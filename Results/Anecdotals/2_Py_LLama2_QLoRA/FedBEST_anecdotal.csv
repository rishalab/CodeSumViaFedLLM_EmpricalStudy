Function,Generated,Ground Truth
"@frappe.whitelist() 
 def setup_complete(args): 
    if cint(frappe.db.get_single_value(u'System   Settings', u'setup_complete')): 
      return 
   args = process_args(args) 
   try: 
      if (args.language and (args.language != u'english')): 
         set_default_language(get_language_code(args.lang)) 
      frappe.clear_cache() 
      update_system_settings(args) 
      update_user_name(args) 
      for method in frappe.get_hooks(u'setup_wizard_complete'): 
         frappe.get_attr(method)(args) 
      disable_future_access() 
      frappe.db.commit() 
      frappe.clear_cache() 
   except: 
      frappe.db.rollback() 
      if args: 
         traceback = frappe.get_traceback() 
         for hook in frappe.get_hooks(u'setup_wizard_exception'): 
            frappe.get_attr(hook)(traceback, args) 
      raise 
   else: 
      for hook in frappe.get_hooks(u'setup_wizard_success'): 
         frappe.get_attr(hook)(args)"," 'Setup complete function. 
 This function is called after the setup is completed. 
 This function is called after the setup is completed. 
 This function is called after the setup is completed. 
 This function is called after the setup is completed. 
 This function is called after the setup is completed. 
 This function is called after the setup is completed. 
 This function is called after the setup is completed. 
 This function is called after the setup is completed. 
 This function is called after the setup is completed. 
 This function is called after the setup is completed. 
 This function is called after the setup is completed. 
 This function is called after the setup is completed. 
 This function is called after the setup is completed. 
 This function is called after the setup is completed. 
 This function is called after the setup is completed. 
 This function is called after the setup is completed. 
 This function is called after the setup is completed. 
 This function is called after the setup is completed. 
 This function is called after the setup is completed. 
 This function is called after the setup is completed. 
 This function is called after the setup is completed","'Calls hooks for `setup_wizard_complete`, sets home page as `desktop` 
 and clears cache. If wizard breaks, calls `setup_wizard_exception` hook'"
"def main(): 
    module = AnsibleModule(argument_spec=ClcGroup._define_module_argument_spec(), supports_check_mode=True) 
   clc_group = ClcGroup(module) 
   clc_group.process_request()", 'Ansible module to manage ClcGroup objects.',"'The main function.  Instantiates the module and calls process_request. 
 :return: none'"
"def __guess_dataset_by_filename(filename): 
    try: 
      fields = os.path.split(filename) 
      if fields: 
         if (fields[(-1)].startswith('dataset_') and fields[(-1)].endswith('.dat')): 
            return Dataset.get(int(fields[(-1)][len('dataset_'):(- len('.dat'))])) 
   except: 
      pass 
   return None"," 'Attempt to guess the dataset from the filename. 
 Parameters 
 filename : str 
 The filename to guess the dataset from. 
 Returns 
 Dataset 
 The dataset, or None if the filename could not be guessed.'",'Return a guessed dataset by filename'
"def resolve_possible_pending_xrefs(app, fromdocname, maybe_xrefs): 
    result = [] 
   for node in maybe_xrefs: 
      if isinstance(node, addnodes.pending_xref): 
         result.extend(resolve_pending_xref(app, fromdocname, node.deepcopy())) 
      else: 
         result.append(node) 
   return result"," 'Resolve possible pending xrefs and return the result. 
 :param app: The app instance. 
 :param fromdocname: The name of the document that is being processed. 
 :param maybe_xrefs: The xref nodes to resolve. 
 :return: The resolved xref nodes. 
 :rtype: list'","'If any node is a pending_xref, attempt to resolve it. If it cannot be 
 resolved, replace it with its children.'"
"def prepare_to_run(): 
    (is_travis, non_pr) = check_environment() 
   if (not is_travis): 
      return 
   if (not non_pr): 
      print('Running   in   Travis   during   non-merge   to   master,   doing   nothing.') 
      sys.exit(0) 
   decrypt_keyfile()"," 'This is run on Travis CI, when the PR is not merged to master. 
 This will decrypt the keyfile and prepare for the next run. 
 This is needed because the keyfile is encrypted on Travis, 
 and the decrypted keyfile is needed to run the tests.'","'Prepare to run system tests. 
 If on Travis during a PR, exit the entire program; there is 
 no need to run the system tests. 
 If on Travis during a build for a non-PR merge to master, 
 decrypts stored keyfile.'"
"def test_download_exit_status_code_when_no_requirements(script): 
    result = script.pip('download', expect_error=True) 
   assert ('You   must   give   at   least   one   requirement   to   download' in result.stderr) 
   assert (result.returncode == ERROR)", 'Test that the script returns an error when no requirements are given','Test download exit status code when no requirements specified'
"def main(): 
    with open('../show_version.txt') as show_ver_file: 
      show_ver = show_ver_file.read() 
   print obtain_os_version(show_ver)"," 'Main entry point for the script. 
 :return: None 
 :rtype: None'","'Obtain the OS version from the show version output 
 Print output to STDOUT'"
"def _out(msg): 
    stdout.write(msg.encode('utf-8'))", 'Print to stdout','Output a string'
"@hook.command('bingimage', 'bis') 
 def bingimage(text, bot): 
    api_key = bot.config.get('api_keys', {}).get('bing_azure') 
   show_nsfw = text.endswith('   nsfw') 
   if show_nsfw: 
      text = text[:(-5)].strip().lower() 
   rating = (NSFW_FILTER if show_nsfw else DEFAULT_FILTER) 
   if (not api_key): 
      return 'Error:   No   Bing   Azure   API   details.' 
   params = {'Sources': bingify('image'), 'Query': bingify(text), 'Adult': bingify(rating), '$format': 'json'} 
   request = requests.get(API_URL, params=params, auth=(api_key, api_key)) 
   j = request.json()['d']['results'][0] 
   if (not j['Image']): 
      return 'No   results.' 
   result = random.choice(j['Image'][:10]) 
   tags = [] 
   tags.append('{}x{}px'.format(result['Width'], result['Height'])) 
   tags.append(result['ContentType']) 
   tags.append(filesize.size(int(result['FileSize']), system=filesize.alternative)) 
   if ('explicit' in result['Thumbnail']['MediaUrl']): 
      tags.append('NSFW') 
   tag_text = ',   '.join(tags) 
   return '{}   ({})'.format(unescape(result['MediaUrl']), tag_text)"," 'Searches Bing for an image, and returns the URL of the first image. 
 If the image is NSFW, it will be returned as a link. 
 .. versionadded:: 0.1'",'<query> - returns the first bing image search result for <query>'
"def create_option_values_for_optionable(optionable_type, **options): 
    if (not issubclass(optionable_type, Optionable)): 
      raise TypeError(u'The   given   `optionable_type`   was   not   a   subclass   of   `Optionable`:   {}'.format(optionable_type)) 
   option_values = {} 
   registration_function = _options_registration_function(option_values) 
   optionable_type.register_options(registration_function) 
   option_values.update(**options) 
   return create_option_values(option_values)"," 'Creates option values for an optionable. 
 :param optionable_type: A subclass of Optionable. 
 :param options: A dictionary of option values. 
 :returns: A dictionary of option values.'","'Create a fake OptionValueContainer with appropriate defaults for the given `Optionable` type."" 
 :param type optionable_type: An :class:`pants.option.optionable.Optionable` subclass. 
 :param **options: Keyword args representing option values explicitly set via the command line. 
 :returns: A fake `OptionValueContainer`, ie: the value returned from `get_options()`.'"
"def probitloglike(params, endog, exog): 
    q = ((2 * endog) - 1) 
   X = exog 
   return np.add.reduce(stats.norm.logcdf((q * np.dot(X, params))))"," 'Probit-Log-Likelihood function. 
 Parameters 
 params : array-like 
 The parameters. 
 endog : 1-D array-like 
 The endogenous variables. 
 exog : 1-D array-like 
 The exogenous variables. 
 Returns 
 logL : float 
 The log-likelihood function. 
 Notes 
 This function is based on the function `loglike` from the `scipy.stats` 
 package. 
 References 
 .. [1] http://en.wikipedia.org/wiki/Probit 
 .. [2] http://en.wikipedia.org/wiki/Log-likelihood_ratio_test 
 Examples 
 >>> from statsmodels.regression.linear_model import probitloglike 
 >>> params = np.array([[0.000000000000000000000000000000000000000000000000000000000000000",'Log likelihood for the probit'
"def test_pix2world(): 
    filename = get_pkg_data_filename(u'data/sip2.fits') 
   with catch_warnings(wcs.wcs.FITSFixedWarning) as caught_warnings: 
      ww = wcs.WCS(filename) 
      assert (len(caught_warnings) == 1) 
   n = 3 
   pixels = (np.arange(n) * np.ones((2, n))).T 
   result = ww.wcs_pix2world(pixels, 0, ra_dec_order=True) 
   ww.wcs_pix2world(pixels[..., 0], pixels[..., 1], 0, ra_dec_order=True) 
   close_enough = 1e-08 
   answer = np.array([[0.00024976, 0.00023018], [0.00023043, (-0.00024997)]]) 
   assert np.all((np.abs((ww.wcs.pc - answer)) < close_enough)) 
   answer = np.array([[202.39265216, 47.17756518], [202.39335826, 47.17754619], [202.39406436, 47.1775272]]) 
   assert np.all((np.abs((result - answer)) < close_enough))", 'Test the wcs_pix2world method','From github issue #1463'
"def influence_plot(results, external=True, alpha=0.05, criterion='cooks', size=48, plot_alpha=0.75, ax=None, **kwargs): 
    (fig, ax) = utils.create_mpl_ax(ax) 
   infl = results.get_influence() 
   if criterion.lower().startswith('coo'): 
      psize = infl.cooks_distance[0] 
   elif criterion.lower().startswith('dff'): 
      psize = np.abs(infl.dffits[0]) 
   else: 
      raise ValueError(('Criterion   %s   not   understood' % criterion)) 
   old_range = np.ptp(psize) 
   new_range = ((size ** 2) - (8 ** 2)) 
   psize = ((((psize - psize.min()) * new_range) / old_range) + (8 ** 2)) 
   leverage = infl.hat_matrix_diag 
   if external: 
      resids = infl.resid_studentized_external 
   else: 
      resids = infl.resid_studentized_internal 
   from scipy import stats 
   cutoff = stats.t.ppf((1.0 - (alpha / 2)), results.df_resid) 
   large_resid = (np.abs(resids) > cutoff) 
   large_leverage = (leverage > _high_leverage(results)) 
   large_points = np.logical_or(large_resid, large_leverage) 
   ax.scatter(leverage, resids, s=psize, alpha=plot_alpha) 
   labels = results.model.data.row_labels 
   if (labels is None): 
      labels = lrange(len(resids)) 
   ax = utils.annotate_axes(np.where(large_points)[0], labels, lzip(leverage, resids), lzip((- ((psize / 2) ** 0.5)), ((psize / 2) ** 0.5)), 'x-large', ax) 
   font = {'fontsize': 16, 'color': 'black'} 
   ax.set_ylabel('Studentized   Residuals', **font) 
   ax.set_xlabel('H   Leverage', **font) 
   ax.set_title('Influence   Plot', **font) 
   return fig"," 'Create an influence plot. 
 Parameters 
 results : Results object 
 The results object to be plotted. 
 external : bool 
 If True, then the external residuals are plotted. 
 alpha : float 
 The significance level for the test. 
 criterion : \'cooks\' or \'dff\' 
 The criterion for determining which residuals are large. 
 size : int 
 The size of the influence plot. 
 plot_alpha : float 
 The alpha value for the points. 
 ax : matplotlib axes 
 The axes to plot the figure on. 
 kwargs : dict 
 Keyword arguments are passed to the `matplotlib.axes.Axes.scatter` method. 
 Returns 
 fig : matplotlib figure 
 The figure containing the plot. 
 Examples 
 >>> results = Results(model, data) 
 >>> results.fit() 
 >>> influence_plot(results, external=True) 
 >>> influence_plot(results, external=False) 
 >>> influence_plot(results, alpha=0.001)'","'Plot of influence in regression. Plots studentized resids vs. leverage. 
 Parameters 
 results : results instance 
 A fitted model. 
 external : bool 
 Whether to use externally or internally studentized residuals. It is 
 recommended to leave external as True. 
 alpha : float 
 The alpha value to identify large studentized residuals. Large means 
 abs(resid_studentized) > t.ppf(1-alpha/2, dof=results.df_resid) 
 criterion : str {\'DFFITS\', \'Cooks\'} 
 Which criterion to base the size of the points on. Options are 
 DFFITS or Cook\'s D. 
 size : float 
 The range of `criterion` is mapped to 10**2 - size**2 in points. 
 plot_alpha : float 
 The `alpha` of the plotted points. 
 ax : matplotlib Axes instance 
 An instance of a matplotlib Axes. 
 Returns 
 fig : matplotlib figure 
 The matplotlib figure that contains the Axes. 
 Notes 
 Row labels for the observations in which the leverage, measured by the 
 diagonal of the hat matrix, is high or the residuals are large, as the 
 combination of large residuals and a high influence value indicates an 
 influence point. The value of large residuals can be controlled using the 
 `alpha` parameter. Large leverage points are identified as 
 hat_i > 2 * (df_model + 1)/nobs.'"
"def send_file(name, data): 
    nf = NetlogFile(name) 
   nf.sock.sendall(data) 
   nf.close()"," 'Send a file to a remote NetlogFile. 
 :param name: the name of the file to send 
 :param data: the data to send'",'Send file to result server'
"def root_create(request): 
    root = get_or_create_root() 
   return redirect('wiki:get', path=root.path)", 'Redirects to the root page.',"'In the edX wiki, we don\'t show the root_create view. Instead, we 
 just create the root automatically if it doesn\'t exist.'"
"def block_device_mapping_update_or_create(context, values): 
    return IMPL.block_device_mapping_update_or_create(context, values)"," 'Update or create a block device mapping. 
 :param values: The values to update or create. 
 :returns: A block device mapping.'","'Update an entry of block device mapping. 
 If not existed, create a new entry'"
"@profiler.trace 
 @memoized 
 def flavor_list(request, is_public=True, get_extras=False): 
    flavors = novaclient(request).flavors.list(is_public=is_public) 
   if get_extras: 
      for flavor in flavors: 
         flavor.extras = flavor_get_extras(request, flavor.id, True, flavor) 
   return flavors"," 'Get a list of flavors. 
 This method returns a list of flavors. 
 If ``get_extras=True`` is set, then the ``extras`` dictionary is also 
 populated. 
 If ``get_extras=False`` is set, then the ``extras`` dictionary is not 
 populated. 
 :param request: The request to use for authentication. 
 :param is_public: Whether to only return public flavors. 
 :param get_extras: Whether to get the extras dictionary. 
 :returns: A list of flavors. 
 :rtype: list'",'Get the list of available instance sizes (flavors).'
"def build_content_type(format, encoding=u'utf-8'): 
    if (u'charset' in format): 
      return format 
   if (format in (u'application/json', u'text/javascript')): 
      return format 
   return (u'%s;   charset=%s' % (format, encoding))"," 'Build a content type from a format string. 
 :param format: a format string 
 :param encoding: the encoding to use 
 :returns: the content type'",'Appends character encoding to the provided format if not already present.'
"def assign_random_category(resource): 
    random_index = randint(0, (TopicCategory.objects.all().count() - 1)) 
   tc = TopicCategory.objects.all()[random_index] 
   resource.category = tc 
   resource.save()"," 'Assign a random category to the resource. 
 :param resource: The resource to assign a category to. 
 :type resource: django.db.models.Model 
 :return: The resource with the category assigned.'",'Assign a random category to a resource'
"@contextlib.contextmanager 
 def temporary_mutation(obj, **kwargs): 
    NOT_PRESENT = object() 
   old_values = {} 
   for (attr, new_value) in kwargs.items(): 
      old_values[attr] = getattr(obj, attr, NOT_PRESENT) 
      setattr(obj, attr, new_value) 
   try: 
      (yield) 
   finally: 
      for (attr, old_value) in old_values.items(): 
         if (old_value is NOT_PRESENT): 
            del obj[attr] 
         else: 
            setattr(obj, attr, old_value)"," 'Context manager that temporarily mutates an object. 
 :param obj: The object to mutate. 
 :param kwargs: The mutation to apply. 
 :raises AssertionError: If the mutation fails. 
 :returns: The original object. 
 :rtype: object'","'Temporarily set the attr on a particular object to a given value then 
 revert when finished. 
 One use of this is to temporarily set the read_deleted flag on a context 
 object: 
 with temporary_mutation(context, read_deleted=""yes""): 
 do_something_that_needed_deleted_objects()'"
"def is_harvard_notes_enabled(course): 
    modules = set(['textannotation', 'imageannotation', 'videoannotation']) 
   return bool(modules.intersection(course.advanced_modules))"," 'Check if Harvard Notes are enabled for a course. 
 Returns True if the course has at least one advanced module enabled 
 that is a Harvard Notes module.'","'Returns True if Harvard Annotation Tool is enabled for the course, 
 False otherwise. 
 Checks for \'textannotation\', \'imageannotation\', \'videoannotation\' in the list 
 of advanced modules of the course.'"
"def seqs_from_file(ids, file_lines): 
    for (label, seq) in parse_fasta(file_lines): 
      if (id_from_fasta_label_line(label) in ids): 
         (yield (label, seq))"," 'Returns a sequence object for each sequence in a fasta file. 
 :param ids: a list of ids to search for in the file 
 :type ids: list 
 :param file_lines: a list of lines from the file 
 :type file_lines: list 
 :return: a list of (label, seq) tuples 
 :rtype: list'",'Extract labels and seqs from file'
"def dice_coe(output, target, epsilon=1e-10): 
    inse = tf.reduce_sum((output * target)) 
   l = tf.reduce_sum((output * output)) 
   r = tf.reduce_sum((target * target)) 
   dice = ((2 * inse) / (l + r)) 
   if (epsilon == 0): 
      return dice 
   else: 
      return tf.clip_by_value(dice, 0, (1.0 - epsilon))"," 'Computes dice coefficient between two tensors. 
 Computes the dice coefficient between two tensors. 
 The dice coefficient is defined as the harmonic mean of the product 
 and sum of the two tensors. 
 This is the same as the equation for dice coefficient in [1]_. 
 Parameters 
 output : tensor 
 The first tensor. 
 target : tensor 
 The second tensor. 
 epsilon : float, optional 
 The epsilon value to clip the dice coefficient by. 
 Returns 
 dice : tensor 
 The dice coefficient. 
 References 
 .. [1] http://en.wikipedia.org/wiki/Dice_coefficient'","'SÃ¸rensenâDice coefficient for comparing the similarity of two distributions, 
 usually be used for binary image segmentation i.e. labels are binary. 
 The coefficient = [0, 1], 1 if totally match. 
 Parameters 
 output : tensor 
 A distribution with shape: [batch_size, ....], (any dimensions). 
 target : tensor 
 A distribution with shape: [batch_size, ....], (any dimensions). 
 epsilon : float 
 An optional name to attach to this layer. 
 Examples 
 >>> outputs = tl.act.pixel_wise_softmax(network.outputs) 
 >>> dice_loss = 1 - tl.cost.dice_coe(outputs, y_, epsilon=1e-5) 
 References 
 - `wiki-dice <https://en.wikipedia.org/wiki/SÃ¸rensenâDice_coefficient>`_'"
"def _skip_create_test_db(self, verbosity=1, autoclobber=False, serialize=True): 
    if callable(getattr(self.connection.features, 'confirm', None)): 
      self.connection.features.confirm() 
   elif hasattr(self, '_rollback_works'): 
      can_rollback = self._rollback_works() 
      self.connection.settings_dict['SUPPORTS_TRANSACTIONS'] = can_rollback 
   return self._get_test_db_name()"," 'Skip test if the test database doesn\'t exist. 
 This is a convenience function for tests that don\'t need to run 
 against a real database. 
 This function will check if the test database exists, and if it doesn\'t 
 create it. 
 :param verbosity: 
 :param autoclobber: 
 :param serialize: 
 :return: 
 :rtype: 
 :raises: 
 :version: 0.2'","'``create_test_db`` implementation that skips both creation and flushing 
 The idea is to re-use the perfectly good test DB already created by an 
 earlier test run, cutting the time spent before any tests run from 5-13s 
 (depending on your I/O luck) down to 3.'"
"def upload_mission(aFileName): 
    missionlist = readmission(aFileName) 
   print ('\nUpload   mission   from   a   file:   %s' % import_mission_filename) 
   print '   Clear   mission' 
   cmds = vehicle.commands 
   cmds.clear() 
   for command in missionlist: 
      cmds.add(command) 
   print '   Upload   mission' 
   vehicle.commands.upload()"," 'Upload a mission from a file. 
 This is useful for when you have a mission that you want to run 
 more than once, but you don\'t want to type it in again. 
 You can use the import_mission_filename command to save the mission 
 to a file, and then upload it with this command.'",'Upload a mission from a file.'
"def evaluator(variables, functions, math_expr, case_sensitive=False): 
    if (math_expr.strip() == ''): 
      return float('nan') 
   math_interpreter = ParseAugmenter(math_expr, case_sensitive) 
   math_interpreter.parse_algebra() 
   (all_variables, all_functions) = add_defaults(variables, functions, case_sensitive) 
   math_interpreter.check_variables(all_variables, all_functions) 
   if case_sensitive: 
      casify = (lambda x: x) 
   else: 
      casify = (lambda x: x.lower()) 
   evaluate_actions = {'number': eval_number, 'variable': (lambda x: all_variables[casify(x[0])]), 'function': (lambda x: all_functions[casify(x[0])](x[1])), 'atom': eval_atom, 'power': eval_power, 'parallel': eval_parallel, 'product': eval_product, 'sum': eval_sum} 
   return math_interpreter.reduce_tree(evaluate_actions)"," 'Evaluates an expression using the given variables and functions. 
 The variables and functions are a dictionary mapping variable names to 
 functions and a dictionary mapping function names to functions. 
 If the given expression is a number, it is returned as-is. 
 If the given expression is a variable, it is replaced by the corresponding 
 function in the variables dictionary. 
 If the given expression is a function, it is replaced by the corresponding 
 function in the functions dictionary. 
 If the given expression is an atom, it is replaced by the corresponding 
 function in the functions dictionary. 
 If the given expression is a power, it is replaced by the corresponding 
 function in the functions dictionary. 
 If the given expression is a parallel, it is replaced by the corresponding 
 function in the functions dictionary. 
 If the given expression is a product, it is replaced by the corresponding 
 function in the functions dictionary. 
 If the given expression is a sum, it is replaced by the corresponding 
 function in the functions dictionary. 
 If the given expression is a function call, it is replaced by the corresponding 
 function in the functions dictionary. 
 If the given expression is a","'Evaluate an expression; that is, take a string of math and return a float. 
 -Variables are passed as a dictionary from string to value. They must be 
 python numbers. 
 -Unary functions are passed as a dictionary from string to function.'"
"def make_pie_chart(data, dir_path, level, prefs, pref_colors, background_color, label_color, generate_image_type, plot_width, plot_height, bar_width, dpi, include_html_legend, file_prefix=None, props={}, others_key='All   Other   Categories', others_color='#eeeeee', should_capitalize=True): 
    if (not data): 
      raise ValueError('No   data   available   for   pie   chart.') 
   all_fracs = [] 
   all_labels = [] 
   colors = [] 
   for (color_ix, (c_label, c_frac)) in enumerate(data): 
      if (c_label == others_key): 
         colors.append(others_color) 
      else: 
         colors.append(data_colors[pref_colors[c_label]].toHex()) 
      all_fracs.append(c_frac) 
      if should_capitalize: 
         capital = ('%s   (%.2f%%)' % (c_label.capitalize(), (c_frac * 100.0))) 
         all_labels.append(capital) 
      else: 
         all_labels.append(('%s   (%.2f%%)' % (c_label, (c_frac * 100.0)))) 
   rc('font', size='10') 
   rc('text', color=label_color) 
   rc('patch', linewidth=0.1) 
   rc('axes', linewidth=0.5, edgecolor=label_color) 
   rc('text', usetex=False) 
   fig = figure(randrange(10000), figsize=(plot_width, plot_height)) 
   fp = FontProperties() 
   fp.set_size('8') 
   if (len(data) > 30): 
      loc = 4 
   else: 
      loc = 5 
   mtitle = 'Pie   Chart' 
   if ('title' in props): 
      mtitle = props['title'] 
   axis('off') 
   title(mtitle, fontsize='10', color=label_color) 
   ax = axes([0.0, 0.0, 0.5, 1]) 
   p1 = pie(all_fracs, shadow=False, colors=colors) 
   if (file_prefix is None): 
      img_name = make_img_name(file_ext='.png') 
   else: 
      img_name = file_prefix 
   img_abs = os.path.join(dir_path, 'charts', img_name) 
   savefig(img_abs, dpi=dpi, facecolor=background_color) 
   eps_link = '' 
   eps_abs = '' 
   if (file_prefix is None): 
      eps_img_name = make_img_name(file_ext=('.%s' % generate_image_type)) 
   else: 
      eps_img_name = (file_prefix + ('.%s' % generate_image_type)) 
   savefig(os.path.join(dir_path, 'charts', eps_img_name), facecolor=background_color) 
   if (generate_image_type == 'eps'): 
      strip_eps_font(os.path.join(dir_path, 'charts', eps_img_name)) 
   eps_abs = os.path.join(dir_path, 'charts', eps_img_name) 
   eps_link = (PDF_LINK % (os.path.join('charts', eps_img_name), ('View   Figure   (.%s)' % generate_image_type))) 
   close(fig) 
   clf() 
   updated_taxa = [] 
   updated_colors = [] 
   for i in data: 
      if (i[0] != others_key): 
         updated_taxa.append(i[0].replace('""', '')) 
         updated_colors.append(data_colors[pref_colors[i[0]]].toHex()) 
      else: 
         updated_taxa.append(others_key) 
         updated_colors.append(others_color) 
   if include_html_legend: 
      legend_fname_png = make_legend(updated_taxa, updated_colors, plot_width, plot_height, label_color, background_color, img_abs, 'png', 80) 
      legend_fpath_png = os.path.join('charts', legend_fname_png) 
   legend_fname = make_legend(updated_taxa, updated_colors, plot_width, plot_height, label_color, background_color, img_abs, generate_image_type, dpi) 
   legend_fpath = os.path.join('charts', legend_fname) 
   legend_link = (LEGEND_LINK % (legend_fpath, ('View   Legend   (.%s)' % generate_image_type))) 
   points_id = '' 
   xmap_html = '' 
   if (not include_html_legend): 
      IMG_TEXT = (IMG_SRC_minus_legend % (os.path.join('charts', img_name), points_id)) 
   else: 
      IMG_TEXT = (IMG_SRC_2 % (os.path.join('charts', img_name), points_id, legend_fpath_png)) 
   return (eps_link, legend_link, IMG_TEXT, xmap_html)"," 'Generate a pie chart. 
 Parameters 
 data : list 
 A list of tuples, where each tuple is a category and its percentage. 
 dir_path : str 
 The directory path to save the chart image. 
 level : int 
 The level of the directory path. 
 prefs : dict 
 A dictionary of preferences. 
 pref_colors : dict 
 A dictionary of color preferences. 
 background_color : str 
 The color of the background. 
 label_color : str 
 The color of the labels. 
 generate_image_type : str 
 The type of image to generate. 
 plot_width : int 
 The width of the chart. 
 plot_height : int 
 The height of the chart. 
 bar_width : int 
 The width of each bar. 
 dpi : int 
 The dots per inch resolution of the image. 
 include_html_legend : bool 
 Whether to include an HTML legend. 
 file_prefix : str 
 The prefix of the chart filename. 
 props : dict 
 A dictionary of properties to apply","'Write interactive piechart 
 data: [fraction:label,...] 
 trunc_len: truncates labels after this many chars'"
"def ismount(path): 
    try: 
      return ismount_raw(path) 
   except OSError: 
      return False"," 'Check if a mount point is mounted. 
 :param path: Path to check. 
 :type path: str 
 :return: True if mounted, False otherwise.'","'Test whether a path is a mount point. This will catch any 
 exceptions and translate them into a False return value 
 Use ismount_raw to have the exceptions raised instead.'"
"def libvlc_vlm_get_media_instance_title(p_instance, psz_name, i_instance): 
    f = (_Cfunctions.get('libvlc_vlm_get_media_instance_title', None) or _Cfunction('libvlc_vlm_get_media_instance_title', ((1,), (1,), (1,)), None, ctypes.c_int, Instance, ctypes.c_char_p, ctypes.c_int)) 
   return f(p_instance, psz_name, i_instance)"," 'Get the title of the media instance. 
 @param p_instance: the media instance. 
 @param psz_name: the name of the media instance. 
 @param i_instance: the instance number. 
 @return the title of the media instance, or -1 if an error occurred.'","'Get vlm_media instance title number by name or instance id. 
 @param p_instance: a libvlc instance. 
 @param psz_name: name of vlm media instance. 
 @param i_instance: instance id. 
 @return: title as number or -1 on error. 
 @bug: will always return 0.'"
"def IE_Dispatcher(s): 
    if (len(s) < 1): 
      return Raw(s) 
   ietype = ord(s[0]) 
   cls = ietypecls.get(ietype, Raw) 
   if ((cls == Raw) and ((ietype & 128) == 128)): 
      cls = IE_NotImplementedTLV 
   return cls(s)"," 'Dispatches IEs to their appropriate TLV class. 
 :param s: the IE to dispatch 
 :type s: bytes 
 :return: the IE as a TLV object 
 :rtype: :class:`TLV`'",'Choose the correct Information Element class.'
"def combine_path_lists(*path_seqs): 
    results = [] 
   for path in combine_lists(*path_seqs): 
      expanded = expand_path(path) 
      paths = (sorted(glob.glob(expanded)) or [expanded]) 
      results.extend(paths) 
   return results"," 'Combine the paths from multiple path sequences. 
 :param path_seqs: A list of path sequences. 
 :return: A list of paths. 
 :rtype: list'","'Concatenate the given sequences into a list. Ignore None values. 
 Resolve ``~`` (home dir) and environment variables, and expand globs 
 that refer to the local filesystem. 
 .. versionchanged:: 0.4.6 
 Can take single strings as well as lists.'"
"def first_ip(network): 
    atoi = (lambda addr: struct.unpack('!I', socket.inet_aton(addr))[0]) 
   itoa = (lambda addr: socket.inet_ntoa(struct.pack('!I', addr))) 
   (address, netmask) = network.split('/') 
   netmask_i = ((4294967295 << (32 - atoi(netmask))) & 4294967295) 
   return itoa(((atoi(address) & netmask_i) + 1))", 'Returns the first IP address of a network.',"'Return the first IPv4 address in network 
 Args: 
 network (str): network in CIDR format 
 Returns: 
 str: first IPv4 address'"
"def load_sparse_dataset(name, normalize=True, transfer=False, randomize_valid=False, randomize_test=False): 
    assert (name in ['harry', 'terry', 'ule']) 
   common = os.path.join(preprocess('${PYLEARN2_DATA_PATH}'), 'UTLC', 'sparse', (name + '_')) 
   (trname, vname, tename) = [((common + subset) + '.npy') for subset in ['train', 'valid', 'test']] 
   train = load_sparse(trname) 
   valid = load_sparse(vname) 
   test = load_sparse(tename) 
   if randomize_valid: 
      rng = make_np_rng(None, [1, 2, 3, 4], which_method='permutation') 
      perm = rng.permutation(valid.shape[0]) 
      valid = valid[perm] 
   if randomize_test: 
      rng = make_np_rng(None, [1, 2, 3, 4], which_method='permutation') 
      perm = rng.permutation(test.shape[0]) 
      test = test[perm] 
   if normalize: 
      if (name == 'ule'): 
         train = (train.astype(theano.config.floatX) / 255) 
         valid = (valid.astype(theano.config.floatX) / 255) 
         test = (test.astype(theano.config.floatX) / 255) 
      elif (name == 'harry'): 
         train = train.astype(theano.config.floatX) 
         valid = valid.astype(theano.config.floatX) 
         test = test.astype(theano.config.floatX) 
         std = 0.6933604603392579 
         train = (train / std) 
         valid = (valid / std) 
         test = (test / std) 
      elif (name == 'terry'): 
         train = train.astype(theano.config.floatX) 
         valid = valid.astype(theano.config.floatX) 
         test = test.astype(theano.config.floatX) 
         train = (train / 300) 
         valid = (valid / 300) 
         test = (test / 300) 
      else: 
         raise Exception(""This   dataset   don't   have   its   normalization   defined"") 
   if transfer: 
      fname = os.path.join(preprocess('${PYLEARN2_DATA_PATH}'), 'UTLC', 'filetensor', (name + '_transfer.ft')) 
      transfer = load_filetensor(fname) 
      return (train, valid, test, transfer) 
   else: 
      return (train, valid, test)"," 'Load a sparse dataset from the UTLC dataset. 
 Parameters 
 name : str 
 Name of the dataset. 
 normalize : bool, optional 
 Whether to normalize the dataset. 
 transfer : bool, optional 
 Whether to load a transfer matrix. 
 randomize_valid : bool, optional 
 Whether to randomly permute the validation set. 
 randomize_test : bool, optional 
 Whether to randomly permute the test set. 
 Returns 
 (train, valid, test) : numpy arrays 
 The train, validation and test sets. 
 Notes 
 The UTLC dataset is a dataset of handwritten digits, which is used in 
 [1]_. 
 References 
 .. [1] http://www.cs.cmu.edu/~rbf/papers/utlc.pdf'","'Load the train,valid,test data for the dataset `name` and return it in 
 sparse format. 
 We suppose the data was created with ift6266h11/pretraitement/to_npy.py 
 that shuffle the train. So the train should already be shuffled. 
 name : \'avicenna\', \'harry\', \'rita\', \'sylvester\' or \'ule\' 
 Which dataset to load 
 normalize : bool 
 If True, we normalize the train dataset before returning it 
 transfer : 
 If True also return the transfer label 
 randomize_valid : bool 
 Do we randomize the order of the valid set?  We always use the same 
 random order If False, return in the same order as downloaded on the 
 web 
 randomize_test : bool 
 Do we randomize the order of the test set?  We always use the same 
 random order If False, return in the same order as downloaded on the 
 web 
 Returns 
 train, valid, test : ndarrays 
 Datasets returned if transfer = False 
 train, valid, test, transfer : ndarrays 
 Datasets returned if transfer = False'"
"def libvlc_video_get_spu_description(p_mi): 
    f = (_Cfunctions.get('libvlc_video_get_spu_description', None) or _Cfunction('libvlc_video_get_spu_description', ((1,),), None, ctypes.POINTER(TrackDescription), MediaPlayer)) 
   return f(p_mi)"," 'Get a description of the SPU track. 
 @param p_mi: the Media Player. 
 @return: the description of the SPU track, or None if not available.'","'Get the description of available video subtitles. 
 @param p_mi: the media player. 
 @return: list containing description of available video subtitles. It must be freed with L{libvlc_track_description_list_release}().'"
"def sum_parts(data): 
    arr = np.asarray(data, dtype=np.float32) 
   out = cuda.device_array(1, dtype=np.float32) 
   gpu_single_block_sum[(1, gpu_block_sum_max_blockdim)](arr, out) 
   return out.copy_to_host()[0]"," 'Sum the parts of a 1D array on the GPU. 
 Parameters 
 data : 1D array 
 The data to sum. 
 Returns 
 out : 1D array 
 The sum of the parts of the input array. 
 Notes 
 This function is used to sum the parts of a 1D array. 
 Examples 
 >>> import numpy as np 
 >>> import cupy as cp 
 >>> import cupyx.cuda.gpuarray as gpuarray 
 >>> data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]) 
 >>> cp.sum_parts(data) 
 array([ 124.,  124.,  124.,  124.,  124.,  124.,  124.,  124.,  124.,  124",'Driver for ``gpu_single_block_sum`` kernel'
"def test_contains_one_of(): 
    assert hug.validate.contains_one_of('no', 'way')(TEST_SCHEMA) 
   assert (not hug.validate.contains_one_of('last', 'place')(TEST_SCHEMA))", 'Test contains_one_of.','Test to ensure hug\'s contains_one_of validation function works as expected to ensure presence of a field'
"def getTransferClosestNestedRing(extrusionHalfWidth, nestedRings, oldOrderedLocation, skein, threadSequence): 
    if (len(nestedRings) > 0): 
      oldOrderedLocation.z = nestedRings[0].z 
   closestDistance = 9.876543219876543e+17 
   closestNestedRing = None 
   for remainingNestedRing in nestedRings: 
      distance = getClosestDistanceIndexToLine(oldOrderedLocation.dropAxis(), remainingNestedRing.boundary).distance 
      if (distance < closestDistance): 
         closestDistance = distance 
         closestNestedRing = remainingNestedRing 
   nestedRings.remove(closestNestedRing) 
   closestNestedRing.addToThreads(extrusionHalfWidth, oldOrderedLocation, skein, threadSequence) 
   return closestNestedRing"," 'Returns the closest nested ring to the oldOrderedLocation. 
 If there are no nested rings, this function returns None. 
 If there are multiple nested rings, this function returns the closest nested ring. 
 If the closest nested ring is already in the skein, it is not added to the skein. 
 If the closest nested ring is not already in the skein, it is added to the skein. 
 If the closest nested ring is not in the skein, it is added to the skein. 
 If the closest nested ring is in the skein, it is not added to the skein. 
 :param extrusionHalfWidth: The half width of the extrusion. 
 :param nestedRings: The list of nested rings. 
 :param oldOrderedLocation: The old ordered location. 
 :param skein: The skein. 
 :param threadSequence: The thread sequence. 
 :return: The closest nested ring to the oldOrderedLocation, or None if there are no nested rings.'",'Get and transfer the closest remaining nested ring.'
"def org_organisation_requires(required=False, realms=None, updateable=False): 
    requires = IS_ONE_OF(current.db, 'org_organisation.id', org_OrganisationRepresent(), realms=realms, updateable=updateable, orderby='org_organisation.name', sort=True) 
   if (not required): 
      requires = IS_EMPTY_OR(requires) 
   return requires"," 'Require an Org to be selected. 
 :param required: If True, the field must be selected, otherwise it can be left blank. 
 :param realms: A list of realm names, if None then all realms are allowed. 
 :param updateable: If True then the field is only required when updating. 
 :returns: A validator for the field.'","'@param required: Whether the selection is optional or mandatory 
 @param realms: Whether the list should be filtered to just those 
 belonging to a list of realm entities 
 @param updateable: Whether the list should be filtered to just those 
 which the user has Write access to 
 @ToDo: Option to remove Branches 
 @ToDo: Option to only include Branches'"
"def get_id(sensorid, feedtag, feedname, feedid, feeduserid): 
    return 'emoncms{}_{}_{}_{}_{}'.format(sensorid, feedtag, feedname, feedid, feeduserid)"," 'Returns a unique id for the sensor, feed, feed name and feed id. 
 :param sensorid: the sensor id 
 :param feedtag: the feed tag 
 :param feedname: the feed name 
 :param feedid: the feed id 
 :param feeduserid: the feed user id 
 :return: a unique id'",'Return unique identifier for feed / sensor.'
"def set_network(ip, netmask, gateway): 
    return __execute_cmd('setniccfg   -s   {0}   {1}   {2}'.format(ip, netmask, gateway))"," 'Set the network configuration of the system. 
 :param ip: IP address of the system 
 :param netmask: Netmask of the system 
 :param gateway: Gateway of the system'","'Configure Network 
 CLI Example: 
 .. code-block:: bash 
 salt dell drac.set_network [DRAC IP] [NETMASK] [GATEWAY] 
 salt dell drac.set_network 192.168.0.2 255.255.255.0 192.168.0.1'"
"def pickle(obj, fname, protocol=2): 
    with smart_open(fname, 'wb') as fout: 
      _pickle.dump(obj, fout, protocol=protocol)"," 'Pickle an object to a file. 
 Parameters 
 obj : object 
 The object to pickle. 
 fname : str 
 The name of the file to pickle to. 
 protocol : int 
 The pickle protocol to use. 
 See Also 
 pickle'","'Pickle object `obj` to file `fname`. 
 `protocol` defaults to 2 so pickled objects are compatible across 
 Python 2.x and 3.x.'"
"def split_file_dummy(changed_file): 
    return (None, changed_file)", 'This is a dummy function to satisfy the split_file function signature.',"'Split the repository-relative filename into a tuple of (branchname, 
 branch_relative_filename). If you have no branches, this should just 
 return (None, changed_file).'"
"def _merge_entries(path, tree1, tree2): 
    entries1 = _tree_entries(path, tree1) 
   entries2 = _tree_entries(path, tree2) 
   i1 = i2 = 0 
   len1 = len(entries1) 
   len2 = len(entries2) 
   result = [] 
   while ((i1 < len1) and (i2 < len2)): 
      entry1 = entries1[i1] 
      entry2 = entries2[i2] 
      if (entry1.path < entry2.path): 
         result.append((entry1, _NULL_ENTRY)) 
         i1 += 1 
      elif (entry1.path > entry2.path): 
         result.append((_NULL_ENTRY, entry2)) 
         i2 += 1 
      else: 
         result.append((entry1, entry2)) 
         i1 += 1 
         i2 += 1 
   for i in range(i1, len1): 
      result.append((entries1[i], _NULL_ENTRY)) 
   for i in range(i2, len2): 
      result.append((_NULL_ENTRY, entries2[i])) 
   return result"," 'Merges two trees into one. 
 The trees are merged by merging entries from the first tree into 
 the second tree. 
 :param path: The path to the tree. 
 :param tree1: The first tree. 
 :param tree2: The second tree. 
 :returns: A list of tuples of entries. 
 :rtype: list(tuple(Entry, Entry))'","'Merge the entries of two trees. 
 :param path: A path to prepend to all tree entry names. 
 :param tree1: The first Tree object to iterate, or None. 
 :param tree2: The second Tree object to iterate, or None. 
 :return: A list of pairs of TreeEntry objects for each pair of entries in 
 the trees. If an entry exists in one tree but not the other, the other 
 entry will have all attributes set to None. If neither entry\'s path is 
 None, they are guaranteed to match.'"
"def register_onaccept(form): 
    req_vars = form.request_vars 
   position = req_vars.get('position', '') 
   reason = req_vars.get('reason', '') 
   db = current.db 
   table = db.auth_user 
   db((table.id == form.vars.id)).update(comments=('%s   |   %s' % (position, reason)))", 'Register the reason for a user being deleted.','Tasks to be performed after a new user registers'
"def gf_mul_ground(f, a, p, K): 
    if (not a): 
      return [] 
   else: 
      return [((a * b) % p) for b in f]"," 'Multiply ``f`` by ``a`` in ``GF(p)[x]``. 
 Examples 
 >>> from sympy.polys.galoistools import gf_mul_ground 
 >>> gf_mul_ground([1, 2, 3], 2, 5, QQ) 
 [2, 4, 6] 
 >>> gf_mul_ground([1, 2, 3], 1, 5, QQ) 
 [1, 2, 3] 
 >>> gf_mul_ground([1, 2, 3], 0, 5, QQ) 
 []'","'Compute ``f * a`` where ``f`` in ``GF(p)[x]`` and ``a`` in ``GF(p)``. 
 Examples 
 >>> from sympy.polys.domains import ZZ 
 >>> from sympy.polys.galoistools import gf_mul_ground 
 >>> gf_mul_ground([3, 2, 4], 2, 5, ZZ) 
 [1, 4, 3]'"
"def close_review_requests(payload, server_url): 
    review_request_id_to_commits_map = defaultdict(list) 
   branch_name = payload.get(u'repository_path') 
   if (not branch_name): 
      return review_request_id_to_commits_map 
   revisions = payload.get(u'revisions', []) 
   for revision in revisions: 
      revision_id = revision.get(u'revision') 
      if (len(revision_id) > 7): 
         revision_id = revision_id[:7] 
      commit_message = revision.get(u'message') 
      review_request_id = get_review_request_id(commit_message, server_url) 
      review_request_id_to_commits_map[review_request_id].append((u'%s   (%s)' % (branch_name, revision_id))) 
   return review_request_id_to_commits_map", 'Returns a dict of review request id to commits.','Closes all review requests for the Google Code repository.'
"def encoding(argument): 
    try: 
      codecs.lookup(argument) 
   except LookupError: 
      raise ValueError(('unknown   encoding:   ""%s""' % argument)) 
   return argument", 'Returns the encoding name of an argument.',"'Verfies the encoding argument by lookup. 
 (Directive option conversion function.) 
 Raises ValueError for unknown encodings.'"
"def setConfigOptions(**opts): 
    for (k, v) in opts.items(): 
      setConfigOption(k, v)"," 'Set configuration options for the current application. 
 :param opts: 
 :type opts: 
 :return: 
 :rtype:'","'Set global configuration options. 
 Each keyword argument sets one global option.'"
"def _getReportItem(itemName, results): 
    subKeys = itemName.split(':') 
   subResults = results 
   for subKey in subKeys: 
      subResults = subResults[subKey] 
   return subResults"," 'Get a report item from a results dictionary. 
 :param itemName: The name of the report item to get. 
 :param results: The results dictionary to get the item from. 
 :return: The report item. 
 :rtype: dict'","'Get a specific item by name out of the results dict. 
 The format of itemName is a string of dictionary keys separated by colons, 
 each key being one level deeper into the results dict. For example, 
 \'key1:key2\' would fetch results[\'key1\'][\'key2\']. 
 If itemName is not found in results, then None is returned'"
"def getInnerText(node): 
    inner_text = [] 
   for child in node.childNodes: 
      if ((child.nodeType == child.TEXT_NODE) or (child.nodeType == child.CDATA_SECTION_NODE)): 
         inner_text.append(child.data) 
      elif (child.nodeType == child.ELEMENT_NODE): 
         inner_text.extend(getInnerText(child)) 
      else: 
         pass 
   return ''.join(inner_text)"," 'Get the inner text of a node. 
 :param node: The node to get the inner text from. 
 :type node: Element 
 :return: The inner text of the node.'",'Get all the inner text of a DOM node (recursively).'
"def resolve_name(name, namespace_, remappings=None): 
    if (not name): 
      return namespace(namespace_) 
   name = canonicalize_name(name) 
   if (name[0] == SEP): 
      resolved_name = name 
   elif is_private(name): 
      resolved_name = canonicalize_name(((namespace_ + SEP) + name[1:])) 
   else: 
      resolved_name = (namespace(namespace_) + name) 
   if (remappings and (resolved_name in remappings)): 
      return remappings[resolved_name] 
   else: 
      return resolved_name"," 'Resolve a name in a namespace. 
 :param name: 
 :param namespace_: 
 :param remappings: 
 :return: 
 :rtype: 
 :raises: 
 :version: 
 :since: 0.1'","'Resolve a ROS name to its global, canonical form. Private ~names 
 are resolved relative to the node name. 
 @param name: name to resolve. 
 @type  name: str 
 @param namespace_: node name to resolve relative to. 
 @type  namespace_: str 
 @param remappings: Map of resolved remappings. Use None to indicate no remapping. 
 @return: Resolved name. If name is empty/None, resolve_name 
 returns parent namespace_. If namespace_ is empty/None, 
 @rtype: str'"
"def separate_qtranslate_content(text): 
    qt_start = u'<!--:' 
   qt_end = u'-->' 
   qt_end_with_lang_len = 5 
   qt_chunks = text.split(qt_start) 
   content_by_lang = {} 
   common_txt_list = [] 
   for c in qt_chunks: 
      if (not c.strip()): 
         continue 
      if c.startswith(qt_end): 
         lang = u'' 
         c = c.lstrip(qt_end) 
         if (not c): 
            continue 
      elif c[2:].startswith(qt_end): 
         lang = c[:2] 
         c = c[qt_end_with_lang_len:] 
      else: 
         lang = u'' 
      if (not lang): 
         common_txt_list.append(c) 
         for l in content_by_lang.keys(): 
            content_by_lang[l].append(c) 
      else: 
         content_by_lang[lang] = (content_by_lang.get(lang, common_txt_list) + [c]) 
   if (common_txt_list and (not content_by_lang)): 
      content_by_lang[u''] = common_txt_list 
   for l in content_by_lang.keys(): 
      content_by_lang[l] = u'   '.join(content_by_lang[l]) 
   return content_by_lang"," 'Separate QTranslate content from the rest of the text. 
 This function is used by the QTranslate plugin. 
 :param text: The text to be translated. 
 :return: A dictionary of translations, keyed by language. 
 :rtype: dict'","'Parse the content of a wordpress post or page and separate qtranslate languages. 
 qtranslate tags: <!--:LL-->blabla<!--:-->'"
"def get(item, default=None): 
    def getter(value): 
      if ET.iselement(value): 
         value = value.attrib 
      try: 
         if _is_re_match(value): 
            return value.group(item) 
         else: 
            return value[item] 
      except (KeyError, IndexError): 
         return default 
      except (TypeError, AttributeError) as err: 
         raise ValueError(err) 
   return transform(getter)"," 'Get the value of the specified attribute. 
 If the attribute is a string, the value is returned. 
 If the attribute is a regular expression, the value is returned 
 as a match object. 
 If the attribute is a dictionary, the value is returned as a dictionary. 
 If the attribute is a list, the value is returned as a list. 
 If the attribute is a tuple, the value is returned as a tuple. 
 If the attribute is an object, the value is returned as an object. 
 If the attribute is not found, the default value is returned. 
 :param item: The attribute to get 
 :param default: The value to return if the attribute is not found 
 :type item: str or int or None 
 :type default: str or int or None 
 :rtype: str or int or None 
 :returns: The value of the specified attribute 
 :raises ValueError: If the attribute is not a string, a regular expression, a 
 dictionary, a list, a tuple, or an object 
 :raises KeyError: If the attribute is a dictionary, a list, a tuple, or an 
","'Get item from value (value[item]). 
 If the item is not found, return the default. 
 Handles XML elements, regex matches and anything that has __getitem__.'"
"def detect_paragraph_type(txt): 
    txt = txt.replace('\r\n', '\n') 
   txt = txt.replace('\r', '\n') 
   txt_line_count = len(re.findall('(?mu)^\\s*.+$', txt)) 
   docanalysis = DocAnalysis('txt', txt) 
   hardbreaks = docanalysis.line_histogram(0.55) 
   if hardbreaks: 
      tab_line_count = len(re.findall('(?mu)^( DCTB |\\s{2,}).+$', txt)) 
      print_percent = (tab_line_count / float(txt_line_count)) 
      empty_line_count = len(re.findall('(?mu)^\\s*$', txt)) 
      block_percent = (empty_line_count / float(txt_line_count)) 
      if (print_percent >= block_percent): 
         if (0.15 <= print_percent <= 0.75): 
            return 'print' 
      elif (0.15 <= block_percent <= 0.75): 
         return 'block' 
      return 'unformatted' 
   return 'single'"," 'Detect paragraph type in a string. 
 :param txt: String to analyze 
 :type txt: str 
 :return: String containing paragraph type 
 :rtype: str'","'Tries to determine the paragraph type of the document. 
 block: Paragraphs are separated by a blank line. 
 single: Each line is a paragraph. 
 print: Each paragraph starts with a 2+ spaces or a tab 
 and ends when a new paragraph is reached. 
 unformatted: most lines have hard line breaks, few/no blank lines or indents 
 returns block, single, print, unformatted'"
"def unique_file_name(base_name, extension=''): 
    idcount = 0 
   if (extension and (not extension.startswith('.'))): 
      extension = ('.%s' % extension) 
   fname = (base_name + extension) 
   while os.path.exists(fname): 
      fname = ('%s-%d%s' % (base_name, idcount, extension)) 
      idcount += 1 
   return fname"," 'Return a unique file name based on the given base_name and extension. 
 This function is used to generate the name of the output files of the 
 ``save_figures`` function. 
 Parameters 
 base_name : str 
 The name of the file without extension. 
 extension : str 
 The extension of the file. 
 Returns 
 fname : str 
 The unique file name. 
 Raises 
 ValueError 
 If base_name is empty.'","'Creates a unique file name based on the specified base name. 
 @base_name - The base name to use for the unique file name. 
 @extension - The file extension to use for the unique file name. 
 Returns a unique file string.'"
"def UniqueIterator(iterator, key=(lambda x: x)): 
    so_far = set() 
   def no_dups(x): 
      k = key(x) 
      if (k in so_far): 
         return False 
      else: 
         so_far.add(k) 
         return True 
   return IteratorFilter(iterator, no_dups)", 'Returns a new iterator which will not yield the same object twice.',"'Takes an iterator and returns an iterator that returns only the 
 first occurence of each entry'"
"def _putmask_smart(v, m, n): 
    if (not is_list_like(n)): 
      n = np.array(([n] * len(m))) 
   elif (isinstance(n, np.ndarray) and (n.ndim == 0)): 
      n = np.repeat(np.array(n, ndmin=1), len(m)) 
   try: 
      nn = n[m] 
      if (not _is_na_compat(v, nn[0])): 
         raise ValueError 
      nn_at = nn.astype(v.dtype) 
      if (not is_numeric_v_string_like(nn, nn_at)): 
         comp = (nn == nn_at) 
         if (is_list_like(comp) and comp.all()): 
            nv = v.copy() 
            nv[m] = nn_at 
            return nv 
   except (ValueError, IndexError, TypeError): 
      pass 
   (dtype, _) = _maybe_promote(n.dtype) 
   if (is_extension_type(v.dtype) and is_object_dtype(dtype)): 
      nv = v.get_values(dtype) 
   else: 
      nv = v.astype(dtype) 
   try: 
      nv[m] = n[m] 
   except ValueError: 
      (idx,) = np.where(np.squeeze(m)) 
      for (mask_index, new_val) in zip(idx, n[m]): 
         nv[mask_index] = new_val 
   return nv"," 'Like putmask, but it also handles non-integer masks. 
 Parameters 
 v : array 
 The value to mask. 
 m : array 
 The mask. 
 n : array 
 The value to replace the masked values with. 
 Returns 
 out : array 
 The output array. 
 Notes 
 If `v` is a string, `n` must be a string as well. 
 Examples 
 >>> a = np.array([1, 2, 3]) 
 >>> a 
 array([1, 2, 3]) 
 >>> a[1:2] 
 array([2]) 
 >>> a[1:2] = 0 
 >>> a 
 array([0, 2, 3]) 
 >>> a[1:2] = 4 
 >>> a 
 array([0, 4, 3]) 
 >>> a[1:2] = 0 
 >>> a 
 array([0, 0, 3]) 
 >>> a[1:2] = 4 
 >>> a 
 array([0, 4,","'Return a new block, try to preserve dtype if possible. 
 Parameters 
 v : `values`, updated in-place (array like) 
 m : `mask`, applies to both sides (array like) 
 n : `new values` either scalar or an array like aligned with `values`'"
"def _traverse_generic(start_node, get_parents, get_children, filter_func=None, yield_descendants_of_unyielded=False): 
    filter_func = (filter_func or (lambda __: True)) 
   stack = deque([start_node]) 
   yield_results = {} 
   while stack: 
      current_node = stack.pop() 
      if (get_parents and (current_node != start_node)): 
         parents = get_parents(current_node) 
         if (not all(((parent in yield_results) for parent in parents))): 
            continue 
         elif ((not yield_descendants_of_unyielded) and (not any((yield_results[parent] for parent in parents)))): 
            continue 
      if (current_node not in yield_results): 
         if get_parents: 
            unvisited_children = list(get_children(current_node)) 
         else: 
            unvisited_children = list((child for child in get_children(current_node) if (child not in yield_results))) 
         unvisited_children.reverse() 
         stack.extend(unvisited_children) 
         should_yield_node = filter_func(current_node) 
         if should_yield_node: 
            (yield current_node) 
         yield_results[current_node] = should_yield_node"," 'Traverse a tree of nodes, yielding nodes that should be yielded. 
 :param start_node: The node to start traversing from. 
 :type start_node: Node 
 :param get_parents: A function that returns the parents of a node. 
 :type get_parents: callable 
 :param get_children: A function that returns the children of a node. 
 :type get_children: callable 
 :param filter_func: A function that returns whether a node should be yielded. 
 :type filter_func: callable 
 :param yield_descendants_of_unyielded: Whether to yield descendants of nodes that have not been yielded. 
 :type yield_descendants_of_unyielded: bool 
 :return: A generator that yields nodes that should be yielded.'","'Helper function to avoid duplicating functionality between 
 traverse_depth_first and traverse_topologically. 
 If get_parents is None, do a pre-order traversal. 
 Else, do a topological traversal. 
 The topological traversal has a worse time complexity than 
 pre-order does, as it needs to check whether each node\'s 
 parents have been visited. 
 Arguments: 
 See description in traverse_topologically.'"
"def getQuadraticPath(elementNode): 
    end = evaluate.getVector3FromElementNode(elementNode) 
   previousElementNode = elementNode.getPreviousElementNode() 
   if (previousElementNode == None): 
      print 'Warning,   can   not   get   previousElementNode   in   getQuadraticPath   in   quadratic   for:' 
      print elementNode 
      return [end] 
   begin = elementNode.getPreviousVertex(Vector3()) 
   controlPoint = evaluate.getVector3ByPrefix(None, elementNode, 'controlPoint') 
   if (controlPoint == None): 
      oldControlPoint = evaluate.getVector3ByPrefixes(previousElementNode, ['controlPoint', 'controlPoint1'], None) 
      if (oldControlPoint == None): 
         oldControlPoint = end 
      controlPoint = ((begin + begin) - oldControlPoint) 
      evaluate.addVector3ToElementNode(elementNode, 'controlPoint', controlPoint) 
   return svg_reader.getQuadraticPoints(begin, controlPoint, end, lineation.getNumberOfBezierPoints(begin, elementNode, end))"," 'Returns a list of quadratic points for the path. 
 The quadratic path is defined as the path between the first and last 
 vertices of the element. 
 Parameters 
 elementNode : ElementNode 
 The element node. 
 Returns 
 A list of points. 
 Raises 
 None 
 Examples 
 >>> getQuadraticPath(elementNode) 
 >>> for point in getQuadraticPath(elementNode): 
 >>>     print point'",'Get the quadratic path.'
"@opt.register_specialize('stabilize', 'fast_compile') 
 @gof.local_optimizer([tensor.Elemwise]) 
 def local_logsoftmax(node): 
    if (isinstance(node.op, tensor.Elemwise) and isinstance(node.op.scalar_op, scalar.basic.Log) and (len(node.inputs) == 1) and (node.inputs[0].owner is not None) and isinstance(node.inputs[0].owner.op, Softmax)): 
      inVars = node.inputs[0].owner.inputs[0] 
      new_op = LogSoftmax() 
      ret = new_op(inVars) 
      ret.tag.values_eq_approx = values_eq_approx_remove_inf 
      copy_stack_trace([node.inputs[0], node.outputs[0]], ret) 
      return [ret]"," 'LogSoftmax is an approximation of LogSoftmax that is faster than 
 the original implementation. 
 Parameters 
 node : Node 
 The node to optimize. 
 Returns 
 A list of optimized nodes.'","'Detect Log(Softmax(x)) and replace it with LogSoftmax(x) 
 Note: only forward pass is affected'"
"def stChromaFeaturesInit(nfft, fs): 
    freqs = numpy.array([(((f + 1) * fs) / (2 * nfft)) for f in range(nfft)]) 
   Cp = 27.5 
   nChroma = numpy.round((12.0 * numpy.log2((freqs / Cp)))).astype(int) 
   nFreqsPerChroma = numpy.zeros((nChroma.shape[0],)) 
   uChroma = numpy.unique(nChroma) 
   for u in uChroma: 
      idx = numpy.nonzero((nChroma == u)) 
      nFreqsPerChroma[idx] = idx[0].shape 
   return (nChroma, nFreqsPerChroma)"," 'Initializes chroma features. 
 Parameters 
 nfft : int 
 The number of fft points in the chroma. 
 fs : float 
 The sampling rate of the audio. 
 Returns 
 nChroma : int 
 The number of chroma bands. 
 nFreqsPerChroma : int 
 The number of frequencies per chroma band. 
 Examples 
 >>> stChromaFeaturesInit(256, 44100) 
 (64, [32, 16, 8, 4, 2, 1]) 
 >>> stChromaFeaturesInit(512, 44100) 
 (128, [32, 16, 8, 4, 2, 1]) 
 >>> stChromaFeaturesInit(1024, 44100) 
 (256, [32, 16, 8, 4, 2, 1])'",'This function initializes the chroma matrices used in the calculation of the chroma features'
"def _set_nxm_headers(nxm_headers): 
    def _set_nxm_headers_dec(self): 
      self.nxm_headers = nxm_headers 
      return self 
   return _set_nxm_headers_dec", 'Sets the nxm headers and returns a new :class:`NXM` object.','Annotate corresponding NXM header'
"def gettext(string, **variables): 
    return get_i18n().gettext(string, **variables)"," 'Returns the translated version of the string. 
 :param string: The string to translate. 
 :param variables: Variables to substitute in the string. 
 :returns: The translated string.'",'See :meth:`I18n.gettext`.'
"def _modified_weiszfeld_step(X, x_old): 
    diff = (X - x_old) 
   diff_norm = np.sqrt(np.sum((diff ** 2), axis=1)) 
   mask = (diff_norm >= _EPSILON) 
   is_x_old_in_X = int((mask.sum() < X.shape[0])) 
   diff = diff[mask] 
   diff_norm = diff_norm[mask][:, np.newaxis] 
   quotient_norm = linalg.norm(np.sum((diff / diff_norm), axis=0)) 
   if (quotient_norm > _EPSILON): 
      new_direction = (np.sum((X[mask, :] / diff_norm), axis=0) / np.sum((1 / diff_norm), axis=0)) 
   else: 
      new_direction = 1.0 
      quotient_norm = 1.0 
   return ((max(0.0, (1.0 - (is_x_old_in_X / quotient_norm))) * new_direction) + (min(1.0, (is_x_old_in_X / quotient_norm)) * x_old))"," 'Modified Weiszfeld step. 
 The step is modified to avoid the step size being zero. 
 Parameters 
 X : ndarray 
 The state vector. 
 x_old : ndarray 
 The previous state vector. 
 Returns 
 new_direction : ndarray 
 The new direction. 
 x_new : ndarray 
 The new state vector. 
 Notes 
 This is the modified Weiszfeld step. 
 References 
 .. [1] [1] ""Linear Systems Theory"" by John G. Kemeny and J. M. Snell, 
 Addison-Wesley, 1968, p. 237. 
 Examples 
 >>> from sympy.physics.mechanics import ( 
 ... StateSpaceModel, 
 ... StateSpaceModelLinear, 
 ... StateSpaceModelLinearDynamics, 
 >>> from sympy.physics.mechanics.dynamics import _modified_weiszfeld_step 
 >>> x = StateSpaceModelLinear(2, 2, [1, 0]) 
 >>> x.step(_mod","'Modified Weiszfeld step. 
 This function defines one iteration step in order to approximate the 
 spatial median (L1 median). It is a form of an iteratively re-weighted 
 least squares method. 
 Parameters 
 X : array, shape = [n_samples, n_features] 
 Training vector, where n_samples is the number of samples and 
 n_features is the number of features. 
 x_old : array, shape = [n_features] 
 Current start vector. 
 Returns 
 x_new : array, shape = [n_features] 
 New iteration step. 
 References 
 - On Computation of Spatial Median for Robust Data Mining, 2005 
 T. KÃ¤rkkÃ¤inen and S. ÃyrÃ¤mÃ¶ 
 http://users.jyu.fi/~samiayr/pdf/ayramo_eurogen05.pdf'"
"def translate_exception(req, e): 
    if (not hasattr(req, 'best_match_language')): 
      return e 
   locale = req.best_match_language() 
   if isinstance(e, webob.exc.HTTPError): 
      e.explanation = i18n.translate(e.explanation, locale) 
      e.detail = i18n.translate(e.detail, locale) 
      if getattr(e, 'body_template', None): 
         e.body_template = i18n.translate(e.body_template, locale) 
   return e"," 'Translate the exception message and detail for the given request 
 :param req: The request object 
 :param e: The exception object'",'Translates all translatable elements of the given exception.'
"def rand_uuid_hex(): 
    return uuid.uuid4().hex", 'Returns a random hex UUID string.',"'Generate a random UUID hex string 
 :return: a random UUID (e.g. \'0b98cf96d90447bda4b46f31aeb1508c\') 
 :rtype: string'"
"def current_time(): 
    return time.time()", 'Returns the current time in seconds since the epoch.',"'Retrieve the current time, this function is mocked out in unit testing.'"
"def check_packages(module, xbps_path, packages, state): 
    would_be_changed = [] 
   for package in packages: 
      (installed, updated) = query_package(module, xbps_path, package) 
      if (((state in ['present', 'latest']) and (not installed)) or ((state == 'absent') and installed) or ((state == 'latest') and (not updated))): 
         would_be_changed.append(package) 
   if would_be_changed: 
      if (state == 'absent'): 
         state = 'removed' 
      module.exit_json(changed=True, msg=('%s   package(s)   would   be   %s' % (len(would_be_changed), state)), packages=would_be_changed) 
   else: 
      module.exit_json(changed=False, msg=('package(s)   already   %s' % state), packages=[])"," 'Check if the package is installed and if it needs to be changed. 
 :param module: Ansible module object 
 :param xbps_path: Path to the XBPS package manager 
 :param packages: List of packages to check 
 :param state: State to check for 
 :return: None'",'Returns change status of command'
"def send_summary(): 
    for d in frappe.get_all(u'Daily   Work   Summary', dict(status=u'Open')): 
      daily_work_summary = frappe.get_doc(u'Daily   Work   Summary', d.name) 
      daily_work_summary.send_summary()", 'Send summary to all open Daily Work Summary.','Send summary to everyone'
"def set_driver(drivers, provider, module, klass): 
    if (provider in drivers): 
      raise AttributeError(('Provider   %s   already   registered' % provider)) 
   drivers[provider] = (module, klass) 
   try: 
      driver = get_driver(drivers, provider) 
   except (ImportError, AttributeError): 
      exp = sys.exc_info()[1] 
      drivers.pop(provider) 
      raise exp 
   return driver"," 'Register a driver for a provider. 
 :param drivers: dict of providers to drivers 
 :param provider: provider to register 
 :param module: module to import 
 :param klass: class to instantiate 
 :returns: driver object 
 :raises: AttributeError, ImportError'","'Sets a driver. 
 :param drivers: Dictionary to store providers. 
 :param provider: Id of provider to set driver for 
 :type provider: :class:`libcloud.types.Provider` 
 :param module: The module which contains the driver 
 :type module: L 
 :param klass: The driver class name 
 :type klass:'"
"def classname_for_table(base, tablename, table): 
    return str(tablename)"," 'Returns the class name for the table. 
 :param base: The base class for the table. 
 :param tablename: The name of the table. 
 :param table: The table object. 
 :returns: The class name for the table. 
 :rtype: str'","'Return the class name that should be used, given the name 
 of a table. 
 The default implementation is:: 
 return str(tablename) 
 Alternate implementations can be specified using the 
 :paramref:`.AutomapBase.prepare.classname_for_table` 
 parameter. 
 :param base: the :class:`.AutomapBase` class doing the prepare. 
 :param tablename: string name of the :class:`.Table`. 
 :param table: the :class:`.Table` object itself. 
 :return: a string class name. 
 .. note:: 
 In Python 2, the string used for the class name **must** be a non-Unicode 
 object, e.g. a ``str()`` object.  The ``.name`` attribute of 
 :class:`.Table` is typically a Python unicode subclass, so the ``str()`` 
 function should be applied to this name, after accounting for any non-ASCII 
 characters.'"
"def getVertexGivenLine(line): 
    splitLine = line.split() 
   return Vector3(float(splitLine[1]), float(splitLine[2]), float(splitLine[3]))"," 'Given a line, return the vertex given by the line. 
 Examples 
 >>> from panda3d.core import Vector3 
 >>> from panda3d.core import Line 
 >>> from panda3d.core import getVertexGivenLine 
 >>> line = Line(Vector3(0, 0, 0), Vector3(1, 0, 0)) 
 >>> getVertexGivenLine(line) 
 Vector3(0.0, 0.0, 0.0)'",'Get vertex given obj vertex line.'
"def filter_new_cons(packet): 
    flags = [] 
   TCP_FIN = 1 
   TCP_SYN = 2 
   TCP_RST = 4 
   TCP_PSH = 8 
   TCP_ACK = 16 
   TCP_URG = 32 
   TCP_ECE = 64 
   TCP_CWK = 128 
   if (packet['tcp']['flags'] & TCP_FIN): 
      flags.append('FIN') 
   elif (packet['tcp']['flags'] & TCP_SYN): 
      flags.append('SYN') 
   elif (packet['tcp']['flags'] & TCP_RST): 
      flags.append('RST') 
   elif (packet['tcp']['flags'] & TCP_PSH): 
      flags.append('PSH') 
   elif (packet['tcp']['flags'] & TCP_ACK): 
      flags.append('ACK') 
   elif (packet['tcp']['flags'] & TCP_URG): 
      flags.append('URG') 
   elif (packet['tcp']['flags'] & TCP_ECE): 
      flags.append('ECE') 
   elif (packet['tcp']['flags'] & TCP_CWK): 
      flags.append('CWK') 
   else: 
      print('UNKNOWN   PACKET') 
   if (packet['tcp']['d_port'] == 4505): 
      if (('SYN' in flags) and (len(flags) == 1)): 
         return 10 
      elif ('FIN' in flags): 
         return 12 
   elif (packet['tcp']['d_port'] == 4506): 
      if (('SYN' in flags) and (len(flags) == 1)): 
         return 100 
      elif ('FIN' in flags): 
         return 120 
   else: 
      return None", 'Filter new connection packets',"'filter packets by there tcp-state and 
 returns codes for specific states'"
"def memcache_set(request): 
    keys = [] 
   for i in request['item']: 
      if ('MemcacheSetRequest_Item' in i): 
         key = i['MemcacheSetRequest_Item']['key'] 
      else: 
         key = i['Item']['key'] 
      keys.append(truncate(key)) 
   return '\n'.join(keys)", 'Returns a string of keys to be set in memcache',"'Pretty-format a memcache.set() request. 
 Arguments: 
 request - The memcache.set() request object, e.g., 
 {\'item\': [{\'Item\': {\'flags\': \'0L\', \'key\': \'memcache_key\' ... 
 Returns: 
 The keys of the memcache.get() response as a string. If there are 
 multiple keys, they are separated by newline characters.'"
"def splantider(tck, n=1): 
    if isinstance(tck, BSpline): 
      return tck.antiderivative(n) 
   else: 
      return _impl.splantider(tck, n)"," 'Returns the antiderivative of a spline, if possible. 
 Parameters 
 tck : spline 
 The spline to antiderivate. 
 n : int, optional 
 The degree of the antiderivative. 
 Returns 
 tck : spline 
 The antiderivative of ``tck``. 
 Raises 
 ValueError 
 If ``tck`` is not a spline.'","'Compute the spline for the antiderivative (integral) of a given spline. 
 Parameters 
 tck : BSpline instance or a tuple of (t, c, k) 
 Spline whose antiderivative to compute 
 n : int, optional 
 Order of antiderivative to evaluate. Default: 1 
 Returns 
 BSpline instance or a tuple of (t2, c2, k2) 
 Spline of order k2=k+n representing the antiderivative of the input 
 spline. 
 A tuple is returned iff the input argument `tck` is a tuple, otherwise 
 a BSpline object is constructed and returned. 
 See Also 
 splder, splev, spalde 
 BSpline 
 Notes 
 The `splder` function is the inverse operation of this function. 
 Namely, ``splder(splantider(tck))`` is identical to `tck`, modulo 
 rounding error. 
 .. versionadded:: 0.13.0 
 Examples 
 >>> from scipy.interpolate import splrep, splder, splantider, splev 
 >>> x = np.linspace(0, np.pi/2, 70) 
 >>> y = 1 / np.sqrt(1 - 0.8*np.sin(x)**2) 
 >>> spl = splrep(x, y) 
 The derivative is the inverse operation of the antiderivative, 
 although some floating point error accumulates: 
 >>> splev(1.7, spl), splev(1.7, splder(splantider(spl))) 
 (array(2.1565429877197317), array(2.1565429877201865)) 
 Antiderivative can be used to evaluate definite integrals: 
 >>> ispl = splantider(spl) 
 >>> splev(np.pi/2, ispl) - splev(0, ispl) 
 2.2572053588768486 
 This is indeed an approximation to the complete elliptic integral 
 :math:`K(m) = \int_0^{\pi/2} [1 - m\sin^2 x]^{-1/2} dx`: 
 >>> from scipy.special import ellipk 
 >>> ellipk(0.8) 
 2.2572053268208538'"
"@contextfunction 
 def core_generic_list(context, objects, skip_group=False, tag=None): 
    if tag: 
      return tag(context, objects) 
   request = context['request'] 
   response_format = 'html' 
   if ('response_format' in context): 
      response_format = context['response_format'] 
   return Markup(render_to_string('core/tags/generic_list', {'objects': objects, 'skip_group': skip_group}, context_instance=RequestContext(request), response_format=response_format))", 'Render a generic list.','Print a list of objects'
"def _get_sysfs_netdev_path(pci_addr, pf_interface): 
    if pf_interface: 
      return ('/sys/bus/pci/devices/%s/physfn/net' % pci_addr) 
   return ('/sys/bus/pci/devices/%s/net' % pci_addr)"," 'Returns the sysfs path of the net device for the given PCI address. 
 The path is returned as a string. 
 :param pci_addr: The PCI address of the device. 
 :param pf_interface: True if the device is a PF interface, False otherwise. 
 :returns: The sysfs path of the net device. 
 :rtype: str'","'Get the sysfs path based on the PCI address of the device. 
 Assumes a networking device - will not check for the existence of the path.'"
"def unpack(desc, formodulename=''): 
    t = desc.type 
   if unpacker_coercions.has_key(t): 
      desc = desc.AECoerceDesc(unpacker_coercions[t]) 
      t = desc.type 
   if (t == typeAEList): 
      l = [] 
      for i in range(desc.AECountItems()): 
         (keyword, item) = desc.AEGetNthDesc((i + 1), '****') 
         l.append(unpack(item, formodulename)) 
      return l 
   if (t == typeAERecord): 
      d = {} 
      for i in range(desc.AECountItems()): 
         (keyword, item) = desc.AEGetNthDesc((i + 1), '****') 
         d[keyword] = unpack(item, formodulename) 
      return d 
   if (t == typeAEText): 
      record = desc.AECoerceDesc('reco') 
      return mkaetext(unpack(record, formodulename)) 
   if (t == typeAlias): 
      return Carbon.File.Alias(rawdata=desc.data) 
   if (t == typeBoolean): 
      return struct.unpack('b', desc.data)[0] 
   if (t == typeChar): 
      return desc.data 
   if (t == typeUnicodeText): 
      return unicode(desc.data, 'utf16') 
   if (t == typeEnumeration): 
      return mkenum(desc.data) 
   if (t == typeFalse): 
      return 0 
   if (t == typeFloat): 
      data = desc.data 
      return struct.unpack('d', data)[0] 
   if (t == typeFSS): 
      return Carbon.File.FSSpec(rawdata=desc.data) 
   if (t == typeFSRef): 
      return Carbon.File.FSRef(rawdata=desc.data) 
   if (t == typeInsertionLoc): 
      record = desc.AECoerceDesc('reco') 
      return mkinsertionloc(unpack(record, formodulename)) 
   if (t == typeIntlText): 
      (script, language) = struct.unpack('hh', desc.data[:4]) 
      return aetypes.IntlText(script, language, desc.data[4:]) 
   if (t == typeIntlWritingCode): 
      (script, language) = struct.unpack('hh', desc.data) 
      return aetypes.IntlWritingCode(script, language) 
   if (t == typeKeyword): 
      return mkkeyword(desc.data) 
   if (t == typeLongInteger): 
      return struct.unpack('l', desc.data)[0] 
   if (t == typeLongDateTime): 
      (a, b) = struct.unpack('lL', desc.data) 
      return ((long(a) << 32) + b) 
   if (t == typeNull): 
      return None 
   if (t == typeMagnitude): 
      v = struct.unpack('l', desc.data) 
      if (v < 0): 
         v = (4294967296L + v) 
      return v 
   if (t == typeObjectSpecifier): 
      record = desc.AECoerceDesc('reco') 
      if formodulename: 
         return mkobjectfrommodule(unpack(record, formodulename), formodulename) 
      return mkobject(unpack(record, formodulename)) 
   if (t == typeQDPoint): 
      (v, h) = struct.unpack('hh', desc.data) 
      return aetypes.QDPoint(v, h) 
   if (t == typeQDRectangle): 
      (v0, h0, v1, h1) = struct.unpack('hhhh', desc.data) 
      return aetypes.QDRectangle(v0, h0, v1, h1) 
   if (t == typeRGBColor): 
      (r, g, b) = struct.unpack('hhh', desc.data) 
      return aetypes.RGBColor(r, g, b) 
   if (t == typeShortFloat): 
      return struct.unpack('f', desc.data)[0] 
   if (t == typeShortInteger): 
      return struct.unpack('h', desc.data)[0] 
   if (t == typeTargetID): 
      return mktargetid(desc.data) 
   if (t == typeTrue): 
      return 1 
   if (t == typeType): 
      return mktype(desc.data, formodulename) 
   if (t == 'rang'): 
      record = desc.AECoerceDesc('reco') 
      return mkrange(unpack(record, formodulename)) 
   if (t == 'cmpd'): 
      record = desc.AECoerceDesc('reco') 
      return mkcomparison(unpack(record, formodulename)) 
   if (t == 'logi'): 
      record = desc.AECoerceDesc('reco') 
      return mklogical(unpack(record, formodulename)) 
   return mkunknown(desc.type, desc.data)"," 'Unpack a Carbon struct. 
 This is the same as the unpack function in the Carbon module, but 
 it has been modified to return the correct type. 
 :param desc: The struct to unpack 
 :param formodulename: The module name to use when unpacking objects 
 :return: The unpacked data'",'Unpack an AE descriptor to a python object'
"def convert_labels(labels, transl): 
    return [transl[l] for l in labels]", 'Converts labels from one language to another.','Convert between strings and numbers.'
"def absent(name, path, mask, cmd, user='root'): 
    mask = ','.join(mask) 
   ret = {'name': name, 'result': True, 'changes': {}, 'comment': ''} 
   if __opts__['test']: 
      status = _check_cron(user, path, mask, cmd) 
      ret['result'] = None 
      if (status == 'absent'): 
         ret['result'] = True 
         ret['comment'] = 'Incron   {0}   is   absent'.format(name) 
      elif ((status == 'present') or (status == 'update')): 
         ret['comment'] = 'Incron   {0}   is   set   to   be   removed'.format(name) 
      return ret 
   data = __salt__['incron.rm_job'](user=user, path=path, mask=mask, cmd=cmd) 
   if (data == 'absent'): 
      ret['comment'] = 'Incron   {0}   already   absent'.format(name) 
      return ret 
   if (data == 'removed'): 
      ret['comment'] = ""Incron   {0}   removed   from   {1}'s   crontab"".format(name, user) 
      ret['changes'] = {user: name} 
      return ret 
   ret['comment'] = 'Incron   {0}   for   user   {1}   failed   to   commit   with   error   {2}'.format(name, user, data) 
   ret['result'] = False 
   return ret"," 'Remove a job from a user\'s crontab. 
 name: The name of the job to remove. 
 path: The path to the crontab. 
 mask: The mask of the crontab. 
 cmd: The command to run. 
 user: The user to remove the job from.'","'Verifies that the specified incron job is absent for the specified user; only 
 the name is matched when removing a incron job. 
 name 
 Unique comment describing the entry 
 path 
 The path that should be watched 
 user 
 The name of the user who\'s crontab needs to be modified, defaults to 
 the root user 
 mask 
 The mask of events that should be monitored for 
 cmd 
 The cmd that should be executed'"
"@blueprint.route('/users') 
 def list_all_users(): 
    return _list_users()", 'List all users.','Return a list of all known user names.'
"def _get_frame_class(frame): 
    import inspect 
   if isinstance(frame, six.string_types): 
      frame_names = frame_transform_graph.get_names() 
      if (frame not in frame_names): 
         raise ValueError(u'Coordinate   frame   {0}   not   in   allowed   values   {1}'.format(frame, sorted(frame_names))) 
      frame_cls = frame_transform_graph.lookup_name(frame) 
   elif (inspect.isclass(frame) and issubclass(frame, BaseCoordinateFrame)): 
      frame_cls = frame 
   else: 
      raise ValueError(u'Coordinate   frame   must   be   a   frame   name   or   frame   class') 
   return frame_cls", 'Returns the class of the frame.',"'Get a frame class from the input `frame`, which could be a frame name 
 string, or frame class.'"
"def dmp_ground_TC(f, u, K): 
    while u: 
      f = dmp_TC(f, K) 
      u -= 1 
   return dup_TC(f, K)"," 'Returns the polynomial ``f`` in ``G(T)[X]``, where ``G`` is the ground 
 field of ``K``. 
 Examples 
 >>> from sympy.polys.domains import ZZ 
 >>> from sympy.polys.densebasic import dmp_ground_TC 
 >>> dmp_ground_TC(ZZ.map([1, 2, 3, 4]), 2, ZZ) 
 [1, 2, 3, 4] 
 >>> dmp_ground_TC(ZZ.map([1, 2, 3, 4]), 2, ZZ).is_ground 
 True 
 >>> dmp_ground_TC(ZZ.map([1, 2, 3, 4]), 2, ZZ).is_ground 
 True 
 References 
 .. [1] http://en.wikipedia.org/wiki/Tschirnhaus_transform'","'Return the ground trailing coefficient. 
 Examples 
 >>> from sympy.polys.domains import ZZ 
 >>> from sympy.polys.densebasic import dmp_ground_TC 
 >>> f = ZZ.map([[[1], [2, 3]]]) 
 >>> dmp_ground_TC(f, 2, ZZ) 
 3'"
"def load_random_chromosome(chr_name): 
    cur_chromosome = BasicChromosome.Chromosome(chr_name) 
   num_segments = random.randrange(num_possible_segments) 
   for seg in range(num_segments): 
      if (seg == 0): 
         cur_segment = BasicChromosome.TelomereSegment() 
      elif (seg == (num_segments - 1)): 
         cur_segment = BasicChromosome.TelomereSegment(1) 
      else: 
         cur_segment = BasicChromosome.ChromosomeSegment() 
      color_chance = random.random() 
      if (color_chance <= color_prob): 
         fill_color = random.choice(color_choices) 
         cur_segment.fill_color = fill_color 
      id_chance = random.random() 
      if (id_chance <= id_prob): 
         id = get_random_id() 
         cur_segment.label = id 
      cur_chromosome.add(cur_segment) 
   return (cur_chromosome, num_segments)"," 'Loads a random chromosome into a BasicChromosome object. 
 Returns a tuple containing the chromosome object and the number of segments 
 in the chromosome. 
 :param chr_name: chromosome name 
 :type chr_name: str 
 :return: chromosome object and number of segments in the chromosome'",'Generate a chromosome with random information about it.'
"def _selective_search_IJCV_top_k(split, year, top_k): 
    imdb = datasets.pascal_voc(split, year) 
   imdb.roidb_handler = imdb.selective_search_IJCV_roidb 
   imdb.config['top_k'] = top_k 
   return imdb"," 'Selective search on the IJCV dataset. 
 :param split: train/test 
 :param year: year of the dataset 
 :param top_k: top k objects to be used for the selective search 
 :return: imdb'","'Return an imdb that uses the top k proposals from the selective search 
 IJCV code.'"
"def is_file_ignored(opts, fname): 
    if opts['file_ignore_regex']: 
      for regex in opts['file_ignore_regex']: 
         if re.search(regex, fname): 
            log.debug('File   matching   file_ignore_regex.   Skipping:   {0}'.format(fname)) 
            return True 
   if opts['file_ignore_glob']: 
      for glob in opts['file_ignore_glob']: 
         if fnmatch.fnmatch(fname, glob): 
            log.debug('File   matching   file_ignore_glob.   Skipping:   {0}'.format(fname)) 
            return True 
   return False"," 'Determine if a file should be ignored. 
 :param opts: The configuration options. 
 :param fname: The name of the file to check. 
 :return: True if the file should be ignored, False otherwise.'","'If file_ignore_regex or file_ignore_glob were given in config, 
 compare the given file path against all of them and return True 
 on the first match.'"
"def _warn_iers(ierserr): 
    msg = u'{0}   Assuming   UT1-UTC=0   for   coordinate   transformations.' 
   warnings.warn(msg.format(ierserr.args[0]), AstropyWarning)"," 'Warn if the IERS has a problem with the UT1-UTC value. 
 This is a warning rather than an error because the IERS 
 doesn\'t know the correct value, so it is not an error. 
 Parameters 
 ierserr : IERSError 
 The error from the IERS. 
 Returns 
 None'","'Generate a warning for an IERSRangeerror 
 Parameters 
 ierserr : An `~astropy.utils.iers.IERSRangeError`'"
"def mean(x, axis=None, keepdims=False): 
    axis = _normalize_axis(axis, ndim(x)) 
   if (x.dtype.base_dtype == tf.bool): 
      x = tf.cast(x, floatx()) 
   return tf.reduce_mean(x, reduction_indices=axis, keep_dims=keepdims)"," 'Return the mean of the elements of a tensor along the given axis. 
 If the axis is None, the mean is taken along the last axis. 
 Args: 
 x: A tensor of type float, double, or bool. 
 axis: An integer, optional. The axis along which the mean is taken. 
 If None, the mean is taken along the last axis. 
 keepdims: If True, the rank of the tensor is reduced by 1. 
 Returns: 
 The mean of the elements of `x` along the given axis. 
 Raises: 
 ValueError: if `axis` is not an integer. 
 Examples: 
 >>> x = tf.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.float32) 
 >>> tf.reduce_mean(x) 
 3.0 
 >>> tf.reduce_mean(x, 1) 
 2.5 
 >>> tf.reduce_mean(x, 0) 
 2.5 
 >>> tf.reduce_mean(x, 2) ","'Mean of a tensor, alongside the specified axis. 
 # Arguments 
 x: A tensor or variable. 
 axis: A list of integer. Axes to compute the mean. 
 keepdims: A boolean, whether to keep the dimensions or not. 
 If `keepdims` is `False`, the rank of the tensor is reduced 
 by 1 for each entry in `axis`. If `keep_dims` is `True`, 
 the reduced dimensions are retained with length 1. 
 # Returns 
 A tensor with the mean of elements of `x`.'"
"@verbose 
 def morph_source_spaces(src_from, subject_to, surf='white', subject_from=None, subjects_dir=None, verbose=None): 
    src_from = _ensure_src(src_from) 
   subject_from = _ensure_src_subject(src_from, subject_from) 
   subjects_dir = get_subjects_dir(subjects_dir, raise_error=True) 
   src_out = list() 
   for fro in src_from: 
      (hemi, idx, id_) = _get_hemi(fro) 
      to = op.join(subjects_dir, subject_to, 'surf', ('%s.%s' % (hemi, surf))) 
      logger.info(('Reading   destination   surface   %s' % (to,))) 
      to = read_surface(to, return_dict=True, verbose=False)[(-1)] 
      complete_surface_info(to, copy=False) 
      best = _get_vertex_map_nn(fro, subject_from, subject_to, hemi, subjects_dir, to['neighbor_tri']) 
      for key in ('neighbor_tri', 'tri_area', 'tri_cent', 'tri_nn', 'use_tris'): 
         del to[key] 
      to['vertno'] = np.sort(best[fro['vertno']]) 
      to['inuse'] = np.zeros(len(to['rr']), int) 
      to['inuse'][to['vertno']] = True 
      to['use_tris'] = best[fro['use_tris']] 
      to.update(nuse=len(to['vertno']), nuse_tri=len(to['use_tris']), nearest=None, nearest_dist=None, patch_inds=None, pinfo=None, dist=None, id=id_, dist_limit=None, type='surf', coord_frame=FIFF.FIFFV_COORD_MRI, subject_his_id=subject_to, rr=(to['rr'] / 1000.0)) 
      src_out.append(to) 
      logger.info('[done]\n') 
   info = dict(working_dir=os.getcwd(), command_line=_get_call_line(in_verbose=True)) 
   return SourceSpaces(src_out, info=info)"," 'Morph the source space from the source space in `src_from` to the 
 surface in `subject_to`. 
 Parameters 
 src_from : list 
 A list of source space structures from which to obtain the source space. 
 subject_to : string 
 The subject in which to obtain the destination surface. 
 surf : string 
 The type of surface to obtain, e.g. \'white\', \'golden\', \'golden_fwhm\', 
 \'golden_fwhm_2mm\', \'golden_fwhm_4mm\', \'golden_fwhm_8mm\', 
 \'golden_fwhm_16mm\', \'golden_fwhm_32mm\', \'golden_fwhm_64mm\', 
 \'golden_fwhm_128mm\', \'golden_fwhm_256mm\', \'golden_fwhm_512mm\',","'Morph an existing source space to a different subject. 
 .. warning:: This can be used in place of morphing source estimates for 
 multiple subjects, but there may be consequences in terms 
 of dipole topology. 
 Parameters 
 src_from : instance of SourceSpaces 
 Surface source spaces to morph. 
 subject_to : str 
 The destination subject. 
 surf : str 
 The brain surface to use for the new source space. 
 subject_from : str | None 
 The ""from"" subject. For most source spaces this shouldn\'t need 
 to be provided, since it is stored in the source space itself. 
 subjects_dir : string, or None 
 Path to SUBJECTS_DIR if it is not set in the environment. 
 verbose : bool, str, int, or None 
 If not None, override default verbose level (see :func:`mne.verbose` 
 and :ref:`Logging documentation <tut_logging>` for more). 
 Returns 
 src : instance of SourceSpaces 
 The morphed source spaces. 
 Notes 
 .. versionadded:: 0.10.0'"
"def _read(filepath_or_buffer, kwds): 
    encoding = kwds.get('encoding', None) 
   if (encoding is not None): 
      encoding = re.sub('_', '-', encoding).lower() 
      kwds['encoding'] = encoding 
   compression = kwds.get('compression') 
   compression = _infer_compression(filepath_or_buffer, compression) 
   (filepath_or_buffer, _, compression) = get_filepath_or_buffer(filepath_or_buffer, encoding, compression) 
   kwds['compression'] = compression 
   if (kwds.get('date_parser', None) is not None): 
      if isinstance(kwds['parse_dates'], bool): 
         kwds['parse_dates'] = True 
   iterator = kwds.get('iterator', False) 
   chunksize = kwds.get('chunksize', None) 
   nrows = _validate_nrows(kwds.pop('nrows', None)) 
   parser = TextFileReader(filepath_or_buffer, **kwds) 
   if ((nrows is not None) and (chunksize is not None)): 
      raise NotImplementedError(""'nrows'   and   'chunksize'   cannot   be   used   together   yet."") 
   elif (nrows is not None): 
      try: 
         data = parser.read(nrows) 
      finally: 
         parser.close() 
      return data 
   elif (chunksize or iterator): 
      return parser 
   try: 
      data = parser.read() 
   finally: 
      parser.close() 
   return data"," 'Reads a file or a file-like object. 
 Parameters 
 filepath_or_buffer : str, file-like object, or None 
 The file path or file-like object to read. 
 kwds : dict 
 Keyword arguments to pass to the TextFileReader. 
 Returns 
 DataFrame or generator 
 The data in the file. 
 See Also 
 read_csv : Reads a CSV file 
 read_html : Reads an HTML file 
 read_excel : Reads an Excel file 
 read_pickle : Reads a pickle file 
 read_table : Reads a tabular file 
 read_fwf : Reads a fixed-width file 
 read_tsv : Reads a tab-separated file 
 read_json : Reads a JSON file 
 read_csv_table : Reads a CSV table 
 read_fwf_table : Reads a fixed-width table 
 read_tsv_table : Reads a tab-separated table 
 read_json_table : Reads a JSON table 
 read_html_table : Reads an HTML",'Generic reader of line files.'
"def get_details_for_etag(options): 
    (tags, noserver) = read_etag_file(options) 
   if (noserver and (not options.noserver)): 
      options.noserver = noserver 
   m = re.match('(?:W/)?""?(.*)""?$', options.etag) 
   if m: 
      options.etag = m.group(1) 
   etag = options.etag 
   if (etag in tags): 
      print ('Found   etag   [%s]   for   version   %s' % (etag, tags[etag][0]['version'])) 
      return tags[etag] 
   short = etag[etag.index('-'):] 
   for t in tags: 
      if (t.find(short) != (-1)): 
         print ('Partial   ETag   match:   [%s],[%s]   for   version   %s' % (etag, t, tags[t][0]['version'])) 
         return tags[t] 
   return None", 'Gets the details for the etag given in options.etag','Get the stack address for a specific ETag from the configuration file.'
"def __virtual__(): 
    if (get_configured_provider() is False): 
      return False 
   if (get_dependencies() is False): 
      return False 
   for (provider, details) in six.iteritems(__opts__['providers']): 
      if ('dimensiondata' not in details): 
         continue 
   return __virtualname__"," 'Checks if the module is available and the configured provider is enabled. 
 If the provider is not enabled, it will return False. 
 :return: True if the module is available and the configured provider is enabled, 
 False otherwise.'",'Set up the libcloud functions and check for dimensiondata configurations.'
"def rs_diff(p, x): 
    R = p.ring 
   n = R.gens.index(x) 
   p1 = R.zero 
   mn = ([0] * R.ngens) 
   mn[n] = 1 
   mn = tuple(mn) 
   for expv in p: 
      if expv[n]: 
         e = monomial_ldiv(expv, mn) 
         p1[e] = (p[expv] * expv[n]) 
   return p1"," 'Compute the difference of two polynomials in ``R[x]``. 
 Examples 
 >>> from sympy.polys import ring, ZZ 
 >>> R, x = ring(""x"", ZZ) 
 >>> R.gens 
 [x] 
 >>> rs_diff(x**2 + x + 1, x) 
 x**2 + 1'","'Return partial derivative of ``p`` with respect to ``x``. 
 Parameters 
 x : :class:`PolyElement` with respect to which ``p`` is differentiated. 
 Examples 
 >>> from sympy.polys.domains import QQ 
 >>> from sympy.polys.rings import ring 
 >>> from sympy.polys.ring_series import rs_diff 
 >>> R, x, y = ring(\'x, y\', QQ) 
 >>> p = x + x**2*y**3 
 >>> rs_diff(p, x) 
 2*x*y**3 + 1'"
"def _generate_python_path(pkg, rospack): 
    if (pkg in _bootstrapped): 
      return [] 
   m = rospack.get_manifest(pkg) 
   if m.is_catkin: 
      _bootstrapped.append(pkg) 
      return [] 
   packages = get_depends(pkg, rospack) 
   packages.append(pkg) 
   paths = [] 
   try: 
      for p in packages: 
         m = rospack.get_manifest(p) 
         d = rospack.get_path(p) 
         _append_package_paths(m, paths, d) 
         _bootstrapped.append(p) 
   except: 
      if (pkg in _bootstrapped): 
         _bootstrapped.remove(pkg) 
      raise 
   return paths"," 'Returns the paths of all packages that are in the python path 
 of the given package. 
 @param pkg: The package to get the python path for 
 @param rospack: The ros package manager'","'Recursive subroutine for building dependency list and python path 
 :raises: :exc:`rospkg.ResourceNotFound` If an error occurs while attempting to load package or dependencies'"
"def follow_files(follow_paths, outstream, lastlines_dirpath=None, waitsecs=5): 
    (procs, pipes) = launch_tails(follow_paths, lastlines_dirpath) 
   while pipes: 
      (lines, bad_pipes) = poll_tail_pipes(pipes, lastlines_dirpath, waitsecs) 
      for bad in bad_pipes: 
         pipes.pop(bad) 
      try: 
         outstream.writelines((['\n'] + lines)) 
         outstream.flush() 
      except (IOError, OSError) as e: 
         break 
   snuff(procs.values())"," 'Follow the given paths, and write the last lines of each file to the 
 output stream. 
 :param follow_paths: A list of paths to follow. 
 :param outstream: An output stream to write to. 
 :param lastlines_dirpath: If provided, the last lines of files in this 
 directory will be written to the output stream. 
 :param waitsecs: The number of seconds to wait between polling the 
 pipes. 
 :return: None'","'Launch tail on a set of files and merge their output into outstream. 
 Args: 
 follow_paths: list; Local paths to launch tail on. 
 outstream: file; Output stream to write aggregated lines to. 
 lastlines_dirpath: Local dirpath to record last lines seen in. 
 waitsecs: int; Timeout for poll_tail_pipes.'"
"def oo_select_keys_from_list(data, keys): 
    if (not isinstance(data, list)): 
      raise errors.AnsibleFilterError('|failed   expects   to   filter   on   a   list') 
   if (not isinstance(keys, list)): 
      raise errors.AnsibleFilterError('|failed   expects   first   param   is   a   list') 
   retval = [oo_select_keys(item, keys) for item in data] 
   return oo_flatten(retval)"," 'Select keys from a list. 
 This is a wrapper for oo_select_keys. 
 The keys argument is a list of keys to select. 
 The data argument is the list of objects to select from. 
 This function will return a list of objects, one for each key. 
 Example: 
 - data: [{\'a\': 1, \'b\': 2}, {\'a\': 3, \'b\': 4}] 
 - keys: [a, b] 
 Will return: 
 - [{\'a\': 1, \'b\': 2}, {\'a\': 3, \'b\': 4}] 
 - [1, 2, 3, 4] 
 This is a wrapper for oo_select_keys. 
 The keys argument is a list of keys to select. 
 The data argument is the list of objects to select from. 
 This function will return a list of objects, one for each key. 
 Example: 
 - data: [{\'a\': 1, \'b\': 2}, {\","'This returns a list, which contains the value portions for the keys 
 Ex: data = { \'a\':1, \'b\':2, \'c\':3 } 
 keys = [\'a\', \'c\'] 
 returns [1, 3]'"
"def is_user_capable(user, api_name): 
    user = urllib.unquote(user) 
   sys.stderr.write((((('checking   permissions   for   user   ' + user) + '   on   api   ') + api_name) + '\n')) 
   secret_file = open('/etc/appscale/secret.key', 'r') 
   secret = secret_file.read() 
   secret = secret[0:(-1)] 
   secret_file.close() 
   uaserver_file = open('/etc/appscale/hypersoap', 'r') 
   uaserver = uaserver_file.read() 
   uaserver_file.close() 
   server = SOAPpy.SOAPProxy((('https://' + uaserver) + ':4343')) 
   capabilities = server.get_capabilities(user, secret) 
   if (not isinstance(capabilities, str)): 
      return False 
   capabilities = capabilities.split(':') 
   sys.stderr.write((((('user   ' + user) + '   has   the   following   capabilities:   ') + str(capabilities)) + '\n')) 
   if (api_name in capabilities): 
      return True 
   else: 
      return False"," 'Checks if a user has the specified capability. 
 :param user: The user to check 
 :param api_name: The API name to check for 
 :return: True if the user has the capability, False otherwise'","'Checks to see if the given user has access to user a particular API. 
 Args: 
 user: The current user email 
 api_name: The API we\'re checking to see if the user has permission 
 Returns: 
 True is capable, False otherwise'"
"def get_c_extract(r, name, sub): 
    if any([getattr(c.op, 'check_input', config.check_input) for (c, _) in r.clients if (not isinstance(c, string_types))]): 
      if any([getattr(c.op, 'check_broadcast', True) for (c, _) in r.clients if (not isinstance(c, string_types))]): 
         c_extract = r.type.c_extract(name, sub, True) 
      else: 
         try: 
            c_extract = r.type.c_extract(name, sub, True, check_broadcast=False) 
         except TypeError as e: 
            c_extract = r.type.c_extract(name, sub, True) 
   else: 
      c_extract = r.type.c_extract(name, sub, False) 
   pre = ('\n            py_%(name)s   =   PyList_GET_ITEM(storage_%(name)s,   0);\n            {Py_XINCREF(py_%(name)s);}\n            ' % locals()) 
   return (pre + c_extract)"," 'Returns the C code for extracting a value from a storage. 
 Parameters 
 r : Runnable 
 The Runnable object. 
 name : str 
 The name of the storage. 
 sub : int 
 The index of the value to extract. 
 Returns 
 str 
 The C code for extracting a value from a storage.'",'Wrapper around c_extract that initializes py_name from storage.'
"def _join(value): 
    return '   '.join(map(_stringify, value))"," 'Join a list of strings into a single string, with spaces in between.'",'Internal function.'
"def produce(url, ident): 
    ctx = zmq.Context.instance() 
   s = ctx.socket(zmq.PUSH) 
   s.connect(url) 
   print(('Producing   %s' % ident)) 
   for i in range(MSGS): 
      s.send((u'%s:   %i' % (ident, time.time())).encode('utf8')) 
      time.sleep(1) 
   print(('Producer   %s   done' % ident)) 
   s.close()", 'Produce a message','Produce messages'
"def getLoopStartingClosest(extrusionHalfWidth, location, loop): 
    closestIndex = getClosestDistanceIndexToLine(location, loop).index 
   loop = getAroundLoop(closestIndex, closestIndex, loop) 
   closestPoint = getClosestPointOnSegment(loop[0], loop[1], location) 
   if ((abs((closestPoint - loop[0])) > extrusionHalfWidth) and (abs((closestPoint - loop[1])) > extrusionHalfWidth)): 
      loop = (([closestPoint] + loop[1:]) + [loop[0]]) 
   elif (abs((closestPoint - loop[0])) > abs((closestPoint - loop[1]))): 
      loop = (loop[1:] + [loop[0]]) 
   return loop"," 'Gets the starting point of the loop closest to the given location. 
 :param extrusionHalfWidth: The half width of the extrusion. 
 :param location: The location to get the closest point to. 
 :param loop: The loop to get the closest point to.'",'Add to threads from the last location from loop.'
"def var_count_error(is_independent, is_plotting): 
    if is_plotting: 
      v = 'Plotting' 
   else: 
      v = 'Registering   plot   modes' 
   if is_independent: 
      (n, s) = (PlotMode._i_var_max, 'independent') 
   else: 
      (n, s) = (PlotMode._d_var_max, 'dependent') 
   return ('%s   with   more   than   %i   %s   variables   is   not   supported.' % (v, n, s))"," 'Returns a string indicating that the number of variables in a plot is 
 too large. 
 Parameters 
 is_independent : bool 
 Whether the plot is independent. 
 is_plotting : bool 
 Whether the plot is being registered or not.'","'Used to format an error message which differs 
 slightly in 4 places.'"
"def train(xs, ys, update_fn=None, typecode=None): 
    if (len(xs) != len(ys)): 
      raise ValueError('xs   and   ys   should   be   the   same   length.') 
   classes = set(ys) 
   if (classes != set([0, 1])): 
      raise ValueError(""Classes   should   be   0's   and   1's"") 
   if (typecode is None): 
      typecode = 'd' 
   (N, ndims) = (len(xs), (len(xs[0]) + 1)) 
   if ((N == 0) or (ndims == 1)): 
      raise ValueError('No   observations   or   observation   of   0   dimension.') 
   X = numpy.ones((N, ndims), typecode) 
   X[:, 1:] = xs 
   Xt = numpy.transpose(X) 
   y = numpy.asarray(ys, typecode) 
   beta = numpy.zeros(ndims, typecode) 
   MAX_ITERATIONS = 500 
   CONVERGE_THRESHOLD = 0.01 
   stepsize = 1.0 
   i = 0 
   old_beta = old_llik = None 
   while (i < MAX_ITERATIONS): 
      ebetaX = numpy.exp(numpy.dot(beta, Xt)) 
      p = (ebetaX / (1 + ebetaX)) 
      logp = ((y * numpy.log(p)) + ((1 - y) * numpy.log((1 - p)))) 
      llik = sum(logp) 
      if (update_fn is not None): 
         update_fn(iter, llik) 
      if (old_llik is not None): 
         if (llik < old_llik): 
            stepsize /= 2.0 
            beta = old_beta 
         if (numpy.fabs((llik - old_llik)) <= CONVERGE_THRESHOLD): 
            break 
      (old_llik, old_beta) = (llik, beta) 
      i += 1 
      W = (numpy.identity(N) * p) 
      Xtyp = numpy.dot(Xt, (y - p)) 
      XtWX = numpy.dot(numpy.dot(Xt, W), X) 
      delta = numpy.linalg.solve(XtWX, Xtyp) 
      if (numpy.fabs((stepsize - 1.0)) > 0.001): 
         delta *= stepsize 
      beta += delta 
   else: 
      raise RuntimeError(""Didn't   converge."") 
   lr = LogisticRegression() 
   lr.beta = [float(x) for x in beta] 
   return lr"," 'Train a logistic regression model on the training set. 
 This function trains a logistic regression model on the training set. 
 The function returns a LogisticRegression object. 
 Parameters 
 xs : array 
 The training set. 
 ys : array 
 The labels of the training set. 
 update_fn : function, optional 
 A function to be called after each iteration. 
 typecode : string, optional 
 The typecode of the data. 
 Returns 
 LogisticRegression 
 The trained logistic regression model.'","'train(xs, ys[, update_fn]) -> LogisticRegression 
 Train a logistic regression classifier on a training set.  xs is a 
 list of observations and ys is a list of the class assignments, 
 which should be 0 or 1.  xs and ys should contain the same number 
 of elements.  update_fn is an optional callback function that 
 takes as parameters that iteration number and log likelihood.'"
"@_api_version(1.21) 
 @_client_version('1.5.0') 
 def create_volume(name, driver=None, driver_opts=None): 
    response = _client_wrapper('create_volume', name, driver=driver, driver_opts=driver_opts) 
   _clear_context() 
   return response"," 'Creates a new volume. 
 If a volume with the same name already exists, it will be deleted and a new 
 volume will be created. 
 :param name: The name of the volume to create. 
 :param driver: The driver to use for the volume. 
 :param driver_opts: Dictionary of options to pass to the driver. 
 :returns: A dictionary containing the volume details.'","'Create a new volume 
 .. versionadded:: 2015.8.4 
 name 
 name of volume 
 driver 
 Driver of the volume 
 driver_opts 
 Options for the driver volume 
 CLI Example: 
 .. code-block:: bash 
 salt myminion dockerng.create_volume my_volume driver=local'"
"def register(linter): 
    linter.register_reporter(HTMLReporter)", 'Registers the HTMLReporter to the given linter.','Register the reporter classes with the linter.'
"def test_scharr_v_vertical(): 
    (i, j) = np.mgrid[(-5):6, (-5):6] 
   image = (j >= 0).astype(float) 
   result = filters.scharr_v(image) 
   j[(np.abs(i) == 5)] = 10000 
   assert np.all((result[(j == 0)] == 1)) 
   assert np.all((result[(np.abs(j) > 1)] == 0))", 'Test vertical Scharr filter','Vertical Scharr on an edge should be a vertical line.'
"def _get_lines_from_file(filename, lineno, context_lines): 
    try: 
      source = open(filename).readlines() 
      lower_bound = max(0, (lineno - context_lines)) 
      upper_bound = (lineno + context_lines) 
      pre_context = [line.strip('\n') for line in source[lower_bound:lineno]] 
      context_line = source[lineno].strip('\n') 
      post_context = [line.strip('\n') for line in source[(lineno + 1):upper_bound]] 
      return (lower_bound, pre_context, context_line, post_context) 
   except (OSError, IOError): 
      return (None, [], None, [])"," 'Get the lines from a file. 
 :param filename: the filename to read from 
 :param lineno: the line number to start from 
 :param context_lines: the number of lines before and after the line to 
 include 
 :returns: (lower_bound, pre_context, context_line, post_context)'","'Returns context_lines before and after lineno from file. 
 Returns (pre_context_lineno, pre_context, context_line, post_context).'"
"def parse_options(): 
    parser = OptionParser(usage=u'%prog   name   [options]', version=(u'Review   Board   ' + get_version_string())) 
   parser.add_option(u'--class-name', dest=u'class_name', default=None, help=u'class   name   of   extension   (capitalized   no   spaces)') 
   parser.add_option(u'--package-name', dest=u'package_name', default=None, help=u'package   name   of   extension   (lower   case   with   underscores)') 
   parser.add_option(u'--description', dest=u'description', default=None, help=u'description   of   extension') 
   parser.add_option(u'--author', dest=u'author', default=None, help=u'author   of   the   extension') 
   parser.add_option(u'--is-configurable', dest=u'is_configurable', action=u'store_true', default=False, help=u'whether   this   extension   is   configurable') 
   (globals()[u'options'], args) = parser.parse_args() 
   if (len(args) != 1): 
      print(u'Error:   incorrect   number   of   arguments') 
      parser.print_help() 
      exit((-1)) 
   options.extension_name = args[0] 
   autofill_unprovided_options()", 'Parse command line options.','Parses the options and stores them in the global options variable.'
"def download_libxml2(dest_dir, version=None): 
    version_re = re.compile('^LATEST_LIBXML2_IS_(.*)$') 
   filename = 'libxml2-%s.tar.gz' 
   return download_library(dest_dir, LIBXML2_LOCATION, 'libxml2', version_re, filename, version=version)", 'Download libxml2 and unpack it into dest_dir.',"'Downloads libxml2, returning the filename where the library was downloaded'"
"def enqueue_push_course_update(update, course_key): 
    if (push_notification_enabled() and update.get('push_notification_selected')): 
      course = modulestore().get_course(course_key) 
      if course: 
         push_course_update_task.delay(unicode(course_key), course.clean_id(padding_char='_'), course.display_name)"," 'Enqueue a push notification for the course update. 
 Args: 
 update: 
 The update dictionary. 
 course_key: 
 The course key. 
 Returns: 
 None'","'Enqueues a task for push notification for the given update for the given course if 
 (1) the feature is enabled and 
 (2) push_notification is selected for the update'"
"def fake_os_walk(paths): 
    paths_dict = dict(paths) 
   def os_walk(top, topdown=True): 
      (dirs, nondirs) = paths_dict[top] 
      (yield (top, dirs, nondirs)) 
      for name in dirs: 
         new_path = '/'.join([top, name]) 
         for x in os_walk(new_path, topdown): 
            (yield x) 
   return os_walk"," 'Fake os.walk() to return a list of tuples (path, dirs, nondirs)'","'Helper function for mocking os.walk() where must test that manipulation 
 of the returned dirs variable works as expected'"
"def get_all_objects(start_obj=None): 
    output = [''] 
   widget_lines = _get_widgets() 
   widget_lines = [('            ' + e) for e in widget_lines] 
   widget_lines.insert(0, 'Qt   widgets   -   {}   objects:'.format(len(widget_lines))) 
   output += widget_lines 
   if (start_obj is None): 
      start_obj = QApplication.instance() 
   pyqt_lines = [] 
   _get_pyqt_objects(pyqt_lines, start_obj) 
   pyqt_lines = [('            ' + e) for e in pyqt_lines] 
   pyqt_lines.insert(0, 'Qt   objects   -   {}   objects:'.format(len(pyqt_lines))) 
   output += [''] 
   output += pyqt_lines 
   output += objreg.dump_objects() 
   return '\n'.join(output)", 'Return a string containing the names of all Qt objects.','Get all children of an object recursively as a string.'
"def send_mail_to_student(student, param_dict, language=None): 
    if ('display_name' in param_dict): 
      param_dict['course_name'] = param_dict['display_name'] 
   param_dict['site_name'] = configuration_helpers.get_value('SITE_NAME', param_dict['site_name']) 
   subject = None 
   message = None 
   message_type = param_dict['message'] 
   email_template_dict = {'allowed_enroll': ('emails/enroll_email_allowedsubject.txt', 'emails/enroll_email_allowedmessage.txt'), 'enrolled_enroll': ('emails/enroll_email_enrolledsubject.txt', 'emails/enroll_email_enrolledmessage.txt'), 'allowed_unenroll': ('emails/unenroll_email_subject.txt', 'emails/unenroll_email_allowedmessage.txt'), 'enrolled_unenroll': ('emails/unenroll_email_subject.txt', 'emails/unenroll_email_enrolledmessage.txt'), 'add_beta_tester': ('emails/add_beta_tester_email_subject.txt', 'emails/add_beta_tester_email_message.txt'), 'remove_beta_tester': ('emails/remove_beta_tester_email_subject.txt', 'emails/remove_beta_tester_email_message.txt'), 'account_creation_and_enrollment': ('emails/enroll_email_enrolledsubject.txt', 'emails/account_creation_and_enroll_emailMessage.txt')} 
   (subject_template, message_template) = email_template_dict.get(message_type, (None, None)) 
   if ((subject_template is not None) and (message_template is not None)): 
      (subject, message) = render_message_to_string(subject_template, message_template, param_dict, language=language) 
   if (subject and message): 
      message = message.strip() 
      subject = ''.join(subject.splitlines()) 
      from_address = configuration_helpers.get_value('email_from_address', settings.DEFAULT_FROM_EMAIL) 
      send_mail(subject, message, from_address, [student], fail_silently=False)"," 'Send an email to a student. 
 :param student: the student to send the email to 
 :param param_dict: a dictionary of parameters to include in the email 
 :param language: the language of the email 
 :return: None'","'Construct the email using templates and then send it. 
 `student` is the student\'s email address (a `str`), 
 `param_dict` is a `dict` with keys 
 `site_name`: name given to edX instance (a `str`) 
 `registration_url`: url for registration (a `str`) 
 `display_name` : display name of a course (a `str`) 
 `course_id`: id of course (a `str`) 
 `auto_enroll`: user input option (a `str`) 
 `course_url`: url of course (a `str`) 
 `email_address`: email of student (a `str`) 
 `full_name`: student full name (a `str`) 
 `message`: type of email to send and template to use (a `str`) 
 `is_shib_course`: (a `boolean`) 
 `language` is the language used to render the email. If None the language 
 of the currently-logged in user (that is, the user sending the email) will 
 be used. 
 Returns a boolean indicating whether the email was sent successfully.'"
"def get_profiler_log_path(autodir): 
    return os.path.join(autodir, 'results', 'default', 'debug', 'client.DEBUG')"," 'Get the path to the profiler log file. 
 :param autodir: the autodir path 
 :type autodir: str 
 :return: the path to the profiler log file'","'Given the directory of a profiler client, find the client log path.'"
"def sh(cmd): 
    return check_call(cmd, shell=True)"," 'Execute a shell command. 
 :param cmd: A string containing the command to execute. 
 :returns: The exit status of the command.'","'Execute command in a subshell, return status code.'"
"def raise_http_conflict_for_instance_invalid_state(exc, action, server_id): 
    attr = exc.kwargs.get('attr') 
   state = exc.kwargs.get('state') 
   if ((attr is not None) and (state is not None)): 
      msg = (_(""Cannot   '%(action)s'   instance   %(server_id)s   while   it   is   in   %(attr)s   %(state)s"") % {'action': action, 'attr': attr, 'state': state, 'server_id': server_id}) 
   else: 
      msg = (_(""Instance   %(server_id)s   is   in   an   invalid   state   for   '%(action)s'"") % {'action': action, 'server_id': server_id}) 
   raise webob.exc.HTTPConflict(explanation=msg)"," 'Raise HTTP Conflict if the instance is in an invalid state for the given 
 action. 
 :param exc: The exception that was raised. 
 :param action: The action that was attempted. 
 :param server_id: The ID of the server. 
 :raises: HTTPConflict if the instance is in an invalid state for the given 
 action.'","'Raises a webob.exc.HTTPConflict instance containing a message 
 appropriate to return via the API based on the original 
 InstanceInvalidState exception.'"
"def random_selection(a, start, end, i): 
    if (start < end): 
      p = choosePivot(start, end) 
      (a[start], a[p]) = (a[p], a[start]) 
      j = partition(a, start, end) 
      if (j == i): 
         return a[i] 
      if (j < i): 
         return random_selection(a, (j + 1), end, i) 
      else: 
         return random_selection(a, start, (j - 1), i) 
   else: 
      return a[start]"," 'Returns the element at position i of a[start..end] 
 If start == end, returns a[start]. 
 If start > end, returns a[end]. 
 If start < end, returns a[i]. 
 If start < end and i > end, returns a[i]. 
 If start < end and i < start, returns a[i]. 
 If start < end and i == start, returns a[i]. 
 If start < end and i == end, returns a[i]. 
 If start < end and i == start, returns a[i]. 
 If start < end and i == end, returns a[i]. 
 If start < end and i == start, returns a[i]. 
 If start < end and i == end, returns a[i]. 
 If start < end and i == start, returns a[i]. 
 If start < end and i == end, returns a[i]. 
 If start < end and i == start, returns a[i]. 
 If start < end and i == end, returns a[i]. 
 If start < end and i == start, returns a","'returns the ith order statistic 
 in the array a in linear time 
 >>> from random import sample 
 >>> test_cases = [sample(range(20), 10) for i in range(10)] 
 >>> orders = [randint(0, 9) for i in range(10)] 
 >>> results = [sorted(test_cases[i])[orders[i]] == random_selection(test_cases[i], 0, len(test_cases[i])-1, orders[i]) for i in range(10)] 
 >>> print sum(results) 
 10'"
"def main(): 
    parser = OptionParser() 
   parser.add_option('-d', '--days', dest='days', action='store', type='int', help='number   of   days   (14)', default=14) 
   parser.add_option('-i', '--info_only', action='store_true', dest='info_only', help='info   about   the   requested   action', default=False) 
   parser.add_option('-v', '--verbose', action='store_true', dest='verbose', help='verbose   mode,   print   the   name   of   each   repository', default=False) 
   (options, args) = parser.parse_args() 
   try: 
      ini_file = args[0] 
   except IndexError: 
      sys.exit(('Usage:   python   %s   <tool   shed   .ini   file>   [options]' % sys.argv[0])) 
   config_parser = ConfigParser.ConfigParser({'here': os.getcwd()}) 
   config_parser.read(ini_file) 
   config_dict = {} 
   for (key, value) in config_parser.items('app:main'): 
      config_dict[key] = value 
   config = tool_shed_config.Configuration(**config_dict) 
   app = DeprecateRepositoriesApplication(config) 
   cutoff_time = (datetime.utcnow() - timedelta(days=options.days)) 
   now = strftime('%Y-%m-%d   %H:%M:%S') 
   print '\n####################################################################################' 
   print ('#   %s   -   Handling   stuff   older   than   %i   days' % (now, options.days)) 
   if options.info_only: 
      print '#   Displaying   info   only   (   --info_only   )' 
   deprecate_repositories(app, cutoff_time, days=options.days, info_only=options.info_only, verbose=options.verbose)", 'Main entry point for the script.',"'Script to deprecate any repositories that are older than n days, and have been empty since creation.'"
"def collect_data_files(package, include_py_files=False, subdir=None): 
    if (not isinstance(package, string_types)): 
      raise ValueError 
   (pkg_base, pkg_dir) = get_package_paths(package) 
   if subdir: 
      pkg_dir = os.path.join(pkg_dir, subdir) 
   datas = [] 
   for (dirpath, dirnames, files) in os.walk(pkg_dir): 
      for f in files: 
         extension = os.path.splitext(f)[1] 
         if (include_py_files or (extension not in PY_IGNORE_EXTENSIONS)): 
            source = os.path.join(dirpath, f) 
            dest = remove_prefix(dirpath, (os.path.dirname(pkg_base) + os.sep)) 
            datas.append((source, dest)) 
   return datas"," 'Collect all data files in a package. 
 This function collects all data files in a package and returns them as a 
 list of (source, dest) tuples. 
 :param package: The package to collect data files for. 
 :param include_py_files: If True, data files with .py extensions will be 
 included. 
 :param subdir: If a subdirectory is specified, only data files in that 
 subdirectory will be included. 
 :returns: A list of (source, dest) tuples.'","'This routine produces a list of (source, dest) non-Python (i.e. data) 
 files which reside in package. Its results can be directly assigned to 
 ``datas`` in a hook script; see, for example, hook-sphinx.py. The 
 package parameter must be a string which names the package. 
 By default, all Python executable files (those ending in .py, .pyc, 
 and so on) will NOT be collected; setting the include_py_files 
 argument to True collects these files as well. This is typically used 
 with Python routines (such as those in pkgutil) that search a given 
 directory for Python executable files then load them as extensions or 
 plugins. The optional subdir give a subdirectory relative to package to 
 search, which is helpful when submodules are imported at run-time from a 
 directory lacking __init__.py 
 This function does not work on zipped Python eggs. 
 This function is used only for hook scripts, but not by the body of 
 PyInstaller.'"
"def encrypt(plaintext): 
    salt = _make_salt() 
   return _encrypt(salt, plaintext, g.tracking_secret)", 'Encrypt the plaintext using the tracking secret.',"'Return the message `plaintext` encrypted. 
 The encrypted message will have its salt prepended and will be URL encoded 
 to make it suitable for use in URLs and Cookies. 
 NOTE: this function is here for backwards compatibility. Please do not 
 use it for new code.'"
"def parse_acl(acl_string): 
    referrers = [] 
   groups = [] 
   if acl_string: 
      for value in acl_string.split(','): 
         if value.startswith('.r:'): 
            referrers.append(value[len('.r:'):]) 
         else: 
            groups.append(value) 
   return (referrers, groups)"," 'Parse a list of ACL values separated by commas. 
 Returns a list of referrers and a list of groups.'","'Parses a standard Swift ACL string into a referrers list and groups list. 
 See :func:`clean_acl` for documentation of the standard Swift ACL format. 
 :param acl_string: The standard Swift ACL string to parse. 
 :returns: A tuple of (referrers, groups) where referrers is a list of 
 referrer designations (without the leading .r:) and groups is a 
 list of groups to allow access.'"
"def popen_nonblock(*args, **kwargs): 
    proc = popen_sp(*args, **kwargs) 
   if proc.stdin: 
      proc.stdin = pipebuf.NonBlockBufferedWriter(proc.stdin) 
   if proc.stdout: 
      proc.stdout = pipebuf.NonBlockBufferedReader(proc.stdout) 
   if proc.stderr: 
      proc.stderr = pipebuf.NonBlockBufferedReader(proc.stderr) 
   return proc"," 'Popen with non-blocking I/O. 
 This is a wrapper around `subprocess.Popen` that uses `pipebuf` 
 to make the stdin, stdout and stderr non-blocking. 
 This is useful when you want to write to stdin but don\'t want 
 the process to wait for the input. 
 :param args: 
 :param kwargs: 
 :return: 
 :rtype: subprocess.Popen'","'Create a process in the same way as popen_sp, but patch the file 
 descriptors so they can be accessed from Python/gevent 
 in a non-blocking manner.'"
"def dup_transform(f, p, q, K): 
    if (not f): 
      return [] 
   n = (len(f) - 1) 
   (h, Q) = ([f[0]], [[K.one]]) 
   for i in range(0, n): 
      Q.append(dup_mul(Q[(-1)], q, K)) 
   for (c, q) in zip(f[1:], Q[1:]): 
      h = dup_mul(h, p, K) 
      q = dup_mul_ground(q, c, K) 
      h = dup_add(h, q, K) 
   return h"," 'Transform ``f`` in ``K[x]`` into ``K[x]`` by multiplying ``f`` by ``p`` 
 and ``q`` and adding the result to ``1``. 
 Examples 
 >>> from sympy.polys.domains import ZZ 
 >>> from sympy.polys.galoistools import dup_transform 
 >>> dup_transform([1, 2, 3], 2, 3, ZZ) 
 [1, 2, 3]'","'Evaluate functional transformation ``q**n * f(p/q)`` in ``K[x]``. 
 Examples 
 >>> from sympy.polys import ring, ZZ 
 >>> R, x = ring(""x"", ZZ) 
 >>> R.dup_transform(x**2 - 2*x + 1, x**2 + 1, x - 1) 
 x**4 - 2*x**3 + 5*x**2 - 4*x + 4'"
"def serialize_item(collection, item): 
    __connect() 
   collection = mongodb[collection.collection_type()] 
   data = collection.find_one({'name': item.name}) 
   if data: 
      collection.update({'name': item.name}, item.to_dict()) 
   else: 
      collection.insert(item.to_dict())", 'Serialize an item in the collection and insert it into the database.',"'Save a collection item to database 
 @param Collection collection collection 
 @param Item item collection item'"
"def create_instance(c_instance): 
    return Serato(c_instance)", 'Create an instance of the Serato class.','Creates and returns the Serato script'
"def req_item(): 
    if (request.function != 'fema'): 
      s3.filter = (FS('req_id$is_template') == False) 
   def prep(r): 
      if (r.interactive or (r.representation == 'aadata')): 
         list_fields = s3db.get_config('req_req_item', 'list_fields') 
         list_fields.insert(1, 'req_id$site_id') 
         levels = gis.get_relevant_hierarchy_levels() 
         levels.reverse() 
         for level in levels: 
            lfield = ('req_id$site_id$location_id$%s' % level) 
            list_fields.insert(1, lfield) 
         s3db.configure('req_req_item', insertable=False, list_fields=list_fields) 
         s3.crud_strings['req_req_item'].title_list = T('Requested   Items') 
         if ((r.method != None) and (r.method != 'update') and (r.method != 'read')): 
            s3db.req_hide_quantities(r.table) 
      return True 
   s3.prep = prep 
   output = s3_rest_controller('req', 'req_item') 
   if settings.get_req_prompt_match(): 
      req_item_inv_item_btn = dict(url=URL(c='req', f='req_item_inv_item', args=['[id]']), _class='action-btn', label=str(T('Request   from   Facility'))) 
      if s3.actions: 
         s3.actions.append(req_item_inv_item_btn) 
      else: 
         s3.actions = [req_item_inv_item_btn] 
   return output", 'Requested Items Controller',"'REST Controller 
 @ToDo: Filter out fulfilled Items?'"
"def service_status(hostname=None, service=None, **kwargs): 
    if (not hostname): 
      raise CommandExecutionError('Missing   hostname   parameter') 
   if (not service): 
      raise CommandExecutionError('Missing   service   parameter') 
   target = 'service' 
   numeric = kwargs.get('numeric') 
   data = _status_query(target, hostname, service=service, enumerate=numeric) 
   ret = {'result': data['result']} 
   if ret['result']: 
      ret['status'] = data.get('json_data', {}).get('data', {}).get(target, {}).get('status', (((not numeric) and 'Unknown') or 2)) 
   else: 
      ret['error'] = data['error'] 
   return ret"," 'Retrieves the status of a service. 
 Example:: 
 from fabtools import require 
 require.service.status(\'httpd\') 
 This will return the status of the httpd service on the current host. 
 If the service is not running, it will return an error. 
 If the service is running, it will return a dictionary with the status 
 of the service.'","'Check status of a particular service on a host on it in Nagios. 
 By default statuses are returned in a numeric format. 
 Parameters: 
 hostname 
 The hostname to check the status of the service in Nagios. 
 service 
 The service to check the status of in Nagios. 
 numeric 
 Turn to false in order to return status in text format 
 (\'OK\' instead of 0, \'Warning\' instead of 1 etc) 
 :return: status:     \'OK\', \'Warning\', \'Critical\' or \'Unknown\' 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' nagios_rpc.service_status hostname=webserver.domain.com service=\'HTTP\' 
 salt \'*\' nagios_rpc.service_status hostname=webserver.domain.com service=\'HTTP\' numeric=False'"
"def command_show(problem): 
    print problem.get_html()", 'Show the problem in a browser.','Display the text for this problem'
"def current_route_url(request, *elements, **kw): 
    return request.current_route_url(*elements, **kw)", 'Return the URL for the current route.',"'This is a backwards compatibility function.  Its result is the same as 
 calling:: 
 request.current_route_url(*elements, **kw) 
 See :meth:`pyramid.request.Request.current_route_url` for more 
 information.'"
"def obfuscatePowershellScript(code): 
    import re 
   newCode = code 
   newCode = remove_comments(newCode) 
   if ('function   Invoke-ReflectivePEInjection' in newCode): 
      newCode = newCode.replace(""$TypeBuilder.DefineLiteral('IMAGE_DLL_CHARACTERISTICS_DYNAMIC_BASE',   [UInt16]   0x0040)   |   Out-Null"", ""$TypeBuilder.DefineLiteral('IMAGE_DLL_CHARACTERIS'+'TICS_DYNAMIC_BASE',   [UInt16]   0x0040)   |   Out-Null"") 
   return newCode"," 'Obfuscate the powershell script to prevent it from being detected by 
 tools like Metasploit.'","'Try to clean powershell script (perhaps in the future \'obfuscation\'...). 
 Comments are deteleted and some strings are replaced in some powershell functions to bypass AV detection'"
"def add_metadata_type(ir): 
    buf = [] 
   for line in ir.splitlines(): 
      if re_metadata_def.match(line): 
         if (None is re_metadata_correct_usage.search(line)): 
            line = line.replace('!{', 'metadata   !{') 
            line = line.replace('!""', 'metadata   !""') 
            def sub_metadata(m): 
               return 'metadata   {0}'.format(m.group(0)) 
            line = re_metadata_ref.sub(sub_metadata, line) 
            line = line.lstrip('metadata   ') 
      buf.append(line) 
   return '\n'.join(buf)"," 'Adds metadata to the input IR. 
 :param ir: input IR 
 :returns: input IR with metadata added'","'Rewrite metadata since llvm3.6 dropped the ""metadata"" type prefix.'"
"def factory(type): 
    return ArrowFactory(type)"," 'Returns a factory for creating Arrow instances of a given type. 
 Parameters 
 type : ArrowType 
 The type of Arrow to return.'","'Returns an :class:`.ArrowFactory` for the specified :class:`Arrow <arrow.arrow.Arrow>` 
 or derived type. 
 :param type: the type, :class:`Arrow <arrow.arrow.Arrow>` or derived.'"
"def get_dl_data(song, mediatype='any'): 
    def mbsize(x): 
      '   Return   size   in   MB.   ' 
      return str(int((x / (1024 ** 2)))) 
   p = util.get_pafy(song) 
   dldata = [] 
   text = '   [Fetching   stream   info]   >' 
   streamlist = [x for x in p.allstreams] 
   if (mediatype == 'audio'): 
      streamlist = [x for x in p.audiostreams] 
   l = len(streamlist) 
   for (n, stream) in enumerate(streamlist): 
      sys.stdout.write(((((text + ('-' * n)) + '>') + ('   ' * ((l - n) - 1))) + '<\r')) 
      sys.stdout.flush() 
      try: 
         size = mbsize(stream.get_filesize()) 
      except TypeError: 
         util.dbg(((c.r + '---Error   getting   stream   size') + c.w)) 
         size = 0 
      item = {'mediatype': stream.mediatype, 'size': size, 'ext': stream.extension, 'quality': stream.quality, 'notes': stream.notes, 'url': stream.url} 
      dldata.append(item) 
   screen.writestatus('') 
   return (dldata, p)"," 'Get info on the stream to download. 
 :param song: The song to download 
 :param mediatype: The type of stream to download. 
 :returns: A tuple containing the downloadable data and the PAFY object 
 :rtype: tuple'","'Get filesize and metadata for all streams, return dict.'"
"def sign(wire, keyname, secret, time, fudge, original_id, error, other_data, request_mac, ctx=None, multi=False, first=True, algorithm=default_algorithm): 
    (algorithm_name, digestmod) = get_algorithm(algorithm) 
   if first: 
      ctx = hmac.new(secret, digestmod=digestmod) 
      ml = len(request_mac) 
      if (ml > 0): 
         ctx.update(struct.pack('!H', ml)) 
         ctx.update(request_mac) 
   id = struct.pack('!H', original_id) 
   ctx.update(id) 
   ctx.update(wire[2:]) 
   if first: 
      ctx.update(keyname.to_digestable()) 
      ctx.update(struct.pack('!H', dns.rdataclass.ANY)) 
      ctx.update(struct.pack('!I', 0)) 
   long_time = (time + 0L) 
   upper_time = ((long_time >> 32) & 65535L) 
   lower_time = (long_time & 4294967295L) 
   time_mac = struct.pack('!HIH', upper_time, lower_time, fudge) 
   pre_mac = (algorithm_name + time_mac) 
   ol = len(other_data) 
   if (ol > 65535): 
      raise ValueError('TSIG   Other   Data   is   >   65535   bytes') 
   post_mac = (struct.pack('!HH', error, ol) + other_data) 
   if first: 
      ctx.update(pre_mac) 
      ctx.update(post_mac) 
   else: 
      ctx.update(time_mac) 
   mac = ctx.digest() 
   mpack = struct.pack('!H', len(mac)) 
   tsig_rdata = ((((pre_mac + mpack) + mac) + id) + post_mac) 
   if multi: 
      ctx = hmac.new(secret, digestmod=digestmod) 
      ml = len(mac) 
      ctx.update(struct.pack('!H', ml)) 
      ctx.update(mac) 
   else: 
      ctx = None 
   return (tsig_rdata, mac, ctx)"," 'Generate a TSIG RDATA. 
 This function generates the TSIG RDATA and the TSIG MAC. 
 It uses the HMAC-MD5 algorithm. 
 :param wire: The wire format wire object. 
 :param keyname: The name of the key. 
 :param secret: The secret key. 
 :param time: The time the signature was created. 
 :param fudge: The fudge value. 
 :param original_id: The original ID of the record. 
 :param error: The error code. 
 :param other_data: The other data. 
 :param request_mac: The request MAC. 
 :param ctx: The context for the HMAC. 
 :param multi: Whether this is a multi-signature. 
 :param first: Whether this is the first signature. 
 :param algorithm: The algorithm to use. 
 :return: The TSIG RDATA, the TSIG MAC, and the HMAC context. 
 :rtype: tuple(bytes, bytes, hmac.HMAC)'","'Return a (tsig_rdata, mac, ctx) tuple containing the HMAC TSIG rdata 
 for the input parameters, the HMAC MAC calculated by applying the 
 TSIG signature algorithm, and the TSIG digest context. 
 @rtype: (string, string, hmac.HMAC object) 
 @raises ValueError: I{other_data} is too long 
 @raises NotImplementedError: I{algorithm} is not supported'"
"def errors_response(status, errors): 
    document = {'errors': errors, 'jsonapi': {'version': JSONAPI_VERSION}} 
   return (jsonpify(document), status)", 'Returns a JSON-API formatted response for a failed request.',"'Return an error response with multiple errors. 
 `status` is an integer representing an HTTP status code corresponding to an 
 error response. 
 `errors` is a list of error dictionaries, each of which must satisfy the 
 requirements of the JSON API specification. 
 This function returns a two-tuple whose left element is a dictionary 
 representing a JSON API response document and whose right element is 
 simply `status`. 
 The keys within each error object are described in the `Errors`_ 
 section of the JSON API specification. 
 .. _Errors: http://jsonapi.org/format/#errors'"
"def dmp_integrate_in(f, m, j, u, K): 
    if ((j < 0) or (j > u)): 
      raise IndexError(('0   <=   j   <=   u   expected,   got   %s' % (u, j))) 
   return _rec_integrate_in(f, m, u, 0, j, K)"," 'Integrate ``f`` in ``[0, j]`` with respect to ``x_j`` for ``j`` in ``[0, u]``. 
 Examples 
 >>> from sympy.polys import ring, ZZ 
 >>> R, x, y = ring(""x, y"", ZZ) 
 >>> f = x**2 + x*y + y**2 
 >>> dmp_integrate_in(f, 2, 2, 1, 1, R) 
 x + y'","'Computes the indefinite integral of ``f`` in ``x_j`` in ``K[X]``. 
 Examples 
 >>> from sympy.polys import ring, QQ 
 >>> R, x,y = ring(""x,y"", QQ) 
 >>> R.dmp_integrate_in(x + 2*y, 1, 0) 
 1/2*x**2 + 2*x*y 
 >>> R.dmp_integrate_in(x + 2*y, 1, 1) 
 x*y + y**2'"
"def dump_thread_stack(): 
    threads = threading.enumerate() 
   output_file = (PROFILING_OUTPUT_FMT % get_filename_fmt()) 
   data = {} 
   for (thread, frame) in sys._current_frames().items(): 
      trace = traceback.format_stack(frame) 
      data[('%x' % thread)] = {'traceback': trace, 'name': get_thread_name(threads, thread)} 
   json.dump(data, file(output_file, 'w'), indent=4)"," 'Dump a thread stack in JSON format to the output file. 
 This function is called by the profiler when profiling is enabled.'",'Dumps all thread stacks to a file'
"@endpoint(u'/ajax/books/{library_id=None}', postprocess=json) 
 def books(ctx, rd, library_id): 
    db = get_db(ctx, rd, library_id) 
   with db.safe_read_lock: 
      id_is_uuid = rd.query.get(u'id_is_uuid', u'false') 
      ids = rd.query.get(u'ids') 
      if ((ids is None) or (ids == u'all')): 
         ids = db.all_book_ids() 
      else: 
         ids = ids.split(u',') 
         if (id_is_uuid == u'true'): 
            ids = {db.lookup_by_uuid(x) for x in ids} 
            ids.discard(None) 
         else: 
            try: 
               ids = {int(x) for x in ids} 
            except Exception: 
               raise HTTPNotFound(u'ids   must   a   comma   separated   list   of   integers') 
      last_modified = None 
      category_urls = (rd.query.get(u'category_urls', u'true').lower() == u'true') 
      device_compatible = (rd.query.get(u'device_compatible', u'false').lower() == u'true') 
      device_for_template = rd.query.get(u'device_for_template', None) 
      ans = {} 
      restricted_to = ctx.allowed_book_ids(rd, db) 
      for book_id in ids: 
         if (book_id not in restricted_to): 
            ans[book_id] = None 
            continue 
         (data, lm) = book_to_json(ctx, rd, db, book_id, get_category_urls=category_urls, device_compatible=device_compatible, device_for_template=device_for_template) 
         last_modified = (lm if (last_modified is None) else max(lm, last_modified)) 
         ans[book_id] = data 
   if (last_modified is not None): 
      rd.outheaders[u'Last-Modified'] = http_date(timestampfromdt(last_modified)) 
   return ans"," 'Returns a JSON object containing the book data for all the books 
 specified in the request. 
 :param rd: The RequestDict. 
 :param library_id: The library ID. 
 :return: A JSON object containing the book data for all the books 
 specified in the request. 
 :rtype: dict'","'Return the metadata for the books as a JSON dictionary. 
 Query parameters: ?ids=all&category_urls=true&id_is_uuid=false&device_for_template=None 
 If category_urls is true the returned dictionary also contains a 
 mapping of category (field) names to URLs that return the list of books in the 
 given category. 
 If id_is_uuid is true then the book_id is assumed to be a book uuid instead.'"
"def get_injected_network_template(network_info, use_ipv6=CONF.use_ipv6, template=CONF.injected_network_template): 
    if (network_info is None): 
      return None 
   if hasattr(network_info, 'legacy'): 
      network_info = network_info.legacy() 
   nets = [] 
   ifc_num = (-1) 
   have_injected_networks = False 
   for (network_ref, mapping) in network_info: 
      ifc_num += 1 
      if (not network_ref['injected']): 
         continue 
      have_injected_networks = True 
      address = mapping['ips'][0]['ip'] 
      netmask = mapping['ips'][0]['netmask'] 
      address_v6 = None 
      gateway_v6 = None 
      netmask_v6 = None 
      if use_ipv6: 
         address_v6 = mapping['ip6s'][0]['ip'] 
         netmask_v6 = mapping['ip6s'][0]['netmask'] 
         gateway_v6 = mapping['gateway_v6'] 
      net_info = {'name': ('eth%d' % ifc_num), 'address': address, 'netmask': netmask, 'gateway': mapping['gateway'], 'broadcast': mapping['broadcast'], 'dns': '   '.join(mapping['dns']), 'address_v6': address_v6, 'gateway_v6': gateway_v6, 'netmask_v6': netmask_v6} 
      nets.append(net_info) 
   if (have_injected_networks is False): 
      return None 
   if (not template): 
      return None 
   _late_load_cheetah() 
   ifc_template = open(template).read() 
   return str(Template(ifc_template, searchList=[{'interfaces': nets, 'use_ipv6': use_ipv6}]))", 'Returns the injected network template.',"'return a rendered network template for the given network_info 
 :param network_info: 
 :py:meth:`~nova.network.manager.NetworkManager.get_instance_nw_info` 
 Note: this code actually depends on the legacy network_info, but will 
 convert the type itself if necessary.'"
"@pytest.fixture 
 def reset_standarddir(no_cachedir_tag): 
    standarddir.init(None) 
   (yield) 
   standarddir.init(None)", 'Reset the standard directory before each test.','Clean up standarddir arguments before and after each test.'
"def refresh_db(full=False): 
    if full: 
      return (__salt__['cmd.retcode']('/bin/pkg   refresh   --full') == 0) 
   else: 
      return (__salt__['cmd.retcode']('/bin/pkg   refresh') == 0)"," 'Refresh the package manager cache. 
 If full is set, then the full cache will be refreshed. 
 Otherwise, only the normal cache will be refreshed.'","'Updates the remote repos database. 
 full : False 
 Set to ``True`` to force a refresh of the pkg DB from all publishers, 
 regardless of the last refresh time. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' pkg.refresh_db 
 salt \'*\' pkg.refresh_db full=True'"
"def from_current_timezone(value): 
    if (settings.USE_TZ and (value is not None) and timezone.is_naive(value)): 
      current_timezone = timezone.get_current_timezone() 
      try: 
         return timezone.make_aware(value, current_timezone) 
      except Exception: 
         raise ValidationError((_(u""%(datetime)s   couldn't   be   interpreted   in   time   zone   %(current_timezone)s;   it   may   be   ambiguous   or   it   may   not   exist."") % {u'datetime': value, u'current_timezone': current_timezone})) 
   return value"," 'Convert a datetime object to the current time zone. 
 If the datetime is naive, it will be converted to UTC and then 
 converted back to the current time zone. 
 If the datetime is not naive, it will be converted to UTC and 
 then converted to the current time zone.'","'When time zone support is enabled, convert naive datetimes 
 entered in the current time zone to aware datetimes.'"
"def DEFINE_boolean(name, default, help): 
    CONFIG.AddOption(type_info.Bool(name=name, default=default, description=help))"," 'Define a boolean option. 
 Parameters 
 name : str 
 The name of the option. 
 default : bool 
 The default value for the option. 
 help : str 
 A help message for the option.'",'A helper for defining boolean options.'
"def load_lang_conf(): 
    if osp.isfile(LANG_FILE): 
      with open(LANG_FILE, 'r') as f: 
         lang = f.read() 
   else: 
      lang = get_interface_language() 
      save_lang_conf(lang) 
   if (lang.strip('\n') in DISABLED_LANGUAGES): 
      lang = DEFAULT_LANGUAGE 
      save_lang_conf(lang) 
   return lang", 'Loads the language configuration file and returns the selected language.',"'Load language setting from language config file if it exists, otherwise 
 try to use the local settings if Spyder provides a translation, or 
 return the default if no translation provided.'"
"def badDecorator(fn): 
    def nameCollision(*args, **kwargs): 
      return fn(*args, **kwargs) 
   return nameCollision", 'Decorator to test for name collisions.',"'Decorate a function without preserving the name of the original function. 
 Always return a function with the same name.'"
"def find_file(path, saltenv='base', **kwargs): 
    if ('env' in kwargs): 
      salt.utils.warn_until('Oxygen', ""Parameter   'env'   has   been   detected   in   the   argument   list.      This   parameter   is   no   longer   used   and   has   been   replaced   by   'saltenv'   as   of   Salt   2016.11.0.      This   warning   will   be   removed   in   Salt   Oxygen."") 
      kwargs.pop('env') 
   path = os.path.normpath(path) 
   fnd = {'path': '', 'rel': ''} 
   if os.path.isabs(path): 
      return fnd 
   if (saltenv not in __opts__['file_roots']): 
      return fnd 
   def _add_file_stat(fnd): 
      '\n                        Stat   the   file   and,   assuming   no   errors   were   found,   convert   the   stat\n                        result   to   a   list   of   values   and   add   to   the   return   dict.\n\n                        Converting   the   stat   result   to   a   list,   the   elements   of   the   list\n                        correspond   to   the   following   stat_result   params:\n\n                        0   =>   st_mode=33188\n                        1   =>   st_ino=10227377\n                        2   =>   st_dev=65026\n                        3   =>   st_nlink=1\n                        4   =>   st_uid=1000\n                        5   =>   st_gid=1000\n                        6   =>   st_size=1056233\n                        7   =>   st_atime=1468284229\n                        8   =>   st_mtime=1456338235\n                        9   =>   st_ctime=1456338235\n                        ' 
      try: 
         fnd['stat'] = list(os.stat(fnd['path'])) 
      except Exception: 
         pass 
      return fnd 
   if ('index' in kwargs): 
      try: 
         root = __opts__['file_roots'][saltenv][int(kwargs['index'])] 
      except IndexError: 
         return fnd 
      except ValueError: 
         return fnd 
      full = os.path.join(root, path) 
      if (os.path.isfile(full) and (not salt.fileserver.is_file_ignored(__opts__, full))): 
         fnd['path'] = full 
         fnd['rel'] = path 
         return _add_file_stat(fnd) 
      return fnd 
   for root in __opts__['file_roots'][saltenv]: 
      full = os.path.join(root, path) 
      if (os.path.isfile(full) and (not salt.fileserver.is_file_ignored(__opts__, full))): 
         fnd['path'] = full 
         fnd['rel'] = path 
         return _add_file_stat(fnd) 
   return fnd"," 'Find the file in the file roots. 
 If the file is not found, return an empty dictionary. 
 If the file is found, return a dictionary with the path and relative path 
 to the file.'",'Search the environment for the relative path.'
"def load_pandas(): 
    filepath = os.path.dirname(os.path.abspath(__file__)) 
   data = pd.read_csv(os.path.join((filepath + '/china_smoking.csv')), index_col='Location') 
   return utils.Dataset(data=data, title='Smoking   and   lung   cancer   in   Chinese   regions')", 'Loads the pandas dataset',"'Load the China smoking/lung cancer data and return a Dataset class. 
 Returns 
 Dataset instance: 
 See DATASET_PROPOSAL.txt for more information.'"
"def flatten(seq, scalarp=is_scalar_or_string): 
    for item in seq: 
      if scalarp(item): 
         (yield item) 
      else: 
         for subitem in flatten(item, scalarp): 
            (yield subitem)"," 'Returns a flattened version of the given sequence. 
 Parameters 
 seq : iterable 
 A sequence of iterables. 
 scalarp : callable 
 A function that takes an item as input and returns True if it is a 
 scalar, and False otherwise. 
 Returns 
 iterable 
 An iterable containing the flattened version of the input sequence. 
 Examples 
 >>> list(flatten([1, [2, 3], 4])) 
 [1, 2, 3, 4] 
 >>> list(flatten([1, [2, [3, 4]], 5])) 
 [1, 2, 3, 4, 5] 
 >>> list(flatten([1, [2, [3, [4, 5]]], 6])) 
 [1, 2, 3, 4, 5, 6] 
 >>> list(flatten([1, [2, [3, [4, [5, 6]]]]], scalarp=lambda x: isinstance(x, str))) 
 [1, 2,","'Returns a generator of flattened nested containers 
 For example: 
 >>> from matplotlib.cbook import flatten 
 >>> l = ((\'John\', [\'Hunter\']), (1, 23), [[([42, (5, 23)], )]]) 
 >>> print(list(flatten(l))) 
 [\'John\', \'Hunter\', 1, 23, 42, 5, 23] 
 By: Composite of Holger Krekel and Luther Blissett 
 From: https://code.activestate.com/recipes/121294/ 
 and Recipe 1.12 in cookbook'"
"def isLineIntersectingLoops(loops, pointBegin, pointEnd): 
    normalizedSegment = (pointEnd - pointBegin) 
   normalizedSegmentLength = abs(normalizedSegment) 
   if (normalizedSegmentLength > 0.0): 
      normalizedSegment /= normalizedSegmentLength 
      segmentYMirror = complex(normalizedSegment.real, (- normalizedSegment.imag)) 
      pointBeginRotated = (segmentYMirror * pointBegin) 
      pointEndRotated = (segmentYMirror * pointEnd) 
      if isLoopListIntersectingInsideXSegment(loops, pointBeginRotated.real, pointEndRotated.real, segmentYMirror, pointBeginRotated.imag): 
         return True 
   return False"," 'Returns True if the point is inside the loop. 
 If the point is not inside the loop, the function returns False. 
 If the point is inside the loop, the function returns True. 
 Parameters 
 loops : list 
 List of loops. 
 pointBegin : list 
 List of points for the beginning of the loop. 
 pointEnd : list 
 List of points for the end of the loop. 
 Returns 
 True if the point is inside the loop. 
 False if the point is not inside the loop.'",'Determine if the line is intersecting loops.'
"def report_expected_diffs(diffs, colorize=False): 
    if (not diffs): 
      return 'No   differences' 
   diffs = diffs.items() 
   diffs.sort() 
   s = [] 
   last = '' 
   for (path, desc) in diffs: 
      t = _space_prefix(last, path, indent=4, include_sep=False) 
      if colorize: 
         t = color_line(t, 11) 
      last = path 
      if (len(desc.splitlines()) > 1): 
         cur_indent = len(re.search('^[   ]*', t).group(0)) 
         desc = indent((cur_indent + 2), desc) 
         if colorize: 
            t += '\n' 
            for line in desc.splitlines(): 
               if line.strip().startswith('+'): 
                  line = color_line(line, 10) 
               elif line.strip().startswith('-'): 
                  line = color_line(line, 9) 
               else: 
                  line = color_line(line, 14) 
               t += (line + '\n') 
         else: 
            t += ('\n' + desc) 
      else: 
         t += ('   ' + desc) 
      s.append(t) 
   s.append(('Files   with   differences:   %s' % len(diffs))) 
   return '\n'.join(s)"," 'Return a diff report. 
 :param diffs: a dict of diffs, where keys are paths and values are 
 lists of lines that differ. 
 :param colorize: if True, colorize the diff report'","'Takes the output of compare_expected, and returns a string 
 description of the differences.'"
"@treeio_login_required 
 @handle_response_format 
 def tax_view(request, tax_id, response_format='html'): 
    tax = get_object_or_404(Tax, pk=tax_id) 
   if ((not request.user.profile.has_permission(tax, mode='r')) and (not request.user.profile.is_admin('treeio_finance'))): 
      return user_denied(request, ""You   don't   have   access   to   this   Tax"", response_format) 
   return render_to_response('finance/tax_view', {'tax': tax}, context_instance=RequestContext(request), response_format=response_format)"," 'View a tax object. 
 :param request: The current request. 
 :param tax_id: The tax id. 
 :param response_format: The format to return the response in. 
 :return: A response to the request.'",'View a tax'
"def timeit(func): 
    before = time.time() 
   res = func() 
   return ((time.time() - before), res)"," 'Decorator to time function calls. 
 This decorator will time the execution of the function and return the 
 time and the result of the function. 
 >>> @timeit 
 ... def foo(): 
 ...     return 1 
 >>> foo() 
 (0.00002997719, 1)'","'Run some function, and return (RunTimeInSeconds,Result)'"
"def http_date_to_dt(http_date, obs_date=False): 
    if (not obs_date): 
      return strptime(http_date, '%a,   %d   %b   %Y   %H:%M:%S   %Z') 
   time_formats = ('%a,   %d   %b   %Y   %H:%M:%S   %Z', '%a,   %d-%b-%Y   %H:%M:%S   %Z', '%A,   %d-%b-%y   %H:%M:%S   %Z', '%a   %b   %d   %H:%M:%S   %Y') 
   for time_format in time_formats: 
      try: 
         return strptime(http_date, time_format) 
      except ValueError: 
         continue 
   raise ValueError(('time   data   %r   does   not   match   known   formats' % http_date))"," 'Convert a date string to a datetime.datetime object 
 Parameters 
 http_date : str 
 A date string in any of the formats accepted by strptime(). 
 obs_date : bool 
 If True, the date string is assumed to be in UTC. If False, it is 
 assumed to be in the local time zone. 
 Returns 
 datetime.datetime 
 A datetime.datetime object. 
 Raises 
 ValueError 
 If the date string does not match any of the known formats.'","'Converts an HTTP date string to a datetime instance. 
 Args: 
 http_date (str): An RFC 1123 date string, e.g.: 
 ""Tue, 15 Nov 1994 12:45:26 GMT"". 
 obs_date (bool, optional): Support obs-date formats according to 
 RFC 7231, e.g.: 
 ""Sunday, 06-Nov-94 08:49:37 GMT"" (default ``False``). 
 Returns: 
 datetime: A UTC datetime instance corresponding to the given 
 HTTP date. 
 Raises: 
 ValueError: http_date doesn\'t match any of the available time formats'"
"def setup_platform(hass, config, add_devices, discovery_info=None): 
    import myusps 
   try: 
      cookie = hass.config.path(COOKIE) 
      session = myusps.get_session(config.get(CONF_USERNAME), config.get(CONF_PASSWORD), cookie_path=cookie) 
   except myusps.USPSError: 
      _LOGGER.exception('Could   not   connect   to   My   USPS') 
      return False 
   add_devices([USPSSensor(session, config.get(CONF_UPDATE_INTERVAL))])", 'Set up the My USPS sensor.','Setup the USPS platform.'
"def task_enable_flocker_control(distribution, action='start'): 
    validate_start_action(action) 
   if is_systemd_distribution(distribution): 
      return sequence([run_from_args(['systemctl', 'enable', 'flocker-control']), run_from_args(['systemctl', action.lower(), 'flocker-control'])]) 
   elif is_ubuntu(distribution): 
      return sequence([put(path='/etc/init/flocker-control.override', content=dedent('                                                            start   on   runlevel   [2345]\n                                                            stop   on   runlevel   [016]\n                                                            ')), run(""echo   'flocker-control-api DCTB 4523/tcp DCTB  DCTB  DCTB #   Flocker   Control   API   port'   >>   /etc/services""), run(""echo   'flocker-control-agent DCTB 4524/tcp DCTB  DCTB  DCTB #   Flocker   Control   Agent   port'   >>   /etc/services""), run_from_args(['service', 'flocker-control', action.lower()])]) 
   else: 
      raise DistributionNotSupported(distribution=distribution)"," 'Enable the Flocker control daemon on the system. 
 :param distribution: The distribution to use. 
 :param action: The action to perform. 
 :returns: A list of commands to run.'","'Enable flocker-control service. We need to be able to indicate whether 
 we want to start the service, when we are deploying a new cluster, 
 or if we want to restart it, when we are using an existent cluster in 
 managed mode. 
 :param bytes distribution: name of the distribution where the flocker 
 controls currently runs. The supported distros are: 
 - ubuntu-14.04 
 - ubuntu-16.04 
 - centos-<centos version> 
 :param bytes action: action to perform with the flocker control service. 
 Currently, we support: 
 -start 
 -stop 
 :raises ``DistributionNotSupported`` if the ``distribution`` is not 
 currently supported 
 ``UnknownAction`` if the action passed is not a valid one'"
"def get_conn(service='SoftLayer_Virtual_Guest'): 
    client = SoftLayer.Client(username=config.get_cloud_config_value('user', get_configured_provider(), __opts__, search_global=False), api_key=config.get_cloud_config_value('apikey', get_configured_provider(), __opts__, search_global=False)) 
   return client[service]"," 'Get a connection to the SoftLayer API. 
 :param service: The service to use. 
 :type service: str 
 :return: A connection to the SoftLayer API.'",'Return a conn object for the passed VM data'
"def autocorr(s, axis=(-1)): 
    N = s.shape[axis] 
   S = np.fft.fft(s, n=((2 * N) - 1), axis=axis) 
   sxx = np.fft.ifft((S * S.conjugate()), axis=axis).real[:N] 
   return (sxx / N)"," 'Compute the auto-correlation of a signal. 
 The auto-correlation of a signal is the cross-correlation of the signal with 
 itself. 
 Parameters 
 s : (N,) ndarray 
 The signal to be correlated. 
 axis : int, optional 
 The axis to correlate along. 
 Returns 
 auto-correlation : (N,) ndarray 
 The auto-correlation of the signal. 
 Examples 
 >>> from scipy import signal 
 >>> s = signal.sin(np.linspace(0, 2 * np.pi, 100)) 
 >>> s 
 array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.       ","'Returns the autocorrelation of signal s at all lags. Adheres to the 
 definition r(k) = E{s(n)s*(n-k)} where E{} is the expectation operator.'"
"def _has_access_course(user, action, courselike): 
    def can_load(): 
      '\n                        Can   this   user   load   this   course?\n\n                        NOTE:   this   is   not   checking   whether   user   is   actually   enrolled   in   the   course.\n                        ' 
      response = (_visible_to_nonstaff_users(courselike) and _can_access_descriptor_with_start_date(user, courselike, courselike.id)) 
      return (ACCESS_GRANTED if (response or _has_staff_access_to_descriptor(user, courselike, courselike.id)) else response) 
   def can_enroll(): 
      '\n                        Returns   whether   the   user   can   enroll   in   the   course.\n                        ' 
      return _can_enroll_courselike(user, courselike) 
   def see_exists(): 
      ""\n                        Can   see   if   can   enroll,   but   also   if   can   load   it:   if   user   enrolled   in   a   course   and   now\n                        it's   past   the   enrollment   period,   they   should   still   see   it.\n                        "" 
      return (ACCESS_GRANTED if (can_load() or can_enroll()) else ACCESS_DENIED) 
   def can_see_in_catalog(): 
      '\n                        Implements   the   ""can   see   course   in   catalog""   logic   if   a   course   should   be   visible   in   the   main   course   catalog\n                        In   this   case   we   use   the   catalog_visibility   property   on   the   course   descriptor\n                        but   also   allow   course   staff   to   see   this.\n                        ' 
      return (_has_catalog_visibility(courselike, CATALOG_VISIBILITY_CATALOG_AND_ABOUT) or _has_staff_access_to_descriptor(user, courselike, courselike.id)) 
   def can_see_about_page(): 
      '\n                        Implements   the   ""can   see   course   about   page""   logic   if   a   course   about   page   should   be   visible\n                        In   this   case   we   use   the   catalog_visibility   property   on   the   course   descriptor\n                        but   also   allow   course   staff   to   see   this.\n                        ' 
      return (_has_catalog_visibility(courselike, CATALOG_VISIBILITY_CATALOG_AND_ABOUT) or _has_catalog_visibility(courselike, CATALOG_VISIBILITY_ABOUT) or _has_staff_access_to_descriptor(user, courselike, courselike.id)) 
   checkers = {'load': can_load, 'view_courseware_with_prerequisites': (lambda : _can_view_courseware_with_prerequisites(user, courselike)), 'load_mobile': (lambda : (can_load() and _can_load_course_on_mobile(user, courselike))), 'enroll': can_enroll, 'see_exists': see_exists, 'staff': (lambda : _has_staff_access_to_descriptor(user, courselike, courselike.id)), 'instructor': (lambda : _has_instructor_access_to_descriptor(user, courselike, courselike.id)), 'see_in_catalog': can_see_in_catalog, 'see_about_page': can_see_about_page} 
   return _dispatch(checkers, action, user, courselike)"," 'Checks if a user has access to a course. 
 :param user: User to check access for. 
 :param action: The action to check for. 
 :param courselike: Course-like object to check access for. 
 :returns: ACCESS_GRANTED, ACCESS_DENIED or ACCESS_UNKNOWN. 
 :raises: NotAuthenticated if user is not authenticated. 
 :raises: CourseAccessDenied if user does not have access to the course. 
 :raises: CourseAccessDenied if user does not have access to the course. 
 :raises: CourseAccessDenied if user does not have access to the course. 
 :raises: CourseAccessDenied if user does not have access to the course. 
 :raises: CourseAccessDenied if user does not have access to the course. 
 :raises: CourseAccessDenied if user does not have access to the course. 
 :raises: CourseAccessDenied if user does not have access to the course. 
 :raises: CourseAccessDen","'Check if user has access to a course. 
 Arguments: 
 user (User): the user whose course access we are checking. 
 action (string): The action that is being checked. 
 courselike (CourseDescriptor or CourseOverview): The object 
 representing the course that the user wants to access. 
 Valid actions: 
 \'load\' -- load the courseware, see inside the course 
 \'load_forum\' -- can load and contribute to the forums (one access level for now) 
 \'load_mobile\' -- can load from a mobile context 
 \'enroll\' -- enroll.  Checks for enrollment window. 
 \'see_exists\' -- can see that the course exists. 
 \'staff\' -- staff access to course. 
 \'see_in_catalog\' -- user is able to see the course listed in the course catalog. 
 \'see_about_page\' -- user is able to see the course about page.'"
"def sequences_add_start_id(sequences, start_id=0, remove_last=False): 
    sequences_out = [[] for _ in range(len(sequences))] 
   for i in range(len(sequences)): 
      if remove_last: 
         sequences_out[i] = ([start_id] + sequences[i][:(-1)]) 
      else: 
         sequences_out[i] = ([start_id] + sequences[i]) 
   return sequences_out"," 'Add start_id to the beginning of each sequence. 
 Parameters 
 sequences : list 
 List of sequences 
 start_id : int 
 Start ID to add to each sequence 
 remove_last : bool 
 If True, remove the last sequence element from each sequence 
 Returns 
 sequences : list 
 List of sequences with start_id added to each sequence'","'Add special start token(id) in the beginning of each sequence. 
 Examples 
 >>> sentences_ids = [[4,3,5,3,2,2,2,2], [5,3,9,4,9,2,2,3]] 
 >>> sentences_ids = sequences_add_start_id(sentences_ids, start_id=2) 
 ... [[2, 4, 3, 5, 3, 2, 2, 2, 2], [2, 5, 3, 9, 4, 9, 2, 2, 3]] 
 >>> sentences_ids = sequences_add_start_id(sentences_ids, start_id=2, remove_last=True) 
 ... [[2, 4, 3, 5, 3, 2, 2, 2], [2, 5, 3, 9, 4, 9, 2, 2]] 
 - For Seq2seq 
 >>> input = [a, b, c] 
 >>> target = [x, y, z] 
 >>> decode_seq = [start_id, a, b] <-- sequences_add_start_id(input, start_id, True)'"
"def test_cnn_sample_wt_fit(): 
    cnn = CondensedNearestNeighbour(random_state=RND_SEED) 
   assert_raises(RuntimeError, cnn.sample, X, Y)", 'Test that sampling with the CondensedNearestNeighbour is not possible',"'Test either if an error is raised when sample is called before 
 fitting'"
"def div_proxy(x, y): 
    f = int_or_true_div((as_scalar(x).type in discrete_types), (as_scalar(y).type in discrete_types)) 
   return f(x, y)"," 'Returns the division of two scalars, if possible. 
 This function will return ``True`` if the division is possible, 
 and ``False`` if not. 
 Parameters 
 x : Scalar 
 The first scalar. 
 y : Scalar 
 The second scalar. 
 Returns 
 bool 
 ``True`` if the division is possible, ``False`` if not. 
 Notes 
 This function will return ``True`` if either the ``x`` or ``y`` 
 scalar is a discrete type, or if both are. 
 Examples 
 >>> from sympy import div_proxy 
 >>> from sympy.abc import x, y 
 >>> div_proxy(x, y) 
 True 
 >>> div_proxy(x, y) 
 False 
 >>> div_proxy(x, x) 
 True 
 >>> div_proxy(x, x) 
 True 
 >>> div_proxy(x, x) 
 True'","'Proxy for either true_div or int_div, depending on types of x, y.'"
"def seteuid(uid): 
    uid = parse_uid(uid) 
   if (uid != os.getuid()): 
      os.seteuid(uid)"," 'Sets the effective user ID to `uid`. 
 If `uid` is not a valid user ID, it is silently ignored. 
 >>> seteuid(1000) 
 >>> os.getuid() 
 1000'",'Set effective user id.'
"def _native_set_to_python_list(typ, payload, c): 
    nitems = payload.used 
   listobj = c.pyapi.list_new(nitems) 
   ok = cgutils.is_not_null(c.builder, listobj) 
   with c.builder.if_then(ok, likely=True): 
      index = cgutils.alloca_once_value(c.builder, ir.Constant(nitems.type, 0)) 
      with payload._iterate() as loop: 
         i = c.builder.load(index) 
         item = loop.entry.key 
         itemobj = c.box(typ.dtype, item) 
         c.pyapi.list_setitem(listobj, i, itemobj) 
         i = c.builder.add(i, ir.Constant(i.type, 1)) 
         c.builder.store(i, index) 
   return (ok, listobj)", 'Convert a native Python list to a C++ list.','Create a Python list from a native set\'s items.'
"def synopsis(filename, cache={}): 
    mtime = os.stat(filename).st_mtime 
   (lastupdate, result) = cache.get(filename, (0, None)) 
   if (lastupdate < mtime): 
      info = inspect.getmoduleinfo(filename) 
      try: 
         file = open(filename) 
      except IOError: 
         return None 
      if (info and ('b' in info[2])): 
         try: 
            module = imp.load_module('__temp__', file, filename, info[1:]) 
         except: 
            return None 
         result = (module.__doc__ or '').splitlines()[0] 
         del sys.modules['__temp__'] 
      else: 
         result = source_synopsis(file) 
         file.close() 
      cache[filename] = (mtime, result) 
   return result"," 'Return the documentation for the module or function. 
 :param filename: name of the module or function 
 :param cache: a dictionary to store the result of previous calls 
 :return: the documentation for the module or function'",'Get the one-line summary out of a module file.'
"def delete_rax_scaling_group(args): 
    print (""---   Cleaning   Autoscale   Groups   matching   '%s'"" % args.match_re) 
   for region in pyrax.identity.services.autoscale.regions: 
      asg = pyrax.connect_to_autoscale(region=region) 
      for group in rax_list_iterator(asg): 
         if re.search(args.match_re, group.name): 
            group.manager._delete = _force_delete_rax_scaling_group(group.manager) 
            prompt_and_delete(group, ('Delete   matching   %s?   [y/n]:   ' % group), args.assumeyes)", 'Delete matching autoscale groups','Function for deleting Autoscale Groups'
"def get_default_ddir(): 
    user_home = os.path.expanduser('~') 
   (join, exists) = (os.path.join, os.path.exists) 
   if mswin: 
      return join(user_home, 'Downloads', 'mps') 
   USER_DIRS = join(user_home, '.config', 'user-dirs.dirs') 
   DOWNLOAD_HOME = join(user_home, 'Downloads') 
   if ('XDG_DOWNLOAD_DIR' in os.environ): 
      ddir = os.environ['XDG_DOWNLOAD_DIR'] 
   elif exists(USER_DIRS): 
      lines = open(USER_DIRS).readlines() 
      defn = [x for x in lines if x.startswith('XDG_DOWNLOAD_DIR')] 
      if (len(defn) == 1): 
         ddir = defn[0].split('=')[1].replace('""', '') 
         ddir = ddir.replace('$HOME', user_home).strip() 
      else: 
         ddir = (DOWNLOAD_HOME if exists(DOWNLOAD_HOME) else user_home) 
   else: 
      ddir = (DOWNLOAD_HOME if exists(DOWNLOAD_HOME) else user_home) 
   ddir = ddir 
   return os.path.join(ddir, 'mps')", 'Get the default download directory for MPS.',"'Get system default Download directory, append mps dir.'"
"def floating_ip_pool_list(call=None): 
    if (call != 'function'): 
      raise SaltCloudSystemExit('The   floating_ip_pool_list   action   must   be   called   with   -f   or   --function') 
   conn = get_conn() 
   return conn.floating_ip_pool_list()", 'List all floating IP pools.',"'List all floating IP pools 
 .. versionadded:: 2016.3.0'"
"def delete(table_name, region=None, key=None, keyid=None, profile=None): 
    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) 
   table = Table(table_name, connection=conn) 
   table.delete() 
   MAX_ATTEMPTS = 30 
   for i in range(MAX_ATTEMPTS): 
      if (not exists(table_name, region, key, keyid, profile)): 
         return True 
      else: 
         time.sleep(1) 
   return False"," 'Delete a table. 
 :param table_name: The name of the table to delete. 
 :param region: The region to use. 
 :param key: The key to use. 
 :param keyid: The key id to use. 
 :param profile: The profile to use. 
 :returns: True if the table was deleted, False if it was not. 
 :raises: :class:`TableDoesNotExist` if the table does not exist.'","'Delete a DynamoDB table. 
 CLI Example: 
 .. code-block:: bash 
 salt myminion boto_dynamodb.delete table_name region=us-east-1'"
"def _qualNameWalker(qualName): 
    (yield (qualName, [])) 
   qualParts = qualName.split('.') 
   for index in range(1, len(qualParts)): 
      (yield ('.'.join(qualParts[:(- index)]), qualParts[(- index):]))", 'Walk a qualified name and yield the parts of it.',"'Given a Python qualified name, this function yields a 2-tuple of the most 
 specific qualified name first, followed by the next-most-specific qualified 
 name, and so on, paired with the remainder of the qualified name. 
 @param qualName: A Python qualified name. 
 @type qualName: L{str}'"
"@pytest.mark.parametrize('specialchars,   count_char', [('   abcde               ', '   '), ('   aaaaaaaaaa', 'a'), ('\xc4\x81\xc3\xa9\xc4\xa9\xc3\xb8\xc3\xb8\xc3\xb8\xc3\xb8\xc3\xb8\xc3\xb8\xc3\xbc', u'\xf8')]) 
 @pytest.mark.django_db 
 def test_clean_specialchars_unique(specialchars, count_char): 
    form_data = {'code': 'foo', 'fullname': 'Foo', 'checkstyle': 'foo', 'nplurals': '2', 'specialchars': specialchars} 
   form = LanguageForm(form_data) 
   assert form.is_valid() 
   assert (form.cleaned_data['specialchars'].count(count_char) == 1)", 'Test that the clean method removes duplicate special characters.','Tests special characters are unique.'
"def present(name, attributes=None, region=None, key=None, keyid=None, profile=None): 
    ret = {'name': name, 'result': True, 'comment': '', 'changes': {}} 
   is_present = __salt__['boto_sqs.exists'](name, region, key, keyid, profile) 
   if (not is_present): 
      if __opts__['test']: 
         msg = 'AWS   SQS   queue   {0}   is   set   to   be   created.'.format(name) 
         ret['comment'] = msg 
         ret['result'] = None 
         return ret 
      created = __salt__['boto_sqs.create'](name, region, key, keyid, profile) 
      if created: 
         ret['changes']['old'] = None 
         ret['changes']['new'] = {'queue': name} 
      else: 
         ret['result'] = False 
         ret['comment'] = 'Failed   to   create   {0}   AWS   queue'.format(name) 
         return ret 
   else: 
      ret['comment'] = '{0}   present.'.format(name) 
   attrs_to_set = {} 
   _attributes = __salt__['boto_sqs.get_attributes'](name, region, key, keyid, profile) 
   if attributes: 
      for (attr, val) in six.iteritems(attributes): 
         _val = _attributes.get(attr, None) 
         if (attr == 'Policy'): 
            if isinstance(_val, six.string_types): 
               _val = json.loads(_val) 
            if isinstance(val, six.string_types): 
               val = json.loads(val) 
            if (_val != val): 
               log.debug('Policies   differ:\n{0}\n{1}'.format(_val, val)) 
               attrs_to_set[attr] = json.dumps(val, sort_keys=True) 
         elif (str(_val) != str(val)): 
            log.debug('Attributes   differ:\n{0}\n{1}'.format(_val, val)) 
            attrs_to_set[attr] = val 
   attr_names = ','.join(attrs_to_set) 
   if attrs_to_set: 
      if __opts__['test']: 
         ret['comment'] = 'Attribute(s)   {0}   to   be   set   on   {1}.'.format(attr_names, name) 
         ret['result'] = None 
         return ret 
      msg = '   Setting   {0}   attribute(s).'.format(attr_names) 
      ret['comment'] = (ret['comment'] + msg) 
      if ('new' in ret['changes']): 
         ret['changes']['new']['attributes_set'] = [] 
      else: 
         ret['changes']['new'] = {'attributes_set': []} 
      for (attr, val) in six.iteritems(attrs_to_set): 
         set_attr = __salt__['boto_sqs.set_attributes'](name, {attr: val}, region, key, keyid, profile) 
         if (not set_attr): 
            ret['result'] = False 
         msg = 'Set   attribute   {0}.'.format(attr) 
         ret['changes']['new']['attributes_set'].append(attr) 
   else: 
      ret['comment'] = (ret['comment'] + '   Attributes   set.') 
   return ret"," 'Create or update an Amazon SQS queue. 
 :param name: The name of the queue to create or update 
 :param attributes: A dictionary of attributes to set 
 :param region: The region to create the queue in 
 :param key: The access key to use when creating the queue 
 :param keyid: The access key ID to use when creating the queue 
 :param profile: The profile to use when creating the queue'","'Ensure the SQS queue exists. 
 name 
 Name of the SQS queue. 
 attributes 
 A dict of key/value SQS attributes. 
 region 
 Region to connect to. 
 key 
 Secret key to be used. 
 keyid 
 Access key to be used. 
 profile 
 A dict with region, key and keyid, or a pillar key (string) 
 that contains a dict with region, key and keyid.'"
"def _sqrt_match(p): 
    from sympy.simplify.radsimp import split_surds 
   p = _mexpand(p) 
   if p.is_Number: 
      res = (p, S.Zero, S.Zero) 
   elif p.is_Add: 
      pargs = sorted(p.args, key=default_sort_key) 
      if all(((x ** 2).is_Rational for x in pargs)): 
         (r, b, a) = split_surds(p) 
         res = (a, b, r) 
         return list(res) 
      v = [(sqrt_depth(x), x, i) for (i, x) in enumerate(pargs)] 
      nmax = max(v, key=default_sort_key) 
      if (nmax[0] == 0): 
         res = [] 
      else: 
         (depth, _, i) = nmax 
         r = pargs.pop(i) 
         v.pop(i) 
         b = S.One 
         if r.is_Mul: 
            bv = [] 
            rv = [] 
            for x in r.args: 
               if (sqrt_depth(x) < depth): 
                  bv.append(x) 
               else: 
                  rv.append(x) 
            b = Mul._from_args(bv) 
            r = Mul._from_args(rv) 
         a1 = [] 
         b1 = [b] 
         for x in v: 
            if (x[0] < depth): 
               a1.append(x[1]) 
            else: 
               x1 = x[1] 
               if (x1 == r): 
                  b1.append(1) 
               elif x1.is_Mul: 
                  x1args = list(x1.args) 
                  if (r in x1args): 
                     x1args.remove(r) 
                     b1.append(Mul(*x1args)) 
                  else: 
                     a1.append(x[1]) 
               else: 
                  a1.append(x[1]) 
         a = Add(*a1) 
         b = Add(*b1) 
         res = (a, b, (r ** 2)) 
   else: 
      (b, r) = p.as_coeff_Mul() 
      if is_sqrt(r): 
         res = (S.Zero, b, (r ** 2)) 
      else: 
         res = [] 
   return list(res)"," 'Returns the square root of the expression p. 
 If p is a number, returns (p, 0, 0). 
 If p is a sum, returns (a, b, r) where 
 a is the sum of the squares of the terms of p 
 b is the sum of the terms of p 
 r is the sum of the squares of the terms of p 
 Examples 
 >>> from sympy import sqrt, sqrt_match 
 >>> sqrt_match(sqrt(2)) 
 [(2, 0, 0)] 
 >>> sqrt_match(sqrt(2 + 3)) 
 [(2, 0, 1)] 
 >>> sqrt_match(sqrt(2 + 3) + sqrt(2)) 
 [(2, 0, 1), (2, 0, 1)] 
 >>> sqrt_match(sqrt(2) + sqrt(2)) 
 [(2, 0, 1), (2, 0, 1)] 
 >>> sqrt_match(sqrt(2) - sqrt(2)) 
 [(2, 0","'Return [a, b, r] for p.match(a + b*sqrt(r)) where, in addition to 
 matching, sqrt(r) also has then maximal sqrt_depth among addends of p. 
 Examples 
 >>> from sympy.functions.elementary.miscellaneous import sqrt 
 >>> from sympy.simplify.sqrtdenest import _sqrt_match 
 >>> _sqrt_match(1 + sqrt(2) + sqrt(2)*sqrt(3) +  2*sqrt(1+sqrt(5))) 
 [1 + sqrt(2) + sqrt(6), 2, 1 + sqrt(5)]'"
"@treeio_login_required 
 @handle_response_format 
 def event_edit(request, event_id, response_format='html'): 
    event = get_object_or_404(Event, pk=event_id) 
   if (not request.user.profile.has_permission(event, mode='w')): 
      return user_denied(request, message=""You   don't   have   access   to   this   Event"") 
   if request.POST: 
      if ('cancel' not in request.POST): 
         form = EventForm(request.user.profile, None, None, request.POST, instance=event) 
         if form.is_valid(): 
            event = form.save() 
            return HttpResponseRedirect(reverse('events_event_view', args=[event.id])) 
      else: 
         return HttpResponseRedirect(reverse('events')) 
   else: 
      form = EventForm(request.user.profile, instance=event) 
   return render_to_response('events/event_edit', {'event': event, 'form': form}, context_instance=RequestContext(request), response_format=response_format)", 'Edit an event','Event edit'
"def is_media_request(request): 
    parsed_media_url = urlparse(settings.MEDIA_URL) 
   if request.path_info.startswith(parsed_media_url.path): 
      if parsed_media_url.netloc: 
         if (request.get_host() == parsed_media_url.netloc): 
            return True 
      else: 
         return True 
   return False", 'Returns True if the request is for a media URL.','Check if a request is a media request.'
"def find_subnets(vpc_conn, vpc_id, identified_subnets): 
    subnet_ids = [] 
   subnet_names = [] 
   subnet_cidrs = [] 
   for subnet in (identified_subnets or []): 
      if re.match(SUBNET_RE, subnet): 
         subnet_ids.append(subnet) 
      elif re.match(CIDR_RE, subnet): 
         subnet_cidrs.append(subnet) 
      else: 
         subnet_names.append(subnet) 
   subnets_by_id = [] 
   if subnet_ids: 
      subnets_by_id = vpc_conn.get_all_subnets(subnet_ids, filters={'vpc_id': vpc_id}) 
      for subnet_id in subnet_ids: 
         if (not any(((s.id == subnet_id) for s in subnets_by_id))): 
            raise AnsibleSubnetSearchException('Subnet   ID   ""{0}""   does   not   exist'.format(subnet_id)) 
   subnets_by_cidr = [] 
   if subnet_cidrs: 
      subnets_by_cidr = vpc_conn.get_all_subnets(filters={'vpc_id': vpc_id, 'cidr': subnet_cidrs}) 
      for cidr in subnet_cidrs: 
         if (not any(((s.cidr_block == cidr) for s in subnets_by_cidr))): 
            raise AnsibleSubnetSearchException('Subnet   CIDR   ""{0}""   does   not   exist'.format(cidr)) 
   subnets_by_name = [] 
   if subnet_names: 
      subnets_by_name = vpc_conn.get_all_subnets(filters={'vpc_id': vpc_id, 'tag:Name': subnet_names}) 
      for name in subnet_names: 
         matching_count = len([1 for s in subnets_by_name if (s.tags.get('Name') == name)]) 
         if (matching_count == 0): 
            raise AnsibleSubnetSearchException('Subnet   named   ""{0}""   does   not   exist'.format(name)) 
         elif (matching_count > 1): 
            raise AnsibleSubnetSearchException('Multiple   subnets   named   ""{0}""'.format(name)) 
   return ((subnets_by_id + subnets_by_cidr) + subnets_by_name)"," 'Searches for subnets in a VPC and returns the matching subnets. 
 :param vpc_conn: VPC connection object. 
 :param vpc_id: VPC ID. 
 :param identified_subnets: List of subnets to identify. 
 :returns: List of subnet IDs, names, and CIDRs.'","'Finds a list of subnets, each identified either by a raw ID, a unique 
 \'Name\' tag, or a CIDR such as 10.0.0.0/8. 
 Note that this function is duplicated in other ec2 modules, and should 
 potentially be moved into potentially be moved into a shared module_utils'"
"def test_sigma_clipped_stats(): 
    data = [0, 1] 
   mask = np.array([True, False]) 
   result = sigma_clipped_stats(data, mask=mask) 
   assert isinstance(result[1], float) 
   assert (result == (1.0, 1.0, 0.0)) 
   result = sigma_clipped_stats(data, mask_value=0.0) 
   assert isinstance(result[1], float) 
   assert (result == (1.0, 1.0, 0.0)) 
   data = [0, 2] 
   result = sigma_clipped_stats(data) 
   assert isinstance(result[1], float) 
   assert (result == (1.0, 1.0, 1.0)) 
   _data = np.arange(10) 
   data = np.ma.MaskedArray([_data, _data, (10 * _data)]) 
   mean = sigma_clip(data, axis=0, sigma=1).mean(axis=0) 
   assert_equal(mean, _data) 
   (mean, median, stddev) = sigma_clipped_stats(data, axis=0, sigma=1) 
   assert_equal(mean, _data) 
   assert_equal(median, _data) 
   assert_equal(stddev, np.zeros_like(_data))", 'Test sigma clipped stats','Test list data with input mask or mask_value (#3268).'
"def test_read_bin_lush_matrix_ubyte_scalar(): 
    path = (example_bin_lush_path + 'ubyte_scalar.lushbin') 
   result = read_bin_lush_matrix(path) 
   assert (str(result.dtype) == 'uint8') 
   assert (len(result.shape) == 3) 
   assert (result.shape[0] == 1) 
   assert (result.shape[1] == 1) 
   assert (result.shape[1] == 1) 
   assert (result[(0, 0)] == 12)", 'Test read_bin_lush_matrix for an image with a scalar value.',"'Read data from a lush file with uint8 data (scalar). 
 Note: When you write a scalar from Koray\'s matlab code it always makes 
 everything 3D. Writing it straight from lush you might be able to get 
 a true scalar'"
"def data_path(path): 
    return (path if isabs(path) else join(project_data_dir(), path))"," 'Returns the path to the given data file. 
 If the path is absolute, it is returned as-is.  Otherwise, it is 
 appended to the project data directory. 
 :param path: The path to the data file. 
 :return: The path to the data file.'","'If path is relative, return the given path inside the project data dir, 
 otherwise return the path unmodified'"
"def add_interface(zone, interface, permanent=True): 
    if (interface in get_interfaces(zone, permanent)): 
      log.info('Interface   is   already   bound   to   zone.') 
   cmd = '--zone={0}   --add-interface={1}'.format(zone, interface) 
   if permanent: 
      cmd += '   --permanent' 
   return __firewall_cmd(cmd)", 'Add an interface to a zone.',"'Bind an interface to a zone 
 .. versionadded:: 2016.3.0 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' firewalld.add_interface zone eth0'"
"def dylib_info(filename): 
    is_dylib = DYLIB_RE.match(filename) 
   if (not is_dylib): 
      return None 
   return is_dylib.groupdict()"," 'Returns a dict with the following keys: 
 ""name"": The name of the dylib 
 ""platform"": The platform the dylib was built for 
 ""arch"": The architecture the dylib was built for 
 ""arch_name"": The architecture name the dylib was built for 
 ""build_version"": The build version of the dylib 
 ""build_version_string"": The build version string of the dylib 
 ""file_type"": The file type of the dylib 
 ""file_type_string"": The file type string of the dylib 
 ""is_fat"": True if the dylib is a fat binary 
 ""is_fat_arch"": True if the dylib is a fat binary and the architecture is 
 a fat architecture 
 ""is_fat_platform"": True if the dylib is a fat binary and the platform is 
 a fat platform 
 ""is_fat_platform_arch"": True if the dylib is a fat binary and the platform 
 and architecture are both fat 
 ""is_fat_platform_arch_name"": True if the dylib is a fat binary and the 
","'A dylib name can take one of the following four forms: 
 Location/Name.SomeVersion_Suffix.dylib 
 Location/Name.SomeVersion.dylib 
 Location/Name_Suffix.dylib 
 Location/Name.dylib 
 returns None if not found or a mapping equivalent to: 
 dict( 
 location=\'Location\', 
 name=\'Name.SomeVersion_Suffix.dylib\', 
 shortname=\'Name\', 
 version=\'SomeVersion\', 
 suffix=\'Suffix\', 
 Note that SomeVersion and Suffix are optional and may be None 
 if not present.'"
"def finite_diff_kauers(sum): 
    function = sum.function 
   for l in sum.limits: 
      function = function.subs(l[0], (l[(-1)] + 1)) 
   return function", 'Returns the finite difference of a sum',"'Takes as input a Sum instance and returns the difference between the sum 
 with the upper index incremented by 1 and the original sum. For example, 
 if S(n) is a sum, then finite_diff_kauers will return S(n + 1) - S(n). 
 Examples 
 >>> from sympy.series.kauers import finite_diff_kauers 
 >>> from sympy import Sum 
 >>> from sympy.abc import x, y, m, n, k 
 >>> finite_diff_kauers(Sum(k, (k, 1, n))) 
 n + 1 
 >>> finite_diff_kauers(Sum(1/k, (k, 1, n))) 
 1/(n + 1) 
 >>> finite_diff_kauers(Sum((x*y**2), (x, 1, n), (y, 1, m))) 
 (m + 1)**2*(n + 1) 
 >>> finite_diff_kauers(Sum((x*y), (x, 1, m), (y, 1, n))) 
 (m + 1)*(n + 1)'"
"def get_disk_list(std_mounts_only=True, get_all_disks=False): 
    mounts = utils.system_output('mount').splitlines() 
   hd_list = [] 
   hd_regexp = re.compile('([hsv]d[a-z]+3)$') 
   partfile = open(_DISKPART_FILE) 
   for partline in partfile: 
      parts = partline.strip().split() 
      if ((len(parts) != 4) or partline.startswith('major')): 
         continue 
      partname = parts[3] 
      if (not get_all_disks): 
         if (not partname[(-1):].isdigit()): 
            continue 
      if (not fd_mgr.use_partition(partname)): 
         continue 
      tunepath = fd_mgr.map_drive_name(partname) 
      mstat = 0 
      fstype = '' 
      fsopts = '' 
      fsmkfs = '?' 
      chkdev = ('/dev/' + partname) 
      mountpt = None 
      for mln in mounts: 
         splt = mln.split() 
         if (splt[0].strip() == chkdev.strip()): 
            mountpt = fd_mgr.check_mount_point(partname, splt[2]) 
            if (not mountpt): 
               mstat = (-1) 
               break 
            fstype = splt[4] 
            fsopts = splt[5] 
            if (fsopts[:3] != '(rw'): 
               mstat = (-1) 
               break 
            mstat = 1 
      if (std_mounts_only and (mstat < 0)): 
         continue 
      if (not get_all_disks): 
         if (not mountpt): 
            mountpt = fd_mgr.check_mount_point(partname, None) 
            if (not mountpt): 
               continue 
      hd_list.append({'device': partname, 'mountpt': mountpt, 'tunable': tunepath, 'fs_type': fstype, 'fs_opts': fsopts, 'fs_mkfs': fsmkfs, 'mounted': mstat}) 
   return hd_list"," 'Returns a list of disks, including their mount points. 
 If std_mounts_only is True, only standard mount points are returned. 
 If get_all_disks is True, all disks are returned, even if they are not 
 mounted. 
 Returns 
 list 
 A list of dictionaries, each containing the mount point, device, and 
 other information about the disk.'","'Get a list of dictionaries with information about disks on this system. 
 :param std_mounts_only: Whether the function should return only disks that 
 have a mount point defined (True) or even devices that doesn\'t 
 (False). 
 :param get_all_disks: Whether the function should return only partitioned 
 disks (False) or return every disk, regardless of being partitioned 
 or not (True). 
 :return: List of dictionaries with disk information (see more below). 
 The \'disk_list\' array returned by get_disk_list() has an entry for each 
 disk drive we find on the box. Each of these entries is a map with the 
 following 3 string values: 
 \'device\'      disk device name (i.e. the part after /dev/) 
 \'mountpt\'     disk mount path 
 \'tunable\'     disk name for setting scheduler tunables (/sys/block/sd??) 
 The last value is an integer that indicates the current mount status 
 of the drive: 
 \'mounted\'     0 = not currently mounted 
 1 = mounted r/w on the expected path 
 -1 = mounted readonly or at an unexpected path 
 When the \'std_mounts_only\' argument is True we don\'t include drives 
 mounted on \'unusual\' mount points in the result. If a given device is 
 partitioned, it will return all partitions that exist on it. If it\'s not, 
 it will return the device itself (ie, if there are /dev/sdb1 and /dev/sdb2, 
 those will be returned but not /dev/sdb. if there is only a /dev/sdc, that 
 one will be returned).'"
"def no_afni(): 
    if (Info.version() is None): 
      return True 
   return False"," 'Returns True if AFNI is not installed, False otherwise.'",'Checks if AFNI is available'
"def initialize(cli): 
    cli.register('building-command-table.main', change_name) 
   cli.register('building-command-table.deploy', inject_commands) 
   cli.register('building-argument-table.deploy.get-application-revision', modify_revision_arguments) 
   cli.register('building-argument-table.deploy.register-application-revision', modify_revision_arguments) 
   cli.register('building-argument-table.deploy.create-deployment', modify_revision_arguments)", 'Initializes the command table','The entry point for CodeDeploy high level commands.'
"def seed(seed=None): 
    get_random_state().seed(seed)"," 'Sets the random seed. 
 seed(seed) sets the random seed to seed. 
 seed(seed1, seed2, ..., seedN) sets the random seed to seed1, seed2, ..., seedN. 
 seed(None) resets the random seed to the global random state. 
 Parameters 
 seed : int, float, or None 
 If seed is None, the random seed is set to the global random state. 
 If seed is an int, the seed is set to seed. 
 If seed is a float, the seed is set to seed * 2**31 - 1. 
 If seed is a tuple, the seed is set to the tuple of seeds. 
 Returns 
 None 
 Examples 
 >>> import numpy.random as NPR 
 >>> NPR.seed(1234) 
 >>> NPR.randint(0, 100) 
 1234 
 >>> NPR.randint(0, 100) 
 4321 
 >>> NPR.seed(None) 
 >>> NPR.randint(","'Resets the state of the random number generator with a seed. 
 This function resets the state of the global random number generator for 
 the current device. Be careful that generators for other devices are not 
 affected. 
 Args: 
 seed (None or int): Seed for the random number generator. If ``None``, 
 it uses :func:`os.urandom` if available or :func:`time.clock` 
 otherwise. Note that this function does not support seeding by an 
 integer array.'"
"def check_paths(): 
    if (not hasattr(settings, 'PEP_REPO_PATH')): 
      raise ImproperlyConfigured('No   PEP_REPO_PATH   in   settings') 
   if (not os.path.exists(settings.PEP_REPO_PATH)): 
      raise ImproperlyConfigured('PEP_REPO_PATH   in   settings   does   not   exist')", 'Check that PEP_REPO_PATH is set and that the directory exists.','Checks to ensure our PEP_REPO_PATH is setup correctly'
"def confirm(message=u'Confirm   (y   or   n)   '): 
    assert isinstance(message, text_type) 
   app = create_confirm_application(message) 
   return run_application(app)"," 'Confirm a yes/no question. 
 The default message is \'Confirm   (y   or   n)\'. 
 :param message: The message to ask the user. 
 :type message: str 
 :return: A boolean value. 
 :rtype: bool'",'Display a confirmation prompt.'
"def dictfind(dictionary, element): 
    for (key, value) in dictionary.iteritems(): 
      if (element is value): 
         return key", 'Finds the value of the element in the dictionary.',"'Returns a key whose value in `dictionary` is `element` 
 or, if none exists, None. 
 >>> d = {1:2, 3:4} 
 >>> dictfind(d, 4) 
 3 
 >>> dictfind(d, 5)'"
"def main_loop(options, modules, sender, tags): 
    next_heartbeat = int((time.time() + 600)) 
   while ALIVE: 
      populate_collectors(options.cdir) 
      reload_changed_config_modules(modules, options, sender, tags) 
      reap_children() 
      check_children(options) 
      spawn_children() 
      time.sleep(15) 
      now = int(time.time()) 
      if (now >= next_heartbeat): 
         LOG.info(('Heartbeat   (%d   collectors   running)' % sum((1 for col in all_living_collectors())))) 
         next_heartbeat = (now + 600)", 'Main loop of the collectd daemon.','The main loop of the program that runs when we\'re not in stdin mode.'
"def uninstall(): 
    global _task 
   _task.stop() 
   _task = None", 'Uninstall the task.',"'Remove the root Tk widget from the reactor. 
 Call this before destroy()ing the root widget.'"
"def _compute_mi(x, y, x_discrete, y_discrete, n_neighbors=3): 
    if (x_discrete and y_discrete): 
      return mutual_info_score(x, y) 
   elif (x_discrete and (not y_discrete)): 
      return _compute_mi_cd(y, x, n_neighbors) 
   elif ((not x_discrete) and y_discrete): 
      return _compute_mi_cd(x, y, n_neighbors) 
   else: 
      return _compute_mi_cc(x, y, n_neighbors)"," 'Compute mutual information between two variables. 
 Parameters 
 x : ndarray 
 The first variable. 
 y : ndarray 
 The second variable. 
 x_discrete : bool 
 If True, x is discrete. 
 y_discrete : bool 
 If True, y is discrete. 
 n_neighbors : int 
 The number of neighbors to consider for computing the mutual information. 
 Returns 
 mi : float 
 The mutual information between x and y. 
 Examples 
 >>> from sklearn.feature_extraction.text import CountVectorizer 
 >>> from sklearn.feature_extraction.text import TfidfTransformer 
 >>> from sklearn.feature_extraction.text import TfidfVectorizer 
 >>> from sklearn.feature_extraction.text import TfidfModel 
 >>> from sklearn.metrics import mutual_info_score 
 >>> x = [[0, 1, 1], [1, 0, 1], [1, 1, 0]] 
 >>> y = [[0, ","'Compute mutual information between two variables. 
 This is a simple wrapper which selects a proper function to call based on 
 whether `x` and `y` are discrete or not.'"
"def _check_bem_size(surfs): 
    if (surfs[0]['np'] > 10000): 
      msg = ('The   bem   surface   has   %s   data   points.   5120   (ico   grade=4)   should   be   enough.' % surfs[0]['np']) 
      if (len(surfs) == 3): 
         msg += '   Dense   3-layer   bems   may   not   save   properly.' 
      warn(msg)", 'Check if the number of data points in the surface is too high.','Helper for checking bem surface sizes.'
"def url_name_for_block(block): 
    return block.location.name", 'Returns the name of the block.',"'Given a block, returns the block\'s URL name. 
 Arguments: 
 block (XModuleMixin|CourseOverview|BlockStructureBlockData): 
 Block that is being accessed'"
"def storify(f, *requireds, **defaults): 
    stor = Storage() 
   for k in (requireds + tuple(f.keys())): 
      v = f[k] 
      if isinstance(v, list): 
         v = v[(-1)] 
      if hasattr(v, 'value'): 
         v = v.value 
      setattr(stor, k, v) 
   for (k, v) in defaults.iteritems(): 
      result = v 
      if hasattr(stor, k): 
         result = stor[k] 
      if ((v == ()) and (not isinstance(result, tuple))): 
         result = (result,) 
      setattr(stor, k, result) 
   return stor"," 'Returns a new Storage instance with the values of the given arguments 
 set. 
 :param f: The function to call to set the values. 
 :param requireds: The keys to set. 
 :param defaults: The values to set. 
 :return: The new Storage instance. 
 :rtype: :class:`django.core.cache.backends.base.BaseCache` 
 :raises TypeError: If the given arguments are not of the expected type. 
 :raises ValueError: If the given arguments are of the wrong type. 
 :raises KeyError: If the given arguments are not of the expected type. 
 :raises AttributeError: If the given arguments are not of the expected type. 
 :raises TypeError: If the given arguments are not of the expected type. 
 :raises ValueError: If the given arguments are of the wrong type. 
 :raises KeyError: If the given arguments are not of the expected type. 
 :raises AttributeError: If the given arguments are not of the expected type. 
 :raises TypeError: If the given arguments are not of the expected type. ","'Creates a `storage` object from dictionary d, raising `KeyError` if 
 d doesn\'t have all of the keys in `requireds` and using the default 
 values for keys found in `defaults`. 
 For example, `storify({\'a\':1, \'c\':3}, b=2, c=0)` will return the equivalent of 
 `storage({\'a\':1, \'b\':2, \'c\':3})`.'"
"def login(session, *args, **kwargs): 
    if ((not session.logged_in) and ('name' in kwargs) and ('password' in kwargs)): 
      from evennia.commands.default.unloggedin import create_normal_player 
      player = create_normal_player(session, kwargs['name'], kwargs['password']) 
      if player: 
         session.sessionhandler.login(session, player)"," 'Login a player. 
 :param session: The session to login. 
 :param args: 
 :param kwargs: 
 :return: 
 :rtype: 
 :raises: 
 :version: 0.1'","'Peform a login. This only works if session is currently not logged 
 in. This will also automatically throttle too quick attempts. 
 Kwargs: 
 name (str): Player name 
 password (str): Plain-text password'"
"def double_coset_can_rep(dummies, sym, b_S, sgens, S_transversals, g): 
    size = g.size 
   g = g.array_form 
   num_dummies = (size - 2) 
   indices = list(range(num_dummies)) 
   all_metrics_with_sym = all([(_ is not None) for _ in sym]) 
   num_types = len(sym) 
   dumx = dummies[:] 
   dumx_flat = [] 
   for dx in dumx: 
      dumx_flat.extend(dx) 
   b_S = b_S[:] 
   sgensx = [h._array_form for h in sgens] 
   if b_S: 
      S_transversals = transversal2coset(size, b_S, S_transversals) 
   dsgsx = [] 
   for i in range(num_types): 
      dsgsx.extend(dummy_sgs(dumx[i], sym[i], num_dummies)) 
   ginv = _af_invert(g) 
   idn = list(range(size)) 
   TAB = [(idn, idn, g)] 
   for i in range((size - 2)): 
      b = i 
      testb = ((b in b_S) and sgensx) 
      if testb: 
         sgensx1 = [_af_new(_) for _ in sgensx] 
         deltab = _orbit(size, sgensx1, b) 
      else: 
         deltab = {b} 
      if all_metrics_with_sym: 
         md = _min_dummies(dumx, sym, indices) 
      else: 
         md = [min(_orbit(size, [_af_new(ddx) for ddx in dsgsx], ii)) for ii in range((size - 2))] 
      p_i = min([min([md[h[x]] for x in deltab]) for (s, d, h) in TAB]) 
      dsgsx1 = [_af_new(_) for _ in dsgsx] 
      Dxtrav = (_orbit_transversal(size, dsgsx1, p_i, False, af=True) if dsgsx else None) 
      if Dxtrav: 
         Dxtrav = [_af_invert(x) for x in Dxtrav] 
      for ii in range(num_types): 
         if (p_i in dumx[ii]): 
            if (sym[ii] is not None): 
               deltap = dumx[ii] 
            else: 
               p_i_index = (dumx[ii].index(p_i) % 2) 
               deltap = dumx[ii][p_i_index::2] 
            break 
      else: 
         deltap = [p_i] 
      TAB1 = [] 
      nTAB = len(TAB) 
      while TAB: 
         (s, d, h) = TAB.pop() 
         if (min([md[h[x]] for x in deltab]) != p_i): 
            continue 
         deltab1 = [x for x in deltab if (md[h[x]] == p_i)] 
         dg = _af_rmul(d, g) 
         dginv = _af_invert(dg) 
         sdeltab = [s[x] for x in deltab1] 
         gdeltap = [dginv[x] for x in deltap] 
         NEXT = [x for x in sdeltab if (x in gdeltap)] 
         for j in NEXT: 
            if testb: 
               s1 = _trace_S(s, j, b, S_transversals) 
               if (not s1): 
                  continue 
               else: 
                  s1 = [s[ix] for ix in s1] 
            else: 
               s1 = s 
            if Dxtrav: 
               d1 = _trace_D(dg[j], p_i, Dxtrav) 
               if (not d1): 
                  continue 
            else: 
               if (p_i != dg[j]): 
                  continue 
               d1 = idn 
            assert (d1[dg[j]] == p_i) 
            d1 = [d1[ix] for ix in d] 
            h1 = [d1[g[ix]] for ix in s1] 
            TAB1.append((s1, d1, h1)) 
      TAB1.sort(key=(lambda x: x[(-1)])) 
      nTAB1 = len(TAB1) 
      prev = ([0] * size) 
      while TAB1: 
         (s, d, h) = TAB1.pop() 
         if (h[:(-2)] == prev[:(-2)]): 
            if (h[(-1)] != prev[(-1)]): 
               return 0 
         else: 
            TAB.append((s, d, h)) 
         prev = h 
      sgensx = [h for h in sgensx if (h[b] == b)] 
      if (b in b_S): 
         b_S.remove(b) 
      _dumx_remove(dumx, dumx_flat, p_i) 
      dsgsx = [] 
      for i in range(num_types): 
         dsgsx.extend(dummy_sgs(dumx[i], sym[i], num_dummies)) 
   return TAB[0][(-1)]"," 'Returns the representative element of the double coset. 
 Parameters 
 dummies : array 
 The list of dummy indices. 
 sym : array 
 The list of symbols. 
 b_S : array 
 The list of basis elements of the subgroup. 
 sgens : array 
 The list of s-generators. 
 S_transversals : array 
 The list of transversals of the subgroup. 
 g : array 
 The list of generators of the group. 
 Returns 
 TAB : list 
 The list of triplets (s, d, h) where s is the set of generators of the 
 subgroup, d is the set of dummy indices of the coset, and h is the 
 representative element of the coset. 
 Examples 
 >>> from sympy.combinatorics.cosets import double_coset_can_rep 
 >>> from sympy.combinatorics.generators import s_generators 
 >>> from sympy.combinatorics.generators import s_transversals 
 >>> from sympy.combinatorics.generators","'Butler-Portugal algorithm for tensor canonicalization with dummy indices 
 dummies 
 list of lists of dummy indices, 
 one list for each type of index; 
 the dummy indices are put in order contravariant, covariant 
 [d0, -d0, d1, -d1, ...]. 
 sym 
 list of the symmetries of the index metric for each type. 
 possible symmetries of the metrics 
 * 0     symmetric 
 * 1     antisymmetric 
 * None  no symmetry 
 b_S 
 base of a minimal slot symmetry BSGS. 
 sgens 
 generators of the slot symmetry BSGS. 
 S_transversals 
 transversals for the slot BSGS. 
 g 
 permutation representing the tensor. 
 Return 0 if the tensor is zero, else return the array form of 
 the permutation representing the canonical form of the tensor. 
 A tensor with dummy indices can be represented in a number 
 of equivalent ways which typically grows exponentially with 
 the number of indices. To be able to establish if two tensors 
 with many indices are equal becomes computationally very slow 
 in absence of an efficient algorithm. 
 The Butler-Portugal algorithm [3] is an efficient algorithm to 
 put tensors in canonical form, solving the above problem. 
 Portugal observed that a tensor can be represented by a permutation, 
 and that the class of tensors equivalent to it under slot and dummy 
 symmetries is equivalent to the double coset `D*g*S` 
 (Note: in this documentation we use the conventions for multiplication 
 of permutations p, q with (p*q)(i) = p[q[i]] which is opposite 
 to the one used in the Permutation class) 
 Using the algorithm by Butler to find a representative of the 
 double coset one can find a canonical form for the tensor. 
 To see this correspondence, 
 let `g` be a permutation in array form; a tensor with indices `ind` 
 (the indices including both the contravariant and the covariant ones) 
 can be written as 
 `t = T(ind[g[0],..., ind[g[n-1]])`, 
 where `n= len(ind)`; 
 `g` has size `n + 2`, the last two indices for the sign of the tensor 
 (trick introduced in [4]). 
 A slot symmetry transformation `s` is a permutation acting on the slots 
 `t -> T(ind[(g*s)[0]],..., ind[(g*s)[n-1]])` 
 A dummy symmetry transformation acts on `ind` 
 `t -> T(ind[(d*g)[0]],..., ind[(d*g)[n-1]])` 
 Being interested only in the transformations of the tensor under 
 these symmetries, one can represent the tensor by `g`, which transforms 
 as 
 `g -> d*g*s`, so it belongs to the coset `D*g*S`. 
 Let us explain the conventions by an example. 
 Given a tensor `T^{d3 d2 d1}{}_{d1 d2 d3}` with the slot symmetries 
 `T^{a0 a1 a2 a3 a4 a5} = -T^{a2 a1 a0 a3 a4 a5}` 
 `T^{a0 a1 a2 a3 a4 a5} = -T^{a4 a1 a2 a3 a0 a5}` 
 and symmetric metric, find the tensor equivalent to it which 
 is the lowest under the ordering of indices: 
 lexicographic ordering `d1, d2, d3` then and contravariant index 
 before covariant index; that is the canonical form of the tensor. 
 The canonical form is `-T^{d1 d2 d3}{}_{d1 d2 d3}` 
 obtained using `T^{a0 a1 a2 a3 a4 a5} = -T^{a2 a1 a0 a3 a4 a5}`. 
 To convert this problem in the input for this function, 
 use the following labelling of the index names 
 (- for covariant for short) `d1, -d1, d2, -d2, d3, -d3` 
 `T^{d3 d2 d1}{}_{d1 d2 d3}` corresponds to `g = [4, 2, 0, 1, 3, 5, 6, 7]` 
 where the last two indices are for the sign 
 `sgens = [Permutation(0, 2)(6, 7), Permutation(0, 4)(6, 7)]` 
 sgens[0] is the slot symmetry `-(0, 2)` 
 `T^{a0 a1 a2 a3 a4 a5} = -T^{a2 a1 a0 a3 a4 a5}` 
 sgens[1] is the slot symmetry `-(0, 4)` 
 `T^{a0 a1 a2 a3 a4 a5} = -T^{a4 a1 a2 a3 a0 a5}` 
 The dummy symmetry group D is generated by the strong base generators 
 `[(0, 1), (2, 3), (4, 5), (0, 1)(2, 3),(2, 3)(4, 5)]` 
 The dummy symmetry acts from the left 
 `d = [1, 0, 2, 3, 4, 5, 6, 7]`  exchange `d1 -> -d1` 
 `T^{d3 d2 d1}{}_{d1 d2 d3} == T^{d3 d2}{}_{d1}{}^{d1}{}_{d2 d3}` 
 `g=[4, 2, 0, 1, 3, 5, 6, 7]  -> [4, 2, 1, 0, 3, 5, 6, 7] = _af_rmul(d, g)` 
 which differs from `_af_rmul(g, d)`. 
 The slot symmetry acts from the right 
 `s = [2, 1, 0, 3, 4, 5, 7, 6]`  exchanges slots 0 and 2 and changes sign 
 `T^{d3 d2 d1}{}_{d1 d2 d3} == -T^{d1 d2 d3}{}_{d1 d2 d3}` 
 `g=[4,2,0,1,3,5,6,7]  -> [0, 2, 4, 1, 3, 5, 7, 6] = _af_rmul(g, s)` 
 Example in which the tensor is zero, same slot symmetries as above: 
 `T^{d3}{}_{d1,d2}{}^{d1}{}_{d3}{}^{d2}` 
 `= -T^{d3}{}_{d1,d3}{}^{d1}{}_{d2}{}^{d2}`   under slot symmetry `-(2,4)`; 
 `= T_{d3 d1}{}^{d3}{}^{d1}{}_{d2}{}^{d2}`    under slot symmetry `-(0,2)`; 
 `= T^{d3}{}_{d1 d3}{}^{d1}{}_{d2}{}^{d2}`    symmetric metric; 
 `= 0`  since two of these lines have tensors differ only for the sign. 
 The double coset D*g*S consists of permutations `h = d*g*s` corresponding 
 to equivalent tensors; if there are two `h` which are the same apart 
 from the sign, return zero; otherwise 
 choose as representative the tensor with indices 
 ordered lexicographically according to `[d1, -d1, d2, -d2, d3, -d3]` 
 that is `rep = min(D*g*S) = min([d*g*s for d in D for s in S])` 
 The indices are fixed one by one; first choose the lowest index 
 for slot 0, then the lowest remaining index for slot 1, etc. 
 Doing this one obtains a chain of stabilizers 
 `S -> S_{b0} -> S_{b0,b1} -> ...` and 
 `D -> D_{p0} -> D_{p0,p1} -> ...` 
 where `[b0, b1, ...] = range(b)` is a base of the symmetric group; 
 the strong base `b_S` of S is an ordered sublist of it; 
 therefore it is sufficient to compute once the 
 strong base generators of S using the Schreier-Sims algorithm; 
 the stabilizers of the strong base generators are the 
 strong base generators of the stabilizer subgroup. 
 `dbase = [p0, p1, ...]` is not in general in lexicographic order, 
 so that one must recompute the strong base generators each time; 
 however this is trivial, there is no need to use the Schreier-Sims 
 algorithm for D. 
 The algorithm keeps a TAB of elements `(s_i, d_i, h_i)` 
 where `h_i = d_i*g*s_i` satisfying `h_i[j] = p_j` for `0 <= j < i` 
 starting from `s_0 = id, d_0 = id, h_0 = g`. 
 The equations `h_0[0] = p_0, h_1[1] = p_1,...` are solved in this order, 
 choosing each time the lowest possible value of p_i 
 For `j < i` 
 `d_i*g*s_i*S_{b_0,...,b_{i-1}}*b_j = D_{p_0,...,p_{i-1}}*p_j` 
 so that for dx in `D_{p_0,...,p_{i-1}}` and sx in 
 `S_{base[0],...,base[i-1]}` one has `dx*d_i*g*s_i*sx*b_j = p_j` 
 Search for dx, sx such that this equation holds for `j = i`; 
 it can be written as `s_i*sx*b_j = J, dx*d_i*g*J = p_j` 
 `sx*b_j = s_i**-1*J; sx = trace(s_i**-1, S_{b_0,...,b_{i-1}})` 
 `dx**-1*p_j = d_i*g*J; dx = trace(d_i*g*J, D_{p_0,...,p_{i-1}})` 
 `s_{i+1} = s_i*trace(s_i**-1*J, S_{b_0,...,b_{i-1}})` 
 `d_{i+1} = trace(d_i*g*J, D_{p_0,...,p_{i-1}})**-1*d_i` 
 `h_{i+1}*b_i = d_{i+1}*g*s_{i+1}*b_i = p_i` 
 `h_n*b_j = p_j` for all j, so that `h_n` is the solution. 
 Add the found `(s, d, h)` to TAB1. 
 At the end of the iteration sort TAB1 with respect to the `h`; 
 if there are two consecutive `h` in TAB1 which differ only for the 
 sign, the tensor is zero, so return 0; 
 if there are two consecutive `h` which are equal, keep only one. 
 Then stabilize the slot generators under `i` and the dummy generators 
 under `p_i`. 
 Assign `TAB = TAB1` at the end of the iteration step. 
 At the end `TAB` contains a unique `(s, d, h)`, since all the slots 
 of the tensor `h` have been fixed to have the minimum value according 
 to the symmetries. The algorithm returns `h`. 
 It is important that the slot BSGS has lexicographic minimal base, 
 otherwise there is an `i` which does not belong to the slot base 
 for which `p_i` is fixed by the dummy symmetry only, while `i` 
 is not invariant from the slot stabilizer, so `p_i` is not in 
 general the minimal value. 
 This algorithm differs slightly from the original algorithm [3]: 
 the canonical form is minimal lexicographically, and 
 the BSGS has minimal base under lexicographic order. 
 Equal tensors `h` are eliminated from TAB. 
 Examples 
 >>> from sympy.combinatorics.permutations import Permutation 
 >>> from sympy.combinatorics.perm_groups import PermutationGroup 
 >>> from sympy.combinatorics.tensor_can import double_coset_can_rep, get_transversals 
 >>> gens = [Permutation(x) for x in [[2, 1, 0, 3, 4, 5, 7, 6], [4, 1, 2, 3, 0, 5, 7, 6]]] 
 >>> base = [0, 2] 
 >>> g = Permutation([4, 2, 0, 1, 3, 5, 6, 7]) 
 >>> transversals = get_transversals(base, gens) 
 >>> double_coset_can_rep([list(range(6))], [0], base, gens, transversals, g) 
 [0, 1, 2, 3, 4, 5, 7, 6] 
 >>> g = Permutation([4, 1, 3, 0, 5, 2, 6, 7]) 
 >>> double_coset_can_rep([list(range(6))], [0], base, gens, transversals, g) 
 0'"
"def setup(app, status, warning): 
    logger = logging.getLogger() 
   logger.setLevel(logging.NOTSET) 
   for handler in logger.handlers[:]: 
      logger.removeHandler(handler) 
   info_handler = NewLineStreamHandler(SafeEncodingWriter(status)) 
   info_handler.addFilter(InfoFilter()) 
   info_handler.setLevel(VERBOSITY_MAP[app.verbosity]) 
   info_handler.setFormatter(ColorizeFormatter()) 
   warning_handler = WarningStreamHandler(SafeEncodingWriter(warning)) 
   warning_handler.addFilter(WarningSuppressor(app)) 
   warning_handler.addFilter(WarningIsErrorFilter(app)) 
   warning_handler.addFilter(WarningLogRecordTranslator(app)) 
   warning_handler.setLevel(logging.WARNING) 
   warning_handler.setFormatter(ColorizeFormatter()) 
   logger.addHandler(info_handler) 
   logger.addHandler(warning_handler)"," 'Sets up the logging system. 
 :param app: The application to be tested. 
 :param status: The status to be logged. 
 :param warning: The warning to be logged. 
 :type app: unittest.TestCase 
 :type status: logging.StreamHandler 
 :type warning: logging.StreamHandler 
 :rtype: None'",'Setup root logger for Sphinx'
"@task(queue='web') 
 def move_files(version_pk, hostname, html=False, localmedia=False, search=False, pdf=False, epub=False): 
    version = Version.objects.get(pk=version_pk) 
   if html: 
      from_path = version.project.artifact_path(version=version.slug, type_=version.project.documentation_type) 
      target = version.project.rtd_build_path(version.slug) 
      Syncer.copy(from_path, target, host=hostname) 
   if ('sphinx' in version.project.documentation_type): 
      if localmedia: 
         from_path = version.project.artifact_path(version=version.slug, type_='sphinx_localmedia') 
         to_path = version.project.get_production_media_path(type_='htmlzip', version_slug=version.slug, include_file=False) 
         Syncer.copy(from_path, to_path, host=hostname) 
      if search: 
         from_path = version.project.artifact_path(version=version.slug, type_='sphinx_search') 
         to_path = version.project.get_production_media_path(type_='json', version_slug=version.slug, include_file=False) 
         Syncer.copy(from_path, to_path, host=hostname) 
      if pdf: 
         from_path = version.project.artifact_path(version=version.slug, type_='sphinx_pdf') 
         to_path = version.project.get_production_media_path(type_='pdf', version_slug=version.slug, include_file=False) 
         Syncer.copy(from_path, to_path, host=hostname) 
      if epub: 
         from_path = version.project.artifact_path(version=version.slug, type_='sphinx_epub') 
         to_path = version.project.get_production_media_path(type_='epub', version_slug=version.slug, include_file=False) 
         Syncer.copy(from_path, to_path, host=hostname) 
   if ('mkdocs' in version.project.documentation_type): 
      if search: 
         from_path = version.project.artifact_path(version=version.slug, type_='mkdocs_json') 
         to_path = version.project.get_production_media_path(type_='json', version_slug=version.slug, include_file=False) 
         Syncer.copy(from_path, to_path, host=hostname)", 'Copies files from the version\'s artifacts to the production media path.',"'Task to move built documentation to web servers 
 :param version_pk: Version id to sync files for 
 :param hostname: Hostname to sync to 
 :param html: Sync HTML 
 :type html: bool 
 :param localmedia: Sync local media files 
 :type localmedia: bool 
 :param search: Sync search files 
 :type search: bool 
 :param pdf: Sync PDF files 
 :type pdf: bool 
 :param epub: Sync ePub files 
 :type epub: bool'"
"def generate_reset_password_token(user): 
    password_hash = (md5(user.password) if user.password else None) 
   data = [str(user.id), password_hash] 
   return _security.reset_serializer.dumps(data)", 'Generate a reset password token for the given user.',"'Generates a unique reset password token for the specified user. 
 :param user: The user to work with'"
"def rrggbb_to_triplet(color): 
    rgbtuple = _namedict.get(color) 
   if (rgbtuple is None): 
      if (color[0] != '#'): 
         raise BadColor(color) 
      red = color[1:3] 
      green = color[3:5] 
      blue = color[5:7] 
      rgbtuple = (int(red, 16), int(green, 16), int(blue, 16)) 
      _namedict[color] = rgbtuple 
   return rgbtuple"," 'Convert a 4-tuple of red, green, blue, and alpha to a 3-tuple of 
 red, green, and blue. 
 >>> rrggbb_to_triplet(""#ff0000"") 
 (255, 0, 0)'","'Converts a #rrggbb color to the tuple (red, green, blue).'"
"@commands(u'suggest') 
 def suggest(bot, trigger): 
    if (not trigger.group(2)): 
      return bot.reply(u'No   query   term.') 
   query = trigger.group(2) 
   uri = u'http://websitedev.de/temp-bin/suggest.pl?q=' 
   answer = web.get((uri + query.replace(u'+', u'%2B'))) 
   if answer: 
      bot.say(answer) 
   else: 
      bot.reply(u'Sorry,   no   result.')"," 'Suggests a term. 
 Example: 
 .. code-block:: 
 @bot.message(u""suggest <query>"") 
 def suggest(trigger, match): 
 if match.group(2): 
 query = match.group(2) 
 uri = u""http://websitedev.de/temp-bin/suggest.pl?q="" 
 answer = web.get((uri + query.replace(u'+', u'%2B'))) 
 if answer: 
 bot.say(answer) 
 else: 
 bot.reply(u""Sorry,   no   result."")'",'Suggest terms starting with given input'
"def assign_role_for_exploration(committer_id, exploration_id, assignee_id, new_role): 
    _assign_role(committer_id, assignee_id, new_role, exploration_id, feconf.ACTIVITY_TYPE_EXPLORATION) 
   if (new_role in [ROLE_OWNER, ROLE_EDITOR]): 
      subscription_services.subscribe_to_exploration(assignee_id, exploration_id)"," 'Assigns a role to an exploration for a user. 
 :param committer_id: The committer ID of the user who is making the change. 
 :param exploration_id: The ID of the exploration. 
 :param assignee_id: The ID of the user to whom the role is being assigned. 
 :param new_role: The new role for the user. 
 :raises PermissionDenied: if the user is not allowed to assign a role to the 
 exploration.'","'Assign `assignee_id` to the given role and subscribes the assignee 
 to future exploration updates. 
 The caller should ensure that assignee_id corresponds to a valid user in 
 the system. 
 Args: 
 - committer_id: str. The user_id of the user who is performing the action. 
 - exploration_id: str. The exploration id. 
 - assignee_id: str. The user_id of the user whose role is being changed. 
 - new_role: str. The name of the new role: either \'owner\', \'editor\' or 
 \'viewer\'.'"
"def _queue_management_worker(executor_reference, processes, pending_work_items, work_ids_queue, call_queue, result_queue): 
    executor = None 
   def shutting_down(): 
      return (_shutdown or (executor is None) or executor._shutdown_thread) 
   def shutdown_worker(): 
      nb_children_alive = sum((p.is_alive() for p in processes.values())) 
      for i in range(0, nb_children_alive): 
         call_queue.put_nowait(None) 
      call_queue.close() 
      for p in processes.values(): 
         p.join() 
   reader = result_queue._reader 
   while True: 
      _add_call_item_to_queue(pending_work_items, work_ids_queue, call_queue) 
      sentinels = [p.sentinel for p in processes.values()] 
      assert sentinels 
      ready = wait(([reader] + sentinels)) 
      if (reader in ready): 
         result_item = reader.recv() 
      else: 
         executor = executor_reference() 
         if (executor is not None): 
            executor._broken = True 
            executor._shutdown_thread = True 
            executor = None 
         for (work_id, work_item) in pending_work_items.items(): 
            work_item.future.set_exception(BrokenProcessPool('A   process   in   the   process   pool   was   terminated   abruptly   while   the   future   was   running   or   pending.')) 
            del work_item 
         pending_work_items.clear() 
         for p in processes.values(): 
            p.terminate() 
         shutdown_worker() 
         return 
      if isinstance(result_item, int): 
         assert shutting_down() 
         p = processes.pop(result_item) 
         p.join() 
         if (not processes): 
            shutdown_worker() 
            return 
      elif (result_item is not None): 
         work_item = pending_work_items.pop(result_item.work_id, None) 
         if (work_item is not None): 
            if result_item.exception: 
               work_item.future.set_exception(result_item.exception) 
            else: 
               work_item.future.set_result(result_item.result) 
            del work_item 
      executor = executor_reference() 
      if shutting_down(): 
         try: 
            if (not pending_work_items): 
               shutdown_worker() 
               return 
         except Full: 
            pass 
      executor = None"," 'The worker that processes work items from the queue. 
 :param executor_reference: The reference to the executor to use. 
 :param processes: The processes to use. 
 :param pending_work_items: The pending work items. 
 :param work_ids_queue: The work_ids queue. 
 :param call_queue: The call queue. 
 :param result_queue: The result queue.'","'Manages the communication between this process and the worker processes. 
 This function is run in a local thread. 
 Args: 
 executor_reference: A weakref.ref to the ProcessPoolExecutor that owns 
 this thread. Used to determine if the ProcessPoolExecutor has been 
 garbage collected and that this function can exit. 
 process: A list of the multiprocessing.Process instances used as 
 workers. 
 pending_work_items: A dict mapping work ids to _WorkItems e.g. 
 {5: <_WorkItem...>, 6: <_WorkItem...>, ...} 
 work_ids_queue: A queue.Queue of work ids e.g. Queue([5, 6, ...]). 
 call_queue: A multiprocessing.Queue that will be filled with _CallItems 
 derived from _WorkItems for processing by the process workers. 
 result_queue: A multiprocessing.Queue of _ResultItems generated by the 
 process workers.'"
"def timefunc(correct, s, func, *args, **kwargs): 
    print(s.ljust(20), end='   ') 
   res = func(*args, **kwargs) 
   if (correct is not None): 
      assert np.allclose(res, correct), (res, correct) 
   print('{:>5.0f}   ms'.format((min(repeat((lambda : func(*args, **kwargs)), number=5, repeat=2)) * 1000))) 
   return res"," 'Run a function and time it. 
 Parameters 
 correct : array 
 The correct answer. 
 s : string 
 The string to print. 
 func : function 
 The function to time. 
 *args 
 Additional arguments to pass to func. 
 **kwargs 
 Additional keyword arguments to pass to func. 
 Returns 
 The result of the function.'",'Benchmark *func* and print out its runtime.'
"def demo(): 
    from nltk import Tree, ProbabilisticTree 
   s = u'(S   (NP   (DT   the)   (NN   cat))   (VP   (VBD   ate)   (NP   (DT   a)   (NN   cookie))))' 
   t = Tree.fromstring(s) 
   print(u'Convert   bracketed   string   into   tree:') 
   print(t) 
   print(t.__repr__()) 
   print(u'Display   tree   properties:') 
   print(t.label()) 
   print(t[0]) 
   print(t[1]) 
   print(t.height()) 
   print(t.leaves()) 
   print(t[1]) 
   print(t[(1, 1)]) 
   print(t[(1, 1, 0)]) 
   the_cat = t[0] 
   the_cat.insert(1, Tree.fromstring(u'(JJ   big)')) 
   print(u'Tree   modification:') 
   print(t) 
   t[(1, 1, 1)] = Tree.fromstring(u'(NN   cake)') 
   print(t) 
   print() 
   print(u'Collapse   unary:') 
   t.collapse_unary() 
   print(t) 
   print(u'Chomsky   normal   form:') 
   t.chomsky_normal_form() 
   print(t) 
   print() 
   pt = ProbabilisticTree(u'x', [u'y', u'z'], prob=0.5) 
   print(u'Probabilistic   Tree:') 
   print(pt) 
   print() 
   t = Tree.fromstring(t.pformat()) 
   print(u'Convert   tree   to   bracketed   string   and   back   again:') 
   print(t) 
   print() 
   print(u'LaTeX   output:') 
   print(t.pformat_latex_qtree()) 
   print() 
   print(u'Production   output:') 
   print(t.productions()) 
   print() 
   t.set_label((u'test', 3)) 
   print(t)"," 'Demo of the Tree class and its subclasses. 
 This demo is based on the example in the NLTK documentation 
 (http://www.nltk.org/doc/nltk/tree.html)'","'A demonstration showing how Trees and Trees can be 
 used.  This demonstration creates a Tree, and loads a 
 Tree from the Treebank corpus, 
 and shows the results of calling several of their methods.'"
"@register_specialize 
 @register_canonicalize 
 @gof.local_optimizer([T.mul]) 
 def local_mul_switch_sink(node): 
    if (node.op != T.mul): 
      return False 
   for (idx, i) in enumerate(node.inputs): 
      if (i.owner and (i.owner.op == T.switch)): 
         switch = i.owner 
         try: 
            if (get_scalar_constant_value(switch.inputs[1], only_process_constants=True) == 0.0): 
               listmul = (node.inputs[:idx] + node.inputs[(idx + 1):]) 
               fmul = T.mul(*(listmul + [switch.inputs[2]])) 
               copy_stack_trace(node.outputs, fmul) 
               fct = [T.switch(switch.inputs[0], 0, fmul)] 
               fct[0].tag.values_eq_approx = values_eq_approx_remove_nan 
               copy_stack_trace((node.outputs + switch.outputs), fct) 
               return fct 
         except NotScalarConstantError: 
            pass 
         try: 
            if (get_scalar_constant_value(switch.inputs[2], only_process_constants=True) == 0.0): 
               listmul = (node.inputs[:idx] + node.inputs[(idx + 1):]) 
               fmul = T.mul(*(listmul + [switch.inputs[1]])) 
               copy_stack_trace(node.outputs, fmul) 
               fct = [T.switch(switch.inputs[0], fmul, 0)] 
               fct[0].tag.values_eq_approx = values_eq_approx_remove_nan 
               copy_stack_trace((node.outputs + switch.outputs), fct) 
               return fct 
         except NotScalarConstantError: 
            pass 
   return False"," 'Switches the mul operation to a switch operation when the switch input 
 is a constant. 
 Parameters 
 node : Theano node 
 Examples 
 >>> from theano import gof 
 >>> x = T.matrix(""x"") 
 >>> y = T.matrix(""y"") 
 >>> z = T.matrix(""z"") 
 >>> f = T.dot(x, y) 
 >>> g = T.dot(x, z) 
 >>> gof.local_mul_switch_sink(f) 
 [f, [g, 0]]'","'This optimization makes the folowing changes in the graph: 
 T.mul(A,T.switch(cond,0,iff),B) -->  T.switch(cond,0,T.mul(A,B,iff)) 
 T.mul(A,T.switch(cond,ift,0),B) -->  T.switch(cond,T.mul(A,B,ift),0) 
 A and B being several (or none) symbolic variables. 
 This is useful because A and B may not be numerically stable and give 
 NaN or inf values for cases where the switch returns 0. 
 With this optimization T.grad(T.switch(...)) has the right behavior. 
 Examples 
 x -> f(x) 
 x -> g(x) 
 y = T.switch(cond,f(x),g(x)) 
 **without the optimization 
 T.grad(y,x) -> grad(f(x),x) * grad(y,f(x)) +  grad(g(x),x) * grad(y,g(x)) 
 **with the optimization 
 T.grad(y,x) -> switch(cond,grad(f(x),x), 0) + switch(cond,0,grad(g(x),x)) 
 This will be particularly useful for the lazyif because we skip 
 an entire part of the graph.'"
"def po_due_followups(): 
    query = ((FS('followup_date') <= datetime.datetime.utcnow().date()) & (FS('completed') != True)) 
   resource = current.s3db.resource('po_household_followup', filter=query) 
   return resource.count()"," 'Number of followups that are due 
 :return: number of followups that are due 
 :rtype: int'",'Number of due follow-ups'
"def lstrips(text, remove): 
    return _strips('l', text, remove)"," 'Strip leading whitespace from a list of strings. 
 >>> lstrips([], []) 
 [] 
 >>> lstrips([], [1]) 
 [1] 
 >>> lstrips([1, 2, 3], []) 
 [1, 2, 3] 
 >>> lstrips([1, 2, 3], [1]) 
 [2, 3] 
 >>> lstrips([1, 2, 3], [2, 3]) 
 [1] 
 >>> lstrips([1, 2, 3], [1, 3]) 
 [2] 
 >>> lstrips([1, 2, 3], [1, 2]) 
 [3] 
 >>> lstrips([1, 2, 3], [1, 2, 3]) 
 [1, 2, 3] 
 >>> lstrips([1, 2, 3], [1, 2, 3, 4]) 
 [1, 2, 3, 4]'",'removes the string `remove` from the right of `text`'
"def writeOutput(fileName): 
    repository = getNewRepository() 
   repository.fileNameInput.value = fileName 
   settings.startMainLoopFromWindow(repository.execute())"," 'Writes the output to the specified file. 
 :param fileName: The name of the file to write to. 
 :type fileName: str'","'Craft a file, display dialog.'"
"def move(src, dst): 
    real_dst = dst 
   if os.path.isdir(dst): 
      if _samefile(src, dst): 
         os.rename(src, dst) 
         return 
      real_dst = os.path.join(dst, _basename(src)) 
      if os.path.exists(real_dst): 
         raise Error, (""Destination   path   '%s'   already   exists"" % real_dst) 
   try: 
      os.rename(src, real_dst) 
   except OSError: 
      if os.path.isdir(src): 
         if _destinsrc(src, dst): 
            raise Error, (""Cannot   move   a   directory   '%s'   into   itself   '%s'."" % (src, dst)) 
         copytree(src, real_dst, symlinks=True) 
         rmtree(src) 
      else: 
         copy2(src, real_dst) 
         os.unlink(src)"," 'Move src to dst. 
 :param src: Source path 
 :param dst: Destination path'","'Recursively move a file or directory to another location. This is 
 similar to the Unix ""mv"" command. 
 If the destination is a directory or a symlink to a directory, the source 
 is moved inside the directory. The destination path must not already 
 exist. 
 If the destination already exists but is not a directory, it may be 
 overwritten depending on os.rename() semantics. 
 If the destination is on our current filesystem, then rename() is used. 
 Otherwise, src is copied to the destination and then removed. 
 A lot more could be done here...  A look at a mv.c shows a lot of 
 the issues this implementation glosses over.'"
"def iri_to_uri(iri): 
    if (iri is None): 
      return iri 
   return quote(force_bytes(iri), safe=""/#%[]=:;$&()+,!?*@'~"")"," 'Convert an IRI to a URI. 
 :param iri: The IRI to convert. 
 :return: The URI.'","'Convert an Internationalized Resource Identifier (IRI) portion to a URI 
 portion that is suitable for inclusion in a URL. 
 This is the algorithm from section 3.1 of RFC 3987.  However, since we are 
 assuming input is either UTF-8 or unicode already, we can simplify things a 
 little from the full method. 
 Returns an ASCII string containing the encoded result.'"
"def perform_check(prerelease=current_version.is_prerelease): 
    pypi = current_version 
   try: 
      pypi = available_on_pypi(prerelease) 
   except Exception: 
      log.warning('An   issue   occurred   while   checking   PyPI') 
   best = max(pypi, current_version) 
   where = None 
   command = None 
   cache = cache_file() 
   if cache: 
      os.utime(cache, None) 
   if (best == current_version): 
      log.info(('You   have   the   latest   version   of   Pwntools   (%s)' % best)) 
      return 
   command = ['pip', 'install', '-U'] 
   if (best == pypi): 
      where = 'pypi' 
      pypi_package = package_name 
      if best.is_prerelease: 
         pypi_package += ('==%s' % best) 
      command += [pypi_package] 
   command_str = '   '.join(command) 
   log.info((('A   newer   version   of   %s   is   available   on   %s   (%s   -->   %s).\n' % (package_name, where, current_version, best)) + ('Update   with:   $   %s' % command_str))) 
   return command"," 'Perform a check for a new version of Pwntools and return the command to 
 update. 
 :return: a list of commands to update Pwntools 
 :rtype: list'","'Perform the update check, and report to the user. 
 Arguments: 
 prerelease(bool): Whether or not to include pre-release versions. 
 Returns: 
 A list of arguments to the update command. 
 >>> from packaging.version import Version 
 >>> pwnlib.update.current_version = Version(""999.0.0"") 
 >>> print perform_check() 
 None 
 >>> pwnlib.update.current_version = Version(""0.0.0"") 
 >>> perform_check() # doctest: +ELLIPSIS 
 [\'pip\', \'install\', \'-U\', ...] 
 >>> def bail(*a): raise Exception() 
 >>> pypi   = pwnlib.update.available_on_pypi 
 >>> perform_check(prerelease=False) 
 [\'pip\', \'install\', \'-U\', \'pwntools\'] 
 >>> perform_check(prerelease=True)  # doctest: +ELLIPSIS 
 [\'pip\', \'install\', \'-U\', \'pwntools...\']'"
"def virtual_root(resource, request): 
    try: 
      reg = request.registry 
   except AttributeError: 
      reg = get_current_registry() 
   url_adapter = reg.queryMultiAdapter((resource, request), IResourceURL) 
   if (url_adapter is None): 
      url_adapter = ResourceURL(resource, request) 
   (vpath, rpath) = (url_adapter.virtual_path, url_adapter.physical_path) 
   if ((rpath != vpath) and rpath.endswith(vpath)): 
      vroot_path = rpath[:(- len(vpath))] 
      return find_resource(resource, vroot_path) 
   try: 
      return request.root 
   except AttributeError: 
      return find_root(resource)"," 'Returns the root of the resource. 
 :param resource: the resource to get the root for 
 :param request: the request to get the root for 
 :return: the root of the resource 
 :rtype: L{django.core.urlresolvers.Resolver}'","'Provided any :term:`resource` and a :term:`request` object, return 
 the resource object representing the :term:`virtual root` of the 
 current :term:`request`.  Using a virtual root in a 
 :term:`traversal` -based :app:`Pyramid` application permits 
 rooting. For example, the resource at the traversal path ``/cms`` will 
 be found at ``http://example.com/`` instead of rooting it at 
 ``http://example.com/cms/``. 
 If the ``resource`` passed in is a context obtained via 
 :term:`traversal`, and if the ``HTTP_X_VHM_ROOT`` key is in the 
 WSGI environment, the value of this key will be treated as a 
 \'virtual root path\': the :func:`pyramid.traversal.find_resource` 
 API will be used to find the virtual root resource using this path; 
 if the resource is found, it will be returned.  If the 
 ``HTTP_X_VHM_ROOT`` key is not present in the WSGI environment, 
 the physical :term:`root` of the resource tree will be returned instead. 
 Virtual roots are not useful at all in applications that use 
 :term:`URL dispatch`. Contexts obtained via URL dispatch don\'t 
 really support being virtually rooted (each URL dispatch context 
 is both its own physical and virtual root).  However if this API 
 is called with a ``resource`` argument which is a context obtained 
 via URL dispatch, the resource passed in will be returned 
 unconditionally.'"
"def config(conf_path='/etc/rsyncd.conf'): 
    ret = '' 
   try: 
      with salt.utils.fopen(conf_path, 'r') as fp_: 
         for line in fp_: 
            ret += line 
   except IOError as exc: 
      if (exc.errno == errno.ENOENT): 
         raise CommandExecutionError('{0}   does   not   exist'.format(conf_path)) 
      elif (exc.errno == errno.EACCES): 
         raise CommandExecutionError('Unable   to   read   {0},   access   denied'.format(conf_path)) 
      elif (exc.errno == errno.EISDIR): 
         raise CommandExecutionError('Unable   to   read   {0},   path   is   a   directory'.format(conf_path)) 
      else: 
         raise CommandExecutionError('Error   {0}:   {1}'.format(exc.errno, exc.strerror)) 
   else: 
      return ret"," 'Get the contents of the rsyncd.conf file. 
 :return: The contents of the rsyncd.conf file 
 :rtype: str'","'.. versionchanged:: 2016.3.0 
 Return data now contains just the contents of the rsyncd.conf as a 
 string, instead of a dictionary as returned from :py:func:`cmd.run_all 
 <salt.modules.cmdmod.run_all>`. 
 Returns the contents of the rsync config file 
 conf_path : /etc/rsyncd.conf 
 Path to the config file 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' rsync.config'"
"def intTime(scale=1): 
    return int((time.time() * scale))"," 'Returns an integer representing the time in seconds since the epoch. 
 :param scale: The scale factor, used to convert from seconds to the 
 appropriate time unit. 
 :return: The integer representing the time in seconds since the epoch.'",'The time in integer seconds. Pass scale=1000 to get milliseconds.'
"def _make_compound_key(table, key): 
    if (not isinstance(key, (list, tuple))): 
      key = [key] 
   return [table.columns[name] for name in key]", 'Return a list of columns for a compound key.',"'Returns a list of columns from `column_key` for `table` representing 
 potentially a compound key. The `column_key` can be a name of a single 
 column or list of column names.'"
"def DNSServiceResolve(flags=0, interfaceIndex=_NO_DEFAULT, name=_NO_DEFAULT, regtype=_NO_DEFAULT, domain=_NO_DEFAULT, callBack=None): 
    _NO_DEFAULT.check(interfaceIndex) 
   _NO_DEFAULT.check(name) 
   _NO_DEFAULT.check(regtype) 
   _NO_DEFAULT.check(domain) 
   @_DNSServiceResolveReply 
   def _callback(sdRef, flags, interfaceIndex, errorCode, fullname, hosttarget, port, txtLen, txtRecord, context): 
      if (callBack is not None): 
         port = socket.ntohs(port) 
         txtRecord = _length_and_void_p_to_string(txtLen, txtRecord) 
         callBack(sdRef, flags, interfaceIndex, errorCode, fullname.decode(), hosttarget.decode(), port, txtRecord) 
   _global_lock.acquire() 
   try: 
      sdRef = _DNSServiceResolve(flags, interfaceIndex, name, regtype, domain, _callback, None) 
   finally: 
      _global_lock.release() 
   sdRef._add_callback(_callback) 
   return sdRef"," 'Perform a DNS service lookup on a host. 
 This function will perform a DNS service lookup on a host. 
 This function is non-blocking and will return immediately. 
 You must call the DNSServiceResolveReply() function to retrieve the 
 results. 
 @param flags: A bitmask of flags that control the behavior of the lookup. 
 See the \'DNSServiceFlags\' enumeration for valid values. 
 @param interfaceIndex: The interface index of the host to look up. 
 @param name: The name of the host to look up. 
 @param regtype: The type of record to look up. 
 @param domain: The domain to look up. 
 @param callBack: A function to call with the results of the lookup. 
 @return: A DNSServiceRef object that can be used to retrieve the results of the lookup. 
 @raise DomainError: If the host name is invalid or the flags are invalid. 
 @raise DNSServiceException: If the lookup fails for any other reason. 
 @see: \'DNSServiceFlags\' 
 @see: \'DNSServiceResolveRep","'Resolve a service name discovered via DNSServiceBrowse() to a 
 target host name, port number, and txt record. 
 Note: Applications should NOT use DNSServiceResolve() solely for 
 txt record monitoring; use DNSServiceQueryRecord() instead, as it 
 is more efficient for this task. 
 Note: When the desired results have been returned, the client MUST 
 terminate the resolve by closing the returned DNSServiceRef. 
 Note: DNSServiceResolve() behaves correctly for typical services 
 that have a single SRV record and a single TXT record.  To resolve 
 non-standard services with multiple SRV or TXT records, 
 DNSServiceQueryRecord() should be used. 
 flags: 
 Currently ignored, reserved for future use. 
 interfaceIndex: 
 The interface on which to resolve the service.  If this 
 resolve call is as a result of a currently active 
 DNSServiceBrowse() operation, then the interfaceIndex should 
 be the index reported in the browse callback.  If this resolve 
 call is using information previously saved (e.g. in a 
 preference file) for later use, then use 
 kDNSServiceInterfaceIndexAny (0), because the desired service 
 may now be reachable via a different physical interface. 
 name: 
 The name of the service instance to be resolved, as reported 
 to the DNSServiceBrowse() callback. 
 regtype: 
 The type of the service instance to be resolved, as reported 
 to the DNSServiceBrowse() callback. 
 domain: 
 The domain of the service instance to be resolved, as reported 
 to the DNSServiceBrowse() callback. 
 callBack: 
 The function to be called when a result is found, or if the 
 call asynchronously fails.  Its signature should be 
 callBack(sdRef, flags, interfaceIndex, errorCode, fullname, 
 hosttarget, port, txtRecord). 
 return value: 
 A DNSServiceRef instance.  The resolve operation will run 
 indefinitely until the client terminates it by closing the 
 DNSServiceRef. 
 Callback Parameters: 
 sdRef: 
 The DNSServiceRef returned by DNSServiceResolve(). 
 flags: 
 Currently unused, reserved for future use. 
 interfaceIndex: 
 The interface on which the service was resolved. 
 errorCode: 
 Will be kDNSServiceErr_NoError (0) on success, otherwise will 
 indicate the failure that occurred.  Other parameters are 
 undefined if an error occurred. 
 fullname: 
 The full service domain name, in the form 
 <servicename>.<protocol>.<domain>. 
 hosttarget: 
 The target hostname of the machine providing the service. 
 port: 
 The port, in host (not network) byte order, on which 
 connections are accepted for this service. 
 txtRecord: 
 A string containing the service\'s primary txt record, in 
 standard txt record format.'"
"def FindRegisterPackage(packageName, knownFile, searchPaths, registryAppName=None): 
    import regutil, string 
   if (not packageName): 
      raise error('A   package   name   must   be   supplied') 
   corePaths = regutil.GetRegisteredNamedPath(None).split(';') 
   if (not searchPaths): 
      searchPaths = corePaths 
   registryAppName = (registryAppName or packageName) 
   try: 
      (pathLook, pathAdd) = FindPackagePath(packageName, knownFile, searchPaths) 
      if (pathAdd is not None): 
         if (pathAdd in corePaths): 
            pathAdd = '' 
         regutil.RegisterNamedPath(registryAppName, pathAdd) 
      return pathLook 
   except error as details: 
      print ('***   The   %s   package   could   not   be   registered   -   %s' % (packageName, details)) 
      print '***   Please   ensure   you   have   passed   the   correct   paths   on   the   command   line.' 
      print '***   -   For   packages,   you   should   pass   a   path   to   the   packages   parent   directory,' 
      print '***   -   and   not   the   package   directory   itself...'"," 'Find the package registry path for the package. 
 This function will attempt to find the path to the package registry 
 and register it. 
 @param packageName: The package name. 
 @param knownFile: The file that was used to find the package. 
 @param searchPaths: The search paths to use. 
 @param registryAppName: The registry application name to use. 
 @return: The path to the package registry.'","'Find and Register a package. 
 Assumes the core registry setup correctly. 
 In addition, if the location located by the package is already 
 in the **core** path, then an entry is registered, but no path. 
 (no other paths are checked, as the application whose path was used 
 may later be uninstalled.  This should not happen with the core)'"
"def kernel_info(attrs=None, where=None): 
    return _osquery_cmd(table='kernel_info', attrs=attrs, where=where)", 'Returns kernel information.',"'Return kernel_info information from osquery 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' osquery.kernel_info'"
"def vsepr_build_correct_answer(geometry, atoms): 
    return {'geometry': geometry, 'atoms': atoms}"," 'Returns the correct answer for the given geometry. 
 Args: 
 geometry: The geometry to be checked. 
 atoms: The atoms in the given geometry. 
 Returns: 
 A dict with the geometry and atoms of the correct answer. 
 Raises: 
 ValueError: If the geometry is not valid.'","'geometry is string. 
 atoms is dict of atoms with proper positions. 
 Example: 
 correct_answer = vsepr_build_correct_answer(geometry=""AX4E0"", atoms={""c0"": ""N"", ""p0"": ""H"", ""p1"": ""(ep)"", ""p2"": ""H"", ""p3"": ""H""}) 
 returns a dictionary composed from input values: 
 {\'geometry\': geometry, \'atoms\': atoms}'"
"def _get_enabled_tax_rules(taxing_context, tax_class): 
    tax_rules = TaxRule.objects.may_match_postal_code(taxing_context.postal_code).filter(enabled=True, tax_classes=tax_class) 
   if taxing_context.customer_tax_group: 
      tax_rules = tax_rules.filter((Q(customer_tax_groups=taxing_context.customer_tax_group) | Q(customer_tax_groups=None))) 
   tax_rules = tax_rules.order_by('-override_group', 'priority') 
   return tax_rules"," 'Returns a list of enabled tax rules. 
 :param taxing_context: taxing context 
 :type taxing_context: TaxingContext 
 :param tax_class: tax class 
 :type tax_class: TaxClass 
 :return: list of tax rules 
 :rtype: list'","'Get enabled tax rules from the db for given parameters. 
 Returned rules are ordered desceding by override group and then 
 ascending by priority (as required by `_filter_and_group_rules`). 
 :type taxing_context: shuup.core.taxing.TaxingContext 
 :type tax_class: shuup.core.models.TaxClass'"
"def hilbert(n): 
    values = (1.0 / (1.0 + np.arange(((2 * n) - 1)))) 
   h = hankel(values[:n], r=values[(n - 1):]) 
   return h"," 'Return the hilbert transform of a sequence. 
 Parameters 
 n : int 
 The order of the transform. 
 Returns 
 h : ndarray 
 The hilbert transform of the input sequence. 
 Examples 
 >>> from sympy.physics.wave import hilbert 
 >>> from sympy.physics.wave import hankel 
 >>> hilbert(1) 
 1/2 
 >>> hilbert(2) 
 1/2 - 1/4 
 >>> hilbert(3) 
 1/2 - 1/4 + 1/8 
 >>> hilbert(4) 
 1/2 - 1/4 + 1/8 - 1/16 
 >>> hilbert(5) 
 1/2 - 1/4 + 1/8 - 1/16 + 1/32 
 >>> hankel(hilbert(3), 3) 
 1/2 
 >>> hankel(hilbert(3), 4) 
 1/2 - ","'Create a Hilbert matrix of order `n`. 
 Returns the `n` by `n` array with entries `h[i,j] = 1 / (i + j + 1)`. 
 Parameters 
 n : int 
 The size of the array to create. 
 Returns 
 h : (n, n) ndarray 
 The Hilbert matrix. 
 See Also 
 invhilbert : Compute the inverse of a Hilbert matrix. 
 Notes 
 .. versionadded:: 0.10.0 
 Examples 
 >>> from scipy.linalg import hilbert 
 >>> hilbert(3) 
 array([[ 1.        ,  0.5       ,  0.33333333], 
 [ 0.5       ,  0.33333333,  0.25      ], 
 [ 0.33333333,  0.25      ,  0.2       ]])'"
"def float_sum(iterable): 
    return float(sum(iterable))", 'Returns the sum of the iterable as a float.',"'Sum the elements of the iterable, and return the result as a float.'"
"def json2csv_entities(tweets_file, outfile, main_fields, entity_type, entity_fields, encoding='utf8', errors='replace', gzip_compress=False): 
    (writer, outf) = outf_writer_compat(outfile, encoding, errors, gzip_compress) 
   header = get_header_field_list(main_fields, entity_type, entity_fields) 
   writer.writerow(header) 
   for line in tweets_file: 
      tweet = json.loads(line) 
      if _is_composed_key(entity_type): 
         (key, value) = _get_key_value_composed(entity_type) 
         object_json = _get_entity_recursive(tweet, key) 
         if (not object_json): 
            continue 
         object_fields = extract_fields(object_json, main_fields) 
         items = _get_entity_recursive(object_json, value) 
         _write_to_file(object_fields, items, entity_fields, writer) 
      else: 
         tweet_fields = extract_fields(tweet, main_fields) 
         items = _get_entity_recursive(tweet, entity_type) 
         _write_to_file(tweet_fields, items, entity_fields, writer) 
   outf.close()"," 'Write a CSV file containing all the entities in the input JSON file. 
 :param tweets_file: the file to read from 
 :type tweets_file: file 
 :param outfile: the file to write to 
 :type outfile: file 
 :param main_fields: the main fields to extract from the JSON file 
 :type main_fields: list 
 :param entity_type: the entity type to extract 
 :type entity_type: str 
 :param entity_fields: the fields to extract from the entity 
 :type entity_fields: list 
 :param encoding: the encoding to use 
 :type encoding: str 
 :param errors: the errors to use 
 :type errors: str 
 :param gzip_compress: if True, compress the file 
 :type gzip_compress: bool 
 :return: None'","'Extract selected fields from a file of line-separated JSON tweets and 
 write to a file in CSV format. 
 This utility function allows a file of full Tweets to be easily converted 
 to a CSV file for easier processing of Twitter entities. For example, the 
 hashtags or media elements of a tweet can be extracted. 
 It returns one line per entity of a Tweet, e.g. if a tweet has two hashtags 
 there will be two lines in the output file, one per hashtag 
 :param tweets_file: the file-like object containing full Tweets 
 :param str outfile: The path of the text file where results should be    written 
 :param list main_fields: The list of fields to be extracted from the main    object, usually the tweet. Useful examples: \'id_str\' for the tweetID. See    <https://dev.twitter.com/overview/api/tweets> for a full list of fields. 
 e. g.: [\'id_str\'], [\'id\', \'text\', \'favorite_count\', \'retweet_count\'] 
 If `entity_type` is expressed with hierarchy, then it is the list of    fields of the object that corresponds to the key of the entity_type,    (e.g., for entity_type=\'user.urls\', the fields in the main_fields list    belong to the user object; for entity_type=\'place.bounding_box\', the    files in the main_field list belong to the place object of the tweet). 
 :param list entity_type: The name of the entity: \'hashtags\', \'media\',    \'urls\' and \'user_mentions\' for the tweet object. For a user object,    this needs to be expressed with a hierarchy: `\'user.urls\'`. For the    bounding box of the Tweet location, use `\'place.bounding_box\'`. 
 :param list entity_fields: The list of fields to be extracted from the    entity. E.g. `[\'text\']` (of the Tweet) 
 :param error: Behaviour for encoding errors, see    https://docs.python.org/3/library/codecs.html#codec-base-classes 
 :param gzip_compress: if `True`, ouput files are compressed with gzip'"
"def quota_allocated_update(context, project_id, resource, allocated): 
    return IMPL.quota_allocated_update(context, project_id, resource, allocated)"," 'Update the allocated quota for the given project. 
 :param context: context to use. 
 :param project_id: project id. 
 :param resource: resource name. 
 :param allocated: new allocated value. 
 :returns: updated allocated value.'","'Update allocated quota to subprojects or raise if it does not exist. 
 :raises: cinder.exception.ProjectQuotaNotFound'"
"def AllocateIdsAsync(model_key, size=None, **kwargs): 
    max = kwargs.pop('max', None) 
   config = _GetConfigFromKwargs(kwargs) 
   if (getattr(config, 'read_policy', None) == EVENTUAL_CONSISTENCY): 
      raise datastore_errors.BadRequestError('read_policy   is   only   supported   on   read   operations.') 
   (keys, _) = NormalizeAndTypeCheckKeys(model_key) 
   if (len(keys) > 1): 
      raise datastore_errors.BadArgumentError('Cannot   allocate   IDs   for   more   than   one   model   key   at   a   time') 
   rpc = _GetConnection().async_allocate_ids(config, keys[0], size, max) 
   return rpc"," 'Allocate a sequence of IDs for a model key. 
 The sequence of IDs will be returned asynchronously. 
 :param model_key: The model key to allocate IDs for. 
 :type model_key: ModelKey 
 :param size: The number of IDs to allocate. 
 :type size: int 
 :param max: The maximum number of IDs to allocate. 
 :type max: int 
 :param kwargs: Additional arguments to pass to the RPC. 
 :returns: An RPC that will return the IDs. 
 :rtype: AsyncRPC'","'Asynchronously allocates a range of IDs. 
 Identical to datastore.AllocateIds() except returns an asynchronous object. 
 Call get_result() on the return value to block on the call and get the 
 results.'"
"def unpack(source): 
    (payload, symtab, radix, count) = _filterargs(source) 
   if (count != len(symtab)): 
      raise UnpackingError('Malformed   p.a.c.k.e.r.   symtab.') 
   try: 
      unbase = Unbaser(radix) 
   except TypeError: 
      raise UnpackingError('Unknown   p.a.c.k.e.r.   encoding.') 
   def lookup(match): 
      'Look   up   symbols   in   the   synthetic   symtab.' 
      word = match.group(0) 
      return (symtab[unbase(word)] or word) 
   source = re.sub('\\b\\w+\\b', lookup, payload) 
   return _replacestrings(source)"," 'Unpack a string into a list of strings. 
 This function parses the payload of a packet and unpacks it into a list of 
 strings. 
 :param source: A string containing a packet to be unpacked. 
 :type source: str 
 :return: A list of strings. 
 :rtype: list of str'",'Unpacks P.A.C.K.E.R. packed js code.'
"def encode_quopri(msg): 
    orig = msg.get_payload(decode=True) 
   encdata = _qencode(orig) 
   msg.set_payload(encdata) 
   msg['Content-Transfer-Encoding'] = 'quoted-printable'"," 'Encode the message to be QOPRI compliant. 
 :param msg: Message to be encoded 
 :type msg: Message'","'Encode the message\'s payload in quoted-printable. 
 Also, add an appropriate Content-Transfer-Encoding header.'"
"def dump_metadata(filename, show_row_group_metadata, out=sys.stdout): 
    def println(value): 
      u'Write   a   new   line   containing   `value`   to   `out`.' 
      out.write((value + u'\n')) 
   footer = read_footer(filename) 
   println(u'File   Metadata:   {0}'.format(filename)) 
   println(u'      Version:   {0}'.format(footer.version)) 
   println(u'      Num   Rows:   {0}'.format(footer.num_rows)) 
   println(u'      k/v   metadata:   ') 
   if (footer.key_value_metadata and (len(footer.key_value_metadata) > 0)): 
      for item in footer.key_value_metadata: 
         println(u'            {0}={1}'.format(item.key, item.value)) 
   else: 
      println(u'            (none)') 
   println(u'      schema:   ') 
   for element in footer.schema: 
      println(u'            {name}   ({type}):   length={type_length},   repetition={repetition_type},   children={num_children},   converted_type={converted_type}'.format(name=element.name, type=(parquet_thrift.Type._VALUES_TO_NAMES[element.type] if element.type else None), type_length=element.type_length, repetition_type=_get_name(parquet_thrift.FieldRepetitionType, element.repetition_type), num_children=element.num_children, converted_type=element.converted_type)) 
   if show_row_group_metadata: 
      println(u'      row   groups:   ') 
      for row_group in footer.row_groups: 
         num_rows = row_group.num_rows 
         size_bytes = row_group.total_byte_size 
         println(u'      rows={num_rows},   bytes={bytes}'.format(num_rows=num_rows, bytes=size_bytes)) 
         println(u'            chunks:') 
         for col_group in row_group.columns: 
            cmd = col_group.meta_data 
            println(u'                  type={type}   file_offset={offset}   compression={codec}   encodings={encodings}   path_in_schema={path_in_schema}   num_values={num_values}   uncompressed_bytes={raw_bytes}   compressed_bytes={compressed_bytes}   data_page_offset={data_page_offset}   dictionary_page_offset={dictionary_page_offset}'.format(type=_get_name(parquet_thrift.Type, cmd.type), offset=col_group.file_offset, codec=_get_name(parquet_thrift.CompressionCodec, cmd.codec), encodings=u','.join([_get_name(parquet_thrift.Encoding, s) for s in cmd.encodings]), path_in_schema=cmd.path_in_schema, num_values=cmd.num_values, raw_bytes=cmd.total_uncompressed_size, compressed_bytes=cmd.total_compressed_size, data_page_offset=cmd.data_page_offset, dictionary_page_offset=cmd.dictionary_page_offset)) 
            with open(filename, u'rb') as file_obj: 
               offset = _get_offset(cmd) 
               file_obj.seek(offset, 0) 
               values_read = 0 
               println(u'                  pages:   ') 
               while (values_read < num_rows): 
                  page_header = _read_page_header(file_obj) 
                  file_obj.seek(page_header.compressed_page_size, 1) 
                  daph = page_header.data_page_header 
                  type_ = _get_name(parquet_thrift.PageType, page_header.type) 
                  raw_bytes = page_header.uncompressed_page_size 
                  num_values = None 
                  if (page_header.type == parquet_thrift.PageType.DATA_PAGE): 
                     num_values = daph.num_values 
                     values_read += num_values 
                  if (page_header.type == parquet_thrift.PageType.DICTIONARY_PAGE): 
                     pass 
                  encoding_type = None 
                  def_level_encoding = None 
                  rep_level_encoding = None 
                  if daph: 
                     encoding_type = _get_name(parquet_thrift.Encoding, daph.encoding) 
                     def_level_encoding = _get_name(parquet_thrift.Encoding, daph.definition_level_encoding) 
                     rep_level_encoding = _get_name(parquet_thrift.Encoding, daph.repetition_level_encoding) 
                  println(u'                        page   header:   type={type}   uncompressed_size={raw_bytes}   num_values={num_values}   encoding={encoding}   def_level_encoding={def_level_encoding}   rep_level_encoding={rep_level_encoding}'.format(type=type_, raw_bytes=raw_bytes, num_values=num_values, encoding=encoding_type, def_level_encoding=def_level_encoding, rep_level_encoding=rep_level_encoding))"," 'Dump metadata from a Parquet file. 
 :param filename: The filename of the Parquet file to dump. 
 :param show_row_group_metadata: Whether to show row group metadata. 
 :param out: The output stream to write to.'","'Dump metadata about the parquet object with the given filename. 
 Dump human-readable metadata to specified `out`. Optionally dump the row group metadata as well.'"
"def make_query_from_filter(sample_filter, require_meter=True): 
    q = {} 
   if sample_filter.user: 
      q['user_id'] = sample_filter.user 
   if sample_filter.project: 
      q['project_id'] = sample_filter.project 
   if sample_filter.meter: 
      q['counter_name'] = sample_filter.meter 
   elif require_meter: 
      raise RuntimeError('Missing   required   meter   specifier') 
   ts_range = make_timestamp_range(sample_filter.start_timestamp, sample_filter.end_timestamp, sample_filter.start_timestamp_op, sample_filter.end_timestamp_op) 
   if ts_range: 
      q['timestamp'] = ts_range 
   if sample_filter.resource: 
      q['resource_id'] = sample_filter.resource 
   if sample_filter.source: 
      q['source'] = sample_filter.source 
   if sample_filter.message_id: 
      q['message_id'] = sample_filter.message_id 
   q.update(dict(((('resource_%s' % k), v) for (k, v) in six.iteritems(improve_keys(sample_filter.metaquery, metaquery=True))))) 
   return q"," 'Return a query dict from a filter dict. 
 :param sample_filter: A filter dict 
 :type sample_filter: dict 
 :param require_meter: Whether the meter specifier is required 
 :type require_meter: bool 
 :return: A dict of query parameters 
 :rtype: dict'","'Return a query dictionary based on the settings in the filter. 
 :param sample_filter: SampleFilter instance 
 :param require_meter: If true and the filter does not have a meter, 
 raise an error.'"
"def get_value_from_user(message, default_value='', hidden=False): 
    return _validate_user_input(InputDialog(message, default_value, is_truthy(hidden)))"," 'Asks the user for a value, and returns the value if it was provided, 
 or the default value otherwise. 
 :param message: The message to display to the user. 
 :param default_value: The default value to return if the user does not 
 provide a value. 
 :param hidden: If ``True``, the user will not be able to see the 
 value that they provide. 
 :return: The value provided by the user, or the default value if none was 
 provided.'","'Pauses test execution and asks user to input a value. 
 Value typed by the user, or the possible default value, is returned. 
 Returning an empty value is fine, but pressing ``Cancel`` fails the keyword. 
 ``message`` is the instruction shown in the dialog and ``default_value`` is 
 the possible default value shown in the input field. 
 If ``hidden`` is given a true value, the value typed by the user is hidden. 
 ``hidden`` is considered true if it is a non-empty string not equal to 
 ``false`` or ``no``, case-insensitively. If it is not a string, its truth 
 value is got directly using same 
 [http://docs.python.org/2/library/stdtypes.html#truth-value-testing|rules 
 as in Python]. 
 Example: 
 | ${username} = | Get Value From User | Input user name | default    | 
 | ${password} = | Get Value From User | Input password  | hidden=yes | 
 Possibility to hide the typed in value is new in Robot Framework 2.8.4. 
 Considering strings ``false`` and ``no`` to be false is new in 2.9.'"
"def is_private_subnet(ip): 
    priv_subnets = [{'subnet': '10.0.0.0', 'mask': '255.0.0.0'}, {'subnet': '172.16.0.0', 'mask': '255.240.0.0'}, {'subnet': '192.168.0.0', 'mask': '255.255.0.0'}] 
   ip = struct.unpack('I', socket.inet_aton(ip))[0] 
   for network in priv_subnets: 
      subnet = struct.unpack('I', socket.inet_aton(network['subnet']))[0] 
      mask = struct.unpack('I', socket.inet_aton(network['mask']))[0] 
      if ((ip & mask) == (subnet & mask)): 
         return True 
   return False"," 'Returns True if the IP address is a private subnet address. 
 :param ip: IP address to test 
 :type ip: int 
 :return: True if the IP address is a private subnet address, False otherwise'","'Utility function to check if an IP address is inside a private subnet. 
 :type ip: ``str`` 
 :param ip: IP address to check 
 :return: ``bool`` if the specified IP address is private.'"
"def emit_certificate_event(event_name, user, course_id, course=None, event_data=None): 
    event_name = '.'.join(['edx', 'certificate', event_name]) 
   if (course is None): 
      course = modulestore().get_course(course_id, depth=0) 
   context = {'org_id': course.org, 'course_id': unicode(course_id)} 
   data = {'user_id': user.id, 'course_id': unicode(course_id), 'certificate_url': get_certificate_url(user.id, course_id)} 
   event_data = (event_data or {}) 
   event_data.update(data) 
   with tracker.get_tracker().context(event_name, context): 
      tracker.emit(event_name, event_data)", 'Emit a certificate event.','Emits certificate event.'
"def write_checkpoint(current_key, ctr, cluster_mapping, ids, bestscores, order, out_fp): 
    checkpoint_dir = (out_fp + '/checkpoints/') 
   if (not exists(checkpoint_dir)): 
      create_dir(checkpoint_dir) 
   out_fp = (checkpoint_dir + ('/checkpoint%d.pickle' % ctr)) 
   out_fh = open(out_fp, 'w') 
   pickle.dump((current_key, ctr, cluster_mapping, ids, bestscores, order), out_fh) 
   return out_fp"," 'Write a checkpoint to disk. 
 :param current_key: The key of the current training iteration. 
 :param ctr: The counter for the current training iteration. 
 :param cluster_mapping: The cluster mapping for the current training iteration. 
 :param ids: The IDs of the documents for the current training iteration. 
 :param bestscores: The best scores for the current training iteration. 
 :param order: The order of the best scores for the current training iteration. 
 :param out_fp: The output file name.'","'write intermediate results to checkpoint file 
 current_key: the identifier of the current denoiser round 
 ctr: a uniq counter to label the checkpoint 
 cluster_mapping: an intermediate cluster mapping as dict 
 ids: the dict of active ids 
 order:  a list of ids, which defines the order of which flowgrams are clustered 
 bestscores: a dict of'"
"def get_nexusport_binding(port_id, vlan_id, switch_ip, instance_id): 
    LOG.debug(_('get_nexusport_binding()   called')) 
   session = db.get_session() 
   try: 
      binding = session.query(nexus_models_v2.NexusPortBinding).filter_by(vlan_id=vlan_id).filter_by(switch_ip=switch_ip).filter_by(port_id=port_id).filter_by(instance_id=instance_id).all() 
      return binding 
   except exc.NoResultFound: 
      raise c_exc.NexusPortBindingNotFound(vlan_id=vlan_id)"," 'Get NexusPortBinding for the specified port_id, vlan_id, switch_ip, and 
 instance_id.'",'Lists a nexusport binding'
"def filter_user(user_ref): 
    if user_ref: 
      user_ref = user_ref.copy() 
      user_ref.pop('password', None) 
      user_ref.pop('tenants', None) 
      user_ref.pop('groups', None) 
      user_ref.pop('domains', None) 
      try: 
         user_ref['extra'].pop('password', None) 
         user_ref['extra'].pop('tenants', None) 
      except KeyError: 
         pass 
      if ('password_expires_at' not in user_ref): 
         user_ref['password_expires_at'] = None 
   return user_ref", 'Filter out sensitive fields from the user object',"'Filter out private items in a user dict. 
 \'password\', \'tenants\' and \'groups\' are never returned. 
 :returns: user_ref'"
"def BdbQuit_excepthook(et, ev, tb, excepthook=None): 
    warnings.warn('`BdbQuit_excepthook`   is   deprecated   since   version   5.1', DeprecationWarning, stacklevel=2) 
   if (et == bdb.BdbQuit): 
      print 'Exiting   Debugger.' 
   elif (excepthook is not None): 
      excepthook(et, ev, tb) 
   else: 
      BdbQuit_excepthook.excepthook_ori(et, ev, tb)"," 'The bdb quit event handler. 
 This function is called when the user hits CTRL-C or when the bdb process 
 exits. 
 Parameters 
 et : BdbEventType 
 The event type. 
 ev : BdbEvent 
 The event object. 
 tb : Exception 
 The exception that caused the bdb process to exit. 
 excepthook : function 
 A function to call when the bdb process exits. 
 If this function is not provided, the BdbQuit_excepthook.excepthook_ori 
 function is called.'","'Exception hook which handles `BdbQuit` exceptions. 
 All other exceptions are processed using the `excepthook` 
 parameter.'"
"def social_auth_login_redirect(request): 
    redirect_value = request.REQUEST.get(REDIRECT_FIELD_NAME) 
   if redirect_value: 
      redirect_querystring = ((REDIRECT_FIELD_NAME + '=') + redirect_value) 
   else: 
      redirect_querystring = '' 
   return {'REDIRECT_FIELD_NAME': REDIRECT_FIELD_NAME, 'REDIRECT_FIELD_VALUE': redirect_value, 'redirect_querystring': redirect_querystring}"," 'Returns a dictionary containing the redirect URL and querystring to use 
 for the social auth login redirect.'",'Load current redirect to context.'
"def get_user_model(): 
    from django.db.models import get_model 
   try: 
      (app_label, model_name) = settings.AUTH_USER_MODEL.split('.') 
   except ValueError: 
      raise ImproperlyConfigured(""AUTH_USER_MODEL   must   be   of   the   form   'app_label.model_name'"") 
   user_model = get_model(app_label, model_name) 
   if (user_model is None): 
      raise ImproperlyConfigured((""AUTH_USER_MODEL   refers   to   model   '%s'   that   has   not   been   installed"" % settings.AUTH_USER_MODEL)) 
   return user_model", 'Return the user model.','Returns the User model that is active in this project.'
"def remove_useless_meta(meta): 
    if meta: 
      for name in USELESS_META: 
         if (name in meta): 
            del meta[name] 
   return meta", 'Remove useless meta from a dict.','Removes useless (for database) attributes from the table\'s meta.'
"def bidirectional_rnn(incoming, rnncell_fw, rnncell_bw, return_seq=False, return_states=False, initial_state_fw=None, initial_state_bw=None, dynamic=False, scope=None, name='BiRNN'): 
    assert (rnncell_fw._num_units == rnncell_bw._num_units), 'RNN   Cells   number   of   units   must   match!' 
   sequence_length = None 
   if dynamic: 
      sequence_length = retrieve_seq_length_op((incoming if isinstance(incoming, tf.Tensor) else tf.pack(incoming))) 
   input_shape = utils.get_incoming_shape(incoming) 
   with tf.variable_scope(scope, name, values=[incoming]) as scope: 
      name = scope.name 
      inference = incoming 
      if (type(inference) not in [list, np.array]): 
         ndim = len(input_shape) 
         assert (ndim >= 3), 'Input   dim   should   be   at   least   3.' 
         axes = ([1, 0] + list(range(2, ndim))) 
         inference = tf.transpose(inference, axes) 
         inference = tf.unpack(inference) 
      (outputs, states_fw, states_bw) = _brnn(rnncell_fw, rnncell_bw, inference, initial_state_fw=initial_state_fw, initial_state_bw=initial_state_bw, sequence_length=sequence_length, dtype=tf.float32) 
      c = ((tf.GraphKeys.LAYER_VARIABLES + '/') + scope.name) 
      for v in [rnncell_fw.W, rnncell_fw.b, rnncell_bw.W, rnncell_bw.b]: 
         if hasattr(v, '__len__'): 
            for var in v: 
               tf.add_to_collection(c, var) 
         else: 
            tf.add_to_collection(c, v) 
      tf.add_to_collection(tf.GraphKeys.ACTIVATIONS, outputs[(-1)]) 
   if dynamic: 
      if return_seq: 
         o = outputs 
      else: 
         outputs = tf.transpose(tf.pack(outputs), [1, 0, 2]) 
         o = advanced_indexing_op(outputs, sequence_length) 
   else: 
      o = (outputs if return_seq else outputs[(-1)]) 
   sfw = states_fw 
   sbw = states_bw 
   tf.add_to_collection(((tf.GraphKeys.LAYER_TENSOR + '/') + name), o) 
   return ((o, sfw, sbw) if return_states else o)"," 'Perform a bidirectional recurrent neural network. 
 Parameters 
 incoming : tensor, (batch_size, sequence_length, input_dim) 
 The input tensor. 
 rnncell_fw : RNNCell 
 The forward RNNCell. 
 rnncell_bw : RNNCell 
 The backward RNNCell. 
 return_seq : bool 
 Whether to return the entire sequence or just the last output. 
 return_states : bool 
 Whether to return the states of the RNN. 
 initial_state_fw : Tensor 
 The initial state of the forward RNN. 
 initial_state_bw : Tensor 
 The initial state of the backward RNN. 
 dynamic : bool 
 Whether to return the sequence length or not. 
 scope : A scope. 
 name : A name for the operation. 
 Returns 
 o : tensor, (batch_size, output_dim) 
 The output of the bidirectional RNN. 
 sfw : tensor, (batch_size, output_dim, num_units) 
 The final state of the","'Bidirectional RNN. 
 Build a bidirectional recurrent neural network, it requires 2 RNN Cells 
 to process sequence in forward and backward order. Any RNN Cell can be 
 used i.e. SimpleRNN, LSTM, GRU... with its own parameters. But the two 
 cells number of units must match. 
 Input: 
 3-D Tensor Layer [samples, timesteps, input dim]. 
 Output: 
 if `return_seq`: 3-D Tensor [samples, timesteps, output dim]. 
 else: 2-D Tensor Layer [samples, output dim]. 
 Arguments: 
 incoming: `Tensor`. The incoming Tensor. 
 rnncell_fw: `RNNCell`. The RNN Cell to use for foward computation. 
 rnncell_bw: `RNNCell`. The RNN Cell to use for backward computation. 
 return_seq: `bool`. If True, returns the full sequence instead of 
 last sequence output only. 
 return_states: `bool`. If True, returns a tuple with output and 
 states: (output, states). 
 initial_state_fw: `Tensor`. An initial state for the forward RNN. 
 This must be a tensor of appropriate type and shape [batch_size 
 x cell.state_size]. 
 initial_state_bw: `Tensor`. An initial state for the backward RNN. 
 This must be a tensor of appropriate type and shape [batch_size 
 x cell.state_size]. 
 dynamic: `bool`. If True, dynamic computation is performed. It will not 
 compute RNN steps above the sequence length. Note that because TF 
 requires to feed sequences of same length, 0 is used as a mask. 
 So a sequence padded with 0 at the end must be provided. When 
 computation is performed, it will stop when it meets a step with 
 a value of 0. 
 scope: `str`. Define this layer scope (optional). A scope can be 
 used to share variables between layers. Note that scope will 
 override name. 
 name: `str`. A name for this layer (optional).'"
"def test_scenario_may_own_outlines(): 
    scenario = Scenario.from_string(OUTLINED_SCENARIO) 
   assert_equals(len(scenario.steps), 4) 
   expected_sentences = ['Given   I   have   entered   <input_1>   into   the   calculator', 'And   I   have   entered   <input_2>   into   the   calculator', 'When   I   press   <button>', 'Then   the   result   should   be   <output>   on   the   screen'] 
   for (step, expected_sentence) in zip(scenario.steps, expected_sentences): 
      assert_equals(type(step), Step) 
      assert_equals(step.sentence, expected_sentence) 
   assert_equals(scenario.name, 'Add   two   numbers') 
   assert_equals(scenario.outlines, [{'input_1': '20', 'input_2': '30', 'button': 'add', 'output': '50'}, {'input_1': '2', 'input_2': '5', 'button': 'add', 'output': '7'}, {'input_1': '0', 'input_2': '40', 'button': 'add', 'output': '40'}])", 'Test that a scenario may own outlines.','A scenario may own outlines'
"def xml_decode(string): 
    string = string.replace('&amp;', '&') 
   string = string.replace('&lt;', '<') 
   string = string.replace('&gt;', '>') 
   string = string.replace('&quot;', '""') 
   string = string.replace('/', SLASH) 
   return string", 'Replace all &amp; with & and &lt; with < and &gt; with >','Returns the string with special characters decoded.'
"def ChiNoncentral(name, k, l): 
    return rv(name, ChiNoncentralDistribution, (k, l))"," 'Random variable with a Chi-squared distribution with noncentral k and 
 degrees of freedom l. 
 The density of the Chi-squared distribution with noncentral k and degrees 
 of freedom l is given by 
 .. math :: 
 f(x) = \frac{k}{\pi} \frac{1}{l !} \frac{(x/l)^{k-1} e^{-x/l} }{\Gamma(k/2)} 
 Parameters 
 k : The noncentrality parameter. 
 l : The degrees of freedom. 
 Returns 
 A RandomSymbol representing a Chi-squared random variable with noncentral 
 k and degrees of freedom l. 
 Examples 
 >>> from sympy.stats import ChiNoncentral 
 >>> from sympy import Symbol 
 >>> x = Symbol(""x"") 
 >>> ChiNoncentral(""x"", 2, 10).density() 
 10*x**(-2)/sqrt(2*pi)*exp(-x/10)**2/Gamma(3/2)/Gamma(5/2) 
 References 
 .. [1] http://en.wikipedia.","'Create a continuous random variable with a non-central Chi distribution. 
 The density of the non-central Chi distribution is given by 
 .. math:: 
 f(x) := \frac{e^{-(x^2+\lambda^2)/2} x^k\lambda} 
 {(\lambda x)^{k/2}} I_{k/2-1}(\lambda x) 
 with `x \geq 0`. Here, `I_\nu (x)` is the 
 :ref:`modified Bessel function of the first kind <besseli>`. 
 Parameters 
 k : A positive Integer, `k > 0`, the number of degrees of freedom 
 l : Shift parameter 
 Returns 
 A RandomSymbol. 
 Examples 
 >>> from sympy.stats import ChiNoncentral, density, E, std 
 >>> from sympy import Symbol, simplify 
 >>> k = Symbol(""k"", integer=True) 
 >>> l = Symbol(""l"") 
 >>> z = Symbol(""z"") 
 >>> X = ChiNoncentral(""x"", k, l) 
 >>> density(X)(z) 
 l*z**k*(l*z)**(-k/2)*exp(-l**2/2 - z**2/2)*besseli(k/2 - 1, l*z) 
 References 
 .. [1] http://en.wikipedia.org/wiki/Noncentral_chi_distribution'"
"def name_for_collection_relationship(base, local_cls, referred_cls, constraint): 
    return (referred_cls.__name__.lower() + '_collection')"," 'Returns the name for the collection relationship for a given 
 relationship. 
 :param base: The base class. 
 :param local_cls: The local class. 
 :param referred_cls: The referred class. 
 :param constraint: The constraint. 
 :return: The name of the relationship.'","'Return the attribute name that should be used to refer from one 
 class to another, for a collection reference. 
 The default implementation is:: 
 return referred_cls.__name__.lower() + ""_collection"" 
 Alternate implementations 
 can be specified using the 
 :paramref:`.AutomapBase.prepare.name_for_collection_relationship` 
 parameter. 
 :param base: the :class:`.AutomapBase` class doing the prepare. 
 :param local_cls: the class to be mapped on the local side. 
 :param referred_cls: the class to be mapped on the referring side. 
 :param constraint: the :class:`.ForeignKeyConstraint` that is being 
 inspected to produce this relationship.'"
"def demo_str_rule_format(): 
    postag(ruleformat='str')",'''',"'Exemplify repr(Rule) (see also str(Rule) and Rule.format(""verbose""))'"
"def get_location(vm_=None): 
    return __opts__.get('location', config.get_cloud_config_value('location', (vm_ or get_configured_provider()), __opts__, default=DEFAULT_LOCATION, search_global=False))"," 'Get the location from the cloud config. 
 :param vm_: the vm to get the location for, if none is given, the default location will be returned 
 :returns: the location 
 :rtype: str'","'Return the EC2 region to use, in this order: 
 - CLI parameter 
 - VM parameter 
 - Cloud profile setting'"
"def test_unicode_column(tmpdir): 
    t = Table([np.array([u'a', u'b', u'cd'])]) 
   t.write(str(tmpdir.join('test.fits')), overwrite=True) 
   with fits.open(str(tmpdir.join('test.fits'))) as hdul: 
      assert np.all((hdul[1].data['col0'] == ['a', 'b', 'cd'])) 
      assert (hdul[1].header['TFORM1'] == '2A') 
   t2 = Table([np.array([u'\u2603'])]) 
   with pytest.raises(UnicodeEncodeError): 
      t2.write(str(tmpdir.join('test.fits')), overwrite=True)", 'Test unicode columns',"'Test that a column of unicode strings is still written as one 
 byte-per-character in the FITS table (so long as the column can be ASCII 
 encoded). 
 Regression test for one of the issues fixed in 
 https://github.com/astropy/astropy/pull/4228'"
"@skip_if_not_win32 
 @with_environment 
 def test_get_home_dir_1(): 
    unfrozen = path.get_home_dir() 
   sys.frozen = True 
   IPython.__file__ = abspath(join(HOME_TEST_DIR, 'Lib/IPython/__init__.py')) 
   home_dir = path.get_home_dir() 
   nt.assert_equal(home_dir, unfrozen)"," 'Test that the home directory is not changed when running from a frozen 
 IPython'","'Testcase for py2exe logic, un-compressed lib'"
"def pct_to_int(value, num_items, min_value=1): 
    if (isinstance(value, string_types) and value.endswith('%')): 
      value_pct = int(value.replace('%', '')) 
      return (int(((value_pct / 100.0) * num_items)) or min_value) 
   else: 
      return int(value)"," 'Converts a percentage into an integer value. 
 :param value: The percentage value to convert. 
 :type value: int or string 
 :param num_items: The number of items to be converted. 
 :type num_items: int 
 :param min_value: The minimum value to return if the percentage is zero. 
 :type min_value: int 
 :return: The converted integer value. 
 :rtype: int'","'Converts a given value to a percentage if specified as ""x%"", 
 otherwise converts the given value to an integer.'"
"def zip_timeseries(*series, **kwargs): 
    next_slice = (max if (kwargs.get('order', 'descending') == 'descending') else min) 
   iterators = [PeekableIterator(s) for s in series] 
   widths = [] 
   for w in iterators: 
      r = w.peek() 
      if r: 
         (date, values) = r 
         widths.append(len(values)) 
      else: 
         widths.append(0) 
   while True: 
      items = [it.peek() for it in iterators] 
      if (not any(items)): 
         return 
      current_slice = next_slice((item[0] for item in items if item)) 
      data = [] 
      for (i, item) in enumerate(items): 
         if (item and (item[0] == current_slice)): 
            data.extend(item[1]) 
            iterators[i].next() 
         else: 
            data.extend(([0] * widths[i])) 
      (yield (current_slice, tuple(data)))"," 'Returns a generator that yields the same data as the input series, 
 but grouped together into a single timeseries. 
 :param series: A list of series 
 :param order: The order of the timeseries. 
 :param order: The order of the timeseries. 
 :param order: The order of the timeseries. 
 :param order: The order of the timeseries. 
 :param order: The order of the timeseries. 
 :param order: The order of the timeseries. 
 :param order: The order of the timeseries. 
 :param order: The order of the timeseries. 
 :param order: The order of the timeseries. 
 :param order: The order of the timeseries. 
 :param order: The order of the timeseries. 
 :param order: The order of the timeseries. 
 :param order: The order of the timeseries. 
 :param order: The order of the timeseries. 
 :param order: The order of the timeseries. 
 :param order: The order of the timeseries. 
 :param order: The order of the timeseries","'Zip timeseries data while gracefully handling gaps in the data. 
 Timeseries data is expected to be a sequence of two-tuples (date, values). 
 Values is expected itself to be a tuple. The width of the values tuples 
 should be the same across all elements in a timeseries sequence. The result 
 will be a single sequence in timeseries format. 
 Gaps in sequences are filled with an appropriate number of zeros based on 
 the size of the first value-tuple of that sequence.'"
"def html_body(input_string, source_path=None, destination_path=None, input_encoding='unicode', output_encoding='unicode', doctitle=True, initial_header_level=1): 
    parts = html_parts(input_string=input_string, source_path=source_path, destination_path=destination_path, input_encoding=input_encoding, doctitle=doctitle, initial_header_level=initial_header_level) 
   fragment = parts['html_body'] 
   if (output_encoding != 'unicode'): 
      fragment = fragment.encode(output_encoding) 
   return fragment"," 'Returns the HTML body of the input string. 
 The HTML body is the content of the body tag, and is the part of the 
 document that is displayed in the browser. 
 :param input_string: The string to be converted to HTML. 
 :param source_path: The source path to use for the input string. 
 :param destination_path: The destination path to use for the input string. 
 :param input_encoding: The encoding to use for the input string. 
 :param output_encoding: The encoding to use for the output string. 
 :param doctitle: If True, a title tag is added to the output. 
 :param initial_header_level: The header level to use for the output. 
 :return: The HTML body of the input string. 
 :rtype: str'","'Given an input string, returns an HTML fragment as a string. 
 The return value is the contents of the <body> element. 
 Parameters (see `html_parts()` for the remainder): 
 - `output_encoding`: The desired encoding of the output.  If a Unicode 
 string is desired, use the default value of ""unicode"" .'"
"def wlPen(wl): 
    l1 = 400 
   l2 = 700 
   hue = np.clip(((((l2 - l1) - (wl - l1)) * 0.8) / (l2 - l1)), 0, 0.8) 
   val = 1.0 
   if (wl > 700): 
      val = (1.0 * (((700 - wl) / 700.0) + 1)) 
   elif (wl < 400): 
      val = ((wl * 1.0) / 400.0) 
   color = pg.hsvColor(hue, 1.0, val) 
   pen = pg.mkPen(color) 
   return pen"," 'Create a pen object. 
 Parameters 
 wl : float 
 The wavelength of the light. 
 Returns 
 pen : pg.Pen 
 The pen object.'",'Return a pen representing the given wavelength'
"@conf.commands.register 
 def defragment(plist): 
    frags = defaultdict((lambda : [])) 
   final = [] 
   pos = 0 
   for p in plist: 
      p._defrag_pos = pos 
      pos += 1 
      if (IP in p): 
         ip = p[IP] 
         if ((ip.frag != 0) or (ip.flags & 1)): 
            ip = p[IP] 
            uniq = (ip.id, ip.src, ip.dst, ip.proto) 
            frags[uniq].append(p) 
            continue 
      final.append(p) 
   defrag = [] 
   missfrag = [] 
   for lst in frags.itervalues(): 
      lst.sort(key=(lambda x: x.frag)) 
      p = lst[0] 
      lastp = lst[(-1)] 
      if ((p.frag > 0) or ((lastp.flags & 1) != 0)): 
         missfrag += lst 
         continue 
      p = p.copy() 
      if (conf.padding_layer in p): 
         del p[conf.padding_layer].underlayer.payload 
      ip = p[IP] 
      if ((ip.len is None) or (ip.ihl is None)): 
         clen = len(ip.payload) 
      else: 
         clen = (ip.len - (ip.ihl << 2)) 
      txt = conf.raw_layer() 
      for q in lst[1:]: 
         if (clen != (q.frag << 3)): 
            if (clen > (q.frag << 3)): 
               warning(('Fragment   overlap   (%i   >   %i)   %r   ||   %r   ||      %r' % (clen, (q.frag << 3), p, txt, q))) 
            missfrag += lst 
            break 
         if ((q[IP].len is None) or (q[IP].ihl is None)): 
            clen += len(q[IP].payload) 
         else: 
            clen += (q[IP].len - (q[IP].ihl << 2)) 
         if (conf.padding_layer in q): 
            del q[conf.padding_layer].underlayer.payload 
         txt.add_payload(q[IP].payload.copy()) 
      else: 
         ip.flags &= (~ 1) 
         del ip.chksum 
         del ip.len 
         p = (p / txt) 
         p._defrag_pos = max((x._defrag_pos for x in lst)) 
         defrag.append(p) 
   defrag2 = [] 
   for p in defrag: 
      q = p.__class__(str(p)) 
      q._defrag_pos = p._defrag_pos 
      defrag2.append(q) 
   final += defrag2 
   final += missfrag 
   final.sort(key=(lambda x: x._defrag_pos)) 
   for p in final: 
      del p._defrag_pos 
   if hasattr(plist, 'listname'): 
      name = ('Defragmented   %s' % plist.listname) 
   else: 
      name = 'Defragmented' 
   return PacketList(final, name=name)", 'Defragments a list of packets','defrag(plist) -> plist defragmented as much as possible'
"def validate_positive_integer_or_none(option, value): 
    if (value is None): 
      return value 
   return validate_positive_integer(option, value)", 'Validate that the given value is a positive integer or None.','Validate that \'value\' is a positive integer or None.'
"def sixteen(data): 
    n = 0 
   for b in serial.iterbytes(data): 
      (yield ('{:02X}   '.format(ord(b)), (b.decode('ascii') if ('   ' <= b < '\x7f') else '.'))) 
      n += 1 
      if (n == 8): 
         (yield ('   ', '')) 
      elif (n >= 16): 
         (yield (None, None)) 
         n = 0 
   if (n > 0): 
      while (n < 16): 
         n += 1 
         if (n == 8): 
            (yield ('   ', '')) 
         (yield ('         ', '   ')) 
      (yield (None, None))"," 'Returns a string of hex values representing the bytes in the data 
 argument. 
 >>> sixteen(b""\x00\x01\x02\x03\x04\x05\x06\x07\x08\x09\x0a\x0b\x0c\x0d\x0e\x0f"") 
 \x00\x01\x02\x03\x04\x05\x06\x07\x08\x09\x0a\x0b\x0c\x0d\x0e\x0f 
 >>> sixteen(b""\x00\x01\x02\x03\x04\x05\x06\x07\x08\x09\x0a\x0b\x0c\x0d\x0e\x0f\x00\x01\x02\x03\x04\x05\x06\","'yield tuples of hex and ASCII display in multiples of 16. Includes a 
 space after 8 bytes and (None, None) after 16 bytes and at the end.'"
"def test_example2(): 
    vert_template = Function('\n            void   main(void)\n            {\n                        gl_Position   =   $position;\n            }\n            ') 
   transformScale = Function('\n            vec4   transform_scale(vec4   pos)\n            {\n                        pos.xyz   *=   $scale;\n                        return   pos;\n            }\n            ') 
   class Transform(object, ): 
      def __init__(self): 
         self.func = Function(transformScale) 
         self.func['scale'] = 'uniform   float' 
      def set_scale(self, scale): 
         self.func['scale'].value = scale 
   transforms = [Transform(), Transform(), Transform()] 
   code = Function(vert_template) 
   ob = Variable('attribute   vec3   a_position') 
   for trans in transforms: 
      ob = trans.func(ob) 
   code['position'] = ob 
   print code", 'Test example 2','Demonstrate how a transform would work.'
"def texts(i, e): 
    return pq(e).text()"," 'Returns the text nodes of the given elements. 
 :param i: the first element to include 
 :param e: the last element to include 
 :return: a generator of text nodes'",'Helper for getting the text of an element'
"def test_conditional_get_vae(): 
    mlp = MLP(layers=[Linear(layer_name='h', dim=5, irange=0.01)]) 
   conditional = DummyConditional(mlp=mlp, name='conditional') 
   vae = DummyVAE() 
   conditional.set_vae(vae) 
   testing.assert_same_object(conditional.get_vae(), vae)", 'Test that the conditional object gets the correct VAE object','Conditional.get_vae returns its VAE'
"def task_create(context, values, session=None): 
    values = values.copy() 
   session = (session or get_session()) 
   with session.begin(): 
      task_info_values = _pop_task_info_values(values) 
      task_ref = models.Task() 
      _task_update(context, task_ref, values, session=session) 
      _task_info_create(context, task_ref.id, task_info_values, session=session) 
   return task_get(context, task_ref.id, session)"," 'Create a task. 
 :param values: dict of task values 
 :param session: database session to use 
 :returns: task object'",'Create a task object'
"def getEvaluatedIntDefault(defaultInt, key, xmlElement=None): 
    evaluatedInt = getEvaluatedInt(key, xmlElement) 
   if (evaluatedInt == None): 
      return defaultInt 
   return evaluatedInt"," 'Get the evaluated int value for the given key in the xml element. 
 If the value is not found, defaultInt is returned. 
 @param defaultInt: The default value to return if the key is not found 
 @param key: The key to look up in the xml element 
 @param xmlElement: The xml element to look up the value in'",'Get the evaluated value as an int.'
"def _ToBlobstoreError(error): 
    error_map = {blobstore_service_pb.BlobstoreServiceError.INTERNAL_ERROR: InternalError, blobstore_service_pb.BlobstoreServiceError.BLOB_NOT_FOUND: BlobNotFoundError, blobstore_service_pb.BlobstoreServiceError.DATA_INDEX_OUT_OF_RANGE: DataIndexOutOfRangeError, blobstore_service_pb.BlobstoreServiceError.BLOB_FETCH_SIZE_TOO_LARGE: BlobFetchSizeTooLargeError, blobstore_service_pb.BlobstoreServiceError.PERMISSION_DENIED: PermissionDeniedError} 
   desired_exc = error_map.get(error.application_error) 
   return (desired_exc(error.error_detail) if desired_exc else error)", 'Convert a BlobstoreError to an appropriate Exception.',"'Translate an application error to a datastore Error, if possible. 
 Args: 
 error: An ApplicationError to translate.'"
"def csolve_prime(f, p, e=1): 
    from sympy.polys.domains import ZZ 
   X1 = [i for i in range(p) if (gf_eval(f, i, p, ZZ) == 0)] 
   if (e == 1): 
      return X1 
   X = [] 
   S = list(zip(X1, ([1] * len(X1)))) 
   while S: 
      (x, s) = S.pop() 
      if (s == e): 
         X.append(x) 
      else: 
         s1 = (s + 1) 
         ps = (p ** s) 
         S.extend([((x + (v * ps)), s1) for v in _raise_mod_power(x, s, p, f)]) 
   return sorted(X)"," 'Solve the polynomial ``f`` modulo ``p`` in ``X1`` and return the 
 solution in ``X`` in increasing order. 
 Examples 
 >>> from sympy import gf, gf_diff, gf_sqr, gf_sqrt, gf_exp, gf_log, gf_eval, gf_degree, gf_clear_denominators 
 >>> from sympy.polys.galoistools import gf_gcd, gf_primitive_gcd, gf_primitive_gcd_list 
 >>> from sympy.polys.galoistools import gf_solve_primitive_gcd, gf_primitive_gcd_list_to_gcd 
 >>> from sympy.polys.galoistools import gf_primitive_gcd_list_to_gcd_list 
 >>> from sympy.polys.galoistools import gf_primitive_gcd_list_to_gcd_list_int 
 >>> from sympy.polys.galoistools import gf","'Solutions of f(x) congruent 0 mod(p**e). 
 Examples 
 >>> from sympy.polys.galoistools import csolve_prime 
 >>> csolve_prime([1, 1, 7], 3, 1) 
 [1] 
 >>> csolve_prime([1, 1, 7], 3, 2) 
 [1, 4, 7] 
 Solutions [7, 4, 1] (mod 3**2) are generated by ``_raise_mod_power()`` 
 from solution [1] (mod 3).'"
"def _get_nets(vif, subnet, version, net_num, link_id): 
    if (subnet.get_meta('dhcp_server') is not None): 
      net_info = {'id': ('network%d' % net_num), 'type': ('ipv%d_dhcp' % version), 'link': link_id, 'network_id': vif['network']['id']} 
      return net_info 
   ip = subnet['ips'][0] 
   address = ip['address'] 
   if (version == 4): 
      netmask = model.get_netmask(ip, subnet) 
   elif (version == 6): 
      netmask = str(subnet.as_netaddr().netmask) 
   net_info = {'id': ('network%d' % net_num), 'type': ('ipv%d' % version), 'link': link_id, 'ip_address': address, 'netmask': netmask, 'routes': _get_default_route(version, subnet), 'network_id': vif['network']['id']} 
   for route in subnet['routes']: 
      route_addr = netaddr.IPNetwork(route['cidr']) 
      new_route = {'network': str(route_addr.network), 'netmask': str(route_addr.netmask), 'gateway': route['gateway']['address']} 
      net_info['routes'].append(new_route) 
   return net_info", 'Return a dictionary containing the network information.',"'Get networks for the given VIF and subnet 
 :param vif: Neutron VIF 
 :param subnet: Neutron subnet 
 :param version: IP version as an int, either \'4\' or \'6\' 
 :param net_num: Network index for generating name of each network 
 :param link_id: Arbitrary identifier for the link the networks are 
 attached to'"
"def inv_item_quantity(): 
    try: 
      item_id = request.args[0] 
   except: 
      raise HTTP(400, current.xml.json_message(False, 400, 'No   value   provided!')) 
   table = s3db.inv_inv_item 
   ptable = db.supply_item_pack 
   query = ((table.id == item_id) & (table.item_pack_id == ptable.id)) 
   record = db(query).select(table.quantity, ptable.quantity, limitby=(0, 1)).first() 
   d = {'iquantity': record.inv_inv_item.quantity, 'pquantity': record.supply_item_pack.quantity} 
   output = json.dumps(d) 
   response.headers['Content-Type'] = 'application/json' 
   return output"," 'Return the quantity of an item in a pack. 
 :param item_id: the ID of the item 
 :return: JSON of the quantity'","'Access via the .json representation to avoid work rendering menus, etc'"
"def get_connection(service, module=None, region=None, key=None, keyid=None, profile=None): 
    module = (module or service) 
   (cxkey, region, key, keyid) = _get_profile(service, region, key, keyid, profile) 
   cxkey = (cxkey + ':conn3') 
   if (cxkey in __context__): 
      return __context__[cxkey] 
   try: 
      session = boto3.session.Session(aws_access_key_id=keyid, aws_secret_access_key=key, region_name=region) 
      if (session is None): 
         raise SaltInvocationError('Region   ""{0}""   is   not   valid.'.format(region)) 
      conn = session.client(module) 
      if (conn is None): 
         raise SaltInvocationError('Region   ""{0}""   is   not   valid.'.format(region)) 
   except boto.exception.NoAuthHandlerFound: 
      raise SaltInvocationError('No   authentication   credentials   found   when   attempting   to   make   boto   {0}   connection   to   region   ""{1}"".'.format(service, region)) 
   __context__[cxkey] = conn 
   return conn"," 'Return a connection to the service 
 :param service: The service to connect to 
 :param module: The module to connect to 
 :param region: The region to connect to 
 :param key: The access key to use 
 :param keyid: The access key id to use 
 :param profile: The profile to use 
 :return: A connection to the service'","'Return a boto connection for the service. 
 .. code-block:: python 
 conn = __utils__[\'boto.get_connection\'](\'ec2\', profile=\'custom_profile\')'"
"def trigger(registry, xml_parent, data): 
    tconfig = XML.SubElement(xml_parent, 'hudson.tasks.BuildTrigger') 
   childProjects = XML.SubElement(tconfig, 'childProjects') 
   childProjects.text = data['project'] 
   tthreshold = XML.SubElement(tconfig, 'threshold') 
   threshold = data.get('threshold', 'SUCCESS') 
   supported_thresholds = ['SUCCESS', 'UNSTABLE', 'FAILURE'] 
   if (threshold not in supported_thresholds): 
      raise JenkinsJobsException(('threshold   must   be   one   of   %s' % ',   '.join(supported_thresholds))) 
   tname = XML.SubElement(tthreshold, 'name') 
   tname.text = hudson_model.THRESHOLDS[threshold]['name'] 
   tordinal = XML.SubElement(tthreshold, 'ordinal') 
   tordinal.text = hudson_model.THRESHOLDS[threshold]['ordinal'] 
   tcolor = XML.SubElement(tthreshold, 'color') 
   tcolor.text = hudson_model.THRESHOLDS[threshold]['color']", 'Create a trigger element in the XML tree.',"'yaml: trigger 
 Trigger non-parametrised builds of other jobs. 
 :arg str project: name of the job to trigger 
 :arg str threshold: when to trigger the other job (default \'SUCCESS\'), 
 alternatives: SUCCESS, UNSTABLE, FAILURE 
 Example: 
 .. literalinclude:: /../../tests/publishers/fixtures/trigger_success.yaml 
 :language: yaml'"
"def logger(_modem, message_, type_): 
    pass"," 'Logs the given message to the modem\'s logger. 
 :param _modem: The Modem object. 
 :param message_: The message to log. 
 :param type_: The type of message to log. 
 :return: None'",'Supress all output from pySerial and gsmmodem'
"def read_double(fid): 
    return _unpack_simple(fid, '>f8', np.float64)", 'Read a double from a file.','Read 64bit float from bti file.'
"def getToothProfileCylinder(derivation, pitchRadius, teeth): 
    toothProfileHalfCylinder = getToothProfileHalfCylinder(derivation, pitchRadius) 
   toothProfileHalfCylinder = getThicknessMultipliedPath(toothProfileHalfCylinder, derivation.toothThicknessMultiplier) 
   toothProfileHalf = [] 
   innerRadius = (pitchRadius - derivation.dedendum) 
   for point in toothProfileHalfCylinder: 
      if (abs(point) >= innerRadius): 
         toothProfileHalf.append(point) 
   return getToothProfileCylinderByProfile(derivation, pitchRadius, teeth, toothProfileHalf)"," 'Get the tooth profile for a given pitch radius and teeth. 
 This function returns the tooth profile for a given pitch radius and teeth. 
 :param derivation: The derivation of the gear. 
 :type derivation: Derivation 
 :param pitchRadius: The pitch radius of the gear. 
 :type pitchRadius: float 
 :param teeth: The number of teeth. 
 :type teeth: int 
 :return: The tooth profile. 
 :rtype: list of floats'",'Get profile for one tooth of a cylindrical gear.'
"def run_simple(hostname, port, application, use_reloader=False, use_debugger=False, use_evalex=True, extra_files=None, reloader_interval=1, reloader_type='auto', threaded=False, processes=1, request_handler=None, static_files=None, passthrough_errors=False, ssl_context=None): 
    if use_debugger: 
      from werkzeug.debug import DebuggedApplication 
      application = DebuggedApplication(application, use_evalex) 
   if static_files: 
      from werkzeug.wsgi import SharedDataMiddleware 
      application = SharedDataMiddleware(application, static_files) 
   def log_startup(sock): 
      display_hostname = (((hostname not in ('', '*')) and hostname) or 'localhost') 
      if (':' in display_hostname): 
         display_hostname = ('[%s]' % display_hostname) 
      quit_msg = '(Press   CTRL+C   to   quit)' 
      port = sock.getsockname()[1] 
      _log('info', '   *   Running   on   %s://%s:%d/   %s', (((ssl_context is None) and 'http') or 'https'), display_hostname, port, quit_msg) 
   def inner(): 
      try: 
         fd = int(os.environ['WERKZEUG_SERVER_FD']) 
      except (LookupError, ValueError): 
         fd = None 
      srv = make_server(hostname, port, application, threaded, processes, request_handler, passthrough_errors, ssl_context, fd=fd) 
      if (fd is None): 
         log_startup(srv.socket) 
      srv.serve_forever() 
   if use_reloader: 
      if (os.environ.get('WERKZEUG_RUN_MAIN') != 'true'): 
         if ((port == 0) and (not can_open_by_fd)): 
            raise ValueError('Cannot   bind   to   a   random   port   with   enabled   reloader   if   the   Python   interpreter   does   not   support   socket   opening   by   fd.') 
         address_family = select_ip_version(hostname, port) 
         s = socket.socket(address_family, socket.SOCK_STREAM) 
         s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) 
         s.bind((hostname, port)) 
         if hasattr(s, 'set_inheritable'): 
            s.set_inheritable(True) 
         if can_open_by_fd: 
            os.environ['WERKZEUG_SERVER_FD'] = str(s.fileno()) 
            s.listen(LISTEN_QUEUE) 
            log_startup(s) 
         else: 
            s.close() 
      from ._reloader import run_with_reloader 
      run_with_reloader(inner, extra_files, reloader_interval, reloader_type) 
   else: 
      inner()"," 'Run a Werkzeug server. 
 :param hostname: Hostname to bind to. 
 :param port: Port to bind to. 
 :param application: Application to run. 
 :param use_reloader: Whether to use a reloader. 
 :param use_debugger: Whether to use the debugger. 
 :param use_evalex: Whether to use the evallex plugin. 
 :param extra_files: Files to include in the reloader. 
 :param reloader_interval: Interval between reloads. 
 :param reloader_type: Type of reloader to use. 
 :param threaded: Whether to use threaded mode. 
 :param processes: Number of processes to use. 
 :param request_handler: Request handler to use. 
 :param static_files: Static files to include in the reloader. 
 :param passthrough_errors: Whether to pass errors through the reloader. 
 :param ssl_context: SSL context to use. 
 :return: None. 
 :raises: ValueError if the Python interpreter does not support socket opening by fd. ","'Start a WSGI application. Optional features include a reloader, 
 multithreading and fork support. 
 This function has a command-line interface too:: 
 python -m werkzeug.serving --help 
 .. versionadded:: 0.5 
 `static_files` was added to simplify serving of static files as well 
 as `passthrough_errors`. 
 .. versionadded:: 0.6 
 support for SSL was added. 
 .. versionadded:: 0.8 
 Added support for automatically loading a SSL context from certificate 
 file and private key. 
 .. versionadded:: 0.9 
 Added command-line interface. 
 .. versionadded:: 0.10 
 Improved the reloader and added support for changing the backend 
 through the `reloader_type` parameter.  See :ref:`reloader` 
 for more information. 
 :param hostname: The host for the application.  eg: ``\'localhost\'`` 
 :param port: The port for the server.  eg: ``8080`` 
 :param application: the WSGI application to execute 
 :param use_reloader: should the server automatically restart the python 
 process if modules were changed? 
 :param use_debugger: should the werkzeug debugging system be used? 
 :param use_evalex: should the exception evaluation feature be enabled? 
 :param extra_files: a list of files the reloader should watch 
 additionally to the modules.  For example configuration 
 files. 
 :param reloader_interval: the interval for the reloader in seconds. 
 :param reloader_type: the type of reloader to use.  The default is 
 auto detection.  Valid values are ``\'stat\'`` and 
 ``\'watchdog\'``. See :ref:`reloader` for more 
 information. 
 :param threaded: should the process handle each request in a separate 
 thread? 
 :param processes: if greater than 1 then handle each request in a new process 
 up to this maximum number of concurrent processes. 
 :param request_handler: optional parameter that can be used to replace 
 the default one.  You can use this to replace it 
 with a different 
 :class:`~BaseHTTPServer.BaseHTTPRequestHandler` 
 subclass. 
 :param static_files: a dict of paths for static files.  This works exactly 
 like :class:`SharedDataMiddleware`, it\'s actually 
 just wrapping the application in that middleware before 
 serving. 
 :param passthrough_errors: set this to `True` to disable the error catching. 
 This means that the server will die on errors but 
 it can be useful to hook debuggers in (pdb etc.) 
 :param ssl_context: an SSL context for the connection. Either an 
 :class:`ssl.SSLContext`, a tuple in the form 
 ``(cert_file, pkey_file)``, the string ``\'adhoc\'`` if 
 the server should automatically create one, or ``None`` 
 to disable SSL (which is the default).'"
"def summary_table(res, alpha=0.05): 
    from scipy import stats 
   from statsmodels.sandbox.regression.predstd import wls_prediction_std 
   infl = OLSInfluence(res) 
   predict_mean_se = np.sqrt((infl.hat_matrix_diag * res.mse_resid)) 
   tppf = stats.t.isf((alpha / 2.0), res.df_resid) 
   predict_mean_ci = np.column_stack([(res.fittedvalues - (tppf * predict_mean_se)), (res.fittedvalues + (tppf * predict_mean_se))]) 
   (predict_se, predict_ci_low, predict_ci_upp) = wls_prediction_std(res) 
   predict_ci = np.column_stack((predict_ci_low, predict_ci_upp)) 
   resid_se = np.sqrt((res.mse_resid * (1 - infl.hat_matrix_diag))) 
   table_sm = np.column_stack([(np.arange(res.nobs) + 1), res.model.endog, res.fittedvalues, predict_mean_se, predict_mean_ci[:, 0], predict_mean_ci[:, 1], predict_ci[:, 0], predict_ci[:, 1], res.resid, resid_se, infl.resid_studentized_internal, infl.cooks_distance[0]]) 
   data = table_sm 
   ss2 = ['Obs', 'Dep   Var\nPopulation', 'Predicted\nValue', 'Std   Error\nMean   Predict', 'Mean   ci\n95%   low', 'Mean   ci\n95%   upp', 'Predict   ci\n95%   low', 'Predict   ci\n95%   upp', 'Residual', 'Std   Error\nResidual', 'Student\nResidual', ""Cook's\nD""] 
   colnames = ss2 
   from statsmodels.iolib.table import SimpleTable, default_html_fmt 
   from statsmodels.iolib.tableformatting import fmt_base 
   from copy import deepcopy 
   fmt = deepcopy(fmt_base) 
   fmt_html = deepcopy(default_html_fmt) 
   fmt['data_fmts'] = (['%4d'] + (['%6.3f'] * (data.shape[1] - 1))) 
   st = SimpleTable(data, headers=colnames, txt_fmt=fmt, html_fmt=fmt_html) 
   return (st, data, ss2)"," 'Generate a table of summary statistics for the regression results. 
 Parameters 
 res : Result object 
 The results object from the regression. 
 alpha : float, optional 
 The significance level for the tests. 
 Returns 
 table : Table object 
 A table of summary statistics for the regression results. 
 data : array 
 The data used to generate the table. 
 ss2 : list 
 A list of strings containing the names of the columns in the table. 
 Notes 
 The table contains the following columns: 
 - Obs : the number of observations 
 - Dep Var : the number of independent variables 
 - Predicted Value : the predicted value 
 - Std Error Mean Predict : the standard error of the mean of the predicted values 
 - Mean Predict : the mean of the predicted values 
 - Mean Predict 95% CI Low : the lower 95% confidence interval of the mean of the predicted values 
 - Mean Predict 95% CI High : the upper 95% confidence interval of the mean of the predicted values 
 - Predict 95% CI Low : the lower 95%","'generate summary table of outlier and influence similar to SAS 
 Parameters 
 alpha : float 
 significance level for confidence interval 
 Returns 
 st : SimpleTable instance 
 table with results that can be printed 
 data : ndarray 
 calculated measures and statistics for the table 
 ss2 : list of strings 
 column_names for table (Note: rows of table are observations)'"
"def get_receptive_field(layers, img_size): 
    receptive_field = np.zeros((len(layers), 2)) 
   conv_mode = True 
   first_conv_layer = True 
   expon = np.ones((1, 2)) 
   for (i, layer) in enumerate(layers[1:]): 
      j = (i + 1) 
      if (not conv_mode): 
         receptive_field[j] = img_size 
         continue 
      if is_conv2d(layer): 
         if (not first_conv_layer): 
            last_field = receptive_field[i] 
            new_field = (last_field + (expon * (np.array(layer.filter_size) - 1))) 
            receptive_field[j] = new_field 
         else: 
            receptive_field[j] = layer.filter_size 
            first_conv_layer = False 
      elif is_maxpool2d(layer): 
         receptive_field[j] = receptive_field[i] 
         expon *= np.array(layer.pool_size) 
      else: 
         conv_mode = False 
         receptive_field[j] = img_size 
   receptive_field[0] = img_size 
   return receptive_field"," 'Returns the receptive field of a network. 
 Parameters 
 layers : list 
 The list of layers in the network. 
 img_size : int 
 The size of the image. 
 Returns 
 receptive_field : list 
 The receptive field of the network. 
 Examples 
 >>> layers = [nn.Conv2d(3, 32, 3, 1), 
 ...           nn.MaxPool2d(2), 
 ...           nn.Conv2d(32, 64, 3, 1), 
 ...           nn.MaxPool2d(2), 
 ...           nn.Conv2d(64, 128, 3, 1), 
 ...           nn.MaxPool2d(2), 
 ...           nn.Conv2d(128, 128, 3, 1), 
 ...           nn.MaxPool2d(2), 
 ...           nn.Conv2d(128, 10, 1, ","'Get the real filter sizes of each layer involved in 
 convoluation. See Xudong Cao: 
 https://www.kaggle.com/c/datasciencebowl/forums/t/13166/happy-lantern-festival-report-and-code 
 This does not yet take into consideration feature pooling, 
 padding, striding and similar gimmicks.'"
"def clear_all_actions(): 
    global _populated 
   _all_actions.clear() 
   _top_level_ids.clear() 
   _populated = False"," 'Clear all actions and their ids. 
 This is used to reset the state of the actions system.'","'Clear all registered actions. 
 This method is really only intended to be used by unit tests. We might be 
 able to remove this hack once we convert to djblets.registries. 
 Warning: 
 This will clear **all** actions, even if they were registered in 
 separate extensions.'"
"def timesince(d=None, now=None, abbreviate=False, separator=','): 
    if abbreviate: 
      chunks = (((((60 * 60) * 24) * 365), (lambda n: 'y')), ((((60 * 60) * 24) * 30), (lambda n: 'm')), ((((60 * 60) * 24) * 7), (lambda n: 'w')), (((60 * 60) * 24), (lambda n: 'd')), ((60 * 60), (lambda n: 'h')), (60, (lambda n: 'm')), (1, (lambda n: 's'))) 
   else: 
      chunks = (((((60 * 60) * 24) * 365), (lambda n: ungettext('year', 'years', n))), ((((60 * 60) * 24) * 30), (lambda n: ungettext('month', 'months', n))), ((((60 * 60) * 24) * 7), (lambda n: ungettext('week', 'weeks', n))), (((60 * 60) * 24), (lambda n: ungettext('day', 'days', n))), ((60 * 60), (lambda n: ungettext('hour', 'hours', n))), (60, (lambda n: ungettext('minute', 'minutes', n))), (1, (lambda n: ungettext('second', 'seconds', n)))) 
   if (not isinstance(d, datetime.datetime)): 
      d = datetime.datetime(d.year, d.month, d.day) 
   if (now and (not isinstance(now, datetime.datetime))): 
      now = datetime.datetime(now.year, now.month, now.day) 
   if (not now): 
      if d.tzinfo: 
         now = datetime.datetime.now(LocalTimezone(d)) 
      else: 
         now = datetime.datetime.now() 
   delta = (now - (d - datetime.timedelta(0, 0, d.microsecond))) 
   since = ((((delta.days * 24) * 60) * 60) + delta.seconds) 
   if (since <= 0): 
      if abbreviate: 
         return (u'0' + ugettext('s')) 
      else: 
         return (u'0   ' + ugettext('seconds')) 
   for (i, (seconds, name)) in enumerate(chunks): 
      count = (since // seconds) 
      if (count != 0): 
         break 
   if abbreviate: 
      s = (ugettext('%(number)d%(type)s') % {'number': count, 'type': name(count)}) 
   else: 
      s = (ugettext('%(number)d   %(type)s') % {'number': count, 'type': name(count)}) 
   if ((i + 1) < len(chunks)): 
      (seconds2, name2) = chunks[(i + 1)] 
      count2 = ((since - (seconds * count)) // seconds2) 
      if (count2 != 0): 
         if abbreviate: 
            s += (ugettext('%(separator)s   %(number)d%(type)s') % {'separator': separator, 'number': count2, 'type': name2(count2)}) 
         else: 
            s += (ugettext('%(separator)s   %(number)d   %(type)s') % {'separator': separator, 'number': count2, 'type': name2(count2)}) 
   return s"," 'Return the time since a given date in the past. 
 :param d: date to compare against 
 :param now: datetime.datetime object to compare against 
 :param abbreviate: boolean to indicate if the output should be 
 abbreviated 
 :param separator: string to use as a separator between units 
 :returns: string representing the time since the given date 
 :rtype: str'","'Takes two datetime objects and returns the time between d and now 
 as a nicely formatted string, e.g. ""10 minutes"".  If d occurs after now, 
 then ""0 seconds"" is returned. If abbreviate is True, it truncates values to, 
 for example, ""10m"" or ""4m 30s"". Alternately it can take a second value 
 and return the proper count. 
 Units used are years, months, weeks, days, hours, minutes, and seconds. 
 Microseconds are ignored.  Up to two adjacent units will be 
 displayed.  For example, ""2 weeks, 3 days"" and ""1 year, 3 months"" are 
 possible outputs, but ""2 weeks, 3 hours"" and ""1 year, 5 days"" are not. 
 Adapted from the timesince filter in Django: 
 http://docs.djangoproject.com/en/dev/ref/templates/builtins/#timesince'"
"def _fill_cdata(cls): 
    funcs = {} 
   for (key, name) in [('b', 'char'), ('h', 'short'), ('i', 'int'), ('q', 'longlong')]: 
      for (echar, esuffix) in [('<', 'le'), ('>', 'be')]: 
         esuffix = ('_' + esuffix) 
         for unsigned in [True, False]: 
            s = struct.Struct((echar + (key.upper() if unsigned else key))) 
            get_wrapper = (lambda f: (lambda *a, **k: f(*a, **k)[0])) 
            unpack = get_wrapper(s.unpack) 
            unpack_from = get_wrapper(s.unpack_from) 
            def get_unpack_from(s): 
               def unpack_from(data, offset=0): 
                  return (s.unpack_from(data, offset)[0], (offset + s.size)) 
               return unpack_from 
            unpack_from = get_unpack_from(s) 
            pack = s.pack 
            prefix = ('u' if unsigned else '') 
            if (s.size == 1): 
               esuffix = '' 
            bits = str((s.size * 8)) 
            funcs[('%s%s%s' % (prefix, name, esuffix))] = unpack 
            funcs[('%sint%s%s' % (prefix, bits, esuffix))] = unpack 
            funcs[('%s%s%s_from' % (prefix, name, esuffix))] = unpack_from 
            funcs[('%sint%s%s_from' % (prefix, bits, esuffix))] = unpack_from 
            funcs[('to_%s%s%s' % (prefix, name, esuffix))] = pack 
            funcs[('to_%sint%s%s' % (prefix, bits, esuffix))] = pack 
   for (key, func) in iteritems(funcs): 
      setattr(cls, key, staticmethod(func))"," 'Fill cdata methods in a class with the appropriate unpack/pack functions. 
 This is done by looking at the struct.Struct object and finding the unpack/pack 
 functions that are appropriate for the data type. 
 :param cls: The class to fill.'",'Add struct pack/unpack functions'
"def get_user_unique_id_and_display_name(request, mapped_properties): 
    user = mapped_properties['user'] 
   user_id = user.get('id') 
   user_name = (user.get('name') or request.remote_user) 
   if (not any([user_id, user_name])): 
      msg = _('Could   not   map   user   while   setting   ephemeral   user   identity.   Either   mapping   rules   must   specify   user   id/name   or   REMOTE_USER   environment   variable   must   be   set.') 
      raise exception.Unauthorized(msg) 
   elif (not user_name): 
      user['name'] = user_id 
   elif (not user_id): 
      user_id = user_name 
   user['id'] = parse.quote(user_id) 
   return (user['id'], user['name'])"," 'Returns user id and name that will be used for ephemeral user identity. 
 :param request: Request object 
 :param mapped_properties: Dictionary of properties that are mapped from 
 :class:`~sqlalchemy.orm.exc.NoReferencedTable` 
 :returns: Tuple of user id and name 
 :raises: :class:`~sqlalchemy.orm.exc.NoReferencedTable` 
 :raises: :class:`~sqlalchemy.orm.exc.NoReferencedColumn`'","'Setup federated username. 
 Function covers all the cases for properly setting user id, a primary 
 identifier for identity objects. Initial version of the mapping engine 
 assumed user is identified by ``name`` and his ``id`` is built from the 
 name. We, however need to be able to accept local rules that identify user 
 by either id or name/domain. 
 The following use-cases are covered: 
 1) If neither user_name nor user_id is set raise exception.Unauthorized 
 2) If user_id is set and user_name not, set user_name equal to user_id 
 3) If user_id is not set and user_name is, set user_id as url safe version 
 of user_name. 
 :param request: current request object 
 :param mapped_properties: Properties issued by a RuleProcessor. 
 :type: dictionary 
 :raises keystone.exception.Unauthorized: If neither `user_name` nor 
 `user_id` is set. 
 :returns: tuple with user identification 
 :rtype: tuple'"
"def disable(name, lbn, target, profile='default', tgt_type='glob', expr_form=None): 
    if (expr_form is not None): 
      salt.utils.warn_until('Fluorine', ""the   target   type   should   be   passed   using   the   'tgt_type'   argument   instead   of   'expr_form'.   Support   for   using   'expr_form'   will   be   removed   in   Salt   Fluorine."") 
      tgt_type = expr_form 
   return _talk2modjk(name, lbn, target, 'worker_disable', profile, tgt_type)"," 'Disable a worker. 
 This is a low-level function that should not be called directly. 
 Instead, use the salt.master.disable() function. 
 :param name: The name of the worker. 
 :param lbn: The logical block number of the worker. 
 :param target: The target of the worker. 
 :param profile: The profile to use for the worker. 
 :param tgt_type: The type of target to use for the worker. 
 :param expr_form: The expression form to use for the target. 
 :return: The response from the master. 
 :rtype: dict'","'.. versionchanged:: Nitrogen 
 The ``expr_form`` argument has been renamed to ``tgt_type``, earlier 
 releases must use ``expr_form``. 
 Disable the named worker from the lbn load balancers at the targeted 
 minions. The worker will get traffic only for current sessions and won\'t 
 get new ones. 
 Example: 
 .. code-block:: yaml 
 disable-before-deploy: 
 modjk_worker.disable: 
 - name: {{ grains[\'id\'] }} 
 - lbn: application 
 - target: \'roles:balancer\' 
 - tgt_type: grain'"
"def swap_inf_nan(val): 
    if isinstance(val, string_types): 
      return val 
   elif isinstance(val, collections.Sequence): 
      return [swap_inf_nan(v) for v in val] 
   elif isinstance(val, collections.Mapping): 
      return dict([(swap_inf_nan(k), swap_inf_nan(v)) for (k, v) in iteritems(val)]) 
   elif isinstance(val, float): 
      if math.isnan(val): 
         return '__NaN__' 
      elif (val == float('inf')): 
         return '__Infinity__' 
      elif (val == float('-inf')): 
         return '__-Infinity__' 
      else: 
         return val 
   else: 
      return val"," 'Convert inf and NaN to __Infinity__ and __NaN__, respectively.'","'This takes an arbitrary object and preps it for jsonifying safely, templating Inf/NaN.'"
"def get_job_count_by_state(request, username): 
    res = {'completed': 0, 'running': 0, 'failed': 0, 'killed': 0, 'all': 0} 
   jobcounts = request.jt.get_job_count_by_user(username) 
   res['completed'] = jobcounts.nSucceeded 
   res['running'] = (jobcounts.nPrep + jobcounts.nRunning) 
   res['failed'] = jobcounts.nFailed 
   res['killed'] = jobcounts.nKilled 
   res['all'] = (((res['completed'] + res['running']) + res['failed']) + res['killed']) 
   return res", 'Get the job count by state for a given user.',"'Returns the number of comlpeted, running, and failed jobs for a user.'"
"def filer_file_from_upload(request, path, upload_data, sha1=None): 
    return _filer_file_from_upload(model=File, request=request, path=path, upload_data=upload_data, sha1=sha1)", 'Return a File object from an uploaded file.',"'Create a filer.models.filemodels.File from an upload (UploadedFile or such). 
 If the `sha1` parameter is passed and a file with said SHA1 is found, it will be returned instead. 
 :param request: Request, to figure out the owner for this file 
 :type request: django.http.request.HttpRequest|None 
 :param path: Pathname string (see `filer_folder_from_path`) or a Filer Folder. 
 :type path: basestring|filer.models.Folder 
 :param upload_data: Upload data 
 :type upload_data: django.core.files.base.File 
 :param sha1: SHA1 checksum. If given and a matching `model` with the SHA1 is found, it is returned instead. 
 :type sha1: basestring 
 :rtype: filer.models.filemodels.File'"
"def last_updated(document): 
    if (config.LAST_UPDATED in document): 
      return document[config.LAST_UPDATED].replace(tzinfo=None) 
   else: 
      return epoch()"," 'Returns the last updated date of the document. 
 :param document: The document. 
 :type document: dict 
 :return: The last updated date of the document. 
 :rtype: datetime.datetime'","'Fixes document\'s LAST_UPDATED field value. Flask-PyMongo returns 
 timezone-aware values while stdlib datetime values are timezone-naive. 
 Comparisons between the two would fail. 
 If LAST_UPDATE is missing we assume that it has been created outside of the 
 API context and inject a default value, to allow for proper computing of 
 Last-Modified header tag. By design all documents return a LAST_UPDATED 
 (and we don\'t want to break existing clients). 
 :param document: the document to be processed. 
 .. versionchanged:: 0.1.0 
 Moved to common.py and renamed as public, so it can also be used by edit 
 methods (via get_document()). 
 .. versionadded:: 0.0.5'"
"def stub_out_registry_image_update(stubs): 
    test_stubs.stub_out_registry_server(stubs) 
   def fake_image_update(ctx, image_id, values, purge_props=False): 
      return {'properties': {}} 
   stubs.Set(db_api, 'image_update', fake_image_update)"," 'Stub out the image_update function of the db API. 
 This stubs out the image_update function of the db API to ensure that the 
 image_update function does not get called during the test. 
 :param stubs: The stubs object to use for the stub. 
 :return: The stubs object that was passed in.'","'Stubs an image update on the registry. 
 :param stubs: Set of stubout stubs'"
"def RemoveFlags(flag_values=FLAGS): 
    for flag_name in NamesOfDefinedFlags(): 
      module_bar.RemoveOneFlag(flag_name, flag_values=flag_values) 
   module_bar.RemoveFlags(flag_values=flag_values)"," 'Removes a flag from the module_bar. 
 :param flag_values: The flag values to remove. 
 :type flag_values: dict 
 :return: None'",'Deletes the flag definitions done by the above DefineFlags().'
"@require_context 
 def volume_get_all_by_group(context, group_id, filters=None): 
    query = _volume_get_query(context).filter_by(consistencygroup_id=group_id) 
   if filters: 
      query = _process_volume_filters(query, filters) 
      if (query is None): 
         return [] 
   return query.all()"," 'Get all volumes in a consistency group. 
 :param context: context to query under 
 :param group_id: id of consistency group 
 :param filters: filters to apply to query 
 :returns: list of volumes 
 :raises: NotFound if group not found'","'Retrieves all volumes associated with the group_id. 
 :param context: context to query under 
 :param group_id: consistency group ID for all volumes being retrieved 
 :param filters: dictionary of filters; values that are in lists, tuples, 
 or sets cause an \'IN\' operation, while exact matching 
 is used for other values, see _process_volume_filters 
 function for more information 
 :returns: list of matching volumes'"
"def XmlToString(content, encoding='utf-8', pretty=False): 
    xml_parts = [('<?xml   version=""1.0""   encoding=""%s""?>' % encoding)] 
   if pretty: 
      xml_parts.append('\n') 
   _ConstructContentList(xml_parts, content, pretty) 
   return ''.join(xml_parts)"," 'Converts the given XML content into a string. 
 :param content: The content to convert. 
 :type content: str 
 :param encoding: The encoding to use. 
 :type encoding: str 
 :param pretty: If true, the output will be pretty-printed. 
 :type pretty: bool 
 :return: The converted content as a string. 
 :rtype: str'","'Writes the XML content to disk, touching the file only if it has changed. 
 Visual Studio files have a lot of pre-defined structures.  This function makes 
 it easy to represent these structures as Python data structures, instead of 
 having to create a lot of function calls. 
 Each XML element of the content is represented as a list composed of: 
 1. The name of the element, a string, 
 2. The attributes of the element, a dictionary (optional), and 
 3+. The content of the element, if any.  Strings are simple text nodes and 
 lists are child elements. 
 Example 1: 
 <test/> 
 becomes 
 [\'test\'] 
 Example 2: 
 <myelement a=\'value1\' b=\'value2\'> 
 <childtype>This is</childtype> 
 <childtype>it!</childtype> 
 </myelement> 
 becomes 
 [\'myelement\', {\'a\':\'value1\', \'b\':\'value2\'}, 
 [\'childtype\', \'This is\'], 
 [\'childtype\', \'it!\'], 
 Args: 
 content:  The structured content to be converted. 
 encoding: The encoding to report on the first XML line. 
 pretty: True if we want pretty printing with indents and new lines. 
 Returns: 
 The XML content as a string.'"
"def add_email_to_campaign(survey, email): 
    token = settings.SURVEYGIZMO_API_TOKEN 
   secret = settings.SURVEYGIZMO_API_TOKEN_SECRET 
   if ((token is None) or (secret is None)): 
      return 
   survey_id = SURVEYS[survey]['exit_survey_id'] 
   campaign_id = SURVEYS[survey]['exit_survey_campaign_id'] 
   try: 
      requests.put('https://restapi.surveygizmo.com/v2/survey/{survey}/surveycampaign/{campaign}/contact?semailaddress={email}&api_token={token}&api_token_secret={secret}'.format(survey=survey_id, campaign=campaign_id, email=email, token=token, secret=secret), timeout=30) 
   except requests.exceptions.Timeout: 
      print ('Timedout   adding:   %s' % email)"," 'Adds the given email to the given survey\'s campaign. 
 :param survey: The survey to add the email to. 
 :param email: The email to add. 
 :return: None'",'Add email to the exit survey campaign.'
"def test_lda_empty_docs(): 
    Z = np.zeros((5, 4)) 
   for X in [Z, csr_matrix(Z)]: 
      lda = LatentDirichletAllocation(max_iter=750).fit(X) 
      assert_almost_equal(lda.components_.sum(axis=0), np.ones(lda.components_.shape[1]))", 'Test that the LDA component counts are all 1 when the data is empty.','Test LDA on empty document (all-zero rows).'
"def _diff(state_data, resource_object): 
    objects_differ = None 
   for (k, v) in state_data['service'].items(): 
      if (k == 'escalation_policy_id'): 
         resource_value = resource_object['escalation_policy']['id'] 
      elif (k == 'service_key'): 
         resource_value = resource_object['service_key'] 
         if ('@' in resource_value): 
            resource_value = resource_value[0:resource_value.find('@')] 
      else: 
         resource_value = resource_object[k] 
      if (v != resource_value): 
         objects_differ = '{0}   {1}   {2}'.format(k, v, resource_value) 
         break 
   if objects_differ: 
      return state_data 
   else: 
      return {}", 'Returns a dictionary of objects that differ from the resource object.',"'helper method to compare salt state info with the PagerDuty API json structure, 
 and determine if we need to update. 
 returns the dict to pass to the PD API to perform the update, or empty dict if no update.'"
"@utils.arg('server', metavar='<server>', help=_('Name   or   ID   of   server.')) 
 def do_resize_confirm(cs, args): 
    _find_server(cs, args.server).confirm_resize()", 'Confirm a resize operation.','Confirm a previous resize.'
"def op_abs_tmul(lin_op, value): 
    if (lin_op.type is lo.NEG): 
      result = value 
   elif (lin_op.type is lo.MUL): 
      coeff = mul(lin_op.data, {}, True) 
      if np.isscalar(coeff): 
         result = (coeff * value) 
      else: 
         result = (coeff.T * value) 
   elif (lin_op.type is lo.DIV): 
      divisor = mul(lin_op.data, {}, True) 
      result = (value / divisor) 
   elif (lin_op.type is lo.CONV): 
      result = conv_mul(lin_op, value, True, True) 
   else: 
      result = op_tmul(lin_op, value) 
   return result"," 'Applies the linear operator to the value. 
 Parameters 
 lin_op : LinearOperator 
 The linear operator to apply. 
 value : float 
 The value to apply the operator to. 
 Returns 
 result : float 
 The result of the application of the linear operator. 
 Examples 
 >>> from sympy.physics.linear import op_abs_tmul 
 >>> from sympy.physics.linear import LinOp 
 >>> lin_op = LinOp([1, 2, 3]) 
 >>> op_abs_tmul(lin_op, 3) 
 1 
 >>> lin_op = LinOp([1, -2, 3]) 
 >>> op_abs_tmul(lin_op, 3) 
 3 
 >>> lin_op = LinOp([1, 2, -3]) 
 >>> op_abs_tmul(lin_op, 3) 
 1 
 >>> lin_op = LinOp([1, 2, 3]) 
 >>> op_abs_tmul(lin_op, 0) 
 0 ","'Applies the linear operator |A.T| to the arguments. 
 Parameters 
 lin_op : LinOp 
 A linear operator. 
 value : NumPy matrix 
 A numeric value to apply the operator\'s transpose to. 
 Returns 
 NumPy matrix or SciPy sparse matrix. 
 The result of applying the linear operator.'"
"def html4annotation(htmlpage, baseurl=None, proxy_resources=None): 
    htmlpage = add_tagids(htmlpage) 
   cleaned_html = descriptify(htmlpage, baseurl, proxy=proxy_resources) 
   return cleaned_html"," 'Converts an HTML document to an HTML4 annotation. 
 :param htmlpage: The HTML document to convert. 
 :type htmlpage: str 
 :param baseurl: The base URL for the HTML document. 
 :type baseurl: str 
 :param proxy_resources: A dictionary of proxy resources. 
 :type proxy_resources: dict 
 :return: The HTML4 annotation. 
 :rtype: str'","'Convert the given html document for the annotation UI 
 This adds tags, removes scripts and optionally adds a base url'"
"def run_as_contextmanager(ctx, fn, *arg, **kw): 
    obj = ctx.__enter__() 
   try: 
      result = fn(obj, *arg, **kw) 
      ctx.__exit__(None, None, None) 
      return result 
   except: 
      exc_info = sys.exc_info() 
      raise_ = ctx.__exit__(*exc_info) 
      if (raise_ is None): 
         raise 
      else: 
         return raise_"," 'Run a function in a context manager. 
 This is useful for functions that are called as context managers, 
 such as with the :meth:`with` statement. 
 :param ctx: The context manager to use. 
 :param fn: The function to run. 
 :param arg: Additional arguments to pass to the function. 
 :param kw: Additional keyword arguments to pass to the function. 
 :returns: The return value of the function. 
 :raises: The exception that was raised by the function.'","'Run the given function under the given contextmanager, 
 simulating the behavior of \'with\' to support older 
 Python versions. 
 This is not necessary anymore as we have placed 2.6 
 as minimum Python version, however some tests are still using 
 this structure.'"
"def continued_indentation(logical_line, tokens, indent_level, hang_closing, indent_char, noqa, verbose): 
    first_row = tokens[0][2][0] 
   nrows = ((1 + tokens[(-1)][2][0]) - first_row) 
   if (noqa or (nrows == 1)): 
      return 
   indent_next = logical_line.endswith(':') 
   row = depth = 0 
   valid_hangs = ((4,) if (indent_char != ' DCTB ') else (4, 8)) 
   parens = ([0] * nrows) 
   rel_indent = ([0] * nrows) 
   open_rows = [[0]] 
   hangs = [None] 
   indent_chances = {} 
   last_indent = tokens[0][2] 
   visual_indent = None 
   last_token_multiline = False 
   indent = [last_indent[1]] 
   if (verbose >= 3): 
      print ('>>>   ' + tokens[0][4].rstrip()) 
   for (token_type, text, start, end, line) in tokens: 
      newline = (row < (start[0] - first_row)) 
      if newline: 
         row = (start[0] - first_row) 
         newline = ((not last_token_multiline) and (token_type not in NEWLINE)) 
      if newline: 
         last_indent = start 
         if (verbose >= 3): 
            print ('...   ' + line.rstrip()) 
         rel_indent[row] = (expand_indent(line) - indent_level) 
         close_bracket = ((token_type == tokenize.OP) and (text in ']})')) 
         for open_row in reversed(open_rows[depth]): 
            hang = (rel_indent[row] - rel_indent[open_row]) 
            hanging_indent = (hang in valid_hangs) 
            if hanging_indent: 
               break 
         if hangs[depth]: 
            hanging_indent = (hang == hangs[depth]) 
         visual_indent = ((not close_bracket) and (hang > 0) and indent_chances.get(start[1])) 
         if (close_bracket and indent[depth]): 
            if (start[1] != indent[depth]): 
               (yield (start, 'E124   closing   bracket   does   not   match   visual   indentation')) 
         elif (close_bracket and (not hang)): 
            if hang_closing: 
               (yield (start, 'E133   closing   bracket   is   missing   indentation')) 
         elif (indent[depth] and (start[1] < indent[depth])): 
            if (visual_indent is not True): 
               (yield (start, 'E128   continuation   line   under-indented   for   visual   indent')) 
         elif (hanging_indent or (indent_next and (rel_indent[row] == 8))): 
            if (close_bracket and (not hang_closing)): 
               (yield (start, ""E123   closing   bracket   does   not   match   indentation   of   opening   bracket's   line"")) 
            hangs[depth] = hang 
         elif (visual_indent is True): 
            indent[depth] = start[1] 
         elif (visual_indent in (text, str)): 
            pass 
         else: 
            if (hang <= 0): 
               error = ('E122', 'missing   indentation   or   outdented') 
            elif indent[depth]: 
               error = ('E127', 'over-indented   for   visual   indent') 
            elif ((not close_bracket) and hangs[depth]): 
               error = ('E131', 'unaligned   for   hanging   indent') 
            else: 
               hangs[depth] = hang 
               if (hang > 4): 
                  error = ('E126', 'over-indented   for   hanging   indent') 
               else: 
                  error = ('E121', 'under-indented   for   hanging   indent') 
            (yield (start, ('%s   continuation   line   %s' % error))) 
      if (parens[row] and (token_type not in (tokenize.NL, tokenize.COMMENT)) and (not indent[depth])): 
         indent[depth] = start[1] 
         indent_chances[start[1]] = True 
         if (verbose >= 4): 
            print ('bracket   depth   %s   indent   to   %s' % (depth, start[1])) 
      elif ((token_type in (tokenize.STRING, tokenize.COMMENT)) or (text in ('u', 'ur', 'b', 'br'))): 
         indent_chances[start[1]] = str 
      elif ((not indent_chances) and (not row) and (not depth) and (text == 'if')): 
         indent_chances[(end[1] + 1)] = True 
      elif ((text == ':') and line[end[1]:].isspace()): 
         open_rows[depth].append(row) 
      if (token_type == tokenize.OP): 
         if (text in '([{'): 
            depth += 1 
            indent.append(0) 
            hangs.append(None) 
            if (len(open_rows) == depth): 
               open_rows.append([]) 
            open_rows[depth].append(row) 
            parens[row] += 1 
            if (verbose >= 4): 
               print ('bracket   depth   %s   seen,   col   %s,   visual   min   =   %s' % (depth, start[1], indent[depth])) 
         elif ((text in ')]}') and (depth > 0)): 
            prev_indent = (indent.pop() or last_indent[1]) 
            hangs.pop() 
            for d in range(depth): 
               if (indent[d] > prev_indent): 
                  indent[d] = 0 
            for ind in list(indent_chances): 
               if (ind >= prev_indent): 
                  del indent_chances[ind] 
            del open_rows[(depth + 1):] 
            depth -= 1 
            if depth: 
               indent_chances[indent[depth]] = True 
            for idx in range(row, (-1), (-1)): 
               if parens[idx]: 
                  parens[idx] -= 1 
                  break 
         assert (len(indent) == (depth + 1)) 
         if (start[1] not in indent_chances): 
            indent_chances[start[1]] = text 
      last_token_multiline = (start[0] != end[0]) 
      if last_token_multiline: 
         rel_indent[(end[0] - first_row)] = rel_indent[row] 
   if (indent_next and (expand_indent(line) == (indent_level + 4))): 
      pos = (start[0], (indent[0] + 4)) 
      if visual_indent: 
         code = 'E129   visually   indented   line' 
      else: 
         code = 'E125   continuation   line' 
      (yield (pos, ('%s   with   same   indent   as   next   logical   line' % code)))"," 'Checks for hanging indents, visual indents, and indentation levels. 
 The logic for hanging indents is based on the following rules: 
 1. If a line is indented more than 4 spaces, it is considered hanging. 
 2. If a line is indented more than 4 spaces, and it is the first line of a 
 block, it is considered hanging. 
 3. If a line is indented more than 4 spaces, and it is the last line of a 
 block, it is considered hanging. 
 4. If a line is indented more than 4 spaces, and it is not the first or 
 last line of a block, it is considered hanging if the line above it is 
 indented less than 4 spaces. 
 5. If a line is indented more than 4 spaces, and it is not the first or 
 last line of a block, it is considered hanging if the line above it is 
 indented more than 4 spaces. 
 6. If a line is indented more than 4 spaces, and it is not the first","'Continuation lines indentation. 
 Continuation lines should align wrapped elements either vertically 
 using Python\'s implicit line joining inside parentheses, brackets 
 and braces, or using a hanging indent. 
 When using a hanging indent these considerations should be applied: 
 - there should be no arguments on the first line, and 
 - further indentation should be used to clearly distinguish itself as a 
 continuation line. 
 Okay: a = (\n) 
 E123: a = (\n    ) 
 Okay: a = (\n    42) 
 E121: a = (\n   42) 
 E122: a = (\n42) 
 E123: a = (\n    42\n    ) 
 E124: a = (24,\n     42\n) 
 E125: if (\n    b):\n    pass 
 E126: a = (\n        42) 
 E127: a = (24,\n      42) 
 E128: a = (24,\n    42) 
 E129: if (a or\n    b):\n    pass 
 E131: a = (\n    42\n 24)'"
"def test_prompt_should_ask_and_rm_repo_dir(mocker, tmpdir): 
    mock_read_user = mocker.patch('cookiecutter.vcs.read_user_yes_no', return_value=True, autospec=True) 
   repo_dir = tmpdir.mkdir('repo') 
   vcs.prompt_and_delete_repo(str(repo_dir)) 
   assert mock_read_user.called 
   assert (not repo_dir.exists())"," 'prompt_and_delete_repo should ask the user to confirm and delete the 
 repo directory.'","'In `prompt_and_delete_repo()`, if the user agrees to delete/reclone the 
 repo, the repo should be deleted.'"
"def ipart(x): 
    return np.modf(x)[1]", 'Returns the integer part of x.','Return integer part of given number.'
"def nlmeans_proxy(in_file, settings, snr=None, smask=None, nmask=None, out_file=None): 
    from dipy.denoise.nlmeans import nlmeans 
   from scipy.ndimage.morphology import binary_erosion 
   from scipy import ndimage 
   if (out_file is None): 
      (fname, fext) = op.splitext(op.basename(in_file)) 
      if (fext == u'.gz'): 
         (fname, fext2) = op.splitext(fname) 
         fext = (fext2 + fext) 
      out_file = op.abspath((u'./%s_denoise%s' % (fname, fext))) 
   img = nb.load(in_file) 
   hdr = img.header 
   data = img.get_data() 
   aff = img.affine 
   if (data.ndim < 4): 
      data = data[..., np.newaxis] 
   data = np.nan_to_num(data) 
   if (data.max() < 0.0001): 
      raise RuntimeError(u'There   is   no   signal   in   the   image') 
   df = 1.0 
   if (data.max() < 1000.0): 
      df = (1000.0 / data.max()) 
      data *= df 
   b0 = data[..., 0] 
   if (smask is None): 
      smask = np.zeros_like(b0) 
      smask[(b0 > np.percentile(b0, 85.0))] = 1 
   smask = binary_erosion(smask.astype(np.uint8), iterations=2).astype(np.uint8) 
   if (nmask is None): 
      nmask = np.ones_like(b0, dtype=np.uint8) 
      bmask = settings[u'mask'] 
      if (bmask is None): 
         bmask = np.zeros_like(b0) 
         bmask[(b0 > np.percentile(b0[(b0 > 0)], 10))] = 1 
         (label_im, nb_labels) = ndimage.label(bmask) 
         sizes = ndimage.sum(bmask, label_im, range((nb_labels + 1))) 
         maxidx = np.argmax(sizes) 
         bmask = np.zeros_like(b0, dtype=np.uint8) 
         bmask[(label_im == maxidx)] = 1 
      nmask[(bmask > 0)] = 0 
   else: 
      nmask = np.squeeze(nmask) 
      nmask[(nmask > 0.0)] = 1 
      nmask[(nmask < 1)] = 0 
      nmask = nmask.astype(bool) 
   nmask = binary_erosion(nmask, iterations=1).astype(np.uint8) 
   den = np.zeros_like(data) 
   est_snr = True 
   if (snr is not None): 
      snr = ([snr] * data.shape[(-1)]) 
      est_snr = False 
   else: 
      snr = [] 
   for i in range(data.shape[(-1)]): 
      d = data[..., i] 
      if est_snr: 
         s = np.mean(d[(smask > 0)]) 
         n = np.std(d[(nmask > 0)]) 
         snr.append((s / n)) 
      den[..., i] = nlmeans(d, snr[i], **settings) 
   den = np.squeeze(den) 
   den /= df 
   nb.Nifti1Image(den.astype(hdr.get_data_dtype()), aff, hdr).to_filename(out_file) 
   return (out_file, snr)"," 'Denoise the input image using NLMeans. 
 Parameters 
 in_file : str 
 The input file name. 
 settings : dict 
 The settings for the denoising algorithm. 
 snr : list 
 The signal-to-noise ratio for each voxel. 
 smask : list 
 The mask for each voxel. 
 nmask : list 
 The mask for each voxel. 
 out_file : str 
 The output file name. 
 Returns 
 out_file : str 
 The output file name. 
 snr : list 
 The signal-to-noise ratio for each voxel.'",'Uses non-local means to denoise 4D datasets'
"def get_zones(permanent=True): 
    cmd = '--get-zones' 
   if permanent: 
      cmd += '   --permanent' 
   return __firewall_cmd(cmd).split()", 'Get all firewall zones.',"'Print predefined zones 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' firewalld.get_zones'"
"def p_test(p): 
    p[0] = p[1]"," 'p_test(p) -> p 
 Test a polynomial. 
 Examples 
 >>> from sympy import p_test 
 >>> from sympy.abc import x, y 
 >>> p_test(x**2 + x + 1) 
 x**2 + x + 1 
 >>> p_test(x**2 + x + 1, x) 
 x**2 + x + 1'",'test : comparison'
"def get_terminal_size(): 
    if (sys.version_info >= (3, 3)): 
      import shutil 
      shutil_get_terminal_size = getattr(shutil, 'get_terminal_size', None) 
      if shutil_get_terminal_size: 
         sz = shutil_get_terminal_size() 
         return (sz.columns, sz.lines) 
   if (get_winterm_size is not None): 
      return get_winterm_size() 
   def ioctl_gwinsz(fd): 
      try: 
         import fcntl 
         import termios 
         cr = struct.unpack('hh', fcntl.ioctl(fd, termios.TIOCGWINSZ, '1234')) 
      except Exception: 
         return 
      return cr 
   cr = (ioctl_gwinsz(0) or ioctl_gwinsz(1) or ioctl_gwinsz(2)) 
   if (not cr): 
      try: 
         fd = os.open(os.ctermid(), os.O_RDONLY) 
         try: 
            cr = ioctl_gwinsz(fd) 
         finally: 
            os.close(fd) 
      except Exception: 
         pass 
   if ((not cr) or (not cr[0]) or (not cr[1])): 
      cr = (os.environ.get('LINES', 25), os.environ.get('COLUMNS', DEFAULT_COLUMNS)) 
   return (int(cr[1]), int(cr[0]))"," 'Get the terminal size (in columns and lines) 
 This is a wrapper around the Python 3.3+ shutil.get_terminal_size() 
 function, which is not available on Python 2.7. 
 If the function is not available, fall back to the old-school 
 ioctl(2) method. 
 If the terminal size cannot be determined, fall back to the default 
 of 80x25. 
 :return: Tuple of (columns, lines) 
 :rtype: tuple'","'Returns the current size of the terminal as tuple in the form 
 ``(width, height)`` in columns and rows.'"
"def _get_forward_relationships(opts): 
    forward_relations = OrderedDict() 
   for field in [field for field in opts.fields if (field.serialize and get_remote_field(field))]: 
      forward_relations[field.name] = RelationInfo(model_field=field, related_model=get_related_model(field), to_many=False, to_field=_get_to_field(field), has_through_model=False, reverse=False) 
   for field in [field for field in opts.many_to_many if field.serialize]: 
      forward_relations[field.name] = RelationInfo(model_field=field, related_model=get_related_model(field), to_many=True, to_field=None, has_through_model=(not get_remote_field(field).through._meta.auto_created), reverse=False) 
   return forward_relations"," 'Returns a dictionary of forward relationships. 
 :param opts: The ModelOptions instance. 
 :return: A dictionary of forward relationships.'",'Returns an `OrderedDict` of field names to `RelationInfo`.'
"def get_qos(tenant_id, qos_id): 
    LOG.debug(_('get_qos()   called')) 
   session = db.get_session() 
   try: 
      qos = session.query(l2network_models.QoS).filter_by(tenant_id=tenant_id).filter_by(qos_id=qos_id).one() 
      return qos 
   except exc.NoResultFound: 
      raise c_exc.QosNotFound(qos_id=qos_id, tenant_id=tenant_id)", 'Get QoS by tenant_id and qos_id.','Lists the qos given a tenant_id and qos_id'
"def get_resources_dests(resources_root, rules): 
    def get_rel_path(base, path): 
      base = base.replace(os.path.sep, '/') 
      path = path.replace(os.path.sep, '/') 
      assert path.startswith(base) 
      return path[len(base):].lstrip('/') 
   destinations = {} 
   for (base, suffix, dest) in rules: 
      prefix = os.path.join(resources_root, base) 
      for abs_base in iglob(prefix): 
         abs_glob = os.path.join(abs_base, suffix) 
         for abs_path in iglob(abs_glob): 
            resource_file = get_rel_path(resources_root, abs_path) 
            if (dest is None): 
               destinations.pop(resource_file, None) 
            else: 
               rel_path = get_rel_path(abs_base, abs_path) 
               rel_dest = dest.replace(os.path.sep, '/').rstrip('/') 
               destinations[resource_file] = ((rel_dest + '/') + rel_path) 
   return destinations"," 'Returns a dict of resources and their destinations. 
 The keys are the relative paths of the resources. 
 The values are the absolute paths of the destination files.'",'Find destinations for resources files'
"def cap_alert_is_template(alert_id): 
    if (not alert_id): 
      return False 
   table = current.s3db.cap_alert 
   query = (table.id == alert_id) 
   r = current.db(query).select(table.is_template, limitby=(0, 1)).first() 
   return (r and r.is_template)"," 'Check if the alert is a template. 
 :param int alert_id: alert_id of the alert 
 :return: bool'",'Tell whether an alert entry is a template'
"def add_close_action(widget): 
    return add_action(widget, N_(u'Close...'), widget.close, hotkeys.CLOSE, hotkeys.QUIT)", 'Adds a close action to a widget.','Adds close action and shortcuts to a widget.'
"@task 
 @timed 
 def check_memcache(): 
    if (not is_memcache_running()): 
      msg = colorize('red', 'Memcache   is   not   running   locally.') 
      print msg 
      sys.exit(1)", 'Checks if memcache is running locally.','Check that memcache is running'
"def abstractmethod(funcobj): 
    funcobj.__isabstractmethod__ = True 
   return funcobj"," 'Decorate a method to make it abstract. 
 This decorator does nothing and is only used to mark methods as 
 abstract. 
 Parameters 
 funcobj : function 
 The method to mark as abstract.'","'A decorator indicating abstract methods. 
 Requires that the metaclass is ABCMeta or derived from it.  A 
 class that has a metaclass derived from ABCMeta cannot be 
 instantiated unless all of its abstract methods are overridden. 
 The abstract methods can be called using any of the normal 
 \'super\' call mechanisms. 
 Usage: 
 class C: 
 __metaclass__ = ABCMeta 
 @abstractmethod 
 def my_abstract_method(self, ...):'"
"def validate_filters(where, resource): 
    operators = getattr(app.data, 'operators', set()) 
   allowed = (config.DOMAIN[resource]['allowed_filters'] + list(operators)) 
   def validate_filter(filter): 
      for (key, value) in filter.items(): 
         if (('*' not in allowed) and (key not in allowed)): 
            return (""filter   on   '%s'   not   allowed"" % key) 
         if (key in ('$or', '$and', '$nor')): 
            if (not isinstance(value, list)): 
               return (""operator   '%s'   expects   a   list   of   sub-queries"" % key) 
            for v in value: 
               if (not isinstance(v, dict)): 
                  return (""operator   '%s'   expects   a   list   of   sub-queries"" % key) 
               r = validate_filter(v) 
               if r: 
                  return r 
         elif config.VALIDATE_FILTERS: 
            res_schema = config.DOMAIN[resource]['schema'] 
            if (key not in res_schema): 
               return ""filter   on   '%s'   is   invalid"" 
            else: 
               field_schema = res_schema.get(key) 
               v = Validator({key: field_schema}) 
               if (not v.validate({key: value})): 
                  return ""filter   on   '%s'   is   invalid"" 
               else: 
                  return None 
   if (('*' in allowed) and (not config.VALIDATE_FILTERS)): 
      return None 
   return validate_filter(where)"," 'Validate the filters against the allowed operators. 
 If the \'*\' operator is used, then validation is not performed.'","'Report any filter which is not allowed by  `allowed_filters` 
 :param where: the where clause, as a dict. 
 :param resource: the resource being inspected. 
 .. versionchanged: 0.5 
 If the data layer supports a list of allowed operators, take them 
 into consideration when validating the query string (#388). 
 Recursively validate the whole query string. 
 .. versionadded: 0.0.9'"
"@pytest.mark.network 
 def test_uptodate_editables_flag(script, data): 
    script.pip('install', '-f', data.find_links, '--no-index', 'simple==1.0') 
   result = script.pip('install', '-e', 'git+https://github.com/pypa/pip-test-package.git#egg=pip-test-package') 
   result = script.pip('list', '-f', data.find_links, '--no-index', '--editable', '--uptodate', '--format=legacy') 
   assert ('simple   (1.0)' not in result.stdout), str(result) 
   assert (os.path.join('src', 'pip-test-package') in result.stdout), str(result)", 'Test that --uptodate flag works with editables','test the behavior of --editable --uptodate flag in the list command'
"def _du(real_path): 
    total_size = 0 
   if os.path.isdir(real_path): 
      for (dirpath, dirnames, filenames) in os.walk(real_path): 
         for filename in filenames: 
            total_size += os.path.getsize(os.path.join(dirpath, filename)) 
   else: 
      total_size += os.path.getsize(real_path) 
   return total_size", 'Calculate the total size of a directory.','Get total size of file or files in dir (recursive).'
"def pr_api_url_from_web_url(url): 
    path = '/'.join(map(partial(replace, 'pull', 'pulls'), url_path_parts(url))) 
   return ((API_BASE_URL + REPOS_API_PATH) + path)"," 'Converts a web URL to a PR API URL. 
 :param url: The web URL to convert. 
 :returns: The PR API URL.'","'get the api url for a pull request from the web one. 
 :param unicode url: the web URL of the pull request. 
 :return unicode: the API URL of the same pull request.'"
"def Client(version=None, unstable=False, session=None, **kwargs): 
    if (not session): 
      session = client_session.Session._construct(kwargs) 
   d = discover.Discover(session=session, **kwargs) 
   return d.create_client(version=version, unstable=unstable)"," 'Create a client object. 
 :param version: The version of the client to create. 
 :param unstable: Whether to create an unstable client. 
 :param session: A :class:`~.client.ClientSession` object. 
 :param kwargs: Additional parameters to pass to the :class:`~.client.ClientSession` object. 
 :return: A :class:`~.client.Client` object. 
 :rtype: :class:`~.client.Client` 
 :raises: :class:`~.client.DiscoveryError` if the version is not supported. 
 :raises: :class:`~.client.ClientError` if the session is invalid. 
 :raises: :class:`~.client.ClientError` if the session is not connected. 
 :raises: :class:`~.client.ClientError` if the session is not authenticated. 
 :raises: :class:`~.client.ClientError` if the session is not authorized. 
 :raises: :class:`~.client.ClientError` if the session is not enabled. 
 :","'Factory function to create a new identity service client. 
 The returned client will be either a V3 or V2 client. Check the version 
 using the :py:attr:`~keystoneclient.v3.client.Client.version` property or 
 the instance\'s class (with instanceof). 
 :param tuple version: The required version of the identity API. If 
 specified the client will be selected such that the 
 major version is equivalent and an endpoint provides 
 at least the specified minor version. For example to 
 specify the 3.1 API use ``(3, 1)``. (optional) 
 :param bool unstable: Accept endpoints not marked as \'stable\'. (optional) 
 :param session: A session object to be used for communication. If one is 
 not provided it will be constructed from the provided 
 kwargs. (optional) 
 :type session: keystoneclient.session.Session 
 :param kwargs: Additional arguments are passed through to the client 
 that is being created. 
 :returns: New keystone client object. 
 :rtype: :py:class:`keystoneclient.v3.client.Client` or 
 :py:class:`keystoneclient.v2_0.client.Client` 
 :raises keystoneclient.exceptions.DiscoveryFailure: if the server\'s 
 response is invalid. 
 :raises keystoneclient.exceptions.VersionNotAvailable: if a suitable client 
 cannot be found.'"
"def NullController(*_args, **_kwargs): 
    return None"," 'Returns a NullController. 
 This is useful for testing purposes. 
 :param _args: 
 :param _kwargs: 
 :return: 
 :rtype: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns:",'Nonexistent controller - simply returns None'
"@pytest.mark.django_db 
 def test_social_media_plugin_ordering(): 
    context = get_jinja_context() 
   icon_classes = SocialMediaLinksPlugin.icon_classes 
   link_1_type = 'Facebook' 
   link_1 = {'url': 'http://www.facebook.com', 'ordering': 2} 
   link_2_type = 'Twitter' 
   link_2 = {'url': 'http://www.twitter.com', 'ordering': 1} 
   links = {link_1_type: link_1, link_2_type: link_2} 
   plugin = SocialMediaLinksPlugin({'links': links}) 
   assert (len(plugin.get_links()) == 2) 
   assert (plugin.get_links()[0][2] == link_2['url'])", 'Test that the social media links are ordered correctly.','Test that social media plugin ordering works as expected'
"def parse_backend_conf(backend, **kwargs): 
    conf = settings.CACHES.get(backend, None) 
   if (conf is not None): 
      args = conf.copy() 
      args.update(kwargs) 
      backend = args.pop('BACKEND') 
      location = args.pop('LOCATION', '') 
      return (backend, location, args) 
   else: 
      try: 
         (mod_path, cls_name) = backend.rsplit('.', 1) 
         mod = importlib.import_module(mod_path) 
         backend_cls = getattr(mod, cls_name) 
      except (AttributeError, ImportError, ValueError): 
         raise InvalidCacheBackendError((""Could   not   find   backend   '%s'"" % backend)) 
      location = kwargs.pop('LOCATION', '') 
      return (backend, location, kwargs) 
   raise InvalidCacheBackendError((""Couldn't   find   a   cache   backend   named   '%s'"" % backend))"," 'Parse the given backend and return the backend, the location, and the 
 remaining kwargs. 
 :param backend: the backend to parse. 
 :param kwargs: the remaining keyword arguments to pass to the cache. 
 :return: the backend, the location, and the remaining keyword arguments.'","'Helper function to parse the backend configuration 
 that doesn\'t use the URI notation.'"
"def shell(cmds, env=None, **kwds): 
    sys = kwds.get('sys', _sys) 
   assert (sys is not None) 
   p = shell_process(cmds, env, **kwds) 
   if redirecting_io(sys=sys): 
      redirect_aware_commmunicate(p, sys=sys) 
      exit = p.returncode 
      return exit 
   else: 
      return p.wait()"," 'Execute a command in a shell. 
 This function executes the given command in a shell and returns its 
 exit status. 
 The ``env`` argument can be used to set environment variables for the 
 shell process. 
 The ``shell_process`` function is used to create the process and 
 communicate with it. 
 The ``redirecting_io`` function is used to determine whether or not 
 the output and/or error streams should be redirected to files. 
 The ``redirect_aware_commmunicate`` function is used to redirect 
 output and error streams to files. 
 :param cmds: A list of strings to be executed in the shell. 
 :param env: A dictionary of environment variables to be set. 
 :param **kwds: Additional keyword arguments are passed to 
 ``shell_process``. 
 :return: The exit status of the shell process.'",'Run shell commands with `shell_process` and wait.'
"def normalize_timestamp(timestamp): 
    return Timestamp(timestamp).normal"," 'Normalize a timestamp to the current timezone. 
 :param timestamp: Timestamp to normalize 
 :type timestamp: Timestamp 
 :return: Normalized timestamp 
 :rtype: Timestamp'","'Format a timestamp (string or numeric) into a standardized 
 xxxxxxxxxx.xxxxx (10.5) format. 
 Note that timestamps using values greater than or equal to November 20th, 
 2286 at 17:46 UTC will use 11 digits to represent the number of 
 seconds. 
 :param timestamp: unix timestamp 
 :returns: normalized timestamp as a string'"
"def simple_parse_to_segments(formatted_text): 
    if ('message_parser' in dir(hangups)): 
      segments = hangups.ChatMessageSegment.from_str(formatted_text) 
   else: 
      segments = kludgy_html_parser.simple_parse_to_segments(formatted_text) 
   return segments"," 'Parse a formatted message into a list of segments. 
 :param formatted_text: formatted message 
 :type formatted_text: str 
 :return: list of segments 
 :rtype: list of hangups.ChatMessageSegment'","'send formatted chat message 
 legacy notice: identical function in kludgy_html_parser 
 the older function is ""overridden"" here for compatibility reasons'"
"def _write_proj(fid, projs): 
    if (len(projs) == 0): 
      return 
   start_block(fid, FIFF.FIFFB_PROJ) 
   for proj in projs: 
      start_block(fid, FIFF.FIFFB_PROJ_ITEM) 
      write_int(fid, FIFF.FIFF_NCHAN, proj['data']['ncol']) 
      write_name_list(fid, FIFF.FIFF_PROJ_ITEM_CH_NAME_LIST, proj['data']['col_names']) 
      write_string(fid, FIFF.FIFF_NAME, proj['desc']) 
      write_int(fid, FIFF.FIFF_PROJ_ITEM_KIND, proj['kind']) 
      if (proj['kind'] == FIFF.FIFFV_PROJ_ITEM_FIELD): 
         write_float(fid, FIFF.FIFF_PROJ_ITEM_TIME, 0.0) 
      write_int(fid, FIFF.FIFF_PROJ_ITEM_NVEC, proj['data']['nrow']) 
      write_int(fid, FIFF.FIFF_MNE_PROJ_ITEM_ACTIVE, proj['active']) 
      write_float_matrix(fid, FIFF.FIFF_PROJ_ITEM_VECTORS, proj['data']['data']) 
      if (proj['explained_var'] is not None): 
         write_float(fid, FIFF.FIFF_MNE_ICA_PCA_EXPLAINED_VAR, proj['explained_var']) 
      end_block(fid, FIFF.FIFFB_PROJ_ITEM) 
   end_block(fid, FIFF.FIFFB_PROJ)"," 'Write a proj file. 
 Parameters 
 fid : open file descriptor 
 projs : list of projectors 
 Returns 
 None'","'Write a projection operator to a file. 
 Parameters 
 fid : file 
 The file descriptor of the open file. 
 projs : dict 
 The projection operator.'"
"def clean_up_tables(db, tmp_prefix): 
    db.execute('BEGIN') 
   for table in ('point', 'line', 'roads', 'polygon'): 
      db.execute(('DROP   TABLE   %(tmp_prefix)s_%(table)s' % locals())) 
      db.execute((""DELETE   FROM   geometry_columns   WHERE   f_table_name   =   '%(tmp_prefix)s_%(table)s'"" % locals())) 
   db.execute('COMMIT')", 'Clean up tables in the temporary schema.','Drop all temporary tables created by prepare_data().'
"def custom_url(generator, metadata): 
    global global_siteurl 
   global_siteurl = generator.settings['SITEURL']", 'Customizes the URL used by the generator.','Saves globally the value of SITEURL configuration parameter'
"def find_sockfiles(): 
    paths = [] 
   for dir in SEARCH_DIRS: 
      if ((not os.path.isdir(dir)) or (not os.access(dir, os.R_OK))): 
         continue 
      for name in os.listdir(dir): 
         subdir = os.path.join(dir, name) 
         if ((not os.path.isdir(subdir)) or (not os.access(subdir, os.R_OK))): 
            continue 
         for subname in os.listdir(subdir): 
            path = os.path.join(subdir, subname) 
            if utils.is_sockfile(path): 
               paths.append(path) 
               break 
   for sockfile in DEFAULT_SOCKFILES: 
      if (not utils.is_sockfile(sockfile)): 
         continue 
      paths.append(sockfile) 
   return paths"," 'Returns a list of socket files that are known to be used by the 
 process. 
 This function is used to find the socket files that are used by the 
 process. 
 The socket files are found in the following directories: 
 * $HOME/.ssh/ 
 * $HOME/.ssh/known_hosts 
 * $HOME/.ssh/known_hosts2 
 * $HOME/.ssh/known_hosts3 
 * $HOME/.ssh/known_hosts4 
 * $HOME/.ssh/known_hosts5 
 * $HOME/.ssh/known_hosts6 
 * $HOME/.ssh/known_hosts7 
 * $HOME/.ssh/known_hosts8 
 * $HOME/.ssh/known_hosts9 
 * $HOME/.ssh/known_hosts10 
 * $HOME/.ssh/known_hosts11 
 * $HOME/.ssh/known_hosts12 
 * $HOME/.ssh/known_hosts13 
 * $HOME/.ssh/known_hosts14 
 * $HOME/.ssh/known_hosts15 
 * $HOME/.ssh/known_hosts",'Returns a list of paths to socket files to monitor.'
"def local_extra_dirs(func): 
    def wraps(self, *args, **kwargs): 
      if (kwargs.get('base_dir', None) is None): 
         return func(self, *args, **kwargs) 
      else: 
         for c in self.__class__.__mro__: 
            if (c.__name__ == 'DiskObjectStore'): 
               return getattr(c, func.__name__)(self, *args, **kwargs) 
         raise Exception((""Could   not   call   DiskObjectStore's   %s   method,   does   your   Object   Store   plugin   inherit   from   DiskObjectStore?"" % func.__name__)) 
   return wraps"," 'Decorator to set the local extra dirs for a class method. 
 :param func: The method to decorate. 
 :return: The wrapped method. 
 :rtype: function'",'Non-local plugin decorator using local directories for the extra_dirs (job_work and temp).'
"def topological_sort(graph, key=None): 
    (V, E) = graph 
   L = [] 
   S = set(V) 
   E = list(E) 
   for (v, u) in E: 
      S.discard(u) 
   if (key is None): 
      key = (lambda value: value) 
   S = sorted(S, key=key, reverse=True) 
   while S: 
      node = S.pop() 
      L.append(node) 
      for (u, v) in list(E): 
         if (u == node): 
            E.remove((u, v)) 
            for (_u, _v) in E: 
               if (v == _v): 
                  break 
            else: 
               kv = key(v) 
               for (i, s) in enumerate(S): 
                  ks = key(s) 
                  if (kv > ks): 
                     S.insert(i, v) 
                     break 
               else: 
                  S.append(v) 
   if E: 
      raise ValueError('cycle   detected') 
   else: 
      return L"," 'Return a topological sort of the vertices of a directed graph. 
 This is the same as the topological sort algorithm, except that 
 the input is a directed graph instead of an undirected graph. 
 Parameters 
 graph : dict 
 A dictionary of vertices and edges. 
 key : function, optional 
 A function that takes a vertex as input and returns a number. 
 The vertices will be sorted in decreasing order of the function\'s 
 output. 
 Returns 
 list 
 A list of vertices in sorted order. 
 Raises 
 ValueError 
 If there is a cycle in the graph.'","'Topological sort of graph\'s vertices. 
 Parameters 
 ``graph`` : ``tuple[list, list[tuple[T, T]]`` 
 A tuple consisting of a list of vertices and a list of edges of 
 a graph to be sorted topologically. 
 ``key`` : ``callable[T]`` (optional) 
 Ordering key for vertices on the same level. By default the natural 
 (e.g. lexicographic) ordering is used (in this case the base type 
 must implement ordering relations). 
 Examples 
 Consider a graph:: 
 | 7 |\    | 5 |     | 3 | 
 V  V           V V   | 
 | 11 |         | 8 |  | 
 V  \     V V   /  V  V 
 | 2 |  |  | 9 | |  | 10 | 
 where vertices are integers. This graph can be encoded using 
 elementary Python\'s data structures as follows:: 
 >>> V = [2, 3, 5, 7, 8, 9, 10, 11] 
 >>> E = [(7, 11), (7, 8), (5, 11), (3, 8), (3, 10), 
 ...      (11, 2), (11, 9), (11, 10), (8, 9)] 
 To compute a topological sort for graph ``(V, E)`` issue:: 
 >>> from sympy.utilities.iterables import topological_sort 
 >>> topological_sort((V, E)) 
 [3, 5, 7, 8, 11, 2, 9, 10] 
 If specific tie breaking approach is needed, use ``key`` parameter:: 
 >>> topological_sort((V, E), key=lambda v: -v) 
 [7, 5, 11, 3, 10, 8, 9, 2] 
 Only acyclic graphs can be sorted. If the input graph has a cycle, 
 then :py:exc:`ValueError` will be raised:: 
 >>> topological_sort((V, E + [(10, 7)])) 
 Traceback (most recent call last): 
 ValueError: cycle detected 
 .. seealso:: http://en.wikipedia.org/wiki/Topological_sorting'"
"def get_transcripts_from_youtube(youtube_id, settings, i18n, youtube_transcript_name=''): 
    _ = i18n.ugettext 
   utf8_parser = etree.XMLParser(encoding='utf-8') 
   youtube_text_api = copy.deepcopy(settings.YOUTUBE['TEXT_API']) 
   youtube_text_api['params']['v'] = youtube_id 
   youtube_transcript_name = youtube_video_transcript_name(youtube_text_api) 
   if youtube_transcript_name: 
      youtube_text_api['params']['name'] = youtube_transcript_name 
   data = requests.get(('http://' + youtube_text_api['url']), params=youtube_text_api['params']) 
   if ((data.status_code != 200) or (not data.text)): 
      msg = _(""Can't   receive   transcripts   from   Youtube   for   {youtube_id}.   Status   code:   {status_code}."").format(youtube_id=youtube_id, status_code=data.status_code) 
      raise GetTranscriptsFromYouTubeException(msg) 
   (sub_starts, sub_ends, sub_texts) = ([], [], []) 
   xmltree = etree.fromstring(data.content, parser=utf8_parser) 
   for element in xmltree: 
      if (element.tag == 'text'): 
         start = float(element.get('start')) 
         duration = float(element.get('dur', 0)) 
         text = element.text 
         end = (start + duration) 
         if text: 
            sub_starts.append(int((start * 1000))) 
            sub_ends.append(int(((end + 0.0001) * 1000))) 
            sub_texts.append(text.replace('\n', '   ')) 
   return {'start': sub_starts, 'end': sub_ends, 'text': sub_texts}"," 'Get transcripts from Youtube. 
 Args: 
 youtube_id (str): Youtube video id. 
 settings (dict): Settings. 
 i18n (i18n.Translations): Translations. 
 Returns: 
 dict: Transcripts. 
 Raises: 
 GetTranscriptsFromYouTubeException: If can\'t receive transcripts from Youtube. 
 Examples: 
 >>> from transcripts.utils import get_transcripts_from_youtube 
 >>> youtube_id = ""RX4nzqxqWfk"" 
 >>> settings = {u""YOUTUBE"": {u""TEXT_API"": {u""url"": ""http://gdata.youtube.com/feeds/api/videos/RX4nzqxqWfk?v=2"", u'params': {u'v': u""RX4nzqxqWfk""}}} 
 >>> transcripts = get_transcripts_from_youtube(youtube_id, settings, i18n, you","'Gets transcripts from youtube for youtube_id. 
 Parses only utf-8 encoded transcripts. 
 Other encodings are not supported at the moment. 
 Returns (status, transcripts): bool, dict.'"
"def coreproperties(title, subject, creator, keywords, lastmodifiedby=None): 
    coreprops = makeelement('coreProperties', nsprefix='cp') 
   coreprops.append(makeelement('title', tagtext=title, nsprefix='dc')) 
   coreprops.append(makeelement('subject', tagtext=subject, nsprefix='dc')) 
   coreprops.append(makeelement('creator', tagtext=creator, nsprefix='dc')) 
   coreprops.append(makeelement('keywords', tagtext=','.join(keywords), nsprefix='cp')) 
   if (not lastmodifiedby): 
      lastmodifiedby = creator 
   coreprops.append(makeelement('lastModifiedBy', tagtext=lastmodifiedby, nsprefix='cp')) 
   coreprops.append(makeelement('revision', tagtext='1', nsprefix='cp')) 
   coreprops.append(makeelement('category', tagtext='Examples', nsprefix='cp')) 
   coreprops.append(makeelement('description', tagtext='Examples', nsprefix='dc')) 
   currenttime = time.strftime('%Y-%m-%dT%H:%M:%SZ') 
   for doctime in ['created', 'modified']: 
      elm_str = ('<dcterms:%s   xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""   xmlns:dcterms=""http://purl.org/dc/terms/""   xsi:type=""dcterms:W3CDTF"">%s</dcterms:%s>' % (doctime, currenttime, doctime)) 
      coreprops.append(etree.fromstring(elm_str)) 
   return coreprops", 'Generate core properties for an example.',"'Create core properties (common document properties referred to in the 
 \'Dublin Core\' specification). See appproperties() for other stuff.'"
"def setup_request_bound_data(config): 
    def attach_bound_data(request): 
      parent = getattr(request, 'parent', None) 
      return (parent.bound_data if parent else {}) 
   config.add_request_method(attach_bound_data, name='bound_data', reify=True)"," 'Adds a method to the request object that returns the bound data 
 for the request. 
 :param config: The request configuration object 
 :type config: RequestConfig'","'Attach custom data on request object, and share it with parent 
 requests during batch.'"
"def has_no_time(at): 
    if isinstance(at, datetime): 
      return False 
   return ((at.hour is None) and (at.minute is None) and (at.second is None) and (at.microsecond is None))", 'Check if a time is None.',"'Returns True if the given object is an ``adatetime`` where ``hour``, 
 ``minute``, ``second`` and ``microsecond`` are all None.'"
"def band_stop_obj(wp, ind, passb, stopb, gpass, gstop, type): 
    passbC = passb.copy() 
   passbC[ind] = wp 
   nat = ((stopb * (passbC[0] - passbC[1])) / ((stopb ** 2) - (passbC[0] * passbC[1]))) 
   nat = min(abs(nat)) 
   if (type == 'butter'): 
      GSTOP = (10 ** (0.1 * abs(gstop))) 
      GPASS = (10 ** (0.1 * abs(gpass))) 
      n = (log10(((GSTOP - 1.0) / (GPASS - 1.0))) / (2 * log10(nat))) 
   elif (type == 'cheby'): 
      GSTOP = (10 ** (0.1 * abs(gstop))) 
      GPASS = (10 ** (0.1 * abs(gpass))) 
      n = (arccosh(sqrt(((GSTOP - 1.0) / (GPASS - 1.0)))) / arccosh(nat)) 
   elif (type == 'ellip'): 
      GSTOP = (10 ** (0.1 * gstop)) 
      GPASS = (10 ** (0.1 * gpass)) 
      arg1 = sqrt(((GPASS - 1.0) / (GSTOP - 1.0))) 
      arg0 = (1.0 / nat) 
      d0 = special.ellipk([(arg0 ** 2), (1 - (arg0 ** 2))]) 
      d1 = special.ellipk([(arg1 ** 2), (1 - (arg1 ** 2))]) 
      n = ((d0[0] * d1[1]) / (d0[1] * d1[0])) 
   else: 
      raise ValueError(('Incorrect   type:   %s' % type)) 
   return n"," 'Band-stop filter. 
 Parameters 
 wp : float 
 Passband cutoff frequency (Hz) 
 ind : int 
 Index of bandstop 
 passb : array 
 Passband frequencies (Hz) 
 stopb : array 
 Stopband frequencies (Hz) 
 gpass : float 
 Passband gain (dB/decade) 
 gstop : float 
 Stopband gain (dB/decade) 
 type : str 
 Type of filter: \'butter\', \'cheby\' or \'ellip\' 
 Returns 
 n : float 
 Order of the filter. 
 Notes 
 The band-stop filter is a low-pass filter with a stopband. 
 The passband is a low-pass filter with a cutoff frequency 
 `wp`. 
 The stopband is a high-pass filter with a cutoff frequency 
 `stopb`. 
 The order of the filter is given by `n`. 
 References 
 .. [1] http://en.wikipedia.org/wiki/Band-stop_filter 
 Examples 
 >>> from sci","'Band Stop Objective Function for order minimization. 
 Returns the non-integer order for an analog band stop filter. 
 Parameters 
 wp : scalar 
 Edge of passband `passb`. 
 ind : int, {0, 1} 
 Index specifying which `passb` edge to vary (0 or 1). 
 passb : ndarray 
 Two element sequence of fixed passband edges. 
 stopb : ndarray 
 Two element sequence of fixed stopband edges. 
 gstop : float 
 Amount of attenuation in stopband in dB. 
 gpass : float 
 Amount of ripple in the passband in dB. 
 type : {\'butter\', \'cheby\', \'ellip\'} 
 Type of filter. 
 Returns 
 n : scalar 
 Filter order (possibly non-integer).'"
"def view(tpl_name, **defaults): 
    def decorator(func): 
      @functools.wraps(func) 
      def wrapper(*args, **kwargs): 
         result = func(*args, **kwargs) 
         if isinstance(result, (dict, DictMixin)): 
            tplvars = defaults.copy() 
            tplvars.update(result) 
            return template(tpl_name, **tplvars) 
         elif (result is None): 
            return template(tpl_name, defaults) 
         return result 
      return wrapper 
   return decorator"," 'Decorator that renders the template with the given name and the given 
 defaults. 
 :param tpl_name: The name of the template to render. 
 :param defaults: The default values to use if the template does not have 
 any values. 
 :return: The decorator function.'","'Decorator: renders a template for a handler. 
 The handler can control its behavior like that: 
 - return a dict of template vars to fill out the template 
 - return something other than a dict and the view decorator will not 
 process the template, but return the handler result as is. 
 This includes returning a HTTPResponse(dict) to get, 
 for instance, JSON with autojson or other castfilters.'"
"def primes(n): 
    if (n == 2): 
      return [2] 
   elif (n < 2): 
      return [] 
   s = list(range(3, (n + 1), 2)) 
   mroot = (n ** 0.5) 
   half = (((n + 1) // 2) - 1) 
   i = 0 
   m = 3 
   while (m <= mroot): 
      if s[i]: 
         j = (((m * m) - 3) // 2) 
         s[j] = 0 
         while (j < half): 
            s[j] = 0 
            j += m 
      i = (i + 1) 
      m = ((2 * i) + 3) 
   return ([2] + [x for x in s if x])"," 'Return the prime numbers less than or equal to n. 
 >>> primes(5) 
 [2, 3, 5] 
 >>> primes(100) 
 [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]'","'Simple test function 
 Taken from http://www.huyng.com/posts/python-performance-analysis/'"
"def subjunctive(sentence, classical=True, **kwargs): 
    S = sentence 
   if (not (hasattr(S, 'words') and hasattr(S, 'parse_token'))): 
      raise TypeError(('%s   object   is   not   a   parsed   Sentence' % repr(S.__class__.__name__))) 
   if question(S): 
      return False 
   for (i, w) in enumerate(S): 
      b = False 
      if w.type.startswith('VB'): 
         if s(w).startswith('wish'): 
            return True 
         if ((s(w) == 'hope') and (i > 0) and (s(S[(i - 1)]) in ('i', 'we'))): 
            return True 
         if ((s(w) == 'were') and (i > 0) and ((s(S[(i - 1)]) in ('i', 'it', 'he', 'she')) or (S[(i - 1)].type == 'NN'))): 
            return True 
         if (s(w) in subjunctive1): 
            b = True 
         elif ((s(w) == 'is') and (0 < i < (len(S) - 1)) and (s(S[(i - 1)]) == 'it') and (s(S[(i + 1)]) in subjunctive2)): 
            b = True 
         elif ((s(w) == 'is') and (0 < i < (len(S) - 3)) and (s(S[(i - 1)]) == 'it') and (s(S[(i + 2)]) in ('good', 'bad')) and (s(S[(i + 3)]) == 'idea')): 
            b = True 
      if b: 
         v = find((lambda w: w.type.startswith('VB')), S[(i + 1):]) 
         if (v and (classical is True) and v and (v.type == 'VB')): 
            return True 
         if (v and (classical is False)): 
            return True 
   return False"," 'Returns True if the sentence is subjunctive, False otherwise. 
 Parameters 
 sentence : string 
 The sentence to test. 
 classical : boolean, optional 
 If True, the test is applied to sentences with classical subjunctive 
 verbs. 
 Returns 
 True if the sentence is subjunctive, False otherwise.'","'The subjunctive mood is a classical mood used to express a wish, judgment or opinion. 
 It is marked by the verb wish/were, or infinitive form of a verb 
 preceded by an ""it is""-statement: 
 ""It is recommended that he bring his own computer.""'"
"def test_MultipleLocator_set_params(): 
    mult = mticker.MultipleLocator(base=0.7) 
   mult.set_params(base=1.7) 
   assert (mult._base == 1.7)", 'Test MultipleLocator set_params',"'Create multiple locator with 0.7 base, and change it to something else. 
 See if change was successful. 
 Should not exception.'"
"@contextmanager 
 def disable_overrides(): 
    prev = _OVERRIDES_DISABLED.disabled 
   _OVERRIDES_DISABLED.disabled += (True,) 
   (yield) 
   _OVERRIDES_DISABLED.disabled = prev"," 'Disable overrides for the duration of this context manager. 
 This is useful for tests that want to disable overrides for a 
 short time. 
 Example:: 
 @disable_overrides 
 def test_something(): 
 @enable_overrides 
 def test_something_else(): 
 .. versionadded:: 0.11'","'A context manager which disables field overrides inside the context of a 
 `with` statement, allowing code to get at the `original` value of a field.'"
"def main(args): 
    if (not find_sockfiles()): 
      return 13 
   if (MySQLdb is None): 
      utils.err(""error:   Python   module   `MySQLdb'   is   missing"") 
      return 1 
   last_db_refresh = now() 
   dbs = find_databases() 
   while True: 
      ts = now() 
      if ((ts - last_db_refresh) >= DB_REFRESH_INTERVAL): 
         find_databases(dbs) 
         last_db_refresh = ts 
      errs = [] 
      for (dbname, db) in dbs.iteritems(): 
         try: 
            collect(db) 
         except (EnvironmentError, EOFError, RuntimeError, socket.error, MySQLdb.MySQLError) as e: 
            if (isinstance(e, IOError) and (e[0] == errno.EPIPE)): 
               return 2 
            utils.err(('error:   failed   to   collect   data   from   %s:   %s' % (db, e))) 
            errs.append(dbname) 
      for dbname in errs: 
         del dbs[dbname] 
      sys.stdout.flush() 
      time.sleep(COLLECTION_INTERVAL)"," 'Collects data from MySQL databases and dumps it to files. 
 This program runs in a loop, collecting data from MySQL databases 
 and writing it to files. 
 The program collects data from MySQL databases using the 
 ``--databases`` option and writes it to files using the 
 ``--outfile`` option. 
 The program can be configured using the ``--config`` option. 
 The ``--config`` option can be used to specify the location of 
 the configuration file. 
 The ``--databases`` option can be used to specify the databases 
 to collect data from. 
 The ``--outfile`` option can be used to specify the output file 
 name. 
 The ``--help`` option can be used to display the usage information. 
 The ``--version`` option can be used to display the version information.'",'Collects and dumps stats from a MySQL server.'
"def stack(xs, axis=0): 
    xs = [expand_dims.expand_dims(x, axis=axis) for x in xs] 
   return concat.concat(xs, axis=axis)"," 'Stacks a list of vectors along an axis. 
 Parameters 
 xs : list of ndarray 
 List of vectors to stack along an axis. 
 axis : int, optional 
 Axis to stack the vectors along. 
 Returns 
 stacked_vector : ndarray 
 Stacked vector. 
 Examples 
 >>> stack([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) 
 array([[1, 2, 3, 4, 5, 6, 7, 8, 9], 
 [1, 2, 3, 4, 5, 6, 7, 8, 9], 
 [1, 2, 3, 4, 5, 6, 7, 8, 9]])'","'Concatenate variables along a new axis. 
 Args: 
 xs (list of chainer.Variable): Variables to be concatenated. 
 axis (int): Axis of result along which variables are stacked. 
 Returns: 
 ~chainer.Variable: Output variable.'"
"def _execute(*cmd, **kwargs): 
    if CONF.fake_network: 
      LOG.debug('FAKE   NET:   %s', '   '.join(map(str, cmd))) 
      return ('fake', 0) 
   else: 
      return utils.execute(*cmd, **kwargs)", 'Execute a command and return the exit code.','Wrapper around utils._execute for fake_network.'
"def which_files(file, mode=(F_OK | X_OK), path=None, pathext=None): 
    (filepath, file) = split(file) 
   if filepath: 
      path = (filepath,) 
   elif (path is None): 
      path = defpath 
   elif isinstance(path, str): 
      path = path.split(pathsep) 
   if (pathext is None): 
      pathext = defpathext 
   elif isinstance(pathext, str): 
      pathext = pathext.split(pathsep) 
   if (not ('' in pathext)): 
      pathext.insert(0, '') 
   for dir in path: 
      basepath = join(dir, file) 
      for ext in pathext: 
         fullpath = (basepath + ext) 
         if (exists(fullpath) and access(fullpath, mode)): 
            (yield fullpath)"," 'Return a generator of the paths of files that match the given pattern 
 and mode. 
 The pattern may include wildcards, and the mode is the same as for 
 :func:`access`. 
 :param file: the file name 
 :param mode: the access mode 
 :param path: the directory to search in, or None 
 :param pathext: the extension to search for, or None 
 :returns: a generator of paths to files matching the pattern 
 :rtype: generator[str]'","'Locate a file in a path supplied as a part of the file name, 
 or the user\'s path, or a supplied path. 
 The function yields full paths (not necessarily absolute paths), 
 in which the given file name matches an existing file in a directory on the path. 
 >>> def test_which(expected, *args, **argd): 
 ...     result = list(which_files(*args, **argd)) 
 ...     assert result == expected, \'which_files: %s != %s\' % (result, expected) 
 ...     try: 
 ...         result = [ which(*args, **argd) ] 
 ...     except IOError: 
 ...         result = [] 
 ...     assert result[:1] == expected[:1], \'which: %s != %s\' % (result[:1], expected[:1]) 
 >>> if windows: cmd = environ[\'COMSPEC\'] 
 >>> if windows: test_which([cmd], \'cmd\') 
 >>> if windows: test_which([cmd], \'cmd.exe\') 
 >>> if windows: test_which([cmd], \'cmd\', path=dirname(cmd)) 
 >>> if windows: test_which([cmd], \'cmd\', pathext=\'.exe\') 
 >>> if windows: test_which([cmd], cmd) 
 >>> if windows: test_which([cmd], cmd, path=\'<nonexistent>\') 
 >>> if windows: test_which([cmd], cmd, pathext=\'<nonexistent>\') 
 >>> if windows: test_which([cmd], cmd[:-4]) 
 >>> if windows: test_which([cmd], cmd[:-4], path=\'<nonexistent>\') 
 >>> if windows: test_which([], \'cmd\', path=\'<nonexistent>\') 
 >>> if windows: test_which([], \'cmd\', pathext=\'<nonexistent>\') 
 >>> if windows: test_which([], \'<nonexistent>/cmd\') 
 >>> if windows: test_which([], cmd[:-4], pathext=\'<nonexistent>\') 
 >>> if not windows: sh = \'/bin/sh\' 
 >>> if not windows: test_which([sh], \'sh\') 
 >>> if not windows: test_which([sh], \'sh\', path=dirname(sh)) 
 >>> if not windows: test_which([sh], \'sh\', pathext=\'<nonexistent>\') 
 >>> if not windows: test_which([sh], sh) 
 >>> if not windows: test_which([sh], sh, path=\'<nonexistent>\') 
 >>> if not windows: test_which([sh], sh, pathext=\'<nonexistent>\') 
 >>> if not windows: test_which([], \'sh\', mode=W_OK)  # not running as root, are you? 
 >>> if not windows: test_which([], \'sh\', path=\'<nonexistent>\') 
 >>> if not windows: test_which([], \'<nonexistent>/sh\')'"
"def runner(): 
    client = salt.runner.RunnerClient(__opts__) 
   ret = client.get_docs() 
   return ret", 'Get the documentation for the salt command line.',"'Return all inline documentation for runner modules 
 CLI Example: 
 .. code-block:: bash 
 salt-run doc.runner'"
"def customized_clean_str(string): 
    string = re.sub('\\n', '   ', string) 
   string = re.sub(""\\'s"", ""   's"", string) 
   string = re.sub('\\\xe2\x80\x99s', ""   's"", string) 
   string = re.sub(""\\'ve"", '   have', string) 
   string = re.sub('\\\xe2\x80\x99ve', '   have', string) 
   string = re.sub(""\\'t"", '   not', string) 
   string = re.sub('\\\xe2\x80\x99t', '   not', string) 
   string = re.sub(""\\'re"", '   are', string) 
   string = re.sub('\\\xe2\x80\x99re', '   are', string) 
   string = re.sub(""\\'d"", '', string) 
   string = re.sub('\\\xe2\x80\x99d', '', string) 
   string = re.sub(""\\'ll"", '   will', string) 
   string = re.sub('\\\xe2\x80\x99ll', '   will', string) 
   string = re.sub('\\\xe2\x80\x9c', '   \xe2\x80\x9c   ', string) 
   string = re.sub('\\\xe2\x80\x9d', '   \xe2\x80\x9d   ', string) 
   string = re.sub('\\""', '   \xe2\x80\x9c   ', string) 
   string = re.sub(""\\'"", ""   '   "", string) 
   string = re.sub('\\\xe2\x80\x99', ""   '   "", string) 
   string = re.sub('\\.', '   .   ', string) 
   string = re.sub('\\,', '   ,   ', string) 
   string = re.sub('\\-', '   ', string) 
   string = re.sub('\\(', '   (   ', string) 
   string = re.sub('\\)', '   )   ', string) 
   string = re.sub('\\!', '   !   ', string) 
   string = re.sub('\\]', '   ]   ', string) 
   string = re.sub('\\[', '   [   ', string) 
   string = re.sub('\\?', '   ?   ', string) 
   string = re.sub('\\>', '   >   ', string) 
   string = re.sub('\\<', '   <   ', string) 
   string = re.sub('\\=', '   =   ', string) 
   string = re.sub('\\;', '   ;   ', string) 
   string = re.sub('\\;', '   ;   ', string) 
   string = re.sub('\\:', '   :   ', string) 
   string = re.sub('\\""', '   ""   ', string) 
   string = re.sub('\\$', '   $   ', string) 
   string = re.sub('\\_', '   _   ', string) 
   string = re.sub('\\s{2,}', '   ', string) 
   return string.strip().lower()", 'Customized clean string function','Tokenization/string cleaning for a datasets.'
"def var_label(var, precision=3): 
    if (var.name is not None): 
      return var.name 
   elif isinstance(var, gof.Constant): 
      h = np.asarray(var.data) 
      is_const = False 
      if (h.ndim == 0): 
         is_const = True 
         h = np.array([h]) 
      dstr = np.array2string(h, precision=precision) 
      if ('\n' in dstr): 
         dstr = dstr[:dstr.index('\n')] 
      if is_const: 
         dstr = dstr.replace('[', '').replace(']', '') 
      return dstr 
   else: 
      return type_to_str(var.type)"," 'Returns a string label for a variable. 
 Parameters 
 var : Variable 
 The variable to label. 
 precision : int 
 The number of digits to display. 
 Returns 
 str 
 The string label for the variable.'",'Return label of variable node.'
"def valid_android_zip(app_dir): 
    try: 
      print '[INFO]   Checking   for   ZIP   Validity   and   Mode' 
      man = os.path.isfile(os.path.join(app_dir, 'AndroidManifest.xml')) 
      src = os.path.exists(os.path.join(app_dir, 'src/')) 
      if (man and src): 
         return ('eclipse', True) 
      man = os.path.isfile(os.path.join(app_dir, 'app/src/main/AndroidManifest.xml')) 
      src = os.path.exists(os.path.join(app_dir, 'app/src/main/java/')) 
      if (man and src): 
         return ('studio', True) 
      xcode = [f for f in os.listdir(app_dir) if f.endswith('.xcodeproj')] 
      if xcode: 
         return ('ios', True) 
      return ('', False) 
   except: 
      PrintException('[ERROR]   Determining   Upload   type')"," 'Checks the type of the project and returns the type of project as a string. 
 Returns a string representing the type of project (e.g. ""studio""). 
 If the project is not a valid project, returns """". 
 :param app_dir: The directory of the project. 
 :type app_dir: str 
 :return: A string representing the type of project. 
 :rtype: str'",'Test if this is an valid android zip.'
"def validate_required_iff(**kwargs): 
    def _validator(form, field): 
      all_conditions_met = True 
      for (key, value) in kwargs.iteritems(): 
         if (getattr(form, key).data != value): 
            all_conditions_met = False 
      if all_conditions_met: 
         if ((field.data is None) or (isinstance(field.data, (str, unicode)) and (not field.data.strip())) or (isinstance(field.data, FileStorage) and (not field.data.filename.strip()))): 
            raise validators.ValidationError('This   field   is   required.') 
      else: 
         field.errors[:] = [] 
         raise validators.StopValidation() 
   return _validator"," 'A validator that will validate a field if all other fields have been 
 validated. 
 :param kwargs: 
 :type kwargs: dict 
 :return: 
 :rtype: 
 :raise: 
 :raise: ValidationError'","'Used as a validator within a wtforms.Form 
 This implements a conditional DataRequired 
 Each of the kwargs is a condition that must be met in the form 
 Otherwise, no validation is done'"
"@pytest.mark.django_db 
 def test_format_registry_reregister(no_formats): 
    registry = FormatRegistry() 
   filetype = registry.register('foo', 'foo') 
   new_filetype = registry.register('foo', 'foo') 
   assert (new_filetype == filetype) 
   _test_formats(registry, ['foo']) 
   new_filetype = registry.register('foo', 'foo', title='Bar') 
   assert (new_filetype == filetype) 
   assert (new_filetype.title == 'Bar') 
   _test_formats(registry, ['foo']) 
   new_filetype = registry.register('foo', 'bar') 
   assert (new_filetype == filetype) 
   assert (new_filetype.title == 'Bar') 
   assert (str(new_filetype.extension) == 'bar') 
   _test_formats(registry, ['foo'])"," 'Test that reregistering a format doesn\'t affect the list of registered 
 formats.'",'Tests the creation of a file extension'
"def import_no_virt_driver_import_deps(physical_line, filename): 
    thisdriver = _get_virt_name(virt_file_re, filename) 
   thatdriver = _get_virt_name(virt_import_re, physical_line) 
   if ((thatdriver is not None) and (thisdriver is not None) and (thisdriver != thatdriver)): 
      return (0, 'N311:   importing   code   from   other   virt   drivers   forbidden')"," 'Checks if the driver being imported is a virt driver and if it is 
 from another virt driver'","'Check virt drivers\' modules aren\'t imported by other drivers 
 Modules under each virt driver\'s directory are 
 considered private to that virt driver. Other drivers 
 in Nova must not access those drivers. Any code that 
 is to be shared should be refactored into a common 
 module 
 N311'"
"def _trim_text(text, max_width): 
    width = get_cwidth(text) 
   if (width > max_width): 
      if (len(text) == width): 
         trimmed_text = (text[:max(1, (max_width - 3))] + u'...')[:max_width] 
         return (trimmed_text, len(trimmed_text)) 
      else: 
         trimmed_text = u'' 
         for c in text: 
            if (get_cwidth((trimmed_text + c)) <= (max_width - 3)): 
               trimmed_text += c 
         trimmed_text += u'...' 
         return (trimmed_text, get_cwidth(trimmed_text)) 
   else: 
      return (text, width)"," 'Trim a text to a given width. 
 If the text is longer than the given width, the last characters are 
 removed until the width is reached. 
 Parameters 
 text : unicode 
 Text to trim. 
 max_width : int 
 Maximum width of the output. 
 Returns 
 (text, width) : tuple 
 Text with width and length.'","'Trim the text to `max_width`, append dots when the text is too long. 
 Returns (text, width) tuple.'"
"def _labels_inertia_precompute_dense(X, x_squared_norms, centers, distances): 
    n_samples = X.shape[0] 
   (labels, mindist) = pairwise_distances_argmin_min(X=X, Y=centers, metric='euclidean', metric_kwargs={'squared': True}) 
   labels = labels.astype(np.int32) 
   if (n_samples == distances.shape[0]): 
      distances[:] = mindist 
   inertia = mindist.sum() 
   return (labels, inertia)"," 'Precomputes the labels and inertia of the dense labels. 
 Parameters 
 X : ndarray 
 The points. 
 x_squared_norms : ndarray 
 The squared norms of the points. 
 centers : ndarray 
 The centers of the clusters. 
 distances : ndarray 
 The distances between the points and the centers. 
 Returns 
 labels : ndarray 
 The labels. 
 inertia : ndarray 
 The inertia.'","'Compute labels and inertia using a full distance matrix. 
 This will overwrite the \'distances\' array in-place. 
 Parameters 
 X : numpy array, shape (n_sample, n_features) 
 Input data. 
 x_squared_norms : numpy array, shape (n_samples,) 
 Precomputed squared norms of X. 
 centers : numpy array, shape (n_clusters, n_features) 
 Cluster centers which data is assigned to. 
 distances : numpy array, shape (n_samples,) 
 Pre-allocated array in which distances are stored. 
 Returns 
 labels : numpy array, dtype=np.int, shape (n_samples,) 
 Indices of clusters that samples are assigned to. 
 inertia : float 
 Sum of distances of samples to their closest cluster center.'"
"def compute_node_statistics(context): 
    return IMPL.compute_node_statistics(context)"," 'Returns the total number of nodes, nodes in each state, and nodes in 
 each state for each project.'","'Get aggregate statistics over all compute nodes. 
 :param context: The security context 
 :returns: Dictionary containing compute node characteristics summed up 
 over all the compute nodes, e.g. \'vcpus\', \'free_ram_mb\' etc.'"
"def common_texification(text): 
    text = re_mathdefault.sub(repl_mathdefault, text) 
   parts = re_mathsep.split(text) 
   for (i, s) in enumerate(parts): 
      if (not (i % 2)): 
         s = re_escapetext.sub(repl_escapetext, s) 
      else: 
         s = (u'\\(\\displaystyle   %s\\)' % s) 
      parts[i] = s 
   return u''.join(parts)", 'Converts all LaTeX-like expressions into LaTeX-like expressions.',"'Do some necessary and/or useful substitutions for texts to be included in 
 LaTeX documents.'"
"@register.filter(name='str_to_list') 
 def str_to_list(info): 
    print ast.literal_eval(info), type(ast.literal_eval(info)) 
   return ast.literal_eval(info)", 'convert string to list','str to list'
"def upload_problem_grade_report(_xmodule_instance_args, _entry_id, course_id, _task_input, action_name): 
    start_time = time() 
   start_date = datetime.now(UTC) 
   status_interval = 100 
   enrolled_students = CourseEnrollment.objects.users_enrolled_in(course_id) 
   task_progress = TaskProgress(action_name, enrolled_students.count(), start_time) 
   header_row = OrderedDict([('id', 'Student   ID'), ('email', 'Email'), ('username', 'Username')]) 
   graded_scorable_blocks = _graded_scorable_blocks_to_header(course_id) 
   rows = [((list(header_row.values()) + ['Grade']) + list(chain.from_iterable(graded_scorable_blocks.values())))] 
   error_rows = [(list(header_row.values()) + ['error_msg'])] 
   current_step = {'step': 'Calculating   Grades'} 
   course = get_course_by_id(course_id) 
   for (student, course_grade, err_msg) in CourseGradeFactory().iter(course, enrolled_students): 
      student_fields = [getattr(student, field_name) for field_name in header_row] 
      task_progress.attempted += 1 
      if (not course_grade): 
         if (not err_msg): 
            err_msg = u'Unknown   error' 
         error_rows.append((student_fields + [err_msg])) 
         task_progress.failed += 1 
         continue 
      earned_possible_values = [] 
      for block_location in graded_scorable_blocks: 
         try: 
            problem_score = course_grade.locations_to_scores[block_location] 
         except KeyError: 
            earned_possible_values.append([u'Not   Available', u'Not   Available']) 
         else: 
            if problem_score.attempted: 
               earned_possible_values.append([problem_score.earned, problem_score.possible]) 
            else: 
               earned_possible_values.append([u'Not   Attempted', problem_score.possible]) 
      rows.append(((student_fields + [course_grade.percent]) + list(chain.from_iterable(earned_possible_values)))) 
      task_progress.succeeded += 1 
      if ((task_progress.attempted % status_interval) == 0): 
         task_progress.update_task_state(extra_meta=current_step) 
   if (len(rows) > 1): 
      upload_csv_to_report_store(rows, 'problem_grade_report', course_id, start_date) 
   if (len(error_rows) > 1): 
      upload_csv_to_report_store(error_rows, 'problem_grade_report_err', course_id, start_date) 
   return task_progress.update_task_state(extra_meta={'step': 'Uploading   CSV'})"," 'Uploads a CSV containing student grade information to the report store. 
 This function is called from the report_task.py script. 
 Args: 
 _xmodule_instance_args (dict): 
 Arguments passed to the xmodule instance. 
 _entry_id (str): 
 The ID of the report entry. 
 course_id (str): 
 The ID of the course. 
 _task_input (dict): 
 A dictionary containing the report task input. 
 action_name (str): 
 The name of the action being performed. 
 Returns: 
 A dictionary containing the task progress. 
 Raises: 
 NotImplementedError: 
 If the report type is not supported.'","'Generate a CSV containing all students\' problem grades within a given 
 `course_id`.'"
"def _ip_to_number(ipstr): 
    net = ([int(digit) for digit in ipstr.split('.')] + [0, 0, 0]) 
   net = net[:4] 
   return (((((((0L + net[0]) << 8) + net[1]) << 8) + net[2]) << 8) + net[3])", 'Convert IP address string to a number.',"'Translate a string ip address to a packed number. 
 @param ipstr: the ip address to transform 
 @type ipstr: a string ""x.x.x.x"" 
 @return: an int32 number representing the ip address'"
"def _validate_dict_keys(dict_to_check, required_keys, optional_keys): 
    assert (set(required_keys) <= set(dict_to_check.keys())), ('Missing   keys:   %s' % dict_to_check) 
   assert (set(dict_to_check.keys()) <= set((required_keys + optional_keys))), ('Extra   keys:   %s' % dict_to_check)"," 'Checks that the given dict has the required keys and no extra keys. 
 Args: 
 dict_to_check: 
 dict to check. 
 required_keys: 
 list of required keys. 
 optional_keys: 
 list of optional keys. 
 Raises: 
 ValueError: if the dict has extra keys or missing required keys. 
 Examples: 
 >>> _validate_dict_keys({""foo"": 1}, [""foo"", ""bar""], [""foo"", ""bar"", ""baz""]) 
 >>> _validate_dict_keys({""foo"": 1}, [""foo"", ""bar""], [""foo"", ""bar"", ""baz""]) 
 >>> _validate_dict_keys({""foo"": 1}, [""foo"", ""bar""], [""foo"", ""bar"", ""baz""]) 
 >>> _validate_dict_keys({""foo"": 1}, [""foo"", ""bar""], [""foo"", ""bar"", ""baz""])'","'Checks that all of the required keys, and possibly some of the optional 
 keys, are in the given dict. 
 Raises: 
 AssertionError: if the validation fails.'"
"def _convert_to_array_of_opt_val(optvals): 
    array_of_optv = DataObject() 
   array_of_optv.OptionValue = optvals 
   return array_of_optv", 'Convert a list of optvals to an array of OptionValue objects.','Wraps the given array into a DataObject.'
"def image_volume_cache_create(context, host, cluster_name, image_id, image_updated_at, volume_id, size): 
    return IMPL.image_volume_cache_create(context, host, cluster_name, image_id, image_updated_at, volume_id, size)"," 'Create image volume cache entry. 
 This is used to cache image volume creation. 
 :param context: context to use 
 :param host: host to use 
 :param cluster_name: cluster name 
 :param image_id: image id 
 :param image_updated_at: image updated at 
 :param volume_id: volume id 
 :param size: volume size 
 :returns: image volume cache entry or None if no entry is created'",'Create a new image volume cache entry.'
"def test_against_hor2eq(): 
    location = EarthLocation(lon=Angle(u'-111d36.0m'), lat=Angle(u'31d57.8m'), height=(2120.0 * u.m)) 
   obstime = Time(2451545.0, format=u'jd', scale=u'ut1') 
   altaz_frame = AltAz(obstime=obstime, location=location, temperature=(0 * u.deg_C), pressure=(0.781 * u.bar)) 
   altaz_frame_noatm = AltAz(obstime=obstime, location=location, temperature=(0 * u.deg_C), pressure=(0.0 * u.bar)) 
   altaz = SkyCoord(u'264d55m06s   37d54m41s', frame=altaz_frame) 
   altaz_noatm = SkyCoord(u'264d55m06s   37d54m41s', frame=altaz_frame_noatm) 
   radec_frame = u'icrs' 
   radec_actual = altaz.transform_to(radec_frame) 
   radec_actual_noatm = altaz_noatm.transform_to(radec_frame) 
   radec_expected = SkyCoord(u'07h36m55.2s   +15d25m08s', frame=radec_frame) 
   distance = radec_actual.separation(radec_expected).to(u'arcsec') 
   radec_expected_noatm = SkyCoord(u'07h36m58.9s   +15d25m37s', frame=radec_frame) 
   distance_noatm = radec_actual_noatm.separation(radec_expected_noatm).to(u'arcsec') 
   assert (distance < (5 * u.arcsec)) 
   assert (distance_noatm < (0.4 * u.arcsec))", 'Test against the Hor2Eq algorithm',"'Check that Astropy gives consistent results with an IDL hor2eq example. 
 See : http://idlastro.gsfc.nasa.gov/ftp/pro/astro/hor2eq.pro 
 Test is against these run outputs, run at 2000-01-01T12:00:00: 
 # NORMAL ATMOSPHERE CASE 
 IDL> hor2eq, ten(37,54,41), ten(264,55,06), 2451545.0d, ra, dec, /verb, obs=\'kpno\', pres=781.0, temp=273.0 
 Latitude = +31 57 48.0   Longitude = *** 36 00.0 
 Julian Date =  2451545.000000 
 Az, El =  17 39 40.4  +37 54 41   (Observer Coords) 
 Az, El =  17 39 40.4  +37 53 40   (Apparent Coords) 
 LMST = +11 15 26.5 
 LAST = +11 15 25.7 
 Hour Angle = +03 38 30.1  (hh:mm:ss) 
 Ra, Dec:  07 36 55.6  +15 25 02   (Apparent Coords) 
 Ra, Dec:  07 36 55.2  +15 25 08   (J2000.0000) 
 Ra, Dec:  07 36 55.2  +15 25 08   (J2000) 
 IDL> print, ra, dec 
 114.23004       15.418818 
 # NO PRESSURE CASE 
 IDL> hor2eq, ten(37,54,41), ten(264,55,06), 2451545.0d, ra, dec, /verb, obs=\'kpno\', pres=0.0, temp=273.0 
 Latitude = +31 57 48.0   Longitude = *** 36 00.0 
 Julian Date =  2451545.000000 
 Az, El =  17 39 40.4  +37 54 41   (Observer Coords) 
 Az, El =  17 39 40.4  +37 54 41   (Apparent Coords) 
 LMST = +11 15 26.5 
 LAST = +11 15 25.7 
 Hour Angle = +03 38 26.4  (hh:mm:ss) 
 Ra, Dec:  07 36 59.3  +15 25 31   (Apparent Coords) 
 Ra, Dec:  07 36 58.9  +15 25 37   (J2000.0000) 
 Ra, Dec:  07 36 58.9  +15 25 37   (J2000) 
 IDL> print, ra, dec 
 114.24554       15.427022'"
"def test_vector_to_conv_c01b_invertible(): 
    rng = np.random.RandomState([2013, 5, 1]) 
   batch_size = 3 
   rows = 4 
   cols = 5 
   channels = 2 
   conv = Conv2DSpace([rows, cols], channels=channels, axes=('c', 0, 1, 'b')) 
   vec = VectorSpace(conv.get_total_dimension()) 
   X = conv.make_batch_theano() 
   Y = conv.format_as(X, vec) 
   Z = vec.format_as(Y, conv) 
   A = vec.make_batch_theano() 
   B = vec.format_as(A, conv) 
   C = conv.format_as(B, vec) 
   f = function([X, A], [Z, C]) 
   X = rng.randn(*conv.get_origin_batch(batch_size).shape).astype(X.dtype) 
   A = rng.randn(*vec.get_origin_batch(batch_size).shape).astype(A.dtype) 
   (Z, C) = f(X, A) 
   np.testing.assert_allclose(Z, X) 
   np.testing.assert_allclose(C, A)", 'Test that a vector can be converted to a conv2d and back.',"'Tests that the format_as methods between Conv2DSpace 
 and VectorSpace are invertible for the (\'c\', 0, 1, \'b\') 
 axis format.'"
"@when(u'we   connect   to   test   database') 
 def step_db_connect_test(context): 
    db_name = context.conf[u'dbname'] 
   context.cli.sendline(u'\\connect   {0}'.format(db_name))", 'Connect to the test database.','Send connect to database.'
"def setup_user_info(): 
    if (os.geteuid() != 0): 
      return 
   global g_user_uid, g_user_gid 
   (g_user_uid, g_user_gid) = desktop.lib.daemon_utils.get_uid_gid(SETUID_USER, SETGID_GROUP)", 'Setup user information for daemon.','Translate the user/group info into uid/gid.'
"def copy_constr(constr, func): 
    expr = func(constr.expr) 
   return type(constr)(expr, constr.constr_id, constr.size)"," 'Copy a constraint into a new one. 
 :param constr: The constraint to copy. 
 :param func: The function to apply to the constraint. 
 :returns: The new constraint.'","'Creates a copy of the constraint modified according to func. 
 Parameters 
 constr : LinConstraint 
 The constraint to modify. 
 func : function 
 Function to modify the constraint expression. 
 Returns 
 LinConstraint 
 A copy of the constraint with the specified changes.'"
"def __virtual__(): 
    if ('postgres.tablespace_exists' not in __salt__): 
      return (False, 'Unable   to   load   postgres   module.      Make   sure   `postgres.bins_dir`   is   set.') 
   return True", 'Load the postgres module and return True if it is available','Only load if the postgres module is present and is new enough (has ts funcs)'
"def mul_elemwise(lh_op, rh_op): 
    return lo.LinOp(lo.MUL_ELEM, lh_op.size, [rh_op], lh_op)"," 'Returns a LinOp that multiplies each element in the rhs by the 
 corresponding element in the lhs. 
 Parameters 
 lh_op : LinOp 
 The left hand side LinOp. 
 rh_op : LinOp 
 The right hand side LinOp. 
 Returns 
 LinOp 
 A LinOp that multiplies each element in the rhs by the 
 corresponding element in the lhs.'","'Multiply two linear operators elementwise. 
 Parameters 
 lh_op : LinOp 
 The left-hand operator in the product. 
 rh_op : LinOp 
 The right-hand operator in the product. 
 Returns 
 LinOp 
 A linear operator representing the product.'"
"def list_headers(general=None, request=None, response=None, entity=None): 
    if (not (general or request or response or entity)): 
      general = request = response = entity = True 
   search = [] 
   for (bool, strval) in ((general, 'general'), (request, 'request'), (response, 'response'), (entity, 'entity')): 
      if bool: 
         search.append(strval) 
   return [head for head in _headers.values() if (head.category in search)]"," 'Return a list of headers that match the given criteria. 
 :param general: 
 :param request: 
 :param response: 
 :param entity: 
 :return: 
 :rtype: 
 :raises: 
 :version: 
 :since: 0.1.0'",'list all headers for a given category'
"def map_vera_device(vera_device, remap): 
    import pyvera as veraApi 
   if isinstance(vera_device, veraApi.VeraDimmer): 
      return 'light' 
   if isinstance(vera_device, veraApi.VeraBinarySensor): 
      return 'binary_sensor' 
   if isinstance(vera_device, veraApi.VeraSensor): 
      return 'sensor' 
   if isinstance(vera_device, veraApi.VeraArmableDevice): 
      return 'switch' 
   if isinstance(vera_device, veraApi.VeraLock): 
      return 'lock' 
   if isinstance(vera_device, veraApi.VeraThermostat): 
      return 'climate' 
   if isinstance(vera_device, veraApi.VeraCurtain): 
      return 'cover' 
   if isinstance(vera_device, veraApi.VeraSwitch): 
      if (vera_device.device_id in remap): 
         return 'light' 
      else: 
         return 'switch' 
   return None"," 'Map a Vera device to a Home Assistant device type. 
 :param vera_device: The Vera device object. 
 :param remap: A dictionary of remap mappings. 
 :return: The Home Assistant device type.'",'Map vera  classes to HA types.'
"def auth(username, password): 
    _cred = __get_yubico_users(username) 
   client = Yubico(_cred['id'], _cred['key']) 
   try: 
      if client.verify(password): 
         return True 
      else: 
         return False 
   except yubico_exceptions.StatusCodeError as e: 
      log.info('Unable   to   verify   YubiKey   `{0}`'.format(e)) 
      return False"," 'Authenticate user with YubiKey. 
 :param username: Username to authenticate 
 :param password: Password to authenticate with 
 :returns: True if authentication was successful, False otherwise'",'Authenticate against yubico server'
"def check_mapping_file(mapping_fp, output_dir='.', has_barcodes=True, char_replace='_', verbose=True, variable_len_barcodes=False, disable_primer_check=False, added_demultiplex_field=None, suppress_html=False): 
    (header, mapping_data, run_description, errors, warnings) = process_id_map(open(mapping_fp, 'U'), disable_primer_check, has_barcodes, char_replace, variable_len_barcodes, added_demultiplex_field, strip_quotes=False, suppress_stripping=True) 
   if (not suppress_html): 
      formatted_html = format_mapping_html_data(header, mapping_data, errors, warnings) 
      output_html = join(((output_dir + basename(mapping_fp).replace('.txt', '')) + '.html')) 
      html_f = open(output_html, 'w') 
      html_f.write(formatted_html) 
      qiime_dir = get_qiime_project_dir() 
      copyfile(join(qiime_dir, 'qiime', 'support_files', 'js/overlib.js'), join(output_dir, 'overlib.js')) 
   corrected_mapping_data = correct_mapping_data(mapping_data, header, char_replace) 
   output_corrected_fp = join(((output_dir + basename(mapping_fp).replace('.txt', '')) + '_corrected.txt')) 
   write_corrected_mapping(output_corrected_fp, header, run_description, corrected_mapping_data) 
   output_log_fp = join(((output_dir + basename(mapping_fp).replace('.txt', '')) + '.log')) 
   write_log_file(output_log_fp, errors, warnings) 
   if verbose: 
      if (errors or warnings): 
         print ('Errors   and/or   warnings   detected   in   mapping   file.      Please   ' + 'check   the   log   and   html   file   for   details.') 
      else: 
         print 'No   errors   or   warnings   were   found   in   mapping   file.'"," 'Checks the mapping file and outputs a log and HTML file. 
 :param mapping_fp: Path to the mapping file. 
 :param output_dir: Directory to output the HTML and log files. 
 :param has_barcodes: Whether or not the mapping file contains barcodes. 
 :param char_replace: Characters to replace in the mapping file. 
 :param verbose: Whether or not to print out errors and warnings. 
 :param variable_len_barcodes: Whether or not the mapping file contains variable length barcodes. 
 :param disable_primer_check: Whether or not to disable primer checking. 
 :param added_demultiplex_field: Whether or not to add a demultiplex field to the mapping file. 
 :param suppress_html: Whether or not to output the HTML file. 
 :return: None'","'Main program function for checking mapping file 
 Checks mapping file for errors, warnings, writes log file, html file, 
 and corrected mapping file. 
 mapping_fp:  path to metadata mapping file 
 output_dir:  output directory for log, html, corrected mapping file. 
 has_barcodes:  If True, will test for perform barcodes test (presence, 
 uniqueness, valid IUPAC DNA chars). 
 char_replace:  Character used to replace invalid characters in data 
 fields.  SampleIDs always use periods to be MIENS compliant. 
 verbose: If True, a message about warnings and/or errors will be printed 
 to stdout. 
 variable_len_barcodes:  If True, suppresses warnings about barcodes of 
 varying length. 
 disable_primer_check:  If True, disables tests for valid primer sequences. 
 added_demultiplex_field:  If specified, references a field in the mapping 
 file to use for demultiplexing.  These are to be read from fasta labels 
 during the actual demultiplexing step.  All combinations of barcodes, 
 primers, and the added_demultiplex_field must be unique.'"
"@with_device 
 def push(local_path, remote_path): 
    msg = ('Pushing   %r   to   %r' % (local_path, remote_path)) 
   remote_filename = os.path.basename(local_path) 
   if log.isEnabledFor(logging.DEBUG): 
      msg += ('   (%s)' % context.device) 
   with log.waitfor(msg) as w: 
      with AdbClient() as c: 
         stat_ = c.stat(remote_path) 
         if (not stat_): 
            remote_filename = os.path.basename(remote_path) 
            remote_path = os.path.dirname(remote_path) 
            stat_ = c.stat(remote_path) 
         if (not stat_): 
            log.error(('Could   not   stat   %r' % remote_path)) 
         mode = stat_['mode'] 
         if stat.S_ISDIR(mode): 
            remote_path = os.path.join(remote_path, remote_filename) 
         return c.write(remote_path, misc.read(local_path), callback=_create_adb_push_pull_callback(w))"," 'Pushes a file from the local device to the remote device. 
 :param local_path: Path to the file on the local device. 
 :param remote_path: Path to the file on the remote device. 
 :returns: None'","'Upload a file to the device. 
 Arguments: 
 local_path(str): Path to the local file to push. 
 remote_path(str): Path or directory to store the file on the device. 
 Example: 
 >>> write(\'./filename\', \'contents\') 
 >>> _=adb.push(\'./filename\', \'/data/local/tmp\') 
 >>> adb.read(\'/data/local/tmp/filename\') 
 \'contents\' 
 >>> adb.push(\'./filename\', \'/does/not/exist\') 
 Traceback (most recent call last): 
 PwnlibException: Could not stat \'/does/not/exist\''"
"def oo_ami_selector(data, image_name): 
    if (not isinstance(data, list)): 
      raise errors.AnsibleFilterError('|failed   expects   first   param   is   a   list') 
   if (not data): 
      return None 
   elif ((image_name is None) or (not image_name.endswith('_*'))): 
      ami = sorted(data, key=itemgetter('name'), reverse=True)[0] 
      return ami['ami_id'] 
   else: 
      ami_info = [(ami, ami['name'].split('_')[(-1)]) for ami in data] 
      ami = sorted(ami_info, key=itemgetter(1), reverse=True)[0][0] 
      return ami['ami_id']"," 'Returns the AMI ID of the first image in the list that matches the 
 image name. 
 If the image name is not provided, the first image in the list is returned. 
 If the image name is provided, the first image in the list that matches the 
 name is returned. 
 If no images match the name, None is returned. 
 :param data: List of AMI information 
 :param image_name: Name of the AMI to match 
 :returns: AMI ID of the matching image, or None if no image matches'","'This takes a list of amis and an image name and attempts to return 
 the latest ami.'"
"def get_lib_extension(): 
    if (sys.platform in ['win32', 'cygwin']): 
      return 'pyd' 
   else: 
      return 'so'", 'Get the extension for the library.','Return the platform-dependent extension for compiled modules.'
"def _sort_names(FQDNs): 
    return sorted(FQDNs, key=(lambda fqdn: fqdn.split('.')[::(-1)][1:]))"," 'Returns a sorted list of FQDNs. 
 The FQDNs are sorted in lexicographical order, with the second part of 
 the FQDN coming first. 
 :param FQDNs: list of FQDNs 
 :returns: sorted list of FQDNs'","'Sort FQDNs by SLD (and if many, by their subdomains) 
 :param list FQDNs: list of domain names 
 :returns: Sorted list of domain names 
 :rtype: list'"
"def tostring(xml=None, xmlns=u'', stream=None, outbuffer=u'', top_level=False, open_only=False, namespaces=None): 
    output = [outbuffer] 
   tag_name = xml.tag.split(u'}', 1)[(-1)] 
   if (u'}' in xml.tag): 
      tag_xmlns = xml.tag.split(u'}', 1)[0][1:] 
   else: 
      tag_xmlns = u'' 
   default_ns = u'' 
   stream_ns = u'' 
   use_cdata = False 
   if stream: 
      default_ns = stream.default_ns 
      stream_ns = stream.stream_ns 
      use_cdata = stream.use_cdata 
   namespace = u'' 
   if tag_xmlns: 
      if ((top_level and (tag_xmlns not in [default_ns, xmlns, stream_ns])) or ((not top_level) and (tag_xmlns != xmlns))): 
         namespace = (u'   xmlns=""%s""' % tag_xmlns) 
   if (stream and (tag_xmlns in stream.namespace_map)): 
      mapped_namespace = stream.namespace_map[tag_xmlns] 
      if mapped_namespace: 
         tag_name = (u'%s:%s' % (mapped_namespace, tag_name)) 
   output.append((u'<%s' % tag_name)) 
   output.append(namespace) 
   new_namespaces = set() 
   for (attrib, value) in xml.attrib.items(): 
      value = escape(value, use_cdata) 
      if (u'}' not in attrib): 
         output.append((u'   %s=""%s""' % (attrib, value))) 
      else: 
         attrib_ns = attrib.split(u'}')[0][1:] 
         attrib = attrib.split(u'}')[1] 
         if (attrib_ns == XML_NS): 
            output.append((u'   xml:%s=""%s""' % (attrib, value))) 
         elif (stream and (attrib_ns in stream.namespace_map)): 
            mapped_ns = stream.namespace_map[attrib_ns] 
            if mapped_ns: 
               if (namespaces is None): 
                  namespaces = set() 
               if (attrib_ns not in namespaces): 
                  namespaces.add(attrib_ns) 
                  new_namespaces.add(attrib_ns) 
                  output.append((u'   xmlns:%s=""%s""' % (mapped_ns, attrib_ns))) 
               output.append((u'   %s:%s=""%s""' % (mapped_ns, attrib, value))) 
   if open_only: 
      output.append(u'>') 
      return u''.join(output) 
   if (len(xml) or xml.text): 
      output.append(u'>') 
      if xml.text: 
         output.append(escape(xml.text, use_cdata)) 
      if len(xml): 
         for child in xml: 
            output.append(tostring(child, tag_xmlns, stream, namespaces=namespaces)) 
      output.append((u'</%s>' % tag_name)) 
   elif xml.text: 
      output.append((u'>%s</%s>' % (escape(xml.text, use_cdata), tag_name))) 
   else: 
      output.append(u'   />') 
   if xml.tail: 
      output.append(escape(xml.tail, use_cdata)) 
   for ns in new_namespaces: 
      namespaces.remove(ns) 
   return u''.join(output)"," 'Convert a :class:`xml.sax.saxutils.XMLSerializer` object to a string 
 :param xml: the :class:`xml.sax.saxutils.XMLSerializer` object to convert 
 :param xmlns: the namespace to use for the root element (defaults to the 
 default namespace of the document) 
 :param stream: the :class:`xml.sax.saxutils.XMLStreamWriter` object to use 
 for writing the output (defaults to :class:`xml.sax.saxutils.XMLSerializer`) 
 :param outbuffer: the buffer to write to (defaults to the :class:`xml.sax.saxutils.XMLSerializer`) 
 :param top_level: whether to use the root element (defaults to True) 
 :param open_only: whether to return the string without the closing tag 
 :param namespaces: the namespaces to use for the root element (defaults to 
 the namespace map of the :class:`xml.sax.saxutils.XMLSerializer`) 
 :return: the string representation of the :class:`xml.sax.sax","'Serialize an XML object to a Unicode string. 
 If an outer xmlns is provided using ``xmlns``, then the current element\'s 
 namespace will not be included if it matches the outer namespace. An 
 exception is made for elements that have an attached stream, and appear 
 at the stream root. 
 :param XML xml: The XML object to serialize. 
 :param string xmlns: Optional namespace of an element wrapping the XML 
 object. 
 :param stream: The XML stream that generated the XML object. 
 :param string outbuffer: Optional buffer for storing serializations 
 during recursive calls. 
 :param bool top_level: Indicates that the element is the outermost 
 element. 
 :param set namespaces: Track which namespaces are in active use so 
 that new ones can be declared when needed. 
 :type xml: :py:class:`~xml.etree.ElementTree.Element` 
 :type stream: :class:`~sleekxmpp.xmlstream.xmlstream.XMLStream` 
 :rtype: Unicode string'"
"def safe_join(directory, *pathnames): 
    for filename in pathnames: 
      if (filename != ''): 
         filename = posixpath.normpath(filename) 
      for sep in _os_alt_seps: 
         if (sep in filename): 
            raise NotFound() 
      if (os.path.isabs(filename) or (filename == '..') or filename.startswith('../')): 
         raise NotFound() 
      directory = os.path.join(directory, filename) 
   return directory"," 'Return a directory path joined to the given directory. 
 If any of the pathnames is absolute, or contains \'../\', or contains a 
 non-POSIX-compatible separator, a NotFound exception is raised. 
 If any of the pathnames is relative, it is resolved relative to the given 
 directory. 
 :param directory: Directory to join to. 
 :param pathnames: Pathnames to join. 
 :return: Directory path.'","'Safely join `directory` and zero or more untrusted `pathnames` 
 components. 
 Example usage:: 
 @app.route(\'/wiki/<path:filename>\') 
 def wiki_page(filename): 
 filename = safe_join(app.config[\'WIKI_FOLDER\'], filename) 
 with open(filename, \'rb\') as fd: 
 content = fd.read()  # Read and process the file content... 
 :param directory: the trusted base directory. 
 :param pathnames: the untrusted pathnames relative to that directory. 
 :raises: :class:`~werkzeug.exceptions.NotFound` if one or more passed 
 paths fall out of its boundaries.'"
"def urldecode(query): 
    if (query and (not (set(query) <= urlencoded))): 
      error = u""Error   trying   to   decode   a   non   urlencoded   string.   Found   invalid   characters:   %s   in   the   string:   '%s'.   Please   ensure   the   request/response   body   is   x-www-form-urlencoded."" 
      raise ValueError((error % ((set(query) - urlencoded), query))) 
   invalid_hex = u'%[^0-9A-Fa-f]|%[0-9A-Fa-f][^0-9A-Fa-f]' 
   if len(re.findall(invalid_hex, query)): 
      raise ValueError(u'Invalid   hex   encoding   in   query   string.') 
   query = (query.encode(u'utf-8') if ((not PY3) and isinstance(query, unicode_type)) else query) 
   params = urlparse.parse_qsl(query, keep_blank_values=True) 
   return decode_params_utf8(params)"," 'Decode a query string. 
 :param query: The query string to decode. 
 :type query: str or unicode 
 :return: The decoded query string. 
 :rtype: dict'","'Decode a query string in x-www-form-urlencoded format into a sequence 
 of two-element tuples. 
 Unlike urlparse.parse_qsl(..., strict_parsing=True) urldecode will enforce 
 correct formatting of the query string by validation. If validation fails 
 a ValueError will be raised. urllib.parse_qsl will only raise errors if 
 any of name-value pairs omits the equals sign.'"
"@contextmanager 
 def context(grpc_context): 
    try: 
      (yield) 
   except KeyError as key_error: 
      grpc_context.code(status.Code.NOT_FOUND) 
      grpc_context.details('Unable   to   find   the   item   keyed   by   {}'.format(key_error))"," 'Context manager for grpc.Context. 
 Context manager that uses the grpc.Context to store the context 
 information. 
 The context information is stored in a dictionary that can be accessed 
 using the key. 
 :param grpc_context: grpc.Context to use 
 :type grpc_context: grpc.Context'",'A context manager that automatically handles KeyError.'
"def delete_device(name, safety_on=True): 
    config = _get_vistara_configuration() 
   if (not config): 
      return False 
   access_token = _get_oath2_access_token(config['client_key'], config['client_secret']) 
   if (not access_token): 
      return 'Vistara   access   token   not   available' 
   query_string = 'dnsName:{0}'.format(name) 
   devices = _search_devices(query_string, config['client_id'], access_token) 
   if (not devices): 
      return 'No   devices   found' 
   device_count = len(devices) 
   if (safety_on and (device_count != 1)): 
      return 'Expected   to   delete   1   device   and   found   {0}.   Set   safety_on=False   to   override.'.format(device_count) 
   delete_responses = [] 
   for device in devices: 
      device_id = device['id'] 
      log.debug(device_id) 
      delete_response = _delete_resource(device_id, config['client_id'], access_token) 
      if (not delete_response): 
         return False 
      delete_responses.append(delete_response) 
   return delete_responses"," 'Delete a device from Vistara. 
 :param name: Name of the device to delete 
 :type name: str 
 :param safety_on: If set to False, will delete all devices. 
 :type safety_on: bool 
 :return: A list of delete responses from Vistara'","'Deletes a device from Vistara based on DNS name or partial name. By default, 
 delete_device will only perform the delete if a single host is returned. Set 
 safety_on=False to delete all matches (up to default API search page size) 
 CLI Example: 
 .. code-block:: bash 
 salt-run vistara.delete_device \'hostname-101.mycompany.com\' 
 salt-run vistara.delete_device \'hostname-101\' 
 salt-run vistara.delete_device \'hostname-1\' safety_on=False'"
"def _write_3(fid, val): 
    f_bytes = np.zeros(3, dtype=np.uint8) 
   f_bytes[0] = ((val >> 16) & 255) 
   f_bytes[1] = ((val >> 8) & 255) 
   f_bytes[2] = (val & 255) 
   fid.write(f_bytes.tostring())"," 'Write a 3-byte integer to the file descriptor. 
 Parameters 
 fid : file descriptor 
 val : integer 
 Returns 
 None'",'Write 3 byte integer to file.'
"def make_avpr_object(json_data): 
    if (hasattr(json_data, 'get') and callable(json_data.get)): 
      name = json_data.get('protocol') 
      namespace = json_data.get('namespace') 
      types = json_data.get('types') 
      messages = json_data.get('messages') 
      return Protocol(name, namespace, types, messages) 
   else: 
      raise ProtocolParseException(('Not   a   JSON   object:   %s' % json_data))", 'Create a Protocol object from a JSON data structure.','Build Avro Protocol from data parsed out of JSON string.'
"@constructor 
 def shape_padaxis(t, axis): 
    _t = as_tensor_variable(t) 
   ndim = (_t.ndim + 1) 
   if (not ((- ndim) <= axis < ndim)): 
      msg = 'axis   {0}   is   out   of   bounds   [-{1},   {1})'.format(axis, ndim) 
      raise IndexError(msg) 
   if (axis < 0): 
      axis += ndim 
   pattern = [i for i in xrange(_t.type.ndim)] 
   pattern.insert(axis, 'x') 
   return DimShuffle(_t.broadcastable, pattern)(_t)"," 'Returns a tensor with the given axis replaced by the \'x\' axis. 
 Parameters 
 t : tensor 
 The input tensor. 
 axis : int 
 The index of the axis to replace. 
 Returns 
 out : tensor 
 A tensor with the given axis replaced by the \'x\' axis.'","'Reshape `t` by inserting 1 at the dimension `axis`. 
 Example 
 >>> tensor = theano.tensor.tensor3() 
 >>> theano.tensor.shape_padaxis(tensor, axis=0) 
 DimShuffle{x,0,1,2}.0 
 >>> theano.tensor.shape_padaxis(tensor, axis=1) 
 DimShuffle{0,x,1,2}.0 
 >>> theano.tensor.shape_padaxis(tensor, axis=3) 
 DimShuffle{0,1,2,x}.0 
 >>> theano.tensor.shape_padaxis(tensor, axis=-1) 
 DimShuffle{0,1,2,x}.0 
 See Also 
 shape_padleft 
 shape_padright 
 Dimshuffle'"
"def dumps_with_persistent_ids(obj, protocol=None): 
    file = BytesIO() 
   pickler = pickle.Pickler(file, protocol) 
   pickler.persistent_id = _persistent_id 
   pickler.dump(obj) 
   return file.getvalue()"," 'Pickle an object with persistent ids. 
 This is used to pickle objects that have persistent ids, so that 
 they can be reloaded without re-pickling.'","'Performs a pickle dumps on the given object, substituting all references to 
 a TradingEnvironment or AssetFinder with tokenized representations. 
 All arguments are passed to pickle.Pickler and are described therein.'"
"def cluster_remove(version, name='main', stop=False): 
    cmd = [salt.utils.which('pg_dropcluster')] 
   if stop: 
      cmd += ['--stop'] 
   cmd += [version, name] 
   cmdstr = '   '.join([pipes.quote(c) for c in cmd]) 
   ret = __salt__['cmd.run_all'](cmdstr, python_shell=False) 
   if (ret.get('retcode', 0) != 0): 
      log.error('Error   removing   a   Postgresql   cluster   {0}/{1}'.format(version, name)) 
   else: 
      ret['changes'] = 'Successfully   removed   cluster   {0}/{1}'.format(version, name) 
   return ret", 'Remove a Postgresql cluster from the cluster manager',"'Remove a cluster on a Postgres server. By default it doesn\'t try 
 to stop the cluster. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' postgres.cluster_remove \'9.3\' 
 salt \'*\' postgres.cluster_remove \'9.3\' \'main\' 
 salt \'*\' postgres.cluster_remove \'9.3\' \'main\' stop=True'"
"def returner(ret): 
    _options = _get_options(ret) 
   sid = _options.get('sid', None) 
   token = _options.get('token', None) 
   sender = _options.get('from', None) 
   receiver = _options.get('to', None) 
   if ((sid is None) or (token is None)): 
      log.error('Twilio   sid/authentication   token   missing') 
      return None 
   if ((sender is None) or (receiver is None)): 
      log.error('Twilio   to/from   fields   are   missing') 
      return None 
   client = TwilioRestClient(sid, token) 
   try: 
      message = client.messages.create(body='Minion:   {0}\nCmd:   {1}\nSuccess:   {2}\n\nJid:   {3}'.format(ret['id'], ret['fun'], ret['success'], ret['jid']), to=receiver, from_=sender) 
   except TwilioRestException as e: 
      log.error('Twilio   [https://www.twilio.com/docs/errors/{0}]'.format(e.code)) 
      return False 
   return True"," 'Send a message to a Twilio account. 
 :param ret: The result of the call. 
 :return: True if the message was sent, False otherwise.'",'Return a response in an SMS message'
"def expanding_quantile(arg, quantile, min_periods=1, freq=None): 
    return ensure_compat('expanding', 'quantile', arg, freq=freq, min_periods=min_periods, func_kw=['quantile'], quantile=quantile)"," 'Apply expanding quantile to a time series. 
 Parameters 
 arg : TimeSeries or TimeIndex 
 The time series to expand. 
 quantile : float 
 The quantile to expand to. 
 min_periods : int 
 The minimum number of periods to use. 
 freq : DateOffset, optional 
 The frequency of the time series. If None, the frequency of the input 
 time series is used. 
 Returns 
 TimeSeries 
 The expanded time series. 
 Examples 
 >>> t = TimeSeries(range(10), freq=None) 
 >>> t 
 0    1    2    3    4    5    6    7    8    9 
 1900-01-01  0  1  2  3  4  5  6  7  8  9 
 >>> t.expanding_quantile(0.5) 
 0    1    2    3    4    5    6    7    8    9 
 1900-01-01  0  1  2  3  4  5","'Expanding quantile. 
 Parameters 
 arg : Series, DataFrame 
 quantile : float 
 0 <= quantile <= 1 
 min_periods : int, default None 
 Minimum number of observations in window required to have a value 
 (otherwise result is NA). 
 freq : string or DateOffset object, optional (default None) 
 Frequency to conform the data to before computing the 
 statistic. Specified as a frequency string or DateOffset object. 
 Returns 
 y : type of input argument 
 Notes 
 The `freq` keyword is used to conform time series data to a specified 
 frequency by resampling the data. This is done with the default parameters 
 of :meth:`~pandas.Series.resample` (i.e. using the `mean`). 
 To learn more about the frequency strings, please see `this link 
 <http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases>`__.'"
"def is_funcvar(expr): 
    assert isinstance(expr, string_types), (u'%s   is   not   a   string' % expr) 
   return (re.match(u'^[A-Z]\\d*$', expr) is not None)"," 'Returns true if expr is a valid function variable. 
 This is a function variable if it is a single letter followed by a 
 number. 
 >>> is_funcvar(\'A1\') 
 True 
 >>> is_funcvar(\'A12\') 
 False 
 >>> is_funcvar(\'A123\') 
 False 
 >>> is_funcvar(\'A1234\') 
 True 
 >>> is_funcvar(\'A12345\') 
 True 
 >>> is_funcvar(\'A123456\') 
 False 
 >>> is_funcvar(\'A1234567\') 
 False'","'A function variable must be a single uppercase character followed by 
 zero or more digits. 
 :param expr: str 
 :return: bool True if expr is of the correct form'"
"def getLocalAndroidPath(client, args): 
    localPath = os.path.join(args.localOutputFolder, '{0}-{1}'.format(client.conn.modules['pupydroid.utils'].getAndroidID(), client.desc['user'])) 
   if (not os.path.exists(localPath)): 
      logging.info('Creating   {0}   folder   locally'.format(localPath)) 
      os.makedirs(localPath) 
   return localPath", 'Return the path to the local Android folder',"'Returns the current local path for saving data locally 
 Create folder if not exists'"
"def main(): 
    parser = ArgumentParser(description=CMDLINE_HELP) 
   parser.add_argument('--noserver', action='store_true', dest='noserver', default=False, help='Do   not   start   Server   process') 
   parser.add_argument('--noportal', action='store_true', dest='noportal', default=False, help='Do   not   start   Portal   process') 
   parser.add_argument('--logserver', action='store_true', dest='logserver', default=False, help='Log   Server   output   to   logfile') 
   parser.add_argument('--iserver', action='store_true', dest='iserver', default=False, help='Server   in   interactive   mode') 
   parser.add_argument('--iportal', action='store_true', dest='iportal', default=False, help='Portal   in   interactive   mode') 
   parser.add_argument('--pserver', action='store_true', dest='pserver', default=False, help='Profile   Server') 
   parser.add_argument('--pportal', action='store_true', dest='pportal', default=False, help='Profile   Portal') 
   parser.add_argument('--nologcycle', action='store_false', dest='nologcycle', default=True, help='Do   not   cycle   log   files') 
   parser.add_argument('--doexit', action='store_true', dest='doexit', default=False, help='Immediately   exit   after   processes   have   started.') 
   parser.add_argument('gamedir', help='path   to   game   dir') 
   parser.add_argument('twistdbinary', help='path   to   twistd   binary') 
   parser.add_argument('slogfile', help='path   to   server   log   file') 
   parser.add_argument('plogfile', help='path   to   portal   log   file') 
   parser.add_argument('hlogfile', help='path   to   http   log   file') 
   args = parser.parse_args() 
   global GAMEDIR 
   global SERVER_LOGFILE, PORTAL_LOGFILE, HTTP_LOGFILE 
   global SERVER_PIDFILE, PORTAL_PIDFILE 
   global SERVER_RESTART, PORTAL_RESTART 
   global SPROFILER_LOGFILE, PPROFILER_LOGFILE 
   GAMEDIR = args.gamedir 
   sys.path.insert(1, os.path.join(GAMEDIR, SERVERDIR)) 
   SERVER_PIDFILE = os.path.join(GAMEDIR, SERVERDIR, 'server.pid') 
   PORTAL_PIDFILE = os.path.join(GAMEDIR, SERVERDIR, 'portal.pid') 
   SERVER_RESTART = os.path.join(GAMEDIR, SERVERDIR, 'server.restart') 
   PORTAL_RESTART = os.path.join(GAMEDIR, SERVERDIR, 'portal.restart') 
   SERVER_LOGFILE = args.slogfile 
   PORTAL_LOGFILE = args.plogfile 
   HTTP_LOGFILE = args.hlogfile 
   TWISTED_BINARY = args.twistdbinary 
   SPROFILER_LOGFILE = os.path.join(GAMEDIR, SERVERDIR, 'logs', 'server.prof') 
   PPROFILER_LOGFILE = os.path.join(GAMEDIR, SERVERDIR, 'logs', 'portal.prof') 
   server_argv = [TWISTED_BINARY, '--nodaemon', ('--logfile=%s' % SERVER_LOGFILE), ('--pidfile=%s' % SERVER_PIDFILE), ('--python=%s' % SERVER_PY_FILE)] 
   portal_argv = [TWISTED_BINARY, ('--logfile=%s' % PORTAL_LOGFILE), ('--pidfile=%s' % PORTAL_PIDFILE), ('--python=%s' % PORTAL_PY_FILE)] 
   pserver_argv = ['--savestats', '--profiler=cprofile', ('--profile=%s' % SPROFILER_LOGFILE)] 
   pportal_argv = ['--savestats', '--profiler=cprofile', ('--profile=%s' % PPROFILER_LOGFILE)] 
   pid = get_pid(SERVER_PIDFILE) 
   if (pid and (not args.noserver)): 
      print(('\nEvennia   Server   is   already   running   as   process   %(pid)s.   Not   restarted.' % {'pid': pid})) 
      args.noserver = True 
   if args.noserver: 
      server_argv = None 
   else: 
      set_restart_mode(SERVER_RESTART, 'shutdown') 
      if (not args.logserver): 
         del server_argv[2] 
         print('\nStarting   Evennia   Server   (output   to   stdout).') 
      else: 
         if (not args.nologcycle): 
            cycle_logfile(SERVER_LOGFILE) 
         print('\nStarting   Evennia   Server   (output   to   server   logfile).') 
      if args.pserver: 
         server_argv.extend(pserver_argv) 
         print('\nRunning   Evennia   Server   under   cProfile.') 
   pid = get_pid(PORTAL_PIDFILE) 
   if (pid and (not args.noportal)): 
      print(('\nEvennia   Portal   is   already   running   as   process   %(pid)s.   Not   restarted.' % {'pid': pid})) 
      args.noportal = True 
   if args.noportal: 
      portal_argv = None 
   else: 
      if args.iportal: 
         portal_argv[1] = '--nodaemon' 
         set_restart_mode(PORTAL_RESTART, True) 
         print('\nStarting   Evennia   Portal   in   non-Daemon   mode   (output   to   stdout).') 
      else: 
         if (not args.nologcycle): 
            cycle_logfile(PORTAL_LOGFILE) 
            cycle_logfile(HTTP_LOGFILE) 
         set_restart_mode(PORTAL_RESTART, False) 
         print('\nStarting   Evennia   Portal   in   Daemon   mode   (output   to   portal   logfile).') 
      if args.pportal: 
         portal_argv.extend(pportal_argv) 
         print('\nRunning   Evennia   Portal   under   cProfile.') 
   if args.doexit: 
      print(PROCESS_DOEXIT) 
   if (os.name == 'nt'): 
      if server_argv: 
         del server_argv[(-2)] 
      if portal_argv: 
         del portal_argv[(-2)] 
   start_services(server_argv, portal_argv, doexit=args.doexit)"," 'Starts the server and portal processes. 
 :returns: None 
 :rtype: None'","'This handles the command line input of the runner, usually created by 
 the evennia launcher'"
"def mult(a, b): 
    try: 
      return (a * b) 
   except TypeError: 
      return (to_decimal(a) * to_decimal(b))"," 'Multiply two Decimal objects. 
 Parameters 
 a : Decimal 
 b : Decimal 
 Returns 
 a * b : Decimal 
 Examples 
 >>> from sympy.core import Decimal 
 >>> from sympy.abc import x 
 >>> Decimal(1).mult(x) 
 0 
 >>> Decimal(1).mult(x, x) 
 1 
 >>> Decimal(1).mult(x, x, x) 
 1 
 >>> Decimal(1).mult(x, x, x, x) 
 1 
 >>> Decimal(1).mult(x, x, x, x, x) 
 1 
 >>> Decimal(1).mult(x, x, x, x, x, x) 
 1 
 >>> Decimal(1).mult(x, x, x, x, x, x, x) 
 1 
 >>> Decimal(1).mult(x, x, x, x, x, x, x, x) 
 1 
 >>> Decimal(1).mult(x","'If we get TypeError from * (possibly because one is float and the other is Decimal), then promote them both to Decimal.'"
"def test_check_dictionary(): 
    def CheckDictionary(C): 
      C.newClassAttr = 'xyz' 
      AreEqual(C.newClassAttr, 'xyz') 
      a = C() 
      a.__dict__[1] = '1' 
      if (object in C.__bases__): 
         try: 
            C.__dict__[2] = '2' 
            AssertUnreachable() 
         except TypeError: 
            pass 
         AreEqual(C.__dict__.has_key(2), False) 
      AreEqual(a.__dict__.has_key(1), True) 
      AreEqual(dir(a).__contains__(1), True) 
      AreEqual(repr(a.__dict__), ""{1:   '1'}"") 
      C.newTypeAttr = 1 
      AreEqual(hasattr(C, 'newTypeAttr'), True) 
      class OldClass: 
         pass 
      if isinstance(C, type(OldClass)): 
         C.__dict__ = dict(C.__dict__) 
         AreEqual(hasattr(C, 'newTypeAttr'), True) 
      else: 
         try: 
            C.__dict__ = {} 
            AssertUnreachable() 
         except AttributeError: 
            pass 
      a.newInstanceAttr = 1 
      AreEqual(hasattr(a, 'newInstanceAttr'), True) 
      a.__dict__ = dict(a.__dict__) 
      AreEqual(hasattr(a, 'newInstanceAttr'), True) 
      a.abc = 'xyz' 
      AreEqual(hasattr(a, 'abc'), True) 
      AreEqual(getattr(a, 'abc'), 'xyz') 
   class OldClass: 
      def __init__(self): 
         pass 
   class NewClass(object, ): 
      def __init__(self): 
         pass 
   CheckDictionary(OldClass) 
   CheckDictionary(NewClass)"," 'Test that a dictionary is correctly checked when it is passed to 
 __new__.'",'tests to verify that Symbol dictionaries do the right thing in dynamic scenarios'
"def create_generic_related_manager(superclass): 
    class GenericRelatedObjectManager(superclass, ): 
      def __init__(self, model=None, instance=None, symmetrical=None, source_col_name=None, target_col_name=None, content_type=None, content_type_field_name=None, object_id_field_name=None, prefetch_cache_name=None): 
         super(GenericRelatedObjectManager, self).__init__() 
         self.model = model 
         self.content_type = content_type 
         self.symmetrical = symmetrical 
         self.instance = instance 
         self.source_col_name = source_col_name 
         self.target_col_name = target_col_name 
         self.content_type_field_name = content_type_field_name 
         self.object_id_field_name = object_id_field_name 
         self.prefetch_cache_name = prefetch_cache_name 
         self.pk_val = self.instance._get_pk_val() 
         self.core_filters = {(u'%s__pk' % content_type_field_name): content_type.id, (u'%s__exact' % object_id_field_name): instance._get_pk_val()} 
      def get_query_set(self): 
         try: 
            return self.instance._prefetched_objects_cache[self.prefetch_cache_name] 
         except (AttributeError, KeyError): 
            db = (self._db or router.db_for_read(self.model, instance=self.instance)) 
            return super(GenericRelatedObjectManager, self).get_query_set().using(db).filter(**self.core_filters) 
      def get_prefetch_query_set(self, instances): 
         db = (self._db or router.db_for_read(self.model, instance=instances[0])) 
         query = {(u'%s__pk' % self.content_type_field_name): self.content_type.id, (u'%s__in' % self.object_id_field_name): set((obj._get_pk_val() for obj in instances))} 
         qs = super(GenericRelatedObjectManager, self).get_query_set().using(db).filter(**query) 
         object_id_converter = instances[0]._meta.pk.to_python 
         return (qs, (lambda relobj: object_id_converter(getattr(relobj, self.object_id_field_name))), (lambda obj: obj._get_pk_val()), False, self.prefetch_cache_name) 
      def add(self, *objs): 
         for obj in objs: 
            if (not isinstance(obj, self.model)): 
               raise TypeError((u""'%s'   instance   expected"" % self.model._meta.object_name)) 
            setattr(obj, self.content_type_field_name, self.content_type) 
            setattr(obj, self.object_id_field_name, self.pk_val) 
            obj.save() 
      add.alters_data = True 
      def remove(self, *objs): 
         db = router.db_for_write(self.model, instance=self.instance) 
         for obj in objs: 
            obj.delete(using=db) 
      remove.alters_data = True 
      def clear(self): 
         db = router.db_for_write(self.model, instance=self.instance) 
         for obj in self.all(): 
            obj.delete(using=db) 
      clear.alters_data = True 
      def create(self, **kwargs): 
         kwargs[self.content_type_field_name] = self.content_type 
         kwargs[self.object_id_field_name] = self.pk_val 
         db = router.db_for_write(self.model, instance=self.instance) 
         return super(GenericRelatedObjectManager, self).using(db).create(**kwargs) 
      create.alters_data = True 
   return GenericRelatedObjectManager"," 'Create a generic related manager for a model. 
 This manager is used to create and manage related objects. 
 This manager is a subclass of :class:`~django.db.models.query.QuerySet` 
 and supports the same methods as a regular QuerySet. 
 :param superclass: The manager class to inherit from. 
 :param model: The model to create the manager for. 
 :param instance: The instance to create the manager for. 
 :param symmetrical: If true, the related objects will be created 
 in the same direction as the instance\'s related objects. 
 :param source_col_name: The name of the source column in the 
 related model. 
 :param target_col_name: The name of the target column in the 
 related model. 
 :param content_type: The content type of the related object. 
 :param content_type_field_name: The name of the content type field 
 in the related model. 
 :param object_id_field_name: The name of the object id field in the 
 related model. 
 :param prefetch_cache_name:","'Factory function for a manager that subclasses \'superclass\' (which is a 
 Manager) and adds behavior for generic related objects.'"
"def get_metadata(stream, extract_cover=True): 
    mi = MetaInformation(_('Unknown'), [_('Unknown')]) 
   stream.seek(0) 
   pml = '' 
   if stream.name.endswith('.pmlz'): 
      with TemporaryDirectory('_unpmlz') as tdir: 
         zf = ZipFile(stream) 
         zf.extractall(tdir) 
         pmls = glob.glob(os.path.join(tdir, '*.pml')) 
         for p in pmls: 
            with open(p, 'r+b') as p_stream: 
               pml += p_stream.read() 
         if extract_cover: 
            mi.cover_data = get_cover(os.path.splitext(os.path.basename(stream.name))[0], tdir, True) 
   else: 
      pml = stream.read() 
      if extract_cover: 
         mi.cover_data = get_cover(os.path.splitext(os.path.basename(stream.name))[0], os.path.abspath(os.path.dirname(stream.name))) 
   for comment in re.findall('(?mus)\\\\v.*?\\\\v', pml): 
      m = re.search('TITLE=""(.*?)""', comment) 
      if m: 
         mi.title = re.sub('[\x00-\x1f]', '', prepare_string_for_xml(m.group(1).strip().decode('cp1252', 'replace'))) 
      m = re.search('AUTHOR=""(.*?)""', comment) 
      if m: 
         if (mi.authors == [_('Unknown')]): 
            mi.authors = [] 
         mi.authors.append(re.sub('[\x00-\x1f]', '', prepare_string_for_xml(m.group(1).strip().decode('cp1252', 'replace')))) 
      m = re.search('PUBLISHER=""(.*?)""', comment) 
      if m: 
         mi.publisher = re.sub('[\x00-\x1f]', '', prepare_string_for_xml(m.group(1).strip().decode('cp1252', 'replace'))) 
      m = re.search('COPYRIGHT=""(.*?)""', comment) 
      if m: 
         mi.rights = re.sub('[\x00-\x1f]', '', prepare_string_for_xml(m.group(1).strip().decode('cp1252', 'replace'))) 
      m = re.search('ISBN=""(.*?)""', comment) 
      if m: 
         mi.isbn = re.sub('[\x00-\x1f]', '', prepare_string_for_xml(m.group(1).strip().decode('cp1252', 'replace'))) 
   return mi"," 'Reads the metadata from the given stream. 
 :param stream: the stream to read the metadata from. 
 :param extract_cover: if True, the cover will be extracted to the given 
 directory (default: True). 
 :return: the metadata object.'",'Return metadata as a L{MetaInfo} object'
"def install_readline(hook): 
    global readline_hook, readline_ref 
   readline_hook = hook 
   PyOS_RFP = c_int.from_address(Console.GetProcAddress(sys.dllhandle, 'PyOS_ReadlineFunctionPointer')) 
   if (sys.version < '2.3'): 
      readline_ref = HOOKFUNC22(hook_wrapper) 
   else: 
      readline_ref = HOOKFUNC23(hook_wrapper_23) 
   func_start = c_int.from_address(addressof(readline_ref)).value 
   PyOS_RFP.value = func_start", 'Install the hook function in readline.','Set up things for the interpreter to call our function like GNU readline.'
"def _run_shell_command(cmd, tmpdir): 
    cmdsuf = '' 
   if (platform.system() == 'Windows'): 
      cmdsuf = '.bat' 
      cmd = ('@echo   off\r\n' + cmd) 
   (handle, path) = tempfile.mkstemp(text=True, dir=tmpdir, suffix=cmdsuf) 
   os.write(handle, cmd.encode('utf-8')) 
   os.close(handle) 
   os.chmod(path, stat.S_IRWXU) 
   proc = Popen(path, shell=True, stdout=PIPE, stderr=PIPE) 
   proc.wait() 
   (stdout, _) = proc.communicate() 
   os.unlink(path) 
   return _chomp(as_unicode(stdout))"," 'Run a command in a temporary directory and return the output. 
 :param cmd: The command to run 
 :param tmpdir: The directory to create the temporary file in 
 :return: The command\'s output 
 :rtype: unicode'",'Write the code to a temporary file.'
"def _run_composer(action, directory=None, composer=None, php=None, runas=None, prefer_source=None, prefer_dist=None, no_scripts=None, no_plugins=None, optimize=None, no_dev=None, quiet=False, composer_home='/root', extra_flags=None): 
    if (composer is not None): 
      if (php is None): 
         php = 'php' 
   else: 
      composer = 'composer' 
   if (not _valid_composer(composer)): 
      raise CommandNotFoundError(""'composer.{0}'   is   not   available.   Couldn't   find   '{1}'."".format(action, composer)) 
   if (action is None): 
      raise SaltInvocationError(""The   'action'   argument   is   required"") 
   if ((directory is None) and (action != 'selfupdate')): 
      raise SaltInvocationError(""The   'directory'   argument   is   required   for   composer.{0}"".format(action)) 
   cmd = [composer, action, '--no-interaction', '--no-ansi'] 
   if (extra_flags is not None): 
      cmd.extend(salt.utils.shlex_split(extra_flags)) 
   if (php is not None): 
      cmd = ([php] + cmd) 
   if (directory is not None): 
      cmd.extend(['--working-dir', directory]) 
   if (quiet is True): 
      cmd.append('--quiet') 
   if (no_dev is True): 
      cmd.append('--no-dev') 
   if (prefer_source is True): 
      cmd.append('--prefer-source') 
   if (prefer_dist is True): 
      cmd.append('--prefer-dist') 
   if (no_scripts is True): 
      cmd.append('--no-scripts') 
   if (no_plugins is True): 
      cmd.append('--no-plugins') 
   if (optimize is True): 
      cmd.append('--optimize-autoloader') 
   result = __salt__['cmd.run_all'](cmd, runas=runas, env={'COMPOSER_HOME': composer_home}, python_shell=False) 
   if (result['retcode'] != 0): 
      raise CommandExecutionError(result['stderr']) 
   if (quiet is True): 
      return True 
   return result"," 'Run composer action 
 This function is a wrapper around the composer action. 
 It is meant to be used by Salt. 
 .. versionadded:: 2014.7.0 
 .. versionchanged:: 2014.7.0 
 Added support for the ``no_dev`` flag. 
 .. versionchanged:: 2014.7.0 
 Added support for the ``no_scripts`` flag. 
 .. versionchanged:: 2014.7.0 
 Added support for the ``no_plugins`` flag. 
 .. versionchanged:: 2014.7.0 
 Added support for the ``optimize`` flag. 
 .. versionchanged:: 2014.7.0 
 Added support for the ``prefer_source`` flag. 
 .. versionchanged:: 2014.7.0 
 Added support for the ``prefer_dist`` flag. 
 .. versionchanged:: 2014.7.0 
 Added support for the ``directory`` flag. 
 .. versionchanged:: 2014.7.0 
 Added support","'Run PHP\'s composer with a specific action. 
 If composer has not been installed globally making it available in the 
 system PATH & making it executable, the ``composer`` and ``php`` parameters 
 will need to be set to the location of the executables. 
 action 
 The action to pass to composer (\'install\', \'update\', \'selfupdate\', etc). 
 directory 
 Directory location of the composer.json file.  Required except when 
 action=\'selfupdate\' 
 composer 
 Location of the composer.phar file. If not set composer will 
 just execute ""composer"" as if it is installed globally. 
 (i.e. /path/to/composer.phar) 
 php 
 Location of the php executable to use with composer. 
 (i.e. /usr/bin/php) 
 runas 
 Which system user to run composer as. 
 prefer_source 
 --prefer-source option of composer. 
 prefer_dist 
 --prefer-dist option of composer. 
 no_scripts 
 --no-scripts option of composer. 
 no_plugins 
 --no-plugins option of composer. 
 optimize 
 --optimize-autoloader option of composer. Recommended for production. 
 no_dev 
 --no-dev option for composer. Recommended for production. 
 quiet 
 --quiet option for composer. Whether or not to return output from composer. 
 composer_home 
 $COMPOSER_HOME environment variable 
 extra_flags 
 None, or a string containing extra flags to pass to composer.'"
"def _setup_styles(conditions, style_dict, style, default): 
    tags = set([tag for cond in conditions for tag in cond.split('/')]) 
   msg = ""Can't   map   between   conditions   and   the   provided   {0}.   Make   sure   you   have   provided   keys   in   the   format   of   '/'-separated   tags,   and   that   these   correspond   to   '/'-separated   tags   for   the   condition   names   (e.g.,   conditions   like   'Visual/Right',   and   styles   like   'colors=dict(Visual='red'))'.   The   offending   tag   was   '{1}'."" 
   for key in style_dict: 
      for tag in key.split('/'): 
         if (tag not in tags): 
            raise ValueError(msg.format(style, tag)) 
   condition_warning = ('Condition   {0}   could   not   be   mapped   to   a   ' + style) 
   style_warning = '.   Using   the   default   of   {0}.'.format(default) 
   for condition in conditions: 
      if (condition not in style_dict): 
         if ('/' not in condition): 
            warn((condition_warning.format(condition) + style_warning)) 
            style_dict[condition] = default 
         for style_ in style_dict: 
            if (style_ in condition.split('/')): 
               style_dict[condition] = style_dict[style_] 
               break 
   return style_dict", 'Setup the style dict for a given set of conditions.','Set linestyles and colors for plot_compare_evokeds.'
"def parse(json_string): 
    try: 
      json_data = json.loads(json_string) 
   except: 
      raise SchemaParseException(('Error   parsing   JSON:   %s' % json_string)) 
   names = Names() 
   return make_avsc_object(json_data, names)"," 'Parses a JSON string into an AVSCO object. 
 :param json_string: The JSON string to parse. 
 :type json_string: str 
 :return: The parsed AVSCO object.'",'Constructs the Schema from the JSON text.'
"def _filter_configured_avoids(module): 
    run_app = False 
   if (hasattr(settings, 'LETTUCE_AVOID_APPS') and isinstance(settings.LETTUCE_AVOID_APPS, (list, tuple))): 
      for appname in settings.LETTUCE_AVOID_APPS: 
         if module.__name__.startswith(appname): 
            run_app = True 
   return (not run_app)"," 'Filter out modules that are configured to avoid in the Lettuce settings. 
 If a list of app names is configured, any module that has an app name that 
 matches one of those names will be filtered out. 
 :param module: The module to check. 
 :return: True if the module should be avoided, False otherwise.'",'returns apps that are not within django.conf.settings.LETTUCE_AVOID_APPS'
"def _grow_nonoverlapping_labels(subject, seeds_, extents_, hemis, vertices_, graphs, names_): 
    labels = [] 
   for hemi in set(hemis): 
      hemi_index = (hemis == hemi) 
      seeds = seeds_[hemi_index] 
      extents = extents_[hemi_index] 
      names = names_[hemi_index] 
      graph = graphs[hemi] 
      n_vertices = len(vertices_[hemi]) 
      n_labels = len(seeds) 
      parc = np.empty(n_vertices, dtype='int32') 
      parc[:] = (-1) 
      sources = {} 
      edge = [] 
      for (label, seed) in enumerate(seeds): 
         if np.any((parc[seed] >= 0)): 
            raise ValueError('Overlapping   seeds') 
         parc[seed] = label 
         for s in np.atleast_1d(seed): 
            sources[s] = (label, 0.0) 
            edge.append(s) 
      while edge: 
         vert_from = edge.pop(0) 
         (label, old_dist) = sources[vert_from] 
         row = graph[vert_from, :] 
         for (vert_to, dist) in zip(row.indices, row.data): 
            new_dist = (old_dist + dist) 
            if (new_dist > extents[label]): 
               continue 
            vert_to_label = parc[vert_to] 
            if (vert_to_label >= 0): 
               (_, vert_to_dist) = sources[vert_to] 
               if (new_dist > vert_to_dist): 
                  continue 
               elif (vert_to in edge): 
                  edge.remove(vert_to) 
            parc[vert_to] = label 
            sources[vert_to] = (label, new_dist) 
            edge.append(vert_to) 
      for i in xrange(n_labels): 
         vertices = np.nonzero((parc == i))[0] 
         name = str(names[i]) 
         label_ = Label(vertices, hemi=hemi, name=name, subject=subject) 
         labels.append(label_) 
   return labels"," 'Grow labels non-overlappingly by growing from seeds. 
 Parameters 
 subject : Subject 
 The subject. 
 seeds : ndarray 
 The seeds. 
 extents : ndarray 
 The extents. 
 hemis : ndarray 
 The hemisphere labels. 
 vertices : ndarray 
 The vertices. 
 graphs : ndarray 
 The graphs. 
 names : ndarray 
 The names. 
 Returns 
 labels : list of Label 
 The labels. 
 Examples 
 >>> import nipype.interfaces.freesurfer as fs 
 >>> subject = fs.subject() 
 >>> seeds = np.array([1, 2, 3]) 
 >>> extents = np.array([1, 2, 3]) 
 >>> hemis = np.array([0, 0, 0]) 
 >>> vertices = np.array([0, 0, 0]) 
 >>> graphs = np.array([[1, 2, 3], [4, 5, 6], [7",'Grow labels while ensuring that they don\'t overlap.'
"def _perform_pairwise_tests(labels, dists, tail_type, num_permutations): 
    result = [] 
   num_tests = 0 
   for (g1_idx, (g1_label, g1_dist)) in enumerate(zip(labels[:(-1)], dists[:(-1)])): 
      for (g2_label, g2_dist) in zip(labels[(g1_idx + 1):], dists[(g1_idx + 1):]): 
         if (((len(g1_dist) == 1) and (len(g2_dist) == 1)) or ((len(g1_dist) < 1) or (len(g2_dist) < 1))): 
            (obs_t, param_p_val, nonparam_p_val) = (nan, nan, nan) 
         else: 
            (obs_t, param_p_val, _, nonparam_p_val) = mc_t_two_sample(g1_dist, g2_dist, tails=tail_type, permutations=num_permutations) 
         result.append([g1_label, g2_label, obs_t, param_p_val, None, nonparam_p_val, None]) 
         if (not isnan(obs_t)): 
            num_tests += 1 
   for stat in result: 
      corr_param_p_val = stat[3] 
      if ((corr_param_p_val is not None) and (not isnan(corr_param_p_val))): 
         corr_param_p_val = min((corr_param_p_val * num_tests), 1) 
      stat[4] = corr_param_p_val 
      corr_nonparam_p_val = stat[5] 
      if ((corr_nonparam_p_val is not None) and (not isnan(corr_nonparam_p_val))): 
         corr_nonparam_p_val = min((corr_nonparam_p_val * num_tests), 1) 
      stat[6] = corr_nonparam_p_val 
   return result"," 'Performs pairwise tests for all possible combinations of labels. 
 :param labels: list of labels 
 :param dists: list of distributions 
 :param tail_type: type of tail of test statistic 
 :param num_permutations: number of permutations to perform 
 :return: list of tuples of labels, test statistic, p-value, 
 correlation of test statistic with the parameter of the test statistic, 
 correlation of test statistic with the nonparametric test statistic, 
 and correlation of test statistic with the nonparametric test statistic'","'Perform t-test for all pairs of distributions. 
 Computes corrected p-values in addition to uncorrected.'"
"def get_num_instances(server=None, version=None): 
    req = servers_service_pb.GetNumInstancesRequest() 
   if server: 
      req.set_server(server) 
   if version: 
      req.set_version(version) 
   resp = servers_service_pb.GetNumInstancesResponse() 
   try: 
      apiproxy_stub_map.MakeSyncCall('servers', 'GetNumInstances', req, resp) 
   except apiproxy_errors.ApplicationError as e: 
      if (e.application_error == servers_service_pb.ServersServiceError.INVALID_VERSION): 
         raise InvalidVersionError() 
      else: 
         raise Error() 
   return resp.instances()"," 'Get the number of instances running on a server. 
 :param server: Name of server to get the number of instances for. 
 :param version: Version of the server to get the number of instances for. 
 :returns: Number of instances running on the server. 
 Raises: 
 InvalidVersionError: if the version is invalid. 
 Error: if an error occurs.'","'Return the number of instances that are set for the given server version. 
 This is only valid for fixed servers, an error will be raised for 
 automatically-scheduled servers.  Support for automatically-servers may be 
 supported in the future. 
 Args: 
 server: String containing the name of the server to fetch this info for, if 
 None the server of the current instance will be used. 
 version: String containing the name of the version to fetch this info for, 
 if None the version of the current instance will be used.  If that version 
 does not exist in the other server, then an InvalidVersionError is raised. 
 Raises: 
 InvalidVersionError on invalid input.'"
"def raw(key=None): 
    if key: 
      ret = __pillar__.get(key, {}) 
   else: 
      ret = __pillar__ 
   return ret"," 'Return the raw dictionary of pillar values. 
 :param key: 
 The key to use to retrieve the pillar value. 
 :returns: 
 The raw dictionary of pillar values.'","'Return the raw pillar data that is currently loaded into the minion. 
 Contrast with :py:func:`items` which calls the master to fetch the most 
 up-to-date Pillar. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' pillar.raw 
 With the optional key argument, you can select a subtree of the 
 pillar raw data.:: 
 salt \'*\' pillar.raw key=\'roles\''"
"def smart_str(s, encoding='utf-8', strings_only=False, errors='strict'): 
    if (strings_only and isinstance(s, (types.NoneType, int))): 
      return s 
   if (not isinstance(s, basestring)): 
      try: 
         return str(s) 
      except UnicodeEncodeError: 
         if isinstance(s, Exception): 
            return '   '.join([smart_str(arg, encoding, strings_only, errors) for arg in s]) 
         return unicode(s).encode(encoding, errors) 
   elif isinstance(s, unicode): 
      return s.encode(encoding, errors) 
   elif (s and (encoding != 'utf-8')): 
      return s.decode('utf-8', errors).encode(encoding, errors) 
   else: 
      return s"," 'Make sure a string is a unicode string, or a string with utf-8 encoding. 
 If a unicode string is passed, it is encoded in utf-8. 
 If a string with utf-8 encoding is passed, it is decoded to unicode. 
 If a string with another encoding is passed, it is encoded to utf-8. 
 If a string with no encoding is passed, it is decoded to utf-8. 
 If a unicode string is passed, it is encoded in utf-8. 
 If a string with utf-8 encoding is passed, it is decoded to unicode. 
 If a string with another encoding is passed, it is encoded to utf-8. 
 If a string with no encoding is passed, it is decoded to utf-8. 
 If a string with no encoding is passed, it is decoded to utf-8. 
 If a unicode string is passed, it is encoded in utf-8. 
 If a string with utf-8 encoding is passed, it is decoded to unicode. 
 If a string with another encoding is passed, it is encoded to utf-8. 
 If a string with no encoding is","'Returns a bytestring version of \'s\', encoded as specified in \'encoding\'. 
 If strings_only is True, don\'t convert (some) non-string-like objects.'"
"def localize(value): 
    return force_unicode(formats.localize(value, use_l10n=True))"," 'Convert a value to a localized string. 
 :param value: the value to localize. 
 :return: the localized string.'","'Forces a value to be rendered as a localized value, 
 regardless of the value of ``settings.USE_L10N``.'"
"def test__FileIO_tell(): 
    pass", 'Test that the file pointer is correct after tell.','TODO'
"@requires_application() 
 def test_functionality_desktop(): 
    _test_functionality('gl2')", 'Test functionality on desktop.','Test desktop GL backend for full functionality.'
"def SearchDataAdapter(results, format, collection): 
    if (results and results['response'] and results['response']['docs']): 
      search_data = results['response']['docs'] 
      if collection['template']['fieldsSelected']: 
         headers = collection['template']['fieldsSelected'] 
      else: 
         headers = [field['name'] for field in collection['fields']] 
      rows = [] 
      for data in search_data: 
         row = [] 
         for column in headers: 
            if (column not in data): 
               row.append('') 
            elif (isinstance(data[column], basestring) or isinstance(data[column], (int, long, float, complex))): 
               row.append(data[column]) 
            elif isinstance(data[column], list): 
               row.append([smart_str(val, errors='replace') for val in data[column]]) 
            else: 
               row.append(smart_str(data[column])) 
         rows.append(row) 
   else: 
      rows = [[]] 
   (yield (headers, rows))"," 'SearchDataAdapter(results, format, collection) 
 Generates a search results table from a search response. 
 :param results: The search response. 
 :type results: dict 
 :param format: The format of the search response. 
 :type format: str 
 :param collection: The collection to generate the table for. 
 :type collection: dict 
 :return: A generator of search results table rows. 
 :rtype: generator'","'SearchDataAdapter(results, format, db) -> headers, 2D array of data.'"
"def delaxes(*args): 
    if (not len(args)): 
      ax = gca() 
   else: 
      ax = args[0] 
   ret = gcf().delaxes(ax) 
   return ret"," 'Remove axes from a plot. 
 Parameters 
 *args* 
 Axes to remove. 
 Returns 
 *ret* 
 The axes that were removed. 
 Examples 
 >>> import matplotlib.pyplot as plt 
 >>> import numpy as np 
 >>> x = np.linspace(0, 2 * np.pi, 10) 
 >>> y = np.sin(x) 
 >>> plt.plot(x, y) 
 >>> plt.show() 
 >>> ax = plt.gca() 
 >>> ax.set_xlabel(""x"") 
 >>> ax.set_ylabel(""y"") 
 >>> ax.set_title(""plot"") 
 >>> plt.draw() 
 >>> ax = plt.gca() 
 >>> ax.set_xlabel(""x"") 
 >>> ax.set_ylabel(""y"") 
 >>> ax.set_title(""plot"") 
 >>> ax.set_xlim([-1, 1]) 
 >>> ax.set_ylim([-1, 1]) 
 >>> plt.draw() 
 >>> ax = plt.g","'Remove an axes from the current figure.  If *ax* 
 doesn\'t exist, an error will be raised. 
 ``delaxes()``: delete the current axes'"
"def volume_get_all_by_group(context, group_id, filters=None): 
    return IMPL.volume_get_all_by_group(context, group_id, filters=filters)"," 'Retrieves all volumes that belong to the given group. 
 :param context: context object 
 :param group_id: group id 
 :param filters: filters to apply to the query 
 :returns: a list of volumes'",'Get all volumes belonging to a consistency group.'
"def __virtual__(): 
    if (not salt.utils.is_darwin()): 
      return (False, 'The   mac_system   module   could   not   be   loaded:   module   only   works   on   MacOS   systems.') 
   if (not _atrun_enabled()): 
      if (not _enable_atrun()): 
         return (False, 'atrun   could   not   be   enabled   on   this   system') 
   return __virtualname__", 'Loads the mac_system module if the system is a Mac OS X system.','Only for MacOS with atrun enabled'
"def to_tornado_future(asyncio_future): 
    tf = tornado.concurrent.Future() 
   tornado.concurrent.chain_future(asyncio_future, tf) 
   return tf", 'Convert an asyncio Future into a tornado Future.',"'Convert an `asyncio.Future` to a `tornado.concurrent.Future`. 
 .. versionadded:: 4.1'"
"def has_open_quotes(s): 
    if (s.count('""') % 2): 
      return '""' 
   elif (s.count(""'"") % 2): 
      return ""'"" 
   else: 
      return False"," 'Returns True if the string contains an open quote, False otherwise.'","'Return whether a string has open quotes. 
 This simply counts whether the number of quote characters of either type in 
 the string is odd. 
 Returns 
 If there is an open quote, the quote character is returned.  Else, return 
 False.'"
"def get_bulk_archive(selected_submissions, zip_directory=''): 
    zip_file = tempfile.NamedTemporaryFile(prefix='tmp_securedrop_bulk_dl_', dir=config.TEMP_DIR, delete=False) 
   sources = set([i.source.journalist_designation for i in selected_submissions]) 
   with zipfile.ZipFile(zip_file, 'w') as zip: 
      for source in sources: 
         submissions = [s for s in selected_submissions if (s.source.journalist_designation == source)] 
         for submission in submissions: 
            filename = path(submission.source.filesystem_id, submission.filename) 
            verify(filename) 
            document_number = submission.filename.split('-')[0] 
            zip.write(filename, arcname=os.path.join(zip_directory, source, ('%s_%s' % (document_number, submission.source.last_updated.date())), os.path.basename(filename))) 
   return zip_file", 'Returns a zip file with all selected submissions from a given source.','Generate a zip file from the selected submissions'
"def _frangi_hessian_common_filter(image, scale_range, scale_step, beta1, beta2): 
    from ..feature import hessian_matrix, hessian_matrix_eigvals 
   sigmas = np.arange(scale_range[0], scale_range[1], scale_step) 
   if np.any((np.asarray(sigmas) < 0.0)): 
      raise ValueError('Sigma   values   less   than   zero   are   not   valid') 
   beta1 = (2 * (beta1 ** 2)) 
   beta2 = (2 * (beta2 ** 2)) 
   filtered_array = np.zeros((sigmas.shape + image.shape)) 
   lambdas_array = np.zeros((sigmas.shape + image.shape)) 
   for (i, sigma) in enumerate(sigmas): 
      (Drr, Drc, Dcc) = hessian_matrix(image, sigma, order='rc') 
      Drr = ((sigma ** 2) * Drr) 
      Drc = ((sigma ** 2) * Drc) 
      Dcc = ((sigma ** 2) * Dcc) 
      (lambda1, lambda2) = hessian_matrix_eigvals(Drr, Drc, Dcc) 
      lambda1[(lambda1 == 0)] = 1e-10 
      rb = ((lambda2 / lambda1) ** 2) 
      s2 = ((lambda1 ** 2) + (lambda2 ** 2)) 
      filtered = (np.exp(((- rb) / beta1)) * (np.ones(np.shape(image)) - np.exp(((- s2) / beta2)))) 
      filtered_array[i] = filtered 
      lambdas_array[i] = lambda1 
   return (filtered_array, lambdas_array)"," 'This function is used to filter the Hessian eigenvalues and the Hessian 
 matrix. 
 Parameters 
 image : ndarray 
 The input image. 
 scale_range : tuple 
 The range of scales to compute the Hessian at. 
 scale_step : int 
 The step size between scales. 
 beta1 : float 
 The first eigenvalue scaling factor. 
 beta2 : float 
 The second eigenvalue scaling factor. 
 Returns 
 filtered_array : ndarray 
 The filtered Hessian eigenvalues. 
 lambdas_array : ndarray 
 The filtered Hessian eigenvalues. 
 Examples 
 >>> image = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]]) 
 >>> scale_range = (0.01, 0.2) 
 >>> scale_step = 0.01 
 >>> beta1 = 10 
 >>> beta2 = 10 
 >>> filtered_array, lambdas_array = _frangi_hessian_common","'This is an intermediate function for Frangi and Hessian filters. 
 Shares the common code for Frangi and Hessian functions. 
 Parameters 
 image : (N, M) ndarray 
 Array with input image data. 
 scale_range : 2-tuple of floats, optional 
 The range of sigmas used. 
 scale_step : float, optional 
 Step size between sigmas. 
 beta1 : float, optional 
 Frangi correction constant that adjusts the filter\'s 
 sensitivity to deviation from a blob-like structure. 
 beta2 : float, optional 
 Frangi correction constant that adjusts the filter\'s 
 sensitivity to areas of high variance/texture/structure. 
 Returns 
 filtered_list : list 
 List of pre-filtered images.'"
"def dns_dhcp(interface='Local   Area   Connection'): 
    cmd = ['netsh', 'interface', 'ip', 'set', 'dns', interface, 'source=dhcp'] 
   return (__salt__['cmd.retcode'](cmd, python_shell=False) == 0)"," 'Enable/disable DHCP DNS server for specified interface. 
 This function can be used to enable/disable DHCP DNS server for specified 
 interface. It can be used to disable DHCP DNS server for all interfaces 
 by passing \'all\' as interface argument. 
 This function is available in Windows Server 2003 and later. 
 :param interface: Interface name (default: \'Local Area Connection\') 
 :return: True if DHCP DNS server is enabled for specified interface, 
 False otherwise.'","'Configure the interface to get its DNS servers from the DHCP server 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' win_dns_client.dns_dhcp <interface>'"
"def dijkstra_predecessor_and_distance(G, source, cutoff=None, weight='weight'): 
    weight = _weight_function(G, weight) 
   pred = {source: []} 
   return (pred, _dijkstra(G, source, weight, pred=pred, cutoff=cutoff))"," 'Return the predecessor and distance of each vertex in a graph 
 from the source vertex. 
 Parameters 
 G : networkx.Graph 
 The graph to find the predecessor and distance for. 
 source : node 
 The source node. 
 cutoff : int 
 The maximum number of nodes to visit. 
 weight : string 
 The key in the graph used for edge weights. 
 Returns 
 predecessor : dict 
 A dictionary mapping each node to its predecessor. 
 distance : dict 
 A dictionary mapping each node to its distance to the source. 
 Examples 
 >>> from networkx.algorithms.shortest_paths import dijkstra_predecessor_and_distance 
 >>> G = nx.DiGraph() 
 >>> G.add_edge(0, 1, weight=1) 
 >>> G.add_edge(1, 2, weight=2) 
 >>> G.add_edge(2, 3, weight=3) 
 >>> G.add_edge(3, 4, weight=4) 
 >>> G.add_edge(","'Compute weighted shortest path length and predecessors. 
 Uses Dijkstra\'s Method to obtain the shortest weighted paths 
 and return dictionaries of predecessors for each node and 
 distance for each node from the `source`. 
 Parameters 
 G : NetworkX graph 
 source : node label 
 Starting node for path 
 cutoff : integer or float, optional 
 Depth to stop the search. Only return paths with length <= cutoff. 
 weight : string or function 
 If this is a string, then edge weights will be accessed via the 
 edge attribute with this key (that is, the weight of the edge 
 joining `u` to `v` will be ``G.edge[u][v][weight]``). If no 
 such edge attribute exists, the weight of the edge is assumed to 
 be one. 
 If this is a function, the weight of an edge is the value 
 returned by the function. The function must accept exactly three 
 positional arguments: the two endpoints of an edge and the 
 dictionary of edge attributes for that edge. The function must 
 return a number. 
 Returns 
 pred, distance : dictionaries 
 Returns two dictionaries representing a list of predecessors 
 of a node and the distance to each node. 
 Notes 
 Edge weight attributes must be numerical. 
 Distances are calculated as sums of weighted edges traversed. 
 The list of predecessors contains more than one element only when 
 there are more than one shortest paths to the key node.'"
"def internJID(jidstring): 
    if (jidstring in __internJIDs): 
      return __internJIDs[jidstring] 
   else: 
      j = JID(jidstring) 
      __internJIDs[jidstring] = j 
      return j"," 'Intern a JID. 
 This function takes a JID and interns it, adding it to the 
 __internJIDs dict. 
 This is used to make sure that the JIDs in the JIDs dict are 
 unique. 
 :param jidstring: The JID to intern. 
 :return: The interned JID. 
 :rtype: JID'","'Return interned JID. 
 @rtype: L{JID}'"
"def get_all_roles(exclude_system=False): 
    if exclude_system: 
      result = Role.query(system=False) 
   else: 
      result = Role.get_all() 
   return result"," 'Returns all roles. 
 :param exclude_system: 
 If true, the system role will not be included in the result. 
 :returns: 
 List of roles.'","'Retrieve all the available roles. 
 :param exclude_system: True to exclude system roles. 
 :type exclude_system: ``bool`` 
 :rtype: ``list`` of :class:`RoleDB`'"
"def get_user_hash(request): 
    ip = request.META.get('REMOTE_ADDR', '') 
   ua = request.META.get('User-Agent', '') 
   session_key = (request.session.session_key or '') 
   return hashlib.sha1('-'.join(map(str, (ip, ua, session_key)))).hexdigest()"," 'Returns a hash of the current request. 
 This is used to cache the user session in the database. 
 This is a hash of the following: 
 - IP address 
 - User agent 
 - Session key 
 This is useful for caching the user session in the database. 
 This is not a secure hash, so do not use it to secure anything.'","'Get a hash identifying an user. 
 It\'s a hash of session key, ip and user agent'"
"def test_max_pool(): 
    X_sym = tensor.tensor4('X') 
   pool_it = max_pool(X_sym, pool_shape=(2, 2), pool_stride=(2, 2), image_shape=(6, 4)) 
   f = theano.function(inputs=[X_sym], outputs=pool_it) 
   X = np.array([[2, 1, 3, 4], [1, 1, 3, 3], [5, 5, 7, 7], [5, 6, 8, 7], [9, 10, 11, 12], [9, 10, 12, 12]], dtype=theano.config.floatX)[np.newaxis, np.newaxis, ...] 
   expected = np.array([[2, 4], [6, 8], [10, 12]], dtype=theano.config.floatX)[np.newaxis, np.newaxis, ...] 
   actual = f(X) 
   assert np.allclose(expected, actual)", 'Test max pool','Test max pooling for known result.'
"def test_alknn_not_good_object(): 
    nn = 'rnd' 
   allknn = AllKNN(n_neighbors=nn, random_state=RND_SEED, kind_sel='mode') 
   assert_raises(ValueError, allknn.fit_sample, X, Y)", 'Testing fit on a non-good object','Test either if an error is raised while a wrong type of NN is given'
"def required_estimates_fields(columns): 
    return metadata_columns.union(viewvalues(columns))", 'Returns a set of required fields for an estimate',"'Compute the set of resource columns required to serve 
 `columns`.'"
"@register.tag 
 def ssi(parser, token): 
    bits = token.split_contents() 
   parsed = False 
   if (len(bits) not in (2, 3)): 
      raise TemplateSyntaxError(u""'ssi'   tag   takes   one   argument:   the   path   to   the   file   to   be   included"") 
   if (len(bits) == 3): 
      if (bits[2] == u'parsed'): 
         parsed = True 
      else: 
         raise TemplateSyntaxError((u""Second   (optional)   argument   to   %s   tag   must   be   'parsed'"" % bits[0])) 
   filepath = parser.compile_filter(bits[1]) 
   return SsiNode(filepath, parsed)"," 'Includes a file as a string, using the \'ssi\' tag. 
 The included file can be a string, or a compiled template. 
 The filepath is evaluated as a string, so it can contain variables 
 that are interpolated at runtime. 
 The optional \'parsed\' argument controls whether the included file is 
 parsed at runtime. 
 If the filepath is a compiled template, the template will be rendered 
 and the rendered string will be returned. 
 If the filepath is a string, the string will be returned unchanged. 
 If the filepath is a variable, the variable will be evaluated at runtime 
 and the resulting string will be returned. 
 If the filepath is a list, the first element will be used as the filepath, 
 and the second element will be used as the variable to interpolate into the 
 filepath. 
 If the filepath is a dictionary, the key will be used as the filepath, and 
 the value will be used as the variable to interpolate into the filepath. 
 If the filepath is a tuple, the first element will be used as the filepath, 
","'Outputs the contents of a given file into the page. 
 Like a simple ""include"" tag, the ``ssi`` tag includes the contents 
 of another file -- which must be specified using an absolute path -- 
 in the current page:: 
 {% ssi ""/home/html/ljworld.com/includes/right_generic.html"" %} 
 If the optional ""parsed"" parameter is given, the contents of the included 
 file are evaluated as template code, with the current context:: 
 {% ssi ""/home/html/ljworld.com/includes/right_generic.html"" parsed %}'"
"def real_path(path): 
    return os.path.normpath(os.path.normcase(os.path.realpath(path)))"," 'Returns the real path to the given path. 
 This is a port of the Python 2.6 version of real_path. 
 This version is more robust against Unicode paths.'","'Returns: the canonicalized absolute pathname. The resulting path will have no symbolic link, \'/./\' or \'/../\' components.'"
"def main(): 
    try: 
      f_mounts = open('/proc/mounts', 'r') 
   except IOError as e: 
      utils.err((""error:   can't   open   /proc/mounts:   %s"" % e)) 
      return 13 
   utils.drop_privileges() 
   while True: 
      devices = [] 
      f_mounts.seek(0) 
      ts = int(time.time()) 
      for line in f_mounts: 
         try: 
            (fs_spec, fs_file, fs_vfstype, fs_mntops, fs_freq, fs_passno) = line.split(None) 
         except ValueError as e: 
            utils.err((""error:   can't   parse   line   at   /proc/mounts:   %s"" % e)) 
            continue 
         if (fs_spec == 'none'): 
            continue 
         elif ((fs_vfstype in FSTYPE_IGNORE) or fs_vfstype.startswith('fuse.')): 
            continue 
         elif (fs_file.startswith('/dev') or fs_file.startswith('/sys') or fs_file.startswith('/proc') or fs_file.startswith('/lib') or fs_file.startswith('net:')): 
            continue 
         device_found = False 
         if fs_spec.startswith('/dev'): 
            for device in devices: 
               if (fs_spec == device[0]): 
                  device_found = True 
                  if (len(fs_file) < len(device[1])): 
                     device[1] = fs_file 
                  break 
            if (not device_found): 
               devices.append([fs_spec, fs_file, fs_vfstype]) 
         else: 
            devices.append([fs_spec, fs_file, fs_vfstype]) 
      for device in devices: 
         (fs_spec, fs_file, fs_vfstype) = device 
         try: 
            r = os.statvfs(fs_file) 
         except OSError as e: 
            utils.err((""can't   get   info   for   mount   point:   %s:   %s"" % (fs_file, e))) 
            continue 
         used = (r.f_blocks - r.f_bfree) 
         if (r.f_blocks == 0): 
            percent_used = 100 
         else: 
            percent_used = ((used * 100.0) / r.f_blocks) 
         print ('df.bytes.total   %d   %s   mount=%s   fstype=%s' % (ts, (r.f_frsize * r.f_blocks), fs_file, fs_vfstype)) 
         print ('df.bytes.used   %d   %s   mount=%s   fstype=%s' % (ts, (r.f_frsize * used), fs_file, fs_vfstype)) 
         print ('df.bytes.percentused   %d   %s   mount=%s   fstype=%s' % (ts, percent_used, fs_file, fs_vfstype)) 
         print ('df.bytes.free   %d   %s   mount=%s   fstype=%s' % (ts, (r.f_frsize * r.f_bfree), fs_file, fs_vfstype)) 
         used = (r.f_files - r.f_ffree) 
         if (r.f_files == 0): 
            percent_used = 100 
         else: 
            percent_used = ((used * 100.0) / r.f_files) 
         print ('df.inodes.total   %d   %s   mount=%s   fstype=%s' % (ts, r.f_files, fs_file, fs_vfstype)) 
         print ('df.inodes.used   %d   %s   mount=%s   fstype=%s' % (ts, used, fs_file, fs_vfstype)) 
         print ('df.inodes.percentused   %d   %s   mount=%s   fstype=%s' % (ts, percent_used, fs_file, fs_vfstype)) 
         print ('df.inodes.free   %d   %s   mount=%s   fstype=%s' % (ts, r.f_ffree, fs_file, fs_vfstype)) 
      sys.stdout.flush() 
      time.sleep(COLLECTION_INTERVAL)"," 'This is a simple program to print out the df information of all mounted 
 filesystems. 
 It is useful to see how much disk space is used by each filesystem. 
 It is also useful to see how much disk space is used by each process. 
 This program is useful to see how much disk space is used by each process. 
 It is also useful to see how much disk space is used by each process. 
 This program is useful to see how much disk space is used by each process. 
 It is also useful to see how much disk space is used by each process. 
 This program is useful to see how much disk space is used by each process. 
 It is also useful to see how much disk space is used by each process. 
 This program is useful to see how much disk space is used by each process. 
 It is also useful to see how much disk space is used by each process. 
 This program is useful to see how much disk space is used by each process. 
 It is also useful to see how much disk space is used by each process. 
 This program is useful to see how much disk space is used by each process. 
 It",'dfstats main loop'
"def _create_base_cipher(dict_parameters): 
    try: 
      key = dict_parameters.pop('key') 
   except KeyError: 
      raise TypeError(""Missing   'key'   parameter"") 
   expect_byte_string(key) 
   if (len(key) != key_size): 
      raise ValueError(('Incorrect   DES   key   length   (%d   bytes)' % len(key))) 
   start_operation = _raw_des_lib.DES_start_operation 
   stop_operation = _raw_des_lib.DES_stop_operation 
   cipher = VoidPointer() 
   result = start_operation(key, c_size_t(len(key)), cipher.address_of()) 
   if result: 
      raise ValueError(('Error   %X   while   instantiating   the   DES   cipher' % result)) 
   return SmartPointer(cipher.get(), stop_operation)"," 'Creates a DES cipher based on the parameters in the dictionary. 
 The key is a byte string, the size of the key is the same as the 
 length of the key. 
 The cipher object is returned as a smart pointer. 
 :param dict_parameters: 
 A dictionary containing the parameters for the DES cipher. 
 :type dict_parameters: 
 :return: 
 A smart pointer to the DES cipher.'","'This method instantiates and returns a handle to a low-level 
 base cipher. It will absorb named parameters in the process.'"
"@register_specialize 
 @register_canonicalize 
 @gof.local_optimizer([Subtensor]) 
 def local_subtensor_inc_subtensor(node): 
    if isinstance(node.op, Subtensor): 
      x = node.inputs[0] 
      if ((not x.owner) or (not isinstance(x.owner.op, IncSubtensor))): 
         return 
      if (not x.owner.op.set_instead_of_inc): 
         return 
      if ((x.owner.inputs[2:] == node.inputs[1:]) and (tuple(x.owner.op.idx_list) == tuple(node.op.idx_list))): 
         out = node.outputs[0] 
         y = x.owner.inputs[1] 
         if (x.dtype != y.dtype): 
            y = y.astype(x.dtype) 
         if (out.type == y.type): 
            return [y] 
         else: 
            assert (out.broadcastable != y.broadcastable) 
            x_subtensor = node.op(x.owner.inputs[0], *x.owner.inputs[2:]) 
            return [T.alloc(y, *x_subtensor.shape)] 
      else: 
         return", 'Subtensor with IncSubtensor',"'Subtensor(SetSubtensor(x, y, idx), idx) -> y'"
"def load_check(agentConfig, hostname, checkname): 
    from jmxfetch import JMX_CHECKS 
   agentConfig['checksd_hostname'] = hostname 
   osname = get_os() 
   checks_places = get_checks_places(osname, agentConfig) 
   for config_path in _file_configs_paths(osname, agentConfig): 
      check_name = _conf_path_to_check_name(config_path) 
      if ((check_name == checkname) and (check_name not in JMX_CHECKS)): 
         (conf_is_valid, check_config, invalid_check) = _load_file_config(config_path, check_name, agentConfig) 
         if (invalid_check and (not conf_is_valid)): 
            return invalid_check 
         (load_success, load_failure) = load_check_from_places(check_config, check_name, checks_places, agentConfig) 
         return (load_success.values()[0] or load_failure) 
   for (check_name, service_disco_check_config) in _service_disco_configs(agentConfig).iteritems(): 
      if (check_name == checkname): 
         (sd_init_config, sd_instances) = service_disco_check_config[1] 
         check_config = {'init_config': sd_init_config, 'instances': sd_instances} 
         (load_success, load_failure) = load_check_from_places(check_config, check_name, checks_places, agentConfig) 
         return (load_success.values()[0] or load_failure) 
   return None"," 'Loads a check from a configuration file. 
 :param agentConfig: Agent configuration 
 :param hostname: Hostname 
 :param checkname: Check name 
 :returns: A tuple containing the following values: 
 * ``success`` - A boolean value indicating whether the check was successfully loaded. 
 * ``load_failure`` - A string describing the failure. 
 * ``load_success`` - A dictionary containing the check\'s configuration.'",'Same logic as load_check_directory except it loads one specific check'
"def import_string(import_name, silent=False): 
    if isinstance(import_name, unicode): 
      import_name = str(import_name) 
   try: 
      if (':' in import_name): 
         (module, obj) = import_name.split(':', 1) 
      elif ('.' in import_name): 
         (module, obj) = import_name.rsplit('.', 1) 
      else: 
         return __import__(import_name) 
      if isinstance(obj, unicode): 
         obj = obj.encode('utf-8') 
      try: 
         return getattr(__import__(module, None, None, [obj]), obj) 
      except (ImportError, AttributeError): 
         modname = ((module + '.') + obj) 
         __import__(modname) 
         return sys.modules[modname] 
   except ImportError as e: 
      if (not silent): 
         raise ImportStringError(import_name, e), None, sys.exc_info()[2]"," 'Import a string as a module. 
 :param import_name: 
 :param silent: 
 :return: 
 :raises: ImportStringError'","'Imports an object based on a string.  This is useful if you want to 
 use import paths as endpoints or something similar.  An import path can 
 be specified either in dotted notation (``xml.sax.saxutils.escape``) 
 or with a colon as object delimiter (``xml.sax.saxutils:escape``). 
 If `silent` is True the return value will be `None` if the import fails. 
 For better debugging we recommend the new :func:`import_module` 
 function to be used instead. 
 :param import_name: the dotted name for the object to import. 
 :param silent: if set to `True` import errors are ignored and 
 `None` is returned instead. 
 :return: imported object'"
"def get(): 
    from frappe.desk.notifications import get_notification_info_for_boot, get_notifications 
   from frappe.boot import get_bootinfo, get_unseen_notes 
   from frappe.limits import get_limits, get_expiry_message 
   bootinfo = None 
   if (not getattr(frappe.conf, u'disable_session_cache', None)): 
      bootinfo = frappe.cache().hget(u'bootinfo', frappe.session.user) 
      if bootinfo: 
         bootinfo[u'from_cache'] = 1 
         bootinfo[u'notification_info'].update(get_notifications()) 
         bootinfo[u'user'][u'recent'] = json.dumps(frappe.cache().hget(u'user_recent', frappe.session.user)) 
   if (not bootinfo): 
      bootinfo = get_bootinfo() 
      bootinfo[u'notification_info'] = get_notification_info_for_boot() 
      frappe.cache().hset(u'bootinfo', frappe.session.user, bootinfo) 
      try: 
         frappe.cache().ping() 
      except redis.exceptions.ConnectionError: 
         message = _(u'Redis   cache   server   not   running.   Please   contact   Administrator   /   Tech   support') 
         if (u'messages' in bootinfo): 
            bootinfo[u'messages'].append(message) 
         else: 
            bootinfo[u'messages'] = [message] 
      if frappe.local.request: 
         bootinfo[u'change_log'] = get_change_log() 
   bootinfo[u'metadata_version'] = frappe.cache().get_value(u'metadata_version') 
   if (not bootinfo[u'metadata_version']): 
      bootinfo[u'metadata_version'] = frappe.reset_metadata_version() 
   bootinfo.notes = get_unseen_notes() 
   for hook in frappe.get_hooks(u'extend_bootinfo'): 
      frappe.get_attr(hook)(bootinfo=bootinfo) 
   bootinfo[u'lang'] = frappe.translate.get_user_lang() 
   bootinfo[u'disable_async'] = frappe.conf.disable_async 
   bootinfo.limits = get_limits() 
   bootinfo.expiry_message = get_expiry_message() 
   return bootinfo", 'Returns the bootinfo dict','get session boot info'
"def english_sort(x, y): 
    return cmp(_spat.sub('', x), _spat.sub('', y))"," 'English sorting, i.e. 
 :param x: 
 :param y: 
 :return: 
 :rtype: 
 :raise: 
 :seealso: 
 :note: 
 :author: 
 :version: 
 :date: 
 :references: 
 :todo: 
 :bugs: 
 :wip: 
 :status: 
 :examples: 
 :examples: 
 :related: 
 :requires: 
 :provides: 
 :depends: 
 :suggests: 
 :deprecates: 
 :replaces: 
 :supersedes: 
 :potential-breaking-changes: 
 :potential-breaking-changes-since: 
 :potential-breaking-changes-until: 
 :potential-breaking-changes-details: 
 :potential-breaking-changes-description: 
 :potential-breaking-changes-url: 
 :potential-breaking-changes-type: 
 :potential-breaking-changes-version:",'Comapare two english phrases ignoring starting prepositions.'
"@register.filter(is_safe=True) 
 def length(value): 
    try: 
      return len(value) 
   except (ValueError, TypeError): 
      return u''", 'Return the length of the string.','Returns the length of the value - useful for lists.'
"def get_event_loop(): 
    ev = _state.event_loop 
   if ((not os.getenv(_EVENT_LOOP_KEY)) and (ev is not None)): 
      ev.clear() 
      _state.event_loop = None 
      ev = None 
   if (ev is None): 
      ev = EventLoop() 
      _state.event_loop = ev 
      os.environ[_EVENT_LOOP_KEY] = '1' 
   return ev"," 'Returns the current event loop. 
 If no event loop is set, the default event loop is returned.'","'Return a EventLoop instance. 
 A new instance is created for each new HTTP request.  We determine 
 that we\'re in a new request by inspecting os.environ, which is reset 
 at the start of each request.  Also, each thread gets its own loop.'"
"def get_visibility(name): 
    if SPECIAL.match(name): 
      visibility = 'special' 
   elif PRIVATE.match(name): 
      visibility = 'private' 
   elif PROTECTED.match(name): 
      visibility = 'protected' 
   else: 
      visibility = 'public' 
   return visibility"," 'Returns the visibility of a class member name. 
 :param name: The name of the class member. 
 :return: The visibility of the member. 
 :rtype: str'","'return the visibility from a name: public, protected, private or special'"
"def _extractCommon(string): 
    elements = string.split(None, 2) 
   if (len(elements) != 3): 
      raise InvalidEntry() 
   (hostnames, keyType, keyAndComment) = elements 
   splitkey = keyAndComment.split(None, 1) 
   if (len(splitkey) == 2): 
      (keyString, comment) = splitkey 
      comment = comment.rstrip('\n') 
   else: 
      keyString = splitkey[0] 
      comment = None 
   key = Key.fromString(keyString.decode('base64')) 
   return (hostnames, keyType, key, comment)"," 'Extracts the common parts of a key entry. 
 The function is used to extract the key entry from a string, and 
 returns a tuple containing the hostnames, the key type, the key, and 
 the comment. 
 :param string string: The key entry. 
 :return: A tuple containing the hostnames, the key type, the key, and 
 the comment.'","'Extract common elements of base64 keys from an entry in a hosts file. 
 @return: a 4-tuple of hostname data (L{str}), ssh key type (L{str}), key 
 (L{Key}), and comment (L{str} or L{None}).  The hostname data is simply the 
 beginning of the line up to the first occurrence of whitespace.'"
"def test_np_rng(): 
    rngs = [make_np_rng(rng_or_seed=42, which_method='uniform'), make_np_rng(rng_or_seed=numpy.random.RandomState(42), which_method='uniform'), make_np_rng(default_seed=42), make_np_rng()] 
   random_numbers = rngs[0].uniform(size=(100,)) 
   equals = numpy.ones((100,)) 
   for rng in rngs[1:]: 
      equal = (random_numbers == rng.uniform(size=(100,))) 
      equals *= equal 
   assert equals.all()", 'Test that np.random.RandomState is used when a rng_or_seed is not provided',"'Tests that the four possible ways of creating 
 a numpy RNG give the same results with the same seed'"
"def _translate_str(sequence, table, stop_symbol='*', to_stop=False, cds=False, pos_stop='X', gap=None): 
    sequence = sequence.upper() 
   amino_acids = [] 
   forward_table = table.forward_table 
   stop_codons = table.stop_codons 
   if (table.nucleotide_alphabet.letters is not None): 
      valid_letters = set(table.nucleotide_alphabet.letters.upper()) 
   else: 
      valid_letters = set((IUPAC.ambiguous_dna.letters.upper() + IUPAC.ambiguous_rna.letters.upper())) 
   n = len(sequence) 
   if cds: 
      if (str(sequence[:3]).upper() not in table.start_codons): 
         raise CodonTable.TranslationError(""First   codon   '{0}'   is   not   a   start   codon"".format(sequence[:3])) 
      if ((n % 3) != 0): 
         raise CodonTable.TranslationError('Sequence   length   {0}   is   not   a   multiple   of   three'.format(n)) 
      if (str(sequence[(-3):]).upper() not in stop_codons): 
         raise CodonTable.TranslationError(""Final   codon   '{0}'   is   not   a   stop   codon"".format(sequence[(-3):])) 
      sequence = sequence[3:(-3)] 
      n -= 6 
      amino_acids = ['M'] 
   elif ((n % 3) != 0): 
      warnings.warn('Partial   codon,   len(sequence)   not   a   multiple   of   three.   Explicitly   trim   the   sequence   or   add   trailing   N   before   translation.   This   may   become   an   error   in   future.', BiopythonWarning) 
   if (gap is not None): 
      if (not isinstance(gap, basestring)): 
         raise TypeError('Gap   character   should   be   a   single   character   string.') 
      elif (len(gap) > 1): 
         raise ValueError('Gap   character   should   be   a   single   character   string.') 
   for i in range(0, (n - (n % 3)), 3): 
      codon = sequence[i:(i + 3)] 
      try: 
         amino_acids.append(forward_table[codon]) 
      except (KeyError, CodonTable.TranslationError): 
         if (codon in table.stop_codons): 
            if cds: 
               raise CodonTable.TranslationError('Extra   in   frame   stop   codon   found.') 
            if to_stop: 
               break 
            amino_acids.append(stop_symbol) 
         elif valid_letters.issuperset(set(codon)): 
            amino_acids.append(pos_stop) 
         elif ((gap is not None) and (codon == (gap * 3))): 
            amino_acids.append(gap) 
         else: 
            raise CodonTable.TranslationError(""Codon   '{0}'   is   invalid"".format(codon)) 
   return ''.join(amino_acids)"," 'Translates a nucleotide sequence into amino acid sequence. 
 Parameters 
 sequence : str 
 The nucleotide sequence to be translated. 
 table : CodonTable 
 The codon table to use for translation. 
 stop_symbol : str, optional 
 The symbol used to mark stop codons. 
 to_stop : bool, optional 
 If True, the sequence is considered to stop at the first stop codon. 
 cds : bool, optional 
 If True, the sequence is assumed to be in the CDS. 
 pos_stop : str, optional 
 The symbol used to mark stop codons in the CDS. 
 gap : str, optional 
 The symbol used to mark gaps in the CDS. 
 Returns 
 amino_acids : str 
 The amino acid sequence. 
 Raises 
 CodonTable.TranslationError 
 If the sequence contains invalid codons. 
 BiopythonWarning 
 If the sequence is not a multiple of three. 
 Notes 
 The amino acid sequence is returned as a single string. 
 Examples 
 >>> from","'Helper function to translate a nucleotide string (PRIVATE). 
 Arguments: 
 - sequence - a string 
 - table - a CodonTable object (NOT a table name or id number) 
 - stop_symbol - a single character string, what to use for terminators. 
 - to_stop - boolean, should translation terminate at the first 
 in frame stop codon?  If there is no in-frame stop codon 
 then translation continues to the end. 
 - pos_stop - a single character string for a possible stop codon 
 (e.g. TAN or NNN) 
 - cds - Boolean, indicates this is a complete CDS.  If True, this 
 checks the sequence starts with a valid alternative start 
 codon (which will be translated as methionine, M), that the 
 sequence length is a multiple of three, and that there is a 
 single in frame stop codon at the end (this will be excluded 
 from the protein sequence, regardless of the to_stop option). 
 If these tests fail, an exception is raised. 
 - gap - Single character string to denote symbol used for gaps. 
 Defaults to None. 
 Returns a string. 
 e.g. 
 >>> from Bio.Data import CodonTable 
 >>> table = CodonTable.ambiguous_dna_by_id[1] 
 >>> _translate_str(""AAA"", table) 
 \'K\' 
 >>> _translate_str(""TAR"", table) 
 >>> _translate_str(""TAN"", table) 
 \'X\' 
 >>> _translate_str(""TAN"", table, pos_stop=""@"") 
 >>> _translate_str(""TA?"", table) 
 Traceback (most recent call last): 
 TranslationError: Codon \'TA?\' is invalid 
 In a change to older versions of Biopython, partial codons are now 
 always regarded as an error (previously only checked if cds=True) 
 and will trigger a warning (likely to become an exception in a 
 future release). 
 If **cds=True**, the start and stop codons are checked, and the start 
 codon will be translated at methionine. The sequence must be an 
 while number of codons. 
 >>> _translate_str(""ATGCCCTAG"", table, cds=True) 
 \'MP\' 
 >>> _translate_str(""AAACCCTAG"", table, cds=True) 
 Traceback (most recent call last): 
 TranslationError: First codon \'AAA\' is not a start codon 
 >>> _translate_str(""ATGCCCTAGCCCTAG"", table, cds=True) 
 Traceback (most recent call last): 
 TranslationError: Extra in frame stop codon found.'"
"def track_time_change(year=None, month=None, day=None, hour=None, minute=None, second=None): 
    def track_time_change_decorator(action): 
      'Decorator   to   track   time   changes.' 
      event.track_time_change(HASS, functools.partial(action, HASS), year, month, day, hour, minute, second) 
      return action 
   return track_time_change_decorator"," 'Decorator to track time changes. 
 This is a decorator that will track time changes on the given function. 
 It will track the time the function is called and the time the function 
 returns. 
 Parameters 
 year : int 
 The year the function was called. 
 month : int 
 The month the function was called. 
 day : int 
 The day the function was called. 
 hour : int 
 The hour the function was called. 
 minute : int 
 The minute the function was called. 
 second : int 
 The second the function was called. 
 Returns 
 The decorated function. 
 Examples 
 >>> @track_time_change 
 ... def my_function(): 
 ...     pass 
 >>> my_function() 
 >>> my_function() 
 >>> my_function() 
 >>> my_function() 
 >>> my_function() 
 >>> my_function() 
 >>> my_function() 
 >>> my_function() 
 >>> my_function() 
 >>> my_function() 
 >>> my_function() 
 >>> my_function() 
",'Decorator factory to track time changes.'
"def lz4_encode(payload): 
    return lz4f.compressFrame(payload)", 'Compresses the payload using lz4','Encode payload using interoperable LZ4 framing. Requires Kafka >= 0.10'
"def rc(group, **kwargs): 
    aliases = {u'lw': u'linewidth', u'ls': u'linestyle', u'c': u'color', u'fc': u'facecolor', u'ec': u'edgecolor', u'mew': u'markeredgewidth', u'aa': u'antialiased'} 
   if is_string_like(group): 
      group = (group,) 
   for g in group: 
      for (k, v) in six.iteritems(kwargs): 
         name = (aliases.get(k) or k) 
         key = (u'%s.%s' % (g, name)) 
         try: 
            rcParams[key] = v 
         except KeyError: 
            raise KeyError((u'Unrecognized   key   ""%s""   for   group   ""%s""   and   name   ""%s""' % (key, g, name)))"," 'Set the value of a rc parameter for a group of axes. 
 Parameters 
 group : string, optional 
 The name of the group of axes to set the rc parameter for. 
 Defaults to the current axes. 
 key : string, optional 
 The name of the rc parameter to set. 
 Defaults to the value of the keyword argument. 
 value : object, optional 
 The value to set the rc parameter to. 
 Defaults to the value of the keyword argument. 
 Examples 
 Set the color of all axes to red: 
 >>> fig = plt.figure() 
 >>> ax = fig.gca() 
 >>> ax.rc(\'color\', \'red\') 
 Set the color of all axes to blue and the linestyle to solid: 
 >>> ax = fig.gca() 
 >>> ax.rc(\'color\', \'blue\', \'ls\', \'solid\') 
 Set the color of all axes to blue and the linestyle to solid and the 
 linewidth to 3: 
 >>> ax = fig.gca()","'Set the current rc params.  Group is the grouping for the rc, e.g., 
 for ``lines.linewidth`` the group is ``lines``, for 
 ``axes.facecolor``, the group is ``axes``, and so on.  Group may 
 also be a list or tuple of group names, e.g., (*xtick*, *ytick*). 
 *kwargs* is a dictionary attribute name/value pairs, e.g.,:: 
 rc(\'lines\', linewidth=2, color=\'r\') 
 sets the current rc params and is equivalent to:: 
 rcParams[\'lines.linewidth\'] = 2 
 rcParams[\'lines.color\'] = \'r\' 
 The following aliases are available to save typing for interactive 
 users: 
 Alias   Property 
 \'lw\'    \'linewidth\' 
 \'ls\'    \'linestyle\' 
 \'c\'     \'color\' 
 \'fc\'    \'facecolor\' 
 \'ec\'    \'edgecolor\' 
 \'mew\'   \'markeredgewidth\' 
 \'aa\'    \'antialiased\' 
 Thus you could abbreviate the above rc command as:: 
 rc(\'lines\', lw=2, c=\'r\') 
 Note you can use python\'s kwargs dictionary facility to store 
 dictionaries of default parameters.  e.g., you can customize the 
 font rc as follows:: 
 font = {\'family\' : \'monospace\', 
 \'weight\' : \'bold\', 
 \'size\'   : \'larger\'} 
 rc(\'font\', **font)  # pass in the font dict as kwargs 
 This enables you to easily switch between several configurations. 
 Use :func:`~matplotlib.pyplot.rcdefaults` to restore the default 
 rc params after changes.'"
"def _comp_method_SERIES(op, name, str_rep, masker=False): 
    def na_op(x, y): 
      if is_categorical_dtype(x): 
         return op(x, y) 
      elif (is_categorical_dtype(y) and (not isscalar(y))): 
         return op(y, x) 
      if is_object_dtype(x.dtype): 
         result = _comp_method_OBJECT_ARRAY(op, x, y) 
      else: 
         if is_datetimelike_v_numeric(x, y): 
            raise TypeError('invalid   type   comparison') 
         if (isscalar(y) and isnull(y)): 
            if (name == '__ne__'): 
               return np.ones(len(x), dtype=bool) 
            else: 
               return np.zeros(len(x), dtype=bool) 
         mask = None 
         if (needs_i8_conversion(x) or ((not isscalar(y)) and needs_i8_conversion(y))): 
            if isscalar(y): 
               mask = isnull(x) 
               y = _index.convert_scalar(x, _values_from_object(y)) 
            else: 
               mask = (isnull(x) | isnull(y)) 
               y = y.view('i8') 
            x = x.view('i8') 
         try: 
            with np.errstate(all='ignore'): 
               result = getattr(x, name)(y) 
            if (result is NotImplemented): 
               raise TypeError('invalid   type   comparison') 
         except AttributeError: 
            result = op(x, y) 
         if ((mask is not None) and mask.any()): 
            result[mask] = masker 
      return result 
   def wrapper(self, other, axis=None): 
      if (axis is not None): 
         self._get_axis_number(axis) 
      if isinstance(other, ABCSeries): 
         name = _maybe_match_name(self, other) 
         if (not self._indexed_same(other)): 
            msg = 'Can   only   compare   identically-labeled   Series   objects' 
            raise ValueError(msg) 
         return self._constructor(na_op(self.values, other.values), index=self.index, name=name) 
      elif isinstance(other, pd.DataFrame): 
         return NotImplemented 
      elif isinstance(other, (np.ndarray, pd.Index)): 
         if ((not lib.isscalar(lib.item_from_zerodim(other))) and (len(self) != len(other))): 
            raise ValueError('Lengths   must   match   to   compare') 
         if isinstance(other, ABCPeriodIndex): 
            return self._constructor(na_op(self.values, other.asobject.values), index=self.index) 
         return self._constructor(na_op(self.values, np.asarray(other)), index=self.index).__finalize__(self) 
      elif isinstance(other, pd.Categorical): 
         if (not is_categorical_dtype(self)): 
            msg = ""Cannot   compare   a   Categorical   for   op   {op}   with   Series   of   dtype   {typ}.\nIf   you   want   to   compare   values,   use   'series   <op>   np.asarray(other)'."" 
            raise TypeError(msg.format(op=op, typ=self.dtype)) 
      if is_categorical_dtype(self): 
         with np.errstate(all='ignore'): 
            res = op(self.values, other) 
      else: 
         values = self.get_values() 
         if isinstance(other, (list, np.ndarray)): 
            other = np.asarray(other) 
         with np.errstate(all='ignore'): 
            res = na_op(values, other) 
         if isscalar(res): 
            raise TypeError(('Could   not   compare   %s   type   with   Series' % type(other))) 
         res = _values_from_object(res) 
      res = pd.Series(res, index=self.index, name=self.name, dtype='bool') 
      return res 
   return wrapper"," 'Compare two Series objects using the specified comparison operator. 
 Parameters 
 op : str 
 The comparison operator to use. 
 name : str 
 The name of the comparison method. 
 str_rep : str 
 The string representation of the comparison method. 
 masker : bool, optional 
 If True, mask the result with the mask from the Series with null values. 
 Returns 
 Series 
 A Series containing the results of the comparison. 
 Raises 
 TypeError 
 If the comparison operator cannot be applied to the data types of the 
 Series objects. 
 Examples 
 >>> s1 = pd.Series([1, 2, 3], name=""foo"") 
 >>> s2 = pd.Series([1, 2, 3], name=""foo"") 
 >>> s1 < s2 
 False 
 >>> s1 < s2.astype(""int64"") 
 False 
 >>> s1 < s2.astype(""float64"") 
 True 
 >>> s1 < s2.astype(""int64"") == s1 < s2 
 True 
 >>>","'Wrapper function for Series arithmetic operations, to avoid 
 code duplication.'"
"@allow_public 
 def contribute_view(request): 
    return serve(request, 'contribute.json', document_root=settings.ROOT)", 'Serve the contribution view.','Generate a contribute.json'
"def load_from_folder(app): 
    blueprints_path = app.config.get('BLUEPRINTS_PATH', 'modules') 
   path = os.path.join(app.config.get('PROJECT_ROOT', '..'), blueprints_path) 
   base_module_name = '.'.join([app.name, blueprints_path]) 
   dir_list = os.listdir(path) 
   mods = {} 
   object_name = app.config.get('BLUEPRINTS_OBJECT_NAME', 'module') 
   module_file = app.config.get('BLUEPRINTS_MODULE_NAME', 'main') 
   blueprint_module = (module_file + '.py') 
   for fname in dir_list: 
      if ((not os.path.exists(os.path.join(path, fname, 'DISABLED'))) and os.path.isdir(os.path.join(path, fname)) and os.path.exists(os.path.join(path, fname, blueprint_module))): 
         module_root = '.'.join([base_module_name, fname]) 
         module_name = '.'.join([module_root, module_file]) 
         mods[fname] = importlib.import_module(module_name) 
         blueprint = getattr(mods[fname], object_name) 
         app.logger.info(('registering   blueprint:   %s' % blueprint.name)) 
         app.register_blueprint(blueprint) 
         try: 
            importlib.import_module('.'.join([module_root, 'admin'])) 
         except ImportError as e: 
            app.logger.info('%s   module   does   not   define   admin   or   error:   %s', fname, e) 
   app.logger.info('%s   modules   loaded', mods.keys())"," 'Loads blueprints from a folder. 
 :param app: The Flask app 
 :return: None'","'This code looks for any modules or packages in the given 
 directory, loads them 
 and then registers a blueprint 
 - blueprints must be created with the name \'module\' 
 Implemented directory scan 
 Bulk of the code taken from: 
 https://github.com/smartboyathome/ 
 Cheshire-Engine/blob/master/ScoringServer/utils.py'"
"def encipher_bifid(msg, key, symbols=None): 
    (msg, key, A) = _prep(msg, key, symbols, bifid10) 
   long_key = (''.join(uniq(key)) or A) 
   n = (len(A) ** 0.5) 
   if (n != int(n)): 
      raise ValueError(('Length   of   alphabet   (%s)   is   not   a   square   number.' % len(A))) 
   N = int(n) 
   if (len(long_key) < (N ** 2)): 
      long_key = (list(long_key) + [x for x in A if (x not in long_key)]) 
   row_col = dict([(ch, divmod(i, N)) for (i, ch) in enumerate(long_key)]) 
   (r, c) = zip(*[row_col[x] for x in msg]) 
   rc = (r + c) 
   ch = {i: ch for (ch, i) in row_col.items()} 
   rv = ''.join((ch[i] for i in zip(rc[::2], rc[1::2]))) 
   return rv"," 'Encrypts a message using the Bifid cipher. 
 The Bifid cipher is a simple substitution cipher that uses a 
 10-letter alphabet. 
 :param msg: The message to encrypt 
 :type msg: str 
 :param key: The key to use 
 :type key: str 
 :param symbols: The symbols to use in the key. 
 :type symbols: str 
 :return: The encrypted message'","'Performs the Bifid cipher encryption on plaintext ``msg``, and 
 returns the ciphertext. 
 This is the version of the Bifid cipher that uses an `n \times n` 
 Polybius square. 
 INPUT: 
 ``msg``: plaintext string 
 ``key``: short string for key; duplicate characters are 
 ignored and then it is padded with the characters in 
 ``symbols`` that were not in the short key 
 ``symbols``: `n \times n` characters defining the alphabet 
 (default is string.printable) 
 OUTPUT: 
 ciphertext (using Bifid5 cipher without spaces) 
 See Also 
 decipher_bifid, encipher_bifid5, encipher_bifid6'"
"def _get_view_to_display_matrix(scene): 
    from mayavi.core.ui.mayavi_scene import MayaviScene 
   from tvtk.pyface.tvtk_scene import TVTKScene 
   if (not isinstance(scene, (MayaviScene, TVTKScene))): 
      raise TypeError(('scene   must   be   an   instance   of   TVTKScene/MayaviScene,   found   type   %s' % type(scene))) 
   (x, y) = tuple(scene.get_size()) 
   view_to_disp_mat = np.array([[(x / 2.0), 0.0, 0.0, (x / 2.0)], [0.0, ((- y) / 2.0), 0.0, (y / 2.0)], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]) 
   return view_to_disp_mat"," 'Returns a view-to-display matrix for the given scene. 
 The view-to-display matrix transforms a view into a display. 
 Parameters 
 scene : TVTKScene/MayaviScene 
 The scene to get the view-to-display matrix for. 
 Returns 
 view_to_disp_mat : 4x4 array 
 The view-to-display matrix for the given scene.'","'Return the 4x4 matrix to convert view coordinates to display coordinates. 
 It\'s assumed that the view should take up the entire window and that the 
 origin of the window is in the upper left corner.'"
"def debug_msg(text, prefix='Debug'): 
    msg(colorize(('%s:   %s' % (prefix, str(text))), 'cyan'))", 'Print a debug message.','Colorize debug message with prefix'
"@pytest.mark.cmd 
 @pytest.mark.django_db 
 def test_list_projects_modified_since(capfd): 
    call_command('list_projects', '--modified-since=5') 
   (out, err) = capfd.readouterr() 
   assert ('project0' in out) 
   assert ('project1' in out)", 'Test that projects modified since 5 are listed.','Projects modified since a revision'
"def get_modules_containing_name(evaluator, mods, name): 
    def check_python_file(path): 
      try: 
         return parser_cache[path].parser.module 
      except KeyError: 
         try: 
            return check_fs(path) 
         except IOError: 
            return None 
   def check_fs(path): 
      with open(path, 'rb') as f: 
         source = source_to_unicode(f.read()) 
         if (name in source): 
            module_name = os.path.basename(path)[:(-3)] 
            module = _load_module(evaluator, path, source) 
            add_module(evaluator, module_name, module) 
            return module 
   mods = set((m for m in mods if (not isinstance(m, compiled.CompiledObject)))) 
   mod_paths = set() 
   for m in mods: 
      mod_paths.add(m.path) 
      (yield m) 
   if settings.dynamic_params_for_other_modules: 
      paths = set(settings.additional_dynamic_modules) 
      for p in mod_paths: 
         if (p is not None): 
            d = os.path.dirname(os.path.abspath(p)) 
            for entry in os.listdir(d): 
               if (entry not in mod_paths): 
                  if entry.endswith('.py'): 
                     paths.add(((d + os.path.sep) + entry)) 
      for p in sorted(paths): 
         c = check_python_file(p) 
         if ((c is not None) and (c not in mods) and (not isinstance(c, compiled.CompiledObject))): 
            (yield c)"," 'Returns all modules that contain the name \'name\'. 
 If the name is a parameter name, then the module is added to 
 the module cache. 
 If the name is a function name, then the function is added to 
 the function cache. 
 If the name is a class name, then the class is added to the 
 class cache. 
 If the name is a module name, then the module is added to the 
 module cache. 
 If the name is a module path, then the module is added to the 
 module cache. 
 If the name is a file path, then the module is added to the module 
 cache. 
 If the name is a string, then the module is added to the module 
 cache. 
 If the name is a unicode object, then the module is added to the 
 module cache. 
 If the name is a list of modules, then the modules are added to the 
 module cache. 
 If the name is a set of modules, then the modules are added to the 
 module cache. 
 If the name is a dictionary of modules, then the modules are added 
 to the module",'Search a name in the directories of modules.'
"def get_next_disk_info(mapping, disk_bus, device_type='disk', boot_index=None, assigned_devices=None): 
    disk_dev = find_disk_dev_for_disk_bus(mapping, disk_bus, assigned_devices) 
   info = {'bus': disk_bus, 'dev': disk_dev, 'type': device_type} 
   if ((boot_index is not None) and (boot_index >= 0)): 
      info['boot_index'] = str(boot_index) 
   return info"," 'Get information about the next disk to be assigned. 
 This is used to determine if the next disk is in use and if so, to 
 determine the bus, device and type of the disk. 
 :param mapping: A dict containing information about the disks in the 
 system. 
 :param disk_bus: The bus number of the disk to be assigned. 
 :param device_type: The type of device to be assigned. 
 :param boot_index: The index of the disk to be assigned. 
 :param assigned_devices: A list of devices that have been assigned. 
 :returns: A dict containing information about the disk to be assigned.'","'Determine the disk info for the next device on disk_bus. 
 Considering the disks already listed in the disk mapping, 
 determine the next available disk dev that can be assigned 
 for the disk bus. 
 Returns the disk_info for the next available disk.'"
"def atleast_2d(*arys): 
    res = [] 
   for a in arys: 
      if (not isinstance(a, cupy.ndarray)): 
         raise TypeError('Only   cupy   arrays   can   be   atleast_2d') 
      if (a.ndim == 0): 
         a = a.reshape(1, 1) 
      elif (a.ndim == 1): 
         a = a[None, :] 
      res.append(a) 
   if (len(res) == 1): 
      res = res[0] 
   return res"," 'Return a 2d array containing the values of each array in `arys` 
 Parameters 
 arys : cupy arrays 
 Arrays to be concatenated. 
 Returns 
 out : cupy array 
 2d array containing the values of each array in `arys`. 
 Examples 
 >>> a = cupy.arange(10).reshape(5, 2) 
 >>> b = cupy.arange(10).reshape(5, 2) 
 >>> c = cupy.arange(10).reshape(5, 2) 
 >>> atleast_2d(a, b, c) 
 array([[0, 1], 
 [2, 3], 
 [4, 5], 
 [6, 7], 
 [8, 9]]) 
 See Also 
 atleast_2d_like'","'Converts arrays to arrays with dimensions >= 2. 
 If an input array has dimensions less than two, then this function inserts 
 new axes at the head of dimensions to make it have two dimensions. 
 Args: 
 arys (tuple of arrays): Arrays to be converted. All arguments must be 
 :class:`cupy.ndarray` objects. 
 Returns: 
 If there are only one input, then it returns its converted version. 
 Otherwise, it returns a list of converted arrays. 
 .. seealso:: :func:`numpy.atleast_2d`'"
"def last_month_day(tm): 
    (year, month) = tm[:2] 
   day = _DAYS[month] 
   if ((day == 28) and ((year % 4) == 0)): 
      day = 29 
   return day", 'Return the last day of the month for the given tm.','Return last day of this month'
"def set_block_scalar_indent(TokenClass): 
    def callback(lexer, match, context): 
      text = match.group() 
      context.block_scalar_indent = None 
      if (not text): 
         return 
      increment = match.group(1) 
      if increment: 
         current_indent = max(context.indent, 0) 
         increment = int(increment) 
         context.block_scalar_indent = (current_indent + increment) 
      if text: 
         (yield (match.start(), TokenClass, text)) 
         context.pos = match.end() 
   return callback"," 'Set the block scalar indent level for the given token class. 
 This is called by the lexer to set the indent level for the current 
 block scalar. 
 :param TokenClass: The token class to set the indent for 
 :return: A callback that sets the indent for the given token class'",'Set an explicit indentation level for a block scalar.'
"@require_context 
 def vol_get_usage_by_time(context, begin): 
    return model_query(context, models.VolumeUsage, read_deleted='yes').filter(or_((models.VolumeUsage.tot_last_refreshed == None), (models.VolumeUsage.tot_last_refreshed > begin), (models.VolumeUsage.curr_last_refreshed == None), (models.VolumeUsage.curr_last_refreshed > begin))).all()"," 'Get all volume usages by time. 
 :param begin: Begin time of the volume usage.'",'Return volumes usage that have been updated after a specified time.'
"def role_list(request): 
    return keystoneclient(request, admin=True).roles.list()", 'List all roles.','Returns a global list of available roles.'
"def run_discovery(entry_points_iter, cached=False): 
    reg_cache = {} 
   if cached: 
      reg_cache = cache.registry_cache() 
   discovery = QtWidgetDiscovery(cached_descriptions=reg_cache) 
   registry = QtWidgetRegistry() 
   discovery.found_category.connect(registry.register_category) 
   discovery.found_widget.connect(registry.register_widget) 
   discovery.run() 
   if cached: 
      cache.save_registry_cache(reg_cache) 
   return registry"," 'Discover Qt Widgets. 
 :param entry_points_iter: Iterable of entry points to discover. 
 :param cached: If True, use the cached registry.'","'Run the default discovery and return an instance of 
 :class:`QtWidgetRegistry`.'"
"def _reduce_function(func, globs): 
    if func.__closure__: 
      cells = [cell.cell_contents for cell in func.__closure__] 
   else: 
      cells = None 
   return (_reduce_code(func.__code__), globs, func.__name__, cells)"," 'Return a tuple of (code, globs, name, cells). 
 code: the code object of the function. 
 globs: the global namespace of the function. 
 name: the name of the function. 
 cells: a list of the cells of the function, if it is a closure.'","'Reduce a Python function and its globals to picklable components. 
 If there are cell variables (i.e. references to a closure), their 
 values will be frozen.'"
"def replace(s, old, new, maxsplit=0): 
    return s.replace(old, new, maxsplit)"," 'Replace all occurrences of old in s with new. 
 maxsplit: 
 If maxsplit > 0, s will be split into at most maxsplit pieces 
 and the returned list will be a concatenation of the 
 results of calling replace() on each piece. 
 If maxsplit is 0, the returned value will be a single string.'","'replace (str, old, new[, maxsplit]) -> string 
 Return a copy of string str with all occurrences of substring 
 old replaced by new. If the optional argument maxsplit is 
 given, only the first maxsplit occurrences are replaced.'"
"@inlineCallbacks 
 def main(reactor, args, base_path, top_level): 
    options = RunOptions(top_level=top_level) 
   configure_eliot_logging_for_acceptance() 
   try: 
      options.parseOptions(args) 
   except UsageError as e: 
      sys.stderr.write(('%s:   %s\n' % (base_path.basename(), e))) 
      raise SystemExit(1) 
   runner = options.runner 
   def cluster_cleanup(): 
      print 'stopping   cluster' 
      return runner.stop_cluster(reactor) 
   cleanup_trigger_id = reactor.addSystemEventTrigger('before', 'shutdown', cluster_cleanup) 
   from flocker.common.script import eliot_logging_service 
   log_writer = eliot_logging_service(destination=FileDestination(file=open(('%s.log' % (base_path.basename(),)), 'a')), reactor=reactor, capture_stdout=False) 
   log_writer.startService() 
   reactor.addSystemEventTrigger('before', 'shutdown', log_writer.stopService) 
   (yield runner.ensure_keys(reactor)) 
   cluster = (yield runner.start_cluster(reactor)) 
   save_managed_config(options['cert-directory'], options['config'], cluster) 
   managed_config_file = options['cert-directory'].child('managed.yaml') 
   managed_config = create_managed_config(options['config'], cluster) 
   managed_config_file.setContent(yaml.safe_dump(managed_config, default_flow_style=False)) 
   if (options['distribution'] in ('centos-7',)): 
      remote_logs_file = open('remote_logs.log', 'a') 
      for node in cluster.all_nodes: 
         capture_journal(reactor, node.address, remote_logs_file).addErrback(write_failure) 
   elif (options['distribution'] in ('ubuntu-14.04',)): 
      remote_logs_file = open('remote_logs.log', 'a') 
      for node in cluster.all_nodes: 
         capture_upstart(reactor, node.address, remote_logs_file).addErrback(write_failure) 
   flocker_client = make_client(reactor, cluster) 
   (yield wait_for_nodes(reactor, flocker_client, len(cluster.agent_nodes))) 
   if options['no-keep']: 
      print 'not   keeping   cluster' 
   else: 
      save_environment(options['cert-directory'], cluster, options.package_source()) 
      reactor.removeSystemEventTrigger(cleanup_trigger_id)", 'The main entry point for the acceptance tests.',"':param reactor: Reactor to use. 
 :param list args: The arguments passed to the script. 
 :param FilePath base_path: The executable being run. 
 :param FilePath top_level: The top-level of the Flocker repository.'"
"def hilbert(x, N=None, axis=(-1)): 
    x = asarray(x) 
   if iscomplexobj(x): 
      raise ValueError('x   must   be   real.') 
   if (N is None): 
      N = x.shape[axis] 
   if (N <= 0): 
      raise ValueError('N   must   be   positive.') 
   Xf = fftpack.fft(x, N, axis=axis) 
   h = zeros(N) 
   if ((N % 2) == 0): 
      h[0] = h[(N // 2)] = 1 
      h[1:(N // 2)] = 2 
   else: 
      h[0] = 1 
      h[1:((N + 1) // 2)] = 2 
   if (x.ndim > 1): 
      ind = ([newaxis] * x.ndim) 
      ind[axis] = slice(None) 
      h = h[ind] 
   x = fftpack.ifft((Xf * h), axis=axis) 
   return x"," 'Return the Hilbert transform of x. 
 Parameters 
 x : array_like 
 Input signal. 
 N : int, optional 
 The length of the Hilbert transform. If None, the length of 
 the array is used. 
 axis : int, optional 
 The axis along which to compute the transform. 
 Returns 
 out : ndarray 
 The Hilbert transform of x. 
 See Also 
 ifft : Inverse Fourier transform. 
 References 
 .. [1] http://en.wikipedia.org/wiki/Hilbert_transform 
 Examples 
 >>> from scipy import fftpack 
 >>> from scipy.signal import hilbert 
 >>> x = fftpack.rfft(randn(100)) 
 >>> h = hilbert(x) 
 >>> h 
 array([ 0.00000000+0.j,  0.00000000+0.j,  0.00000000+0.j,  0.00000000+0","'Compute the analytic signal, using the Hilbert transform. 
 The transformation is done along the last axis by default. 
 Parameters 
 x : array_like 
 Signal data.  Must be real. 
 N : int, optional 
 Number of Fourier components.  Default: ``x.shape[axis]`` 
 axis : int, optional 
 Axis along which to do the transformation.  Default: -1. 
 Returns 
 xa : ndarray 
 Analytic signal of `x`, of each 1-D array along `axis` 
 See Also 
 scipy.fftpack.hilbert : Return Hilbert transform of a periodic sequence x. 
 Notes 
 The analytic signal ``x_a(t)`` of signal ``x(t)`` is: 
 .. math:: x_a = F^{-1}(F(x) 2U) = x + i y 
 where `F` is the Fourier transform, `U` the unit step function, 
 and `y` the Hilbert transform of `x`. [1]_ 
 In other words, the negative half of the frequency spectrum is zeroed 
 out, turning the real-valued signal into a complex signal.  The Hilbert 
 transformed signal can be obtained from ``np.imag(hilbert(x))``, and the 
 original signal from ``np.real(hilbert(x))``. 
 Examples 
 In this example we use the Hilbert transform to determine the amplitude 
 envelope and instantaneous frequency of an amplitude-modulated signal. 
 >>> import numpy as np 
 >>> import matplotlib.pyplot as plt 
 >>> from scipy.signal import hilbert, chirp 
 >>> duration = 1.0 
 >>> fs = 400.0 
 >>> samples = int(fs*duration) 
 >>> t = np.arange(samples) / fs 
 We create a chirp of which the frequency increases from 20 Hz to 100 Hz and 
 apply an amplitude modulation. 
 >>> signal = chirp(t, 20.0, t[-1], 100.0) 
 >>> signal *= (1.0 + 0.5 * np.sin(2.0*np.pi*3.0*t) ) 
 The amplitude envelope is given by magnitude of the analytic signal. The 
 instantaneous frequency can be obtained by differentiating the 
 instantaneous phase in respect to time. The instantaneous phase corresponds 
 to the phase angle of the analytic signal. 
 >>> analytic_signal = hilbert(signal) 
 >>> amplitude_envelope = np.abs(analytic_signal) 
 >>> instantaneous_phase = np.unwrap(np.angle(analytic_signal)) 
 >>> instantaneous_frequency = (np.diff(instantaneous_phase) / 
 ...                            (2.0*np.pi) * fs) 
 >>> fig = plt.figure() 
 >>> ax0 = fig.add_subplot(211) 
 >>> ax0.plot(t, signal, label=\'signal\') 
 >>> ax0.plot(t, amplitude_envelope, label=\'envelope\') 
 >>> ax0.set_xlabel(""time in seconds"") 
 >>> ax0.legend() 
 >>> ax1 = fig.add_subplot(212) 
 >>> ax1.plot(t[1:], instantaneous_frequency) 
 >>> ax1.set_xlabel(""time in seconds"") 
 >>> ax1.set_ylim(0.0, 120.0) 
 References 
 .. [1] Wikipedia, ""Analytic signal"". 
 http://en.wikipedia.org/wiki/Analytic_signal 
 .. [2] Leon Cohen, ""Time-Frequency Analysis"", 1995. Chapter 2. 
 .. [3] Alan V. Oppenheim, Ronald W. Schafer. Discrete-Time Signal 
 Processing, Third Edition, 2009. Chapter 12. 
 ISBN 13: 978-1292-02572-8'"
"def unpickleStringO(val, sek): 
    x = _cStringIO() 
   x.write(val) 
   x.seek(sek) 
   return x"," 'Unpickle a string object. 
 :param val: the string to unpickle 
 :param sek: the position to seek to 
 :return: the string object'","'Convert the output of L{pickleStringO} into an appropriate type for the 
 current python version.  This may be called on Python 3 and will convert a 
 cStringIO into an L{io.StringIO}. 
 @param val: The content of the file. 
 @type val: L{bytes} 
 @param sek: The seek position of the file. 
 @type sek: L{int} 
 @return: a file-like object which you can write bytes to. 
 @rtype: L{cStringIO.OutputType} on Python 2, L{io.StringIO} on Python 3.'"
"@FileSystem.in_directory(current_directory, 'django', 'alfaces') 
 def test_django_background_server_running_in_background(): 
    import tornado.ioloop 
   import tornado.web 
   class MainHandler(tornado.web.RequestHandler, ): 
      def get(self): 
         self.write('Hello,   world') 
         raise SystemExit() 
   def runserver(): 
      application = tornado.web.Application([('/', MainHandler)]) 
      application.listen(8000) 
      tornado.ioloop.IOLoop.instance().start() 
   server = multiprocessing.Process(target=runserver) 
   server.start() 
   time.sleep(1) 
   e = 'Lettuce   could   not   run   the   builtin   Django   server   at   0.0.0.0:8000""\nmaybe   you   forgot   a   ""runserver""   instance   running   ?\n\nwell   if   you   really   do   not   want   lettuce   to   run   the   server   for   you,   then   just   run:\n\npython   manage.py   --no-server' 
   try: 
      (status, out) = commands.getstatusoutput('python   manage.py   harvest   --verbosity=3   --no-color') 
      assert_equals(out, e) 
      assert_not_equals(status, 0) 
   finally: 
      os.kill(server.pid, 9)", 'Test that the builtin Django server is running in the background','the django builtin server fails if the HTTP port is not available'
"def do_lower(s): 
    return soft_unicode(s).lower()", 'Convert a string to lowercase.','Convert a value to lowercase.'
"def human_readable_to_bytes(value): 
    value = value.lower() 
   if (value[(-2):] == 'ib'): 
      suffix = value[(-3):].lower() 
   else: 
      suffix = value[(-2):].lower() 
   has_size_identifier = ((len(value) >= 2) and (suffix in SIZE_SUFFIX)) 
   if (not has_size_identifier): 
      try: 
         return int(value) 
      except ValueError: 
         raise ValueError(('Invalid   size   value:   %s' % value)) 
   else: 
      multiplier = SIZE_SUFFIX[suffix] 
      return (int(value[:(- len(suffix))]) * multiplier)"," 'Convert a human readable size to bytes. 
 If the value is not a human readable size, or the size identifier is not 
 valid, raise ValueError. 
 If the size identifier is valid, the value is converted to bytes by multiplying 
 the size identifier by the multiplier. 
 :param value: The human readable size to convert to bytes. 
 :return: The converted value in bytes. 
 :raises ValueError: If the value is not a human readable size, or the size 
 identifier is not valid. 
 :raises ValueError: If the value is not a human readable size, or the size 
 identifier is not valid.'","'Converts a human readable size to bytes. 
 :param value: A string such as ""10MB"".  If a suffix is not included, 
 then the value is assumed to be an integer representing the size 
 in bytes. 
 :returns: The converted value in bytes as an integer'"
"@mock_streams('stdout') 
 def test_global_parallel_honors_runs_once(): 
    @decorators.runs_once 
   def mytask(): 
      print 'yolo' 
   with settings(hide('everything'), parallel=True): 
      execute(mytask, hosts=['localhost', '127.0.0.1']) 
   result = sys.stdout.getvalue() 
   eq_(result, 'yolo\n') 
   assert (result != 'yolo\nyolo\n')"," 'When parallel=True, runs_once decorator honors the setting.'",'fab -P (or env.parallel) should honor @runs_once'
"def request_latency(results, limit): 
    scenario_results = [r['scenario'] for r in results if (r['scenario'].get('metrics') and r['scenario']['metrics'].get('call_durations'))] 
   if (len(scenario_results) > 0): 
      unique_metrics = [] 
      for result in scenario_results: 
         if (result['metrics'] not in unique_metrics): 
            unique_metrics.append(result['metrics']) 
      total_requests = 0 
      requests_under_limit = 0 
      for metric in unique_metrics: 
         for (duration, num_requests) in metric['call_durations'].iteritems(): 
            if (float(duration) <= limit): 
               requests_under_limit += num_requests 
         total_requests += (metric['ok_count'] + metric['err_count']) 
      return (float(requests_under_limit) / total_requests) 
   return None"," 'Returns the percentage of requests that are under the given limit 
 :param results: The results from the test 
 :param limit: The maximum duration of a request'","'Calculate the percentage of scenario requests have a latency under the 
 specified time limit. 
 :param results: Results to extract values from. 
 :param limit: Request latency limit in seconds.'"
"@with_setup(prepare_stdout, registry.clear) 
 def test_jsonreport_output_with_no_steps(): 
    with check_jsonreport(u'missing_steps'): 
      runner = Runner(feature_name(u'missing_steps'), enable_jsonreport=True) 
      runner.run()", 'Test that a report is generated when no steps are run.','Test jsonreport output with no steps'
"def is_full_slice(obj, l): 
    return (isinstance(obj, slice) and (obj.start == 0) and (obj.stop == l) and (obj.step is None))"," 'Return True if obj is a full slice, i.e. a slice with start=0 and stop=l and step=None. 
 >>> is_full_slice(slice(0, 10, None), 10) 
 True 
 >>> is_full_slice(slice(0, 10, 1), 10) 
 False 
 >>> is_full_slice(slice(0, 10, None), 10, 1) 
 False 
 >>> is_full_slice(slice(0, 10, None), 10, 2) 
 True 
 >>> is_full_slice(slice(0, 10, None), 10, 3) 
 False 
 >>> is_full_slice(slice(0, 10, None), 10, 4) 
 True 
 >>> is_full_slice(slice(0, 10, None), 10, 5) 
 False 
 >>> is_full_slice(slice(0, 10, None), 10, 6) 
 True",'we have a full length slice'
"def second(seq): 
    return next(itertools.islice(seq, 1, None))"," 'Return the second element of a sequence. 
 >>> list(second([1, 2, 3])) 
 [2]'","'The second element in a sequence 
 >>> second(\'ABC\') 
 \'B\''"
"@contextmanager 
 def collect_profile(file_prefix): 
    import cProfile 
   import uuid 
   profiler = cProfile.Profile() 
   profiler.enable() 
   try: 
      (yield) 
   finally: 
      profiler.disable() 
      profiler.dump_stats('{0}_{1}_master.profile'.format(file_prefix, uuid.uuid4()))"," 'Collects a profile of the given function and writes it to a file. 
 :param file_prefix: A prefix for the file name.'",'Context manager to collect profile information.'
"def guard_null(context, builder, value, exc_tuple): 
    with builder.if_then(is_scalar_zero(builder, value), likely=False): 
      exc = exc_tuple[0] 
      exc_args = (exc_tuple[1:] or None) 
      context.call_conv.return_user_exc(builder, exc, exc_args)"," 'Guard null value with exception. 
 This is a helper function for :meth:`~.Expr.guard_null` to make sure that 
 the guard is only evaluated if the expression is a scalar zero. 
 :param context: The :class:`~.Context` to use for this computation. 
 :param builder: The :class:`~.Builder` to use for this computation. 
 :param value: The expression to guard. 
 :param exc_tuple: The tuple of exception and arguments to raise if the 
 expression is null. 
 :return: The :class:`~.Builder` instance. 
 :rtype: :class:`~.Builder`'","'Guard against *value* being null or zero. 
 *exc_tuple* should be a (exception type, arguments...) tuple.'"
"def ode_separable(eq, func, order, match): 
    x = func.args[0] 
   f = func.func 
   C1 = get_numbered_constants(eq, num=1) 
   r = match 
   u = r.get('hint', f(x)) 
   return Eq(Integral(((r['m2']['coeff'] * r['m2'][r['y']]) / r['m1'][r['y']]), (r['y'], None, u)), (Integral((((- r['m1']['coeff']) * r['m1'][x]) / r['m2'][x]), x) + C1))"," 'Separable ODE: ``y'' = f(x) + g(x) * y`` 
 ``g(x) = 0`` 
 ``f(x) = 0`` 
 ``f(x) = 1`` 
 ``g(x) = 1`` 
 Parameters 
 eq : sympy.Eq 
 The equation to solve. 
 func : sympy.Function 
 The function to differentiate. 
 order : int 
 The order of the derivative. 
 match : dict 
 A dictionary of match information. 
 Returns 
 A sympy.Eq object. 
 Examples 
 >>> from sympy.solvers.ode import ode_separable 
 >>> from sympy import sin, exp 
 >>> from sympy.abc import x 
 >>> eq = sin(x)*x'' + exp(x)*x'' + 1 
 >>> func = x*x'' + 1 
 >>> ode_separable(eq, func, 2, match={\'m1\': 1, \'m2\': [1, 1]}) 
 x","'Solves separable 1st order differential equations. 
 This is any differential equation that can be written as `P(y) 
 \tfrac{dy}{dx} = Q(x)`.  The solution can then just be found by 
 rearranging terms and integrating: `\int P(y) \,dy = \int Q(x) \,dx`. 
 This hint uses :py:meth:`sympy.simplify.simplify.separatevars` as its back 
 end, so if a separable equation is not caught by this solver, it is most 
 likely the fault of that function. 
 :py:meth:`~sympy.simplify.simplify.separatevars` is 
 smart enough to do most expansion and factoring necessary to convert a 
 separable equation `F(x, y)` into the proper form `P(x)\cdot{}Q(y)`.  The 
 general solution is:: 
 >>> from sympy import Function, dsolve, Eq, pprint 
 >>> from sympy.abc import x 
 >>> a, b, c, d, f = map(Function, [\'a\', \'b\', \'c\', \'d\', \'f\']) 
 >>> genform = Eq(a(x)*b(f(x))*f(x).diff(x), c(x)*d(f(x))) 
 >>> pprint(genform) 
 d 
 a(x)*b(f(x))*--(f(x)) = c(x)*d(f(x)) 
 dx 
 >>> pprint(dsolve(genform, f(x), hint=\'separable_Integral\')) 
 f(x) 
 |  b(y)            | c(x) 
 |  ---- dy = C1 +  | ---- dx 
 |  d(y)            | a(x) 
 Examples 
 >>> from sympy import Function, dsolve, Eq 
 >>> from sympy.abc import x 
 >>> f = Function(\'f\') 
 >>> pprint(dsolve(Eq(f(x)*f(x).diff(x) + x, 3*x*f(x)**2), f(x), 
 ... hint=\'separable\', simplify=False)) 
 /   2       \         2 
 log\3*f (x) - 1/        x 
 ---------------- = C1 + -- 
 6                2 
 References 
 - M. Tenenbaum & H. Pollard, ""Ordinary Differential Equations"", 
 Dover 1963, pp. 52 
 # indirect doctest'"
"def _update_rs_from_primary(sds, replica_set_name, server_description, max_set_version, max_election_id): 
    if (replica_set_name is None): 
      replica_set_name = server_description.replica_set_name 
   elif (replica_set_name != server_description.replica_set_name): 
      sds.pop(server_description.address) 
      return (_check_has_primary(sds), replica_set_name, max_set_version, max_election_id) 
   max_election_tuple = (max_set_version, max_election_id) 
   if (None not in server_description.election_tuple): 
      if ((None not in max_election_tuple) and (max_election_tuple > server_description.election_tuple)): 
         address = server_description.address 
         sds[address] = ServerDescription(address) 
         return (_check_has_primary(sds), replica_set_name, max_set_version, max_election_id) 
      max_election_id = server_description.election_id 
   if ((server_description.set_version is not None) and ((max_set_version is None) or (server_description.set_version > max_set_version))): 
      max_set_version = server_description.set_version 
   for server in sds.values(): 
      if ((server.server_type is SERVER_TYPE.RSPrimary) and (server.address != server_description.address)): 
         sds[server.address] = ServerDescription(server.address) 
         break 
   for new_address in server_description.all_hosts: 
      if (new_address not in sds): 
         sds[new_address] = ServerDescription(new_address) 
   for addr in (set(sds) - server_description.all_hosts): 
      sds.pop(addr) 
   return (_check_has_primary(sds), replica_set_name, max_set_version, max_election_id)"," 'Update the replica set description from the primary. 
 :param sds: the dictionary of server descriptions 
 :param replica_set_name: the name of the replica set 
 :param server_description: the server description of the primary 
 :param max_set_version: the max set version of the primary 
 :param max_election_id: the max election id of the primary 
 :return: (bool, str, int, int)'","'Update topology description from a primary\'s ismaster response. 
 Pass in a dict of ServerDescriptions, current replica set name, the 
 ServerDescription we are processing, and the TopologyDescription\'s 
 max_set_version and max_election_id if any. 
 Returns (new topology type, new replica_set_name, new max_set_version, 
 new max_election_id).'"
"def langnames_to_langcodes(names): 
    iso639 = _load_iso639() 
   translate = _ 
   ans = {} 
   names = set(names) 
   for (k, v) in iso639['by_3t'].iteritems(): 
      tv = translate(v) 
      if (tv in names): 
         names.remove(tv) 
         ans[tv] = k 
      if (not names): 
         break 
   for x in names: 
      ans[x] = None 
   return ans", 'Convert ISO 639-1 language names to codes.',"'Given a list of localized language names return a mapping of the names to 3 
 letter ISO 639 language codes. If a name is not recognized, it is mapped to 
 None.'"
"@pytest.fixture() 
 def celery_app(request, celery_config, celery_parameters, celery_enable_logging, use_celery_app_trap): 
    mark = request.node.get_marker(u'celery') 
   config = dict(celery_config, **(mark.kwargs if mark else {})) 
   with _create_app(request, enable_logging=celery_enable_logging, use_trap=use_celery_app_trap, parameters=celery_parameters, **config) as app: 
      (yield app)"," 'Fixture to create a Celery app. 
 This fixture creates a Celery app with the given configuration and parameters. 
 The app is returned as a :class:`~.celery.Celery` object. 
 :param request: The current request. 
 :param celery_config: The Celery configuration. 
 :param celery_parameters: The Celery parameters. 
 :param celery_enable_logging: Whether to enable logging. 
 :param use_celery_app_trap: Whether to use the ``celery_app`` trap. 
 :return: The Celery app. 
 :rtype: :class:`~.celery.Celery'`",'Fixture creating a Celery application instance.'
"def checkMatch(input, prediction, sparse=True, verbosity=0): 
    if sparse: 
      activeElementsInInput = set(input) 
      activeElementsInPrediction = set(prediction) 
   else: 
      activeElementsInInput = set(input.nonzero()[0]) 
      activeElementsInPrediction = set(prediction.nonzero()[0]) 
   totalActiveInPrediction = len(activeElementsInPrediction) 
   totalActiveInInput = len(activeElementsInInput) 
   foundInInput = len(activeElementsInPrediction.intersection(activeElementsInInput)) 
   missingFromInput = len(activeElementsInPrediction.difference(activeElementsInInput)) 
   missingFromPrediction = len(activeElementsInInput.difference(activeElementsInPrediction)) 
   if (verbosity >= 1): 
      print 'preds.   found   in   input:', foundInInput, 'out   of', totalActiveInPrediction, 
      print ';   preds.   missing   from   input:', missingFromInput, 'out   of', totalActiveInPrediction, 
      print ';   unexpected   active   in   input:', missingFromPrediction, 'out   of', totalActiveInInput 
   return (foundInInput, totalActiveInInput, missingFromInput, totalActiveInPrediction)"," 'Checks the match between the prediction and the input. 
 This function returns a tuple with the following values: 
 - the number of elements in the prediction that are found in the input 
 - the number of elements in the input that are not found in the prediction 
 - the number of elements in the input that are not found in the prediction 
 - the number of elements in the prediction that are not found in the input 
 :param input: 
 :param prediction: 
 :param sparse: 
 :param verbosity: 
 :return: 
 :rtype: 
 :raises: 
 :example: 
 >>> checkMatch(input, prediction, sparse=True, verbosity=0) 
 (4, 4, 0, 4)'","'Compares the actual input with the predicted input and returns results 
 Parameters: 
 input:          The actual input 
 prediction:     the predicted input 
 verbosity:        If > 0, print debugging messages 
 sparse:         If true, they are in sparse form (list of 
 active indices) 
 retval         (foundInInput, totalActiveInInput, missingFromInput, 
 totalActiveInPrediction) 
 foundInInput:       The number of predicted active elements that were 
 found in the actual input 
 totalActiveInInput: The total number of active elements in the input. 
 missingFromInput:   The number of predicted active elements that were not 
 found in the actual input 
 totalActiveInPrediction:  The total number of active elements in the prediction'"
"def _user_has_module_perms(user, app_label): 
    for backend in auth.get_backends(): 
      if (not hasattr(backend, 'has_module_perms')): 
         continue 
      try: 
         if backend.has_module_perms(user, app_label): 
            return True 
      except PermissionDenied: 
         return False 
   return False", 'Check if the given user has the module permissions for the given app_label.','A backend can raise `PermissionDenied` to short-circuit permission checking.'
"def tostring(raw, **kwargs): 
    xml_declaration = kwargs.pop(u'xml_declaration', False) 
   encoding = kwargs.pop(u'encoding', u'UTF-8') 
   kwargs[u'encoding'] = unicode 
   kwargs[u'xml_declaration'] = False 
   ans = etree.tostring(raw, **kwargs) 
   if xml_declaration: 
      ans = ((u'<?xml   version=""1.0""   encoding=""%s""?>\n' % encoding) + ans) 
   return re.sub(u'&#x([0-9A-Fa-f]+);', (lambda m: mychr(int(m.group(1), 16))), ans).encode(encoding)"," 'Convert an ET tree to a string. 
 :param raw: an ET tree 
 :param encoding: the encoding to use for the string 
 :param xml_declaration: whether to include an XML declaration 
 :param kwargs: any other keyword arguments to pass to etree.tostring() 
 :returns: a string containing the XML 
 :rtype: unicode'","'lxml *sometimes* represents non-ascii characters as hex entities in 
 attribute values. I can\'t figure out exactly what circumstances cause it. 
 It seems to happen when serializing a part of a larger tree. Since we need 
 serialization to be the same when serializing full and partial trees, we 
 manually replace all hex entities with their unicode codepoints.'"
"def decode_entities(html): 
    def decode(m): 
      html = m.group(0) 
      if (html[:2] == u'&#'): 
         try: 
            if (html[:3] == u'&#x'): 
               return chr(int(html[3:(-1)], 16)) 
            else: 
               return chr(int(html[2:(-1)])) 
         except ValueError: 
            pass 
      else: 
         try: 
            html = chr(name2codepoint[html[1:(-1)]]) 
         except KeyError: 
            pass 
      return html 
   return re.sub(u'&#?\\w+;', decode, html.replace(u'&amp;', u'&'))", 'Decodes HTML entities.',"'Remove HTML entities from a string. 
 Adapted from http://effbot.org/zone/re-sub.htm#unescape-html'"
"@with_setup(setup, teardown) 
 def test_show_negative_chains(): 
    negative_chains.show_negative_chains('dbm.pkl')", 'Test negative chains in the dbm file.','Test the show_negative_chains script main function'
"def escape_ajax(url): 
    (defrag, frag) = urldefrag(url) 
   if (not frag.startswith('!')): 
      return url 
   return add_or_replace_parameter(defrag, '_escaped_fragment_', frag[1:])"," 'Escape the URL fragment for use in an AJAX request. 
 :param url: The URL to be escaped. 
 :returns: The escaped URL.'","'Return the crawleable url according to: 
 http://code.google.com/web/ajaxcrawling/docs/getting-started.html 
 >>> escape_ajax(""www.example.com/ajax.html#!key=value"") 
 \'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue\' 
 >>> escape_ajax(""www.example.com/ajax.html?k1=v1&k2=v2#!key=value"") 
 \'www.example.com/ajax.html?k1=v1&k2=v2&_escaped_fragment_=key%3Dvalue\' 
 >>> escape_ajax(""www.example.com/ajax.html?#!key=value"") 
 \'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue\' 
 >>> escape_ajax(""www.example.com/ajax.html#!"") 
 \'www.example.com/ajax.html?_escaped_fragment_=\' 
 URLs that are not ""AJAX crawlable"" (according to Google) returned as-is: 
 >>> escape_ajax(""www.example.com/ajax.html#key=value"") 
 \'www.example.com/ajax.html#key=value\' 
 >>> escape_ajax(""www.example.com/ajax.html#"") 
 \'www.example.com/ajax.html#\' 
 >>> escape_ajax(""www.example.com/ajax.html"") 
 \'www.example.com/ajax.html\''"
"def verbose_lookup_expr(lookup_expr): 
    from .conf import settings as app_settings 
   VERBOSE_LOOKUPS = (app_settings.VERBOSE_LOOKUPS or {}) 
   lookups = [force_text(VERBOSE_LOOKUPS.get(lookup, _(lookup))) for lookup in lookup_expr.split(LOOKUP_SEP)] 
   return '   '.join(lookups)", 'Returns a string of the lookup_expr for verbose logging.',"'Get a verbose, more humanized expression for a given ``lookup_expr``. 
 Each part in the expression is looked up in the ``FILTERS_VERBOSE_LOOKUPS`` 
 dictionary. Missing keys will simply default to itself. 
 ex:: 
 >>> verbose_lookup_expr(\'year__lt\') 
 \'year is less than\' 
 # with `FILTERS_VERBOSE_LOOKUPS = {}` 
 >>> verbose_lookup_expr(\'year__lt\') 
 \'year lt\''"
"def insured(pool, fun, args, kwargs, errback=None, on_revive=None, **opts): 
    errback = (errback or _ensure_errback) 
   with pool.acquire(block=True) as conn: 
      conn.ensure_connection(errback=errback) 
      channel = conn.default_channel 
      revive = partial(revive_connection, conn, on_revive=on_revive) 
      insured = conn.autoretry(fun, channel, errback=errback, on_revive=revive, **opts) 
      (retval, _) = insured(*args, **dict(kwargs, connection=conn)) 
      return retval"," 'Returns a function that insures the given function against connection 
 failures. 
 The returned function will raise a :class:`ConnectionFailure` exception if 
 the connection fails. 
 :param pool: The connection pool to use. 
 :param fun: The function to wrap. 
 :param args: The arguments to pass to the wrapped function. 
 :param kwargs: The keyword arguments to pass to the wrapped function. 
 :param errback: An optional callback function to call when the connection 
 fails. 
 :param on_revive: An optional callback function to call when the connection 
 is revived. 
 :param opts: A dict of keyword arguments to pass to the wrapped function. 
 :returns: The wrapped function.'","'Ensures function performing broker commands completes 
 despite intermittent connection failures.'"
"def read_png_depth(filename): 
    result = None 
   f = open(filename, 'rb') 
   try: 
      f.seek((- (LEN_IEND + LEN_DEPTH)), 2) 
      depthchunk = f.read(LEN_DEPTH) 
      if (not depthchunk.startswith((DEPTH_CHUNK_LEN + DEPTH_CHUNK_START))): 
         return None 
      result = struct.unpack('!i', depthchunk[14:18])[0] 
   finally: 
      f.close() 
   return result"," 'Read the depth information from a PNG file. 
 :param filename: the name of the PNG file to read. 
 :returns: the depth of the PNG file, or None if the file is not a PNG file 
 or if the depth chunk is not present.'",'Read the special tEXt chunk indicating the depth from a PNG file.'
"def _infer_decorator_callchain(node): 
    if (not isinstance(node, Function)): 
      return 
   if (not node.parent): 
      return 
   try: 
      result = next(node.infer_call_result(node.parent)) 
   except (StopIteration, InferenceError): 
      return 
   if isinstance(result, Instance): 
      result = result._proxied 
   if isinstance(result, Class): 
      if result.is_subtype_of(('%s.classmethod' % BUILTINS)): 
         return 'classmethod' 
      if result.is_subtype_of(('%s.staticmethod' % BUILTINS)): 
         return 'staticmethod'"," 'Returns the decorator type inferred from the callchain. 
 If the node is a function, the function is inferred to be decorated 
 with one of the decorators in the callchain. 
 :param node: The node to infer the decorator from. 
 :return: The inferred decorator type.'","'Detect decorator call chaining and see if the end result is a 
 static or a classmethod.'"
"def txt_records_for_name(name): 
    if (not DNS_AVAILABLE): 
      raise errors.DependencyError('{0}   is   required   to   use   this   function'.format(DNS_REQUIREMENT)) 
   try: 
      dns_response = dns.resolver.query(name, 'TXT') 
   except dns.resolver.NXDOMAIN as error: 
      return [] 
   except dns.exception.DNSException as error: 
      logger.error('Error   resolving   %s:   %s', name, str(error)) 
      return [] 
   return [txt_rec.decode('utf-8') for rdata in dns_response for txt_rec in rdata.strings]"," 'Returns the text records for the given name. 
 :param name: the name to look up 
 :type name: str 
 :returns: the text records for the given name 
 :rtype: list of str'","'Resolve the name and return the TXT records. 
 :param unicode name: Domain name being verified. 
 :returns: A list of txt records, if empty the name could not be resolved 
 :rtype: list of unicode'"
"def spherical_yn(n, z, derivative=False): 
    if derivative: 
      return _spherical_yn_d(n, z) 
   else: 
      return _spherical_yn(n, z)"," 'Return the spherical harmonics basis functions for ``n`` on the sphere 
 ``z`` using the Y function. 
 This is the spherical harmonics basis for the ``Y`` function on the sphere. 
 See :ref:`here <spherical_harmonics_basis_functions>` for more details. 
 Parameters 
 n : int 
 The order of the basis function. 
 z : array 
 The position on the sphere. 
 derivative : bool, optional 
 If True, the derivative of the basis function is returned. 
 Returns 
 y : ndarray 
 The spherical harmonics basis functions. 
 References 
 .. [1] http://en.wikipedia.org/wiki/Spherical_harmonics 
 Examples 
 >>> from sympy.physics.cartesian_to_spherical import spherical_yn 
 >>> from sympy.physics.spherical import z 
 >>> y = spherical_yn(0, z) 
 >>> y 
 array([[-1.0,  0.0,  0.0], 
 [ 0.0","'Spherical Bessel function of the second kind or its derivative. 
 Defined as [1]_, 
 .. math:: y_n(z) = \sqrt{\frac{\pi}{2z}} Y_{n + 1/2}(z), 
 where :math:`Y_n` is the Bessel function of the second kind. 
 Parameters 
 n : int, array_like 
 Order of the Bessel function (n >= 0). 
 z : complex or float, array_like 
 Argument of the Bessel function. 
 derivative : bool, optional 
 If True, the value of the derivative (rather than the function 
 itself) is returned. 
 Returns 
 yn : ndarray 
 Notes 
 For real arguments, the function is computed using the ascending 
 recurrence [2]_.  For complex arguments, the definitional relation to 
 the cylindrical Bessel function of the second kind is used. 
 The derivative is computed using the relations [3]_, 
 .. math:: 
 y_n\' = y_{n-1} - \frac{n + 1}{2} y_n. 
 y_0\' = -y_1 
 .. versionadded:: 0.18.0 
 References 
 .. [1] http://dlmf.nist.gov/10.47.E4 
 .. [2] http://dlmf.nist.gov/10.51.E1 
 .. [3] http://dlmf.nist.gov/10.51.E2'"
"def write_csv_file(path, app_messages, lang_dict): 
    app_messages.sort((lambda x, y: cmp(x[1], y[1]))) 
   from csv import writer 
   with open(path, u'wb') as msgfile: 
      w = writer(msgfile, lineterminator=u'\n') 
      for (p, m) in app_messages: 
         t = lang_dict.get(m, u'') 
         t = re.sub(u'{\\s?([0-9]+)\\s?}', u'{\\g<1>}', t) 
         w.writerow([(p.encode(u'utf-8') if p else u''), m.encode(u'utf-8'), t.encode(u'utf-8')])", 'Write the app messages to a CSV file.',"'Write translation CSV file. 
 :param path: File path, usually `[app]/translations`. 
 :param app_messages: Translatable strings for this app. 
 :param lang_dict: Full translated dict.'"
"def current_year_and_week(): 
    _update_week_number() 
   return (_cur_year, _cur_week)"," 'Returns the current year and week number. 
 :returns: year and week number as a tuple 
 :rtype: tuple'","'Returns a tuple (year, week) where year is the current year of the season 
 and week is the current week number of games being played. 
 i.e., (2012, 3). 
 N.B. This always downloads the schedule XML data.'"
"def both(): 
    return 23"," 'both() -> int 
 :rtype: int 
 :return: 23'",'@rtype: str'
"def popen_wrapper(args, os_err_exc_type=CommandError): 
    try: 
      p = Popen(args, shell=False, stdout=PIPE, stderr=PIPE, close_fds=(os.name != 'nt'), universal_newlines=True) 
   except OSError as e: 
      six.reraise(os_err_exc_type, os_err_exc_type(('Error   executing   %s:   %s' % (args[0], e.strerror))), sys.exc_info()[2]) 
   (output, errors) = p.communicate() 
   return (output, force_text(errors, DEFAULT_LOCALE_ENCODING, strings_only=True), p.returncode)"," 'Wrapper around the Popen() function. 
 This function is a wrapper around the Popen() function. 
 It allows us to control the encoding of the output and the errors, 
 as well as the returncode. 
 :param args: 
 :param os_err_exc_type: 
 :return: 
 :raises: 
 :rtype: 
 :see: 
 :note: 
 :version: 
 :author: 
 :date: 
 :copyright: 
 :license: 
 :credits: 
 :requires: 
 :provides: 
 :modifies: 
 :deprecated: 
 :todo: 
 :bugs: 
 :tracker: 
 :status: 
 :milestone: 
 :scheduled: 
 :scheduled-by: 
 :scheduled-date: 
 :scheduled-time: 
 :comment: 
 :contributors: 
 :doc: 
 :examples: 
 :binaries: 
 :source: 
 :depends: 
 :suggest","'Friendly wrapper around Popen. 
 Returns stdout output, stderr output and OS status code.'"
"def staticfiles_urlpatterns(prefix=None): 
    if (prefix is None): 
      prefix = settings.STATIC_URL 
   return static(prefix, view='django.contrib.staticfiles.views.serve')"," 'Adds staticfiles urls to the urlconf. 
 If the prefix is not specified, it defaults to the value of the 
 STATIC_URL setting. 
 This is used by the staticfiles app. 
 .. versionchanged:: 0.9 
 Added support for serving static files via the Django staticfiles app.'",'Helper function to return a URL pattern for serving static files.'
"def match_patterns(pathname, patterns): 
    for pattern in patterns: 
      if fnmatch(pathname, pattern): 
         return True 
   return False"," 'Matches pathname against the patterns. 
 If any pattern matches, return True, otherwise return False.'",'Returns ``True`` if the pathname matches any of the given patterns.'
"def itemlist(tparams): 
    return [vv for (kk, vv) in tparams.iteritems()]", 'Returns a list of all parameters in the tree parameter list.',"'Get the list of parameters. 
 Note that tparams must be OrderedDict'"
"def tty(*args, **kwargs): 
    return 'ERROR:   This   function   has   been   moved   to   cmd.tty'", 'This function has been moved to cmd.tty',"'Deprecated! Moved to cmdmod. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' test.tty tty0 \'This is a test\' 
 salt \'*\' test.tty pts3 \'This is a test\''"
"def main(argv): 
    parser = optparse.OptionParser() 
   parser.add_option('-e', '--expire-date', dest='expire_date', type='int', default=30, help='number   of   days   before   builds   expire') 
   parser.add_option('-i', '--include', dest='include', type='str', action='append', help='Globs   of   files   to   include') 
   parser.add_option('-n', '--no-exec', dest='fake', action='store_true', help='Do   not   remove   files') 
   parser.add_option('-X', '--exclude', dest='exclude', type='str', action='append', help='Globs   of   files   to   exclude') 
   (args, dirs) = parser.parse_args() 
   if (not dirs): 
      parser.error('you   must   specify   one   or   more   directories') 
   for buildroot in dirs: 
      rm_old_files(buildroot, expire_time=(args.expire_date * DAY), excludes=args.exclude, fake=args.fake, includes=args.include)"," 'Remove old files from build directories. 
 This will remove old files from the build directories. 
 Arguments: 
 argv: 
 The script arguments. 
 Returns: 
 None'",'main'
"def GetTokenInformation(token, information_class): 
    data_size = ctypes.wintypes.DWORD() 
   ctypes.windll.advapi32.GetTokenInformation(token, information_class.num, 0, 0, ctypes.byref(data_size)) 
   data = ctypes.create_string_buffer(data_size.value) 
   handle_nonzero_success(ctypes.windll.advapi32.GetTokenInformation(token, information_class.num, ctypes.byref(data), ctypes.sizeof(data), ctypes.byref(data_size))) 
   return ctypes.cast(data, ctypes.POINTER(TOKEN_USER)).contents"," 'Returns the specified token information. 
 :param token: The token to get the information for. 
 :param information_class: The class of information to get. 
 :return: The token information. 
 :rtype: :py:class:`ctypes.Structure` 
 :raises WindowsError: If the function fails.'","'Given a token, get the token information for it.'"
"def fill_diagonal(a, val, wrap=False): 
    if (a.ndim < 2): 
      raise ValueError('array   must   be   at   least   2-d') 
   end = None 
   if (a.ndim == 2): 
      step = (a.shape[1] + 1) 
      if (not wrap): 
         end = (a.shape[1] * a.shape[1]) 
   else: 
      if (not numpy.alltrue((numpy.diff(a.shape) == 0))): 
         raise ValueError('All   dimensions   of   input   must   be   of   equal   length') 
      step = (1 + numpy.cumprod(a.shape[:(-1)]).sum()) 
   a.ravel()[:end:step] = val"," 'Fill a diagonal in an array with a value. 
 Parameters 
 a : array 
 The array to be filled. 
 val : scalar 
 The value to fill the diagonal with. 
 wrap : bool 
 Whether to wrap around the array (default: False). 
 Examples 
 >>> a = np.arange(10).reshape(5,2) 
 >>> fill_diagonal(a, 10) 
 array([[0, 10], 
 [10, 20], 
 [20, 30], 
 [30, 40], 
 [40, 50]]) 
 >>> fill_diagonal(a, 10, wrap=True) 
 array([[0, 10], 
 [10, 20], 
 [20, 30], 
 [30, 40], 
 [40, 50]]) 
 >>> fill_diagonal(a, 10, wrap=True) 
 array([[0, 10], 
 [10","'Fill the main diagonal of the given array of any dimensionality. 
 For an array `a` with ``a.ndim > 2``, the diagonal is the list of 
 locations with indices ``a[i, i, ..., i]`` all identical. This function 
 modifies the input array in-place, it does not return a value. 
 Args: 
 a (cupy.ndarray): The array, at least 2-D. 
 val (scalar): The value to be written on the diagonal. 
 Its type must be compatible with that of the array a. 
 wrap (bool): If specified, the diagonal is ""wrapped"" after N columns. 
 This affects only tall matrices. 
 Examples 
 >>> a = cupy.zeros((3, 3), int) 
 >>> cupy.fill_diagonal(a, 5) 
 >>> a 
 array([[5, 0, 0], 
 [0, 5, 0], 
 [0, 0, 5]]) 
 .. seealso:: :func:`numpy.fill_diagonal`'"
"def test_multiclass_error(): 
    y = np.linspace(0, 1, 20) 
   bc = BalanceCascade(random_state=RND_SEED) 
   assert_warns(UserWarning, bc.fit, X, y) 
   y = np.array(((([0] * 3) + ([1] * 2)) + ([2] * 15))) 
   bc = BalanceCascade(random_state=RND_SEED) 
   assert_warns(UserWarning, bc.fit, X, y)", 'Test that BalanceCascade throws a warning when multiclass data is used.',"'Test either if an error is raised when the target are not binary 
 type.'"
"def full_like(array, fill_value, stream=None): 
    warnings.warn('chainer.cuda.full_like   is   deprecated.   Use   cupy.full_like   instead.', DeprecationWarning) 
   check_cuda_available() 
   assert (stream is None) 
   if isinstance(array, cupy.ndarray): 
      return cupy.full_like(array, fill_value) 
   return cupy.full(array.shape, fill_value, dtype=array.dtype)"," 'Constructs a full-like array with the same shape as the input array 
 and the same dtype. 
 Parameters 
 array : cupy.ndarray or numpy.ndarray 
 Input array. 
 fill_value : scalar 
 The value to be filled in the empty locations. 
 stream : cupy.cuda.Stream 
 Stream to be used for the copy. 
 Returns 
 out : cupy.ndarray 
 A full-like array.'","'Creates a constant-filled :class:`cupy.ndarray` object like the given array. 
 Args: 
 array (cupy.ndarray or numpy.ndarray): Base array. 
 fill_value: Constant value to fill the array by. 
 stream (cupy.cuda.Stream): CUDA stream. 
 Returns: 
 cupy.ndarray: Constant-filled array.'"
"def query(uuid): 
    timings = [] 
   errors = [] 
   for _ in range(0, constants.NUM_SAMPLES): 
      start = time.time() 
      try: 
         query = TestModel.all() 
         query.filter('test_string   =', uuid) 
         query.fetch(constants.NUM_SAMPLES) 
         total_time = (time.time() - start) 
      except Exception as exception: 
         logging.exception(exception) 
         errors.append(str(exception)) 
         total_time = 0 
      timings.append((total_time * constants.SECONDS_TO_MILLI)) 
   return (timings, errors)"," 'Query the database for the specified UUID and return the timings and 
 errors.'","'Query stored entities and time them. 
 Returns: 
 A tuple of two lists. A list of float times to query 
 all entities, and a list of errors. A zero value signifies 
 a failure.'"
"def instance_add_security_group(context, instance_uuid, security_group_id): 
    sec_group_ref = models.SecurityGroupInstanceAssociation() 
   sec_group_ref.update({'instance_uuid': instance_uuid, 'security_group_id': security_group_id}) 
   sec_group_ref.save()", 'Associate a security group with an instance.','Associate the given security group with the given instance.'
"def parseOptions(args=None): 
    global WORKDIR, DEPSRC, SDKPATH, SRCDIR, DEPTARGET 
   global UNIVERSALOPTS, UNIVERSALARCHS, ARCHLIST, CC, CXX 
   global FW_VERSION_PREFIX 
   global FW_SSL_DIRECTORY 
   if (args is None): 
      args = sys.argv[1:] 
   try: 
      (options, args) = getopt.getopt(args, '?hb', ['build-dir=', 'third-party=', 'sdk-path=', 'src-dir=', 'dep-target=', 'universal-archs=', 'help']) 
   except getopt.GetoptError: 
      print sys.exc_info()[1] 
      sys.exit(1) 
   if args: 
      print 'Additional   arguments' 
      sys.exit(1) 
   deptarget = None 
   for (k, v) in options: 
      if (k in ('-h', '-?', '--help')): 
         print USAGE 
         sys.exit(0) 
      elif (k in ('-d', '--build-dir')): 
         WORKDIR = v 
      elif (k in ('--third-party',)): 
         DEPSRC = v 
      elif (k in ('--sdk-path',)): 
         SDKPATH = v 
      elif (k in ('--src-dir',)): 
         SRCDIR = v 
      elif (k in ('--dep-target',)): 
         DEPTARGET = v 
         deptarget = v 
      elif (k in ('--universal-archs',)): 
         if (v in UNIVERSALOPTS): 
            UNIVERSALARCHS = v 
            ARCHLIST = universal_opts_map[UNIVERSALARCHS] 
            if (deptarget is None): 
               DEPTARGET = default_target_map.get(v, '10.3') 
         else: 
            raise NotImplementedError(v) 
      else: 
         raise NotImplementedError(k) 
   SRCDIR = os.path.abspath(SRCDIR) 
   WORKDIR = os.path.abspath(WORKDIR) 
   SDKPATH = os.path.abspath(SDKPATH) 
   DEPSRC = os.path.abspath(DEPSRC) 
   (CC, CXX) = getTargetCompilers() 
   FW_VERSION_PREFIX = (FW_PREFIX[:] + ['Versions', getVersion()]) 
   FW_SSL_DIRECTORY = (FW_VERSION_PREFIX[:] + ['etc', 'openssl']) 
   print '--   Settings:' 
   print ('         *   Source   directory:            %s' % SRCDIR) 
   print ('         *   Build   directory:               %s' % WORKDIR) 
   print ('         *   SDK   location:                        %s' % SDKPATH) 
   print ('         *   Third-party   source:      %s' % DEPSRC) 
   print ('         *   Deployment   target:         %s' % DEPTARGET) 
   print ('         *   Universal   archs:               %s' % str(ARCHLIST)) 
   print ('         *   C   compiler:                              %s' % CC) 
   print ('         *   C++   compiler:                        %s' % CXX) 
   print '' 
   print ('   --   Building   a   Python   %s   framework   at   patch   level   %s' % (getVersion(), getFullVersion())) 
   print ''"," 'Parses the command line options and sets the global variables. 
 :param args: 
 :return: 
 :rtype: 
 :raise: 
 :see: 
 :author: 
 :date: 
 :version: 
 :status: 
 :doc: 
 :examples: 
 :todo: 
 :bugs: 
 :tracker: 
 :os: 
 :platform: 
 :related: 
 :depends: 
 :deprecated: 
 :deprecated: 
 :deprecated: 
 :deprecated: 
 :deprecated: 
 :deprecated: 
 :deprecated: 
 :deprecated: 
 :deprecated: 
 :deprecated: 
 :deprecated: 
 :deprecated: 
 :deprecated: 
 :deprecated: 
 :deprecated: 
 :deprecated: 
 :deprecated: 
 :deprecated: 
 :deprecated: 
 :deprecated: 
 :deprecated",'Parse arguments and update global settings.'
"@login_required 
 @expect_json 
 def delete_item(request, usage_key): 
    _delete_item(usage_key, request.user)",'./api/usage/delete/:usage_key','Exposes internal helper method without breaking existing bindings/dependencies'
"def request(http, uri, method='GET', body=None, headers=None, redirections=httplib2.DEFAULT_MAX_REDIRECTS, connection_type=None): 
    http_callable = getattr(http, 'request', http) 
   return http_callable(uri, method=method, body=body, headers=headers, redirections=redirections, connection_type=connection_type)"," 'Requests a URL. 
 :param http: An instance of :class:`~google.auth.httplib2.Http` 
 :param uri: The URL to request 
 :param method: The HTTP method to use 
 :param body: The body of the request 
 :param headers: The headers of the request 
 :param redirections: The number of redirections to follow 
 :param connection_type: The connection type to use 
 :returns: The response from the request'","'Make an HTTP request with an HTTP object and arguments. 
 Args: 
 http: httplib2.Http, an http object to be used to make requests. 
 uri: string, The URI to be requested. 
 method: string, The HTTP method to use for the request. Defaults 
 to \'GET\'. 
 body: string, The payload / body in HTTP request. By default 
 there is no payload. 
 headers: dict, Key-value pairs of request headers. By default 
 there are no headers. 
 redirections: int, The number of allowed 203 redirects for 
 the request. Defaults to 5. 
 connection_type: httplib.HTTPConnection, a subclass to be used for 
 establishing connection. If not set, the type 
 will be determined from the ``uri``. 
 Returns: 
 tuple, a pair of a httplib2.Response with the status code and other 
 headers and the bytes of the content returned.'"
"def _chk_asarray(a, axis): 
    if (axis is None): 
      a = ravel(a) 
      outaxis = 0 
   else: 
      a = asarray(a) 
      outaxis = axis 
   return (a, outaxis)"," 'Check that a is an array. 
 Parameters 
 a : array_like 
 Input array. 
 axis : int 
 Axis to check. 
 Returns 
 a : array 
 Input array. 
 outaxis : int 
 Axis to check. 
 Raises 
 ValueError 
 If `a` is not an array.'",'Converts a list into an numpy array'
"def simsam_range_to_files(table, tree, simulated_sample_sizes, dissimilarities, output_dir, mapping_f=None, output_table_basename='table', output_map_basename='map'): 
    create_dir(output_dir) 
   for e in simsam_range(table, tree, simulated_sample_sizes, dissimilarities, mapping_f): 
      output_table = e[0] 
      output_mapping_lines = e[1] 
      simulated_sample_size = e[2] 
      dissimilarity = e[3] 
      output_table_fp = join(output_dir, ('%s_n%d_d%r.biom' % (output_table_basename, simulated_sample_size, dissimilarity))) 
      write_biom_table(output_table, output_table_fp) 
      if (output_mapping_lines is not None): 
         output_map_fp = join(output_dir, ('%s_n%d_d%r.txt' % (output_map_basename, simulated_sample_size, dissimilarity))) 
         output_map_f = open(output_map_fp, 'w') 
         output_map_f.write(''.join(output_mapping_lines)) 
         output_map_f.close()"," 'Simulate sample sizes and dissimilarities for each node in a tree. 
 This function simulates sample sizes and dissimilarities for each node in a tree. 
 The output is a table with the sample size and dissimilarity for each node. 
 The output is also a text file with the mapping between the sample sizes and 
 dissimilarities. 
 Parameters 
 table : table 
 A table with the nodes of the tree and the sample sizes and dissimilarities. 
 tree : tree 
 A tree with the nodes of the tree. 
 simulated_sample_sizes : list 
 A list of sample sizes. 
 dissimilarities : list 
 A list of dissimilarities. 
 output_dir : str 
 The output directory. 
 mapping_f : None or file, optional 
 A file with the mapping between the sample sizes and dissimilarities. 
 output_table_basename : str, optional 
 The basename of the table. 
 output_map_basename : str, optional 
 The basename of the mapping file. 
 Returns 
 None 
 Examples 
 >>> from bio.","'Applies sim_otu_table over a range of parameters, writing output to file 
 table: the input table to simulate samples from 
 tree: tree related OTUs in input table 
 simulated_sample_sizes: a list of ints defining how many 
 output samples should be create per input sample 
 dissimilarities: a list of floats containing the 
 dissimilarities to use in simulating tables 
 output_dir: the directory where all output tables and 
 mapping files should be written 
 mapping_f: file handle for metadata mapping file, if 
 a mapping file should be created with the samples from 
 each simulated table 
 output_table_basename: basename for output table files 
 (default: table) 
 output_map_basename: basename for output mapping files 
 (default: map)'"
"def parse_function_plugin(logger, line, state): 
    try: 
      acc = (state['test_acc'] + 1) 
   except KeyError: 
      acc = 1 
   state['test_acc'] = acc 
   res = line.split() 
   res[2] = acc 
   res[3] = {'metric_type': 'counter'} 
   return tuple(res)"," 'Parse function plugin. 
 :param logger: logger 
 :param line: line to parse 
 :param state: state dict'",'Simple stateful parser'
"def set_subnet_name(name): 
    cmd = 'systemsetup   -setlocalsubnetname   ""{0}""'.format(name) 
   salt.utils.mac_utils.execute_return_success(cmd) 
   return salt.utils.mac_utils.confirm_updated(name, get_subnet_name)"," 'Set the subnet name to the given value. 
 The subnet name is used to identify the local subnet and is used by 
 the mac address to determine which subnet a given mac address is 
 part of. 
 :param name: The new subnet name. 
 :return: The updated subnet name. 
 :rtype: str'","'Set the local subnet name 
 :param str name: The new local subnet name 
 .. note:: 
 Spaces are changed to dashes. Other special characters are removed. 
 :return: True if successful, False if not 
 :rtype: bool 
 CLI Example: 
 .. code-block:: bash 
 The following will be set as \'Mikes-Mac\' 
 salt \'*\' system.set_subnet_name ""Mike\'s Mac""'"
"@ignore_warnings 
 def test_sensitivity_specificity_ignored_labels(): 
    y_true = [1, 1, 2, 3] 
   y_pred = [1, 3, 3, 3] 
   specificity_13 = partial(specificity_score, y_true, y_pred, labels=[1, 3]) 
   specificity_all = partial(specificity_score, y_true, y_pred, labels=None) 
   assert_allclose([1.0, 0.33], specificity_13(average=None), rtol=R_TOL) 
   assert_allclose(np.mean([1.0, 0.33]), specificity_13(average='macro'), rtol=R_TOL) 
   assert_allclose(np.average([1.0, 0.33], weights=[2.0, 1.0]), specificity_13(average='weighted'), rtol=R_TOL) 
   assert_allclose((3.0 / (3.0 + 2.0)), specificity_13(average='micro'), rtol=R_TOL) 
   for average in ['macro', 'weighted', 'micro']: 
      assert_not_equal(specificity_13(average=average), specificity_all(average=average))"," 'Sensitivity and specificity should be calculated differently for ignored 
 labels'",'Test a subset of labels may be requested for SS'
"def description(): 
    for desc in _description.splitlines(): 
      print desc", 'Print the description of the command.','Get description of brainstorm (bst_phantom_ctf) dataset.'
"def estimate_beta_ridge(x, y, alpha): 
    beta_initial = [random.random() for x_i in x[0]] 
   return minimize_stochastic(partial(squared_error_ridge, alpha=alpha), partial(squared_error_ridge_gradient, alpha=alpha), x, y, beta_initial, 0.001)"," 'Estimate the beta parameters for the beta-ridge model. 
 Parameters 
 x : array-like 
 The independent variables. 
 y : array-like 
 The dependent variables. 
 alpha : float 
 The alpha parameter of the squared error. 
 Returns 
 beta : array-like 
 The estimated beta parameters. 
 Notes 
 The beta parameters are estimated by minimizing the squared error 
 between the predicted values and the actual values. 
 References 
 .. [1] http://en.wikipedia.org/wiki/Ridge_regression 
 Examples 
 >>> from sklearn.linear_model import Ridge 
 >>> from sklearn.utils import check_random_state 
 >>> X, y = make_regression(n_samples=100, n_features=10, noise=0.1, random_state=0) 
 >>> alpha = 1.0 
 >>> ridge = Ridge(alpha=alpha) 
 >>> check_random_state(0) 
 >>> ridge.fit(X, y) 
 Ridge(alpha=1.0) 
","'use gradient descent to fit a ridge regression 
 with penalty alpha'"
"def for_name(fq_name, recursive=False): 
    fq_name = str(fq_name) 
   module_name = __name__ 
   short_name = fq_name 
   if (fq_name.rfind('.') >= 0): 
      (module_name, short_name) = (fq_name[:fq_name.rfind('.')], fq_name[(fq_name.rfind('.') + 1):]) 
   try: 
      result = __import__(module_name, None, None, [short_name]) 
      return result.__dict__[short_name] 
   except KeyError: 
      if recursive: 
         raise 
      else: 
         raise ImportError((""Could   not   find   '%s'   on   path   '%s'"" % (short_name, module_name))) 
   except ImportError: 
      try: 
         module = for_name(module_name, recursive=True) 
         if hasattr(module, short_name): 
            return getattr(module, short_name) 
         else: 
            raise KeyError() 
      except KeyError: 
         raise ImportError((""Could   not   find   '%s'   on   path   '%s'"" % (short_name, module_name))) 
      except ImportError: 
         pass 
      raise"," 'Import a module by its fully qualified name. 
 The module is imported into the current module\'s namespace, and 
 the name of the module is returned. 
 If the module cannot be found on the current path, the search is 
 recursively continued. 
 :param fq_name: fully qualified name of the module to import 
 :type fq_name: str 
 :param recursive: if True, continue the search if the module cannot be found 
 :type recursive: bool 
 :return: the module imported 
 :rtype: Module'","'Find class/function/method specified by its fully qualified name. 
 Fully qualified can be specified as: 
 * <module_name>.<class_name> 
 * <module_name>.<function_name> 
 * <module_name>.<class_name>.<method_name> (an unbound method will be 
 returned in this case). 
 for_name works by doing __import__ for <module_name>, and looks for 
 <class_name>/<function_name> in module\'s __dict__/attrs. If fully qualified 
 name doesn\'t contain \'.\', the current module will be used. 
 Args: 
 fq_name: fully qualified name of something to find 
 Returns: 
 class object. 
 Raises: 
 ImportError: when specified module could not be loaded or the class 
 was not found in the module.'"
"def vgcreate(vgname, devices, **kwargs): 
    if ((not vgname) or (not devices)): 
      return 'Error:   vgname   and   device(s)   are   both   required' 
   if isinstance(devices, six.string_types): 
      devices = devices.split(',') 
   cmd = ['vgcreate', vgname] 
   for device in devices: 
      cmd.append(device) 
   valid = ('clustered', 'maxlogicalvolumes', 'maxphysicalvolumes', 'vgmetadatacopies', 'metadatacopies', 'physicalextentsize') 
   for var in kwargs: 
      if (kwargs[var] and (var in valid)): 
         cmd.append('--{0}'.format(var)) 
         cmd.append(kwargs[var]) 
   out = __salt__['cmd.run'](cmd, python_shell=False).splitlines() 
   vgdata = vgdisplay(vgname) 
   vgdata['Output   from   vgcreate'] = out[0].strip() 
   return vgdata"," 'Create a new volume group. 
 This function creates a new volume group using the passed devices. 
 It also sets the maxlogicalvolumes, maxphysicalvolumes, vgmetadatacopies, 
 metadatacopies, and physicalextentsize options if they are passed. 
 This function will not create a new volume group if it already exists. 
 :vgname: Name of the volume group to create 
 :devices: Devices to be used to create the volume group 
 :maxlogicalvolumes: Maximum number of logical volumes to be created 
 :maxphysicalvolumes: Maximum number of physical volumes to be created 
 :vgmetadatacopies: Number of copies of the volume group metadata to be 
 created 
 :metadatacopies: Number of copies of the volume group metadata to be 
 created 
 :physicalextentsize: Size of physical extents in bytes 
 :return: Dictionary containing the volume group data 
 :rtype: dict'","'Create an LVM volume group 
 CLI Examples: 
 .. code-block:: bash 
 salt mymachine lvm.vgcreate my_vg /dev/sdb1,/dev/sdb2 
 salt mymachine lvm.vgcreate my_vg /dev/sdb1 clustered=y'"
"@scope.define 
 def call(fn, args=(), kwargs={}): 
    return fn(*args, **kwargs)"," 'Decorator for wrapping a function in a scope. 
 This decorator will make a function available to the scope 
 as a variable. 
 This is useful for functions that are only used inside a scope. 
 Example:: 
 @scope.define 
 def do_stuff(): 
 @scope.define 
 def call(fn, args=(), kwargs={}): 
 return fn(*args, **kwargs) 
 @scope.define 
 def do_more_stuff(): 
 return do_stuff() 
 The above will make the functions do_stuff and do_more_stuff 
 available to the scope as do_stuff and do_more_stuff. 
 :param fn: The function to wrap. 
 :param args: The arguments to pass to the function. 
 :param kwargs: The keyword arguments to pass to the function. 
 :type fn: callable 
 :type args: tuple 
 :type kwargs: dict 
 :return: The wrapped function. 
 :rtype: callable'","'call fn with given args and kwargs. 
 This is used to represent Apply.__call__'"
"def test_raise_exception_spatial(): 
    sbn1 = SpatialBatchNormalization((5,)) 
   (yield (assert_raises, (ValueError, sbn1.allocate))) 
   sbn2 = SpatialBatchNormalization(3) 
   (yield (assert_raises, (ValueError, sbn2.allocate))) 
   def do_not_fail(*input_dim): 
      try: 
         sbn = SpatialBatchNormalization(input_dim) 
         sbn.allocate() 
      except ValueError: 
         assert False 
   (yield (do_not_fail, 5, 4, 3)) 
   (yield (do_not_fail, 7, 6)) 
   (yield (do_not_fail, 3, 9, 2, 3))"," 'Test that the SpatialBatchNormalization layer raises an exception when 
 input_dim is invalid.'",'Test that SpatialBatchNormalization raises an expected exception.'
"def new_table(table, family='ipv4'): 
    if (not table): 
      return 'Error:   table   needs   to   be   specified' 
   if check_table(table, family=family): 
      return 'Error:   table   {0}   in   family   {1}   already   exists'.format(table, family) 
   nft_family = _NFTABLES_FAMILIES[family] 
   cmd = '{0}   add   table   {1}   {2}'.format(_nftables_cmd(), nft_family, table) 
   out = __salt__['cmd.run'](cmd, python_shell=False) 
   if (not out): 
      out = True 
   return out"," 'Create a new table in the kernel. 
 This function creates a new table in the kernel. 
 :param table: The name of the table to create. 
 :param family: The family of the table to create. 
 :return: True if the table was created, False otherwise.'","'.. versionadded:: 2014.7.0 
 Create new custom table. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' nftables.new_table filter 
 IPv6: 
 salt \'*\' nftables.new_table filter family=ipv6'"
"def function_exponentiation(tokens, local_dict, global_dict): 
    result = [] 
   exponent = [] 
   consuming_exponent = False 
   level = 0 
   for (tok, nextTok) in zip(tokens, tokens[1:]): 
      if ((tok[0] == NAME) and (nextTok[0] == OP) and (nextTok[1] == '**')): 
         if _token_callable(tok, local_dict, global_dict): 
            consuming_exponent = True 
      elif consuming_exponent: 
         exponent.append(tok) 
         if ((tok[0] == nextTok[0] == OP) and (tok[1] == ')') and (nextTok[1] == '(')): 
            consuming_exponent = False 
         if ((tok[0] == nextTok[0] == OP) and (tok[1] == '*') and (nextTok[1] == '(')): 
            consuming_exponent = False 
            del exponent[(-1)] 
         continue 
      elif (exponent and (not consuming_exponent)): 
         if (tok[0] == OP): 
            if (tok[1] == '('): 
               level += 1 
            elif (tok[1] == ')'): 
               level -= 1 
         if (level == 0): 
            result.append(tok) 
            result.extend(exponent) 
            exponent = [] 
            continue 
      result.append(tok) 
   if tokens: 
      result.append(tokens[(-1)]) 
   if exponent: 
      result.extend(exponent) 
   return result"," 'Functions are treated as if they are callable. 
 If a function is called, the arguments are treated as if they are 
 the function\'s arguments. 
 Examples: 
 >>> from sympy.parsing.sympy_parser import parse_expr 
 >>> parse_expr(\'f(g(x))**2\') 
 [f(g(x)), 2] 
 >>> parse_expr(\'f(g(x))**2\') 
 [f(g(x)), 2] 
 >>> parse_expr(\'f(g(x))**2\') 
 [f(g(x)), 2] 
 >>> parse_expr(\'f(g(x))**2\') 
 [f(g(x)), 2] 
 >>> parse_expr(\'f(g(x))**2\') 
 [f(g(x)), 2] 
 >>> parse_expr(\'f(g(x))**2\') 
 [f(g(x)), 2] 
 >>> parse_expr(\'f(g(x))**2\","'Allows functions to be exponentiated, e.g. ``cos**2(x)``. 
 Examples 
 >>> from sympy.parsing.sympy_parser import (parse_expr, 
 ... standard_transformations, function_exponentiation) 
 >>> transformations = standard_transformations + (function_exponentiation,) 
 >>> parse_expr(\'sin**4(x)\', transformations=transformations) 
 sin(x)**4'"
"@testing.requires_testing_data 
 def test_fine_calibration(): 
    raw = read_crop(raw_fname, (0.0, 1.0)) 
   sss_fine_cal = read_crop(sss_fine_cal_fname) 
   raw_sss = maxwell_filter(raw, calibration=fine_cal_fname, origin=mf_head_origin, regularize=None, bad_condition='ignore') 
   assert_meg_snr(raw_sss, sss_fine_cal, 82, 611) 
   py_cal = raw_sss.info['proc_history'][0]['max_info']['sss_cal'] 
   assert_true((py_cal is not None)) 
   assert_true((len(py_cal) > 0)) 
   mf_cal = sss_fine_cal.info['proc_history'][0]['max_info']['sss_cal'] 
   mf_cal['cal_chans'][((mf_cal['cal_chans'][:, 1] == 3022), 1)] = 3024 
   assert_allclose(py_cal['cal_chans'], mf_cal['cal_chans']) 
   assert_allclose(py_cal['cal_corrs'], mf_cal['cal_corrs'], rtol=0.001, atol=0.001) 
   raw_missing = raw.copy().load_data() 
   raw_missing.info['bads'] = ['MEG0111', 'MEG0943'] 
   raw_missing.info._check_consistency() 
   raw_sss_bad = maxwell_filter(raw_missing, calibration=fine_cal_fname, origin=mf_head_origin, regularize=None, bad_condition='ignore') 
   raw_missing.pick_types() 
   raw_sss_bad.pick_channels(raw_missing.ch_names) 
   with warnings.catch_warnings(record=True): 
      raw_sss_missing = maxwell_filter(raw_missing, calibration=fine_cal_fname, origin=mf_head_origin, regularize=None, bad_condition='ignore') 
   assert_meg_snr(raw_sss_missing, raw_sss_bad, 1000.0, 10000.0) 
   raw_sss_3D = maxwell_filter(raw, calibration=fine_cal_fname_3d, origin=mf_head_origin, regularize=None, bad_condition='ignore') 
   assert_meg_snr(raw_sss_3D, sss_fine_cal, 1.0, 6.0) 
   raw_ctf = read_crop(fname_ctf_raw).apply_gradient_compensation(0) 
   assert_raises(RuntimeError, maxwell_filter, raw_ctf, origin=(0.0, 0.0, 0.04), calibration=fine_cal_fname)", 'Test fine calibration.','Test Maxwell filter fine calibration.'
"def _parse_core_site(): 
    global _CORE_SITE_DICT 
   global _CORE_SITE_PATH 
   for indentifier in conf.HDFS_CLUSTERS.get(): 
      try: 
         _CORE_SITE_PATH = os.path.join(conf.HDFS_CLUSTERS[indentifier].HADOOP_CONF_DIR.get(), 'core-site.xml') 
         data = file(_CORE_SITE_PATH, 'r').read() 
         break 
      except KeyError: 
         data = '' 
      except IOError as err: 
         if (err.errno != errno.ENOENT): 
            LOG.error(('Cannot   read   from   ""%s"":   %s' % (_CORE_SITE_PATH, err))) 
            return 
         data = '' 
   _CORE_SITE_DICT = confparse.ConfParse(data)"," 'Parse the core-site.xml file. 
 This function parses the core-site.xml file to get the values of the 
 properties in the core-site.xml file. 
 :returns: core-site.xml parsed data'",'Parse core-site.xml and store in _CORE_SITE_DICT'
"def compute_norms(array, norm_axes=None): 
    if ((not isinstance(array, theano.Variable)) and (not isinstance(array, np.ndarray))): 
      raise RuntimeError('Unsupported   type   {}.   Only   theano   variables   and   numpy   arrays   are   supported'.format(type(array))) 
   ndim = array.ndim 
   if (norm_axes is not None): 
      sum_over = tuple(norm_axes) 
   elif (ndim == 1): 
      sum_over = () 
   elif (ndim == 2): 
      sum_over = (0,) 
   elif (ndim in [3, 4, 5]): 
      sum_over = tuple(range(1, ndim)) 
   else: 
      raise ValueError('Unsupported   tensor   dimensionality   {}.   Must   specify   `norm_axes`'.format(array.ndim)) 
   if isinstance(array, theano.Variable): 
      if (len(sum_over) == 0): 
         norms = T.abs_(array) 
      else: 
         norms = T.sqrt(T.sum((array ** 2), axis=sum_over)) 
   elif isinstance(array, np.ndarray): 
      if (len(sum_over) == 0): 
         norms = abs(array) 
      else: 
         norms = np.sqrt(np.sum((array ** 2), axis=sum_over)) 
   return norms"," 'Compute the norms of an array. 
 Parameters 
 array : tensor 
 The tensor to compute the norms of. 
 norm_axes : tuple 
 If not None, specify the axes to compute the norms on. 
 Returns 
 norms : tensor 
 The norms of the array. 
 Examples 
 >>> x = T.matrix() 
 >>> x.tag.testvalue = T.scalar() 
 >>> x.tag.testvalue 
 tensor(0.0) 
 >>> norms = compute_norms(x) 
 >>> norms.tag.testvalue 
 tensor(1.0)'","'Compute incoming weight vector norms. 
 Parameters 
 array : numpy array or Theano expression 
 Weight or bias. 
 norm_axes : sequence (list or tuple) 
 The axes over which to compute the norm.  This overrides the 
 default norm axes defined for the number of dimensions 
 in `array`. When this is not specified and `array` is a 2D array, 
 this is set to `(0,)`. If `array` is a 3D, 4D or 5D array, it is 
 set to a tuple listing all axes but axis 0. The former default is 
 useful for working with dense layers, the latter is useful for 1D, 
 2D and 3D convolutional layers. 
 Finally, in case `array` is a vector, `norm_axes` is set to an empty 
 tuple, and this function will simply return the absolute value for 
 each element. This is useful when the function is applied to all 
 parameters of the network, including the bias, without distinction. 
 (Optional) 
 Returns 
 norms : 1D array or Theano vector (1D) 
 1D array or Theano vector of incoming weight/bias vector norms. 
 Examples 
 >>> array = np.random.randn(100, 200) 
 >>> norms = compute_norms(array) 
 >>> norms.shape 
 (200,) 
 >>> norms = compute_norms(array, norm_axes=(1,)) 
 >>> norms.shape 
 (100,)'"
"def hrm_person_controller(**attr): 
    T = current.T 
   db = current.db 
   s3db = current.s3db 
   auth = current.auth 
   response = current.response 
   session = current.session 
   settings = current.deployment_settings 
   s3 = response.s3 
   configure = s3db.configure 
   set_method = s3db.set_method 
   contacts_tabs = settings.get_pr_contacts_tabs() 
   if ('all' in contacts_tabs): 
      set_method('pr', 'person', method='contacts', action=s3db.pr_Contacts) 
   if ('public' in contacts_tabs): 
      set_method('pr', 'person', method='public_contacts', action=s3db.pr_Contacts) 
   if ('private' in contacts_tabs): 
      set_method('pr', 'person', method='private_contacts', action=s3db.pr_Contacts) 
   set_method('pr', 'person', method='cv', action=hrm_CV) 
   set_method('pr', 'person', method='record', action=hrm_Record) 
   if settings.has_module('asset'): 
      s3db.add_components('pr_person', asset_asset='assigned_to_id') 
      configure('asset_asset', deletable=False, editable=False, insertable=False) 
   get_vars = current.request.get_vars 
   group = get_vars.get('group', 'staff') 
   hr_id = get_vars.get('human_resource.id', None) 
   if (not str(hr_id).isdigit()): 
      hr_id = None 
   table = s3db.hrm_human_resource 
   table.type.default = 1 
   get_vars['xsltmode'] = 'staff' 
   if hr_id: 
      hr = db((table.id == hr_id)).select(table.type, limitby=(0, 1)).first() 
      if hr: 
         group = (((hr.type == 2) and 'volunteer') or 'staff') 
         get_vars['group'] = group 
   table = db.pr_person 
   tablename = 'pr_person' 
   configure(tablename, deletable=False) 
   mode = session.s3.hrm.mode 
   if (mode is not None): 
      s3.crud_strings[tablename].update(title_display=T('Personal   Profile'), title_update=T('Personal   Profile')) 
      configure('hrm_human_resource', deletable=False, editable=False, insertable=False) 
      configure('hrm_certification', deletable=True, editable=True, insertable=True) 
      configure('hrm_credential', deletable=False, editable=False, insertable=False) 
      configure('hrm_competency', deletable=False, editable=False, insertable=True) 
      configure('hrm_training', deletable=False, editable=False, insertable=True) 
      configure('hrm_experience', deletable=False, editable=False, insertable=False) 
      configure('pr_group_membership', deletable=False, editable=False, insertable=False) 
   elif (settings.get_hrm_staff_label() == T('Contacts')): 
      s3.crud_strings[tablename].update(title_upload=T('Import   Contacts'), title_display=T('Contact   Details'), title_update=T('Contact   Details')) 
   else: 
      s3.crud_strings[tablename].update(title_upload=T('Import   Staff'), title_display=T('Staff   Member   Details'), title_update=T('Staff   Member   Details')) 
   s3.importerPrep = (lambda : dict(ReplaceOption=T('Remove   existing   data   before   import'))) 
   def import_prep(data, group=group): 
      '\n                                    Deletes   all   HR   records   (of   the   given   group)   of   the\n                                    organisation/branch   before   processing   a   new   data   import\n                        ' 
      (resource, tree) = data 
      xml = current.xml 
      tag = xml.TAG 
      att = xml.ATTRIBUTE 
      if s3.import_replace: 
         if (tree is not None): 
            if (group == 'staff'): 
               group = 1 
            elif (group == 'volunteer'): 
               group = 2 
            else: 
               return 
            root = tree.getroot() 
            expr = (""/%s/%s[@%s='org_organisation']/%s[@%s='name']"" % (tag.root, tag.resource, att.name, tag.data, att.field)) 
            orgs = root.xpath(expr) 
            for org in orgs: 
               org_name = (org.get('value', None) or org.text) 
               if org_name: 
                  try: 
                     org_name = json.loads(xml.xml_decode(org_name)) 
                  except: 
                     pass 
               if org_name: 
                  htable = s3db.hrm_human_resource 
                  otable = s3db.org_organisation 
                  query = (((otable.name == org_name) & (htable.organisation_id == otable.id)) & (htable.type == group)) 
                  resource = s3db.resource('hrm_human_resource', filter=query) 
                  resource.delete(format='xml', cascade=True) 
   s3.import_prep = import_prep 
   def prep(r): 
      S3PersonRoleManager.set_method(r, entity='pr_person') 
      if s3.rtl: 
         f = s3db.pr_phone_contact.value 
         f.represent = s3_phone_represent 
         f.widget = S3PhoneWidget() 
      method = r.method 
      if (r.representation == 's3json'): 
         current.xml.show_ids = True 
      elif (r.interactive and (method != 'import')): 
         if (not r.component): 
            table = r.table 
            table.pe_label.readable = table.pe_label.writable = False 
            table.missing.readable = table.missing.writable = False 
            table.age_group.readable = table.age_group.writable = False 
            dob = table.date_of_birth 
            dob.widget = S3CalendarWidget(past_months=1440, future_months=(-60)) 
            person_details_table = s3db.pr_person_details 
            person_details_table.occupation.readable = person_details_table.occupation.writable = False 
            set_org_dependent_field = settings.set_org_dependent_field 
            set_org_dependent_field('pr_person', 'middle_name') 
            set_org_dependent_field('pr_person_details', 'father_name') 
            set_org_dependent_field('pr_person_details', 'mother_name') 
            set_org_dependent_field('pr_person_details', 'grandfather_name') 
            set_org_dependent_field('pr_person_details', 'affiliations') 
            set_org_dependent_field('pr_person_details', 'company') 
         else: 
            component_name = r.component_name 
            if (component_name == 'physical_description'): 
               table = r.component.table 
               for field in table.fields: 
                  table[field].writable = table[field].readable = False 
               table.ethnicity.writable = table.ethnicity.readable = True 
               table.blood_type.writable = table.blood_type.readable = True 
               table.medical_conditions.writable = table.medical_conditions.readable = True 
               table.other_details.writable = table.other_details.readable = True 
            elif (component_name == 'appraisal'): 
               mission_id = r.get_vars.get('mission_id', None) 
               if mission_id: 
                  hatable = r.component.table 
                  mtable = s3db.deploy_mission 
                  mission = db((mtable.id == mission_id)).select(mtable.code, limitby=(0, 1)).first() 
                  if mission: 
                     hatable.code.default = mission.code 
                  atable = db.deploy_assignment 
                  htable = db.hrm_human_resource 
                  query = (((atable.mission_id == mission_id) & (atable.human_resource_id == htable.id)) & (htable.person_id == r.id)) 
                  assignment = db(query).select(atable.job_title_id, limitby=(0, 1)).first() 
                  if assignment: 
                     hatable.job_title_id.default = assignment.job_title_id 
            elif (component_name == 'asset'): 
               configure('asset_asset', insertable=False, editable=False, deletable=False) 
            elif (component_name == 'group_membership'): 
               hrm_configure_pr_group_membership() 
            elif (component_name == 'salary'): 
               hrm_configure_salary(r) 
         if ((method == 'record') or (r.component_name == 'human_resource')): 
            table = s3db.hrm_human_resource 
            table.person_id.writable = table.person_id.readable = False 
            table.site_id.readable = table.site_id.writable = True 
            org = session.s3.hrm.org 
            f = table.organisation_id 
            if (org is None): 
               f.widget = None 
            else: 
               f.default = org 
               f.readable = f.writable = False 
               table.site_id.requires = IS_EMPTY_OR(IS_ONE_OF(db, ('org_site.%s' % s3db.super_key(db.org_site)), s3db.org_site_represent, filterby='organisation_id', filter_opts=(session.s3.hrm.org,))) 
         elif ((method == 'cv') or (r.component_name == 'training')): 
            list_fields = ['course_id', 'grade'] 
            if settings.get_hrm_course_pass_marks: 
               list_fields.append('grade_details') 
            list_fields.append('date') 
            s3db.configure('hrm_training', list_fields=list_fields) 
         resource = r.resource 
         if (mode is not None): 
            resource.build_query(id=auth.s3_logged_in_person()) 
         elif (method not in ('deduplicate', 'search_ac')): 
            if ((not r.id) and (not hr_id)): 
               if response.error: 
                  session.error = response.error 
               redirect(URL(r=r, f='staff')) 
            if (resource.count() == 1): 
               resource.load() 
               r.record = resource.records().first() 
               if r.record: 
                  r.id = r.record.id 
            if (not r.record): 
               session.error = T('Record   not   found') 
               redirect(URL(f='staff')) 
            if (hr_id and (r.component_name == 'human_resource')): 
               r.component_id = hr_id 
            configure('hrm_human_resource', insertable=False) 
      elif (r.representation == 'aadata'): 
         if (r.component_name == 'group_membership'): 
            hrm_configure_pr_group_membership() 
         elif ((method == 'cv') or (r.component_name == 'training')): 
            list_fields = ['course_id', 'grade'] 
            if settings.get_hrm_course_pass_marks: 
               list_fields.append('grade_details') 
            list_fields.append('date') 
            s3db.configure('hrm_training', list_fields=list_fields) 
      return True 
   s3.prep = prep 
   def postp(r, output): 
      if (r.interactive and r.component): 
         if (r.component_name == 'asset'): 
            output['add_btn'] = A(T('Assign   Asset'), _href=URL(c='asset', f='asset'), _id='add-btn', _class='action-btn') 
      return output 
   s3.postp = postp 
   if (session.s3.hrm.orgname and (mode is None)): 
      orgname = session.s3.hrm.orgname 
   else: 
      orgname = None 
   _attr = dict(csv_stylesheet=('hrm', 'person.xsl'), csv_template='staff', csv_extra_fields=[dict(label='Type', field=s3db.hrm_human_resource.type)], orgname=orgname, replace_option=T('Remove   existing   data   before   import'), rheader=hrm_rheader) 
   _attr.update(attr) 
   output = current.rest_controller('pr', 'person', **_attr) 
   return output"," 'Person controller for HRM 
 :param r: the S3Request instance 
 :return: the S3Response instance'","'Persons Controller, defined in the model for use from 
 multiple controllers for unified menus 
 - used for access to component Tabs, Personal Profile & Imports 
 - includes components relevant to HRM'"
"def _get_basic_stream(stream_name, conn): 
    return _execute_with_retries(conn, 'describe_stream', StreamName=stream_name)"," 'Get basic stream info. 
 :param stream_name: Name of stream. 
 :type stream_name: str 
 :param conn: Connection to DynamoDB. 
 :type conn: boto3.resource.Resource 
 :return: Stream info. 
 :rtype: dict'","'Stream info from AWS, via describe_stream 
 Only returns the first ""page"" of shards (up to 100); use _get_full_stream() for all shards. 
 CLI example:: 
 salt myminion boto_kinesis._get_basic_stream my_stream existing_conn'"
"@protocol.commands.add(u'addid', songpos=protocol.UINT) 
 def addid(context, uri, songpos=None): 
    if (not uri): 
      raise exceptions.MpdNoExistError(u'No   such   song') 
   length = context.core.tracklist.get_length() 
   if ((songpos is not None) and (songpos > length.get())): 
      raise exceptions.MpdArgError(u'Bad   song   index') 
   tl_tracks = context.core.tracklist.add(uris=[uri], at_position=songpos).get() 
   if (not tl_tracks): 
      raise exceptions.MpdNoExistError(u'No   such   song') 
   return (u'Id', tl_tracks[0].tlid)"," 'Add a song to the current playlist. 
 :param songpos: Position of the song in the playlist, if not specified the 
 song will be added to the end of the playlist. 
 :type songpos: int 
 :return: The ID of the added song'","'*musicpd.org, current playlist section:* 
 ``addid {URI} [POSITION]`` 
 Adds a song to the playlist (non-recursive) and returns the song id. 
 ``URI`` is always a single file or URL. For example:: 
 addid ""foo.mp3"" 
 Id: 999 
 OK 
 *Clarifications:* 
 - ``addid """"`` should return an error.'"
"def SampleRows(df, nrows, replace=False): 
    indices = np.random.choice(df.index, nrows, replace=replace) 
   sample = df.loc[indices] 
   return sample"," 'Sample a random number of rows from a DataFrame. 
 Parameters 
 df : DataFrame 
 DataFrame to sample from. 
 nrows : int 
 Number of rows to sample. 
 replace : bool 
 If True, replace the sampled rows with NaNs. 
 Returns 
 DataFrame 
 DataFrame containing the sampled rows.'","'Choose a sample of rows from a DataFrame. 
 df: DataFrame 
 nrows: number of rows 
 replace: whether to sample with replacement 
 returns: DataDf'"
"def flushall(host=None, port=None, db=None, password=None): 
    server = _connect(host, port, db, password) 
   return server.flushall()", 'Flush all queued writes to the server.',"'Remove all keys from all databases 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' redis.flushall'"
"def get_common_path(pathlist): 
    common = osp.normpath(osp.commonprefix(pathlist)) 
   if (len(common) > 1): 
      if (not osp.isdir(common)): 
         return abspardir(common) 
      else: 
         for path in pathlist: 
            if (not osp.isdir(osp.join(common, path[(len(common) + 1):]))): 
               return abspardir(common) 
         else: 
            return osp.abspath(common)"," 'Get common prefix of a list of paths. 
 If the common prefix is a directory, return it. 
 Otherwise, return the directory containing the common prefix.'",'Return common path for all paths in pathlist'
"def _getwindowview(folder_alias): 
    attrs = {} 
   args = {} 
   finder = _getfinder() 
   args = {} 
   attrs = {} 
   aeobj_00 = aetypes.ObjectSpecifier(want=aetypes.Type('cfol'), form='alis', seld=folder_alias, fr=None) 
   aeobj_01 = aetypes.ObjectSpecifier(want=aetypes.Type('prop'), form='prop', seld=aetypes.Type('cwnd'), fr=aeobj_00) 
   aeobj_02 = aetypes.ObjectSpecifier(want=aetypes.Type('prop'), form='prop', seld=aetypes.Type('pvew'), fr=aeobj_01) 
   args['----'] = aeobj_02 
   (_reply, args, attrs) = finder.send('core', 'getd', args, attrs) 
   if ('errn' in args): 
      raise Error, aetools.decodeerror(args) 
   views = {'iimg': 0, 'pnam': 1, 'lgbu': 2} 
   if ('----' in args): 
      return views[args['----'].enum]"," 'Returns the window view of the given folder. 
 :param folder_alias: The folder alias to get the window view of. 
 :returns: The window view of the given folder. 
 :rtype: int'",'get the windowview'
"def group_backend_by_type(items): 
    result = defaultdict(list) 
   backends_defined = get_backends() 
   for item in items: 
      name = getattr(item, 'provider', item) 
      backend = backends_defined[name] 
      if issubclass(backend, OpenIdAuth): 
         result['openid'].append(item) 
      elif issubclass(backend, BaseOAuth2): 
         result['oauth2'].append(item) 
      elif issubclass(backend, BaseOAuth1): 
         result['oauth'].append(item) 
   return dict(result)", 'Groups the items by their backend type.','Group items by backend type.'
"def test_large_angle_representation(): 
    a = (Angle(350, u.deg) + Angle(350, u.deg)) 
   a.to_string() 
   a.to_string(u.hourangle) 
   repr(a) 
   repr(a.to(u.hourangle)) 
   str(a) 
   str(a.to(u.hourangle))", 'Test Angle repr and str.',"'Test that angles above 360 degrees can be output as strings, 
 in repr, str, and to_string.  (regression test for #1413)'"
"def check_fasta_seqs_lens(input_fasta_fp): 
    seq_lens = defaultdict(int) 
   input_fasta_f = open(input_fasta_fp, 'U') 
   for (label, seq) in parse_fasta(input_fasta_f): 
      seq_lens[len(seq)] += 1 
   input_fasta_f.close() 
   formatted_seq_lens = [] 
   for curr_key in seq_lens: 
      formatted_seq_lens.append((seq_lens[curr_key], curr_key)) 
   formatted_seq_lens.sort(reverse=True) 
   return formatted_seq_lens"," 'Checks if the input fasta file has all sequence lengths. 
 Parameters 
 input_fasta_fp : string 
 Path to the input fasta file. 
 Returns 
 formatted_seq_lens : list 
 List of (sequence length, sequence label) tuples. 
 Examples 
 >>> check_fasta_seqs_lens(\'test.fasta\') 
 [[1, \'A\'], [1, \'B\'], [1, \'C\'], [2, \'D\'], [2, \'E\'], [2, \'F\'], [3, \'G\'], [3, \'H\'], [3, \'I\'], [3, \'J\'], [3, \'K\'], [4, \'L\'], [4, \'M\'], [4, \'N\'], [5, \'O\'], [5, \'P\'], [6, \'Q\'], [6, \'R\'], [6, \'S\'], [6, \'T\'], [6, \","'Creates bins of sequence lens 
 Useful for checking for valid aligned sequences. 
 input_fasta_fp:  input fasta filepath'"
"def py_scanstring(s, end, encoding=None, strict=True, _b=BACKSLASH, _m=STRINGCHUNK.match): 
    if (encoding is None): 
      encoding = DEFAULT_ENCODING 
   chunks = [] 
   _append = chunks.append 
   begin = (end - 1) 
   while 1: 
      chunk = _m(s, end) 
      if (chunk is None): 
         raise JSONDecodeError('Unterminated   string   starting   at', s, begin) 
      end = chunk.end() 
      (content, terminator) = chunk.groups() 
      if content: 
         if (not isinstance(content, unicode)): 
            content = unicode(content, encoding) 
         _append(content) 
      if (terminator == '""'): 
         break 
      elif (terminator != '\\'): 
         if strict: 
            msg = ('Invalid   control   character   %r   at' % (terminator,)) 
            raise JSONDecodeError(msg, s, end) 
         else: 
            _append(terminator) 
            continue 
      try: 
         esc = s[end] 
      except IndexError: 
         raise JSONDecodeError('Unterminated   string   starting   at', s, begin) 
      if (esc != 'u'): 
         try: 
            char = _b[esc] 
         except KeyError: 
            msg = ('Invalid   \\escape:   ' + repr(esc)) 
            raise JSONDecodeError(msg, s, end) 
         end += 1 
      else: 
         esc = s[(end + 1):(end + 5)] 
         next_end = (end + 5) 
         if (len(esc) != 4): 
            msg = 'Invalid   \\uXXXX   escape' 
            raise JSONDecodeError(msg, s, end) 
         uni = int(esc, 16) 
         if ((55296 <= uni <= 56319) and (sys.maxunicode > 65535)): 
            msg = 'Invalid   \\uXXXX\\uXXXX   surrogate   pair' 
            if (not (s[(end + 5):(end + 7)] == '\\u')): 
               raise JSONDecodeError(msg, s, end) 
            esc2 = s[(end + 7):(end + 11)] 
            if (len(esc2) != 4): 
               raise JSONDecodeError(msg, s, end) 
            uni2 = int(esc2, 16) 
            uni = (65536 + (((uni - 55296) << 10) | (uni2 - 56320))) 
            next_end += 6 
         char = unichr(uni) 
         end = next_end 
      _append(char) 
   return (u''.join(chunks), end)"," 'Scan a string, handling backslash escapes. 
 This is a port of the Python 2.7 code, with the following changes: 
 - It now handles Unicode surrogate pairs. 
 - It now handles \uXXXX escapes. 
 - It now handles \uXXXX\uXXXX escapes. 
 - It now handles \UXXXXXXXX escapes. 
 - It now handles \UXXXXXXXX\UXXXXXXXX escapes. 
 - It now handles \uXXXX\uXXXX\uXXXX escapes. 
 - It now handles \uXXXX\uXXXX\uXXXX\uXXXX escapes. 
 - It now handles \uXXXX\uXXXX\uXXXX\uXXXX\uXXXX escapes. 
 - It now handles \uXXXX\uXXXX\uXXXX\uXXXX\uXXXX\uXXXX escapes. 
 - It now handles \uXXXX\uXXXX\uXXXX\uXXXX\uXXXX\uXXXX\uXXXX escapes. 
 - It now handles \uXXXX\uXXXX\uXXXX\uXXXX\uXXXX\uXXXX\uXXXX\uXXXX escapes. 
 -","'Scan the string s for a JSON string. End is the index of the 
 character in s after the quote that started the JSON string. 
 Unescapes all valid JSON string escape sequences and raises ValueError 
 on attempt to decode an invalid string. If strict is False then literal 
 control characters are allowed in the string. 
 Returns a tuple of the decoded string and the index of the character in s 
 after the end quote.'"
"def harmonicModel(x, fs, w, N, t, nH, minf0, maxf0, f0et): 
    hN = (N / 2) 
   hM1 = int(math.floor(((w.size + 1) / 2))) 
   hM2 = int(math.floor((w.size / 2))) 
   x = np.append(np.zeros(hM2), x) 
   x = np.append(x, np.zeros(hM1)) 
   Ns = 512 
   H = (Ns / 4) 
   hNs = (Ns / 2) 
   pin = max(hNs, hM1) 
   pend = (x.size - max(hNs, hM1)) 
   fftbuffer = np.zeros(N) 
   yh = np.zeros(Ns) 
   y = np.zeros(x.size) 
   w = (w / sum(w)) 
   sw = np.zeros(Ns) 
   ow = triang((2 * H)) 
   sw[(hNs - H):(hNs + H)] = ow 
   bh = blackmanharris(Ns) 
   bh = (bh / sum(bh)) 
   sw[(hNs - H):(hNs + H)] = (sw[(hNs - H):(hNs + H)] / bh[(hNs - H):(hNs + H)]) 
   hfreqp = [] 
   f0t = 0 
   f0stable = 0 
   while (pin < pend): 
      x1 = x[(pin - hM1):(pin + hM2)] 
      (mX, pX) = DFT.dftAnal(x1, w, N) 
      ploc = UF.peakDetection(mX, t) 
      (iploc, ipmag, ipphase) = UF.peakInterp(mX, pX, ploc) 
      ipfreq = ((fs * iploc) / N) 
      f0t = UF.f0Twm(ipfreq, ipmag, f0et, minf0, maxf0, f0stable) 
      if (((f0stable == 0) & (f0t > 0)) or ((f0stable > 0) & (np.abs((f0stable - f0t)) < (f0stable / 5.0)))): 
         f0stable = f0t 
      else: 
         f0stable = 0 
      (hfreq, hmag, hphase) = harmonicDetection(ipfreq, ipmag, ipphase, f0t, nH, hfreqp, fs) 
      hfreqp = hfreq 
      Yh = UF.genSpecSines(hfreq, hmag, hphase, Ns, fs) 
      fftbuffer = np.real(ifft(Yh)) 
      yh[:(hNs - 1)] = fftbuffer[(hNs + 1):] 
      yh[(hNs - 1):] = fftbuffer[:(hNs + 1)] 
      y[(pin - hNs):(pin + hNs)] += (sw * yh) 
      pin += H 
   y = np.delete(y, range(hM2)) 
   y = np.delete(y, range((y.size - hM1), y.size)) 
   return y"," 'Generates a harmonic model from a set of harmonic frequencies. 
 Parameters 
 x : array 
 The input signal. 
 fs : float 
 The sampling frequency of the input signal. 
 w : array 
 The harmonic frequencies. 
 N : int 
 The size of the DFT. 
 t : array 
 The time stamps of the input signal. 
 nH : int 
 The number of harmonics to detect. 
 minf0 : float 
 The minimum frequency of the harmonic model. 
 maxf0 : float 
 The maximum frequency of the harmonic model. 
 f0et : float 
 The threshold of the energy threshold. 
 Returns 
 y : array 
 The harmonic model. 
 Notes 
 The harmonic model is generated using the harmonic detection algorithm 
 described in [1]_. 
 References 
 .. [1] P. G. M. de Boer and J. W. M. Verheijen, ""The harmonic detection 
 algorithm: A new method for detecting harmonics in signals,"" 
 IEEE Transactions on Acoust","'Analysis/synthesis of a sound using the sinusoidal harmonic model 
 x: input sound, fs: sampling rate, w: analysis window, 
 N: FFT size (minimum 512), t: threshold in negative dB, 
 nH: maximum number of harmonics, minf0: minimum f0 frequency in Hz, 
 maxf0: maximim f0 frequency in Hz, 
 f0et: error threshold in the f0 detection (ex: 5), 
 returns y: output array sound'"
"def _make_allocated_size_testcases(): 
    for unit in (Byte, MB, MiB, GB, GiB): 
      for size in (1, 2, 4, 8): 
         test_case = make_allocated_size_tests(unit(size)) 
         globals()[test_case.__name__] = test_case"," 'Generate a test case for each possible combination of unit and size. 
 The test cases are stored in the global variables under the names 
 _make_allocated_size_tests(unit, size). 
 :return: None'",'Build test cases for some common allocation_units.'
"def Gamma(name, k, theta): 
    return rv(name, GammaDistribution, (k, theta))"," 'Creates a Gamma random variable. 
 Parameters 
 name : str 
 A string giving the name of the random variable. 
 k : int 
 The shape parameter. 
 theta : float 
 The shape parameter. 
 Returns 
 A RandomSymbol. 
 Examples 
 >>> from sympy import symbols, Gamma, sqrt, pi 
 >>> x = symbols(\'x\') 
 >>> Gamma(x).evalf(20) 
 1.000000000000000 
 >>> Gamma(x).evalf(10) 
 1.000000000000000 
 References 
 .. [1] http://en.wikipedia.org/wiki/Gamma_distribution 
 .. [2] http://mathworld.wolfram.com/GammaDistribution.html'","'Create a continuous random variable with a Gamma distribution. 
 The density of the Gamma distribution is given by 
 .. math:: 
 f(x) := \frac{1}{\Gamma(k) \theta^k} x^{k - 1} e^{-\frac{x}{\theta}} 
 with :math:`x \in [0,1]`. 
 Parameters 
 k : Real number, `k > 0`, a shape 
 theta : Real number, `\theta > 0`, a scale 
 Returns 
 A RandomSymbol. 
 Examples 
 >>> from sympy.stats import Gamma, density, cdf, E, variance 
 >>> from sympy import Symbol, pprint, simplify 
 >>> k = Symbol(""k"", positive=True) 
 >>> theta = Symbol(""theta"", positive=True) 
 >>> z = Symbol(""z"") 
 >>> X = Gamma(""x"", k, theta) 
 >>> D = density(X)(z) 
 >>> pprint(D, use_unicode=False) 
 -z 
 -k  k - 1  theta 
 theta  *z     *e 
 gamma(k) 
 >>> C = cdf(X, meijerg=True)(z) 
 >>> pprint(C, use_unicode=False) 
 /                                   /     z  \ 
 |                       k*lowergamma|k, -----| 
 |  k*lowergamma(k, 0)               \   theta/ 
 <- ------------------ + ----------------------  for z >= 0 
 |     gamma(k + 1)           gamma(k + 1) 
 \                      0                        otherwise 
 >>> E(X) 
 theta*gamma(k + 1)/gamma(k) 
 >>> V = simplify(variance(X)) 
 >>> pprint(V, use_unicode=False) 
 2 
 k*theta 
 References 
 .. [1] http://en.wikipedia.org/wiki/Gamma_distribution 
 .. [2] http://mathworld.wolfram.com/GammaDistribution.html'"
"def remove_wsgi_intercept(host, port): 
    key = (host, port) 
   if _wsgi_intercept.has_key(key): 
      del _wsgi_intercept[key]", 'Remove a WSGI intercept from the list.',"'Remove the WSGI intercept call for (host, port).'"
"def build_full_traversal(): 
    TraversalSpec = vmodl.query.PropertyCollector.TraversalSpec 
   SelectionSpec = vmodl.query.PropertyCollector.SelectionSpec 
   rpToRp = TraversalSpec(name='rpToRp', type=vim.ResourcePool, path='resourcePool', skip=False) 
   rpToRp.selectSet.extend((SelectionSpec(name='rpToRp'), SelectionSpec(name='rpToVm'))) 
   rpToVm = TraversalSpec(name='rpToVm', type=vim.ResourcePool, path='vm', skip=False) 
   crToRp = TraversalSpec(name='crToRp', type=vim.ComputeResource, path='resourcePool', skip=False) 
   crToRp.selectSet.extend((SelectionSpec(name='rpToRp'), SelectionSpec(name='rpToVm'))) 
   crToH = TraversalSpec(name='crToH', type=vim.ComputeResource, path='host', skip=False) 
   dcToHf = TraversalSpec(name='dcToHf', type=vim.Datacenter, path='hostFolder', skip=False) 
   dcToHf.selectSet.extend((SelectionSpec(name='visitFolders'),)) 
   dcToVmf = TraversalSpec(name='dcToVmf', type=vim.Datacenter, path='vmFolder', skip=False) 
   dcToVmf.selectSet.extend((SelectionSpec(name='visitFolders'),)) 
   dcToNet = TraversalSpec(name='dcToNet', type=vim.Datacenter, path='networkFolder', skip=False) 
   dcToNet.selectSet.extend((SelectionSpec(name='visitFolders'),)) 
   dcToDs = TraversalSpec(name='dcToDs', type=vim.Datacenter, path='datastore', skip=False) 
   dcToDs.selectSet.extend((SelectionSpec(name='visitFolders'),)) 
   hToVm = TraversalSpec(name='hToVm', type=vim.HostSystem, path='vm', skip=False) 
   hToVm.selectSet.extend((SelectionSpec(name='visitFolders'),)) 
   visitFolders = TraversalSpec(name='visitFolders', type=vim.Folder, path='childEntity', skip=False) 
   visitFolders.selectSet.extend((SelectionSpec(name='visitFolders'), SelectionSpec(name='dcToHf'), SelectionSpec(name='dcToVmf'), SelectionSpec(name='dcToNet'), SelectionSpec(name='crToH'), SelectionSpec(name='crToRp'), SelectionSpec(name='dcToDs'), SelectionSpec(name='hToVm'), SelectionSpec(name='rpToVm'))) 
   fullTraversal = SelectionSpec.Array((visitFolders, dcToHf, dcToVmf, dcToNet, crToH, crToRp, dcToDs, rpToRp, hToVm, rpToVm)) 
   return fullTraversal", 'Builds the full traversal spec',"'Builds a traversal spec that will recurse through all objects .. or at 
 least I think it does. additions welcome. 
 See com.vmware.apputils.vim25.ServiceUtil.buildFullTraversal in the java 
 API. Extended by Sebastian Tello\'s examples from pysphere to reach networks 
 and datastores.'"
"def make_subprocess(cmdline, stdout=False, stderr=False, stdin=False, universal_newlines=False): 
    cmdline = cmdline.encode('ascii') 
   logging.info((""Running   cmd   '%s'"" % cmdline)) 
   kwargs = {} 
   kwargs['stdout'] = ((stdout and subprocess.PIPE) or None) 
   kwargs['stderr'] = ((stderr and subprocess.PIPE) or None) 
   kwargs['stdin'] = ((stdin and subprocess.PIPE) or None) 
   kwargs['universal_newlines'] = universal_newlines 
   args = shlex.split(cmdline) 
   logging.info((""Running   args   '%s'"" % args)) 
   proc = subprocess.Popen(args, **kwargs) 
   return proc"," 'Run a command with subprocess.Popen() 
 cmdline is a list of arguments to the command. 
 stdout is a boolean indicating whether the stdout of the command 
 should be redirected to the stdout of the calling process. 
 stderr is a boolean indicating whether the stderr of the command 
 should be redirected to the stderr of the calling process. 
 stdin is a boolean indicating whether the stdin of the command 
 should be redirected to the stdin of the calling process. 
 universal_newlines is a boolean indicating whether the 
 universal_newlines flag should be passed to the subprocess. 
 Returns: 
 A subprocess.Popen() object.'",'Make a subprocess according to the given command-line string'
"def __virtual__(): 
    if salt.utils.is_windows(): 
      return __virtualname__ 
   return (False, 'Module   cyg:   module   only   works   on   Windows   systems.')"," 'Check if the module is available. 
 Return False if not available.'",'Only works on Windows systems.'
"def addStreamHandler(): 
    addHandler(logging.StreamHandler(stream=sys.stdout))"," 'Add a handler to log to stdout. 
 This handler is used by default, so you probably don\'t need to call this 
 function. 
 :return: 
 :rtype: 
 :returns: 
 :returncode: 
 :returnvalue: 
 :returns: 
 :returncode: 
 :returnvalue: 
 :returns: 
 :returncode: 
 :returnvalue: 
 :returns: 
 :returncode: 
 :returnvalue: 
 :returns: 
 :returncode: 
 :returnvalue: 
 :returns: 
 :returncode: 
 :returnvalue: 
 :returns: 
 :returncode: 
 :returnvalue: 
 :returns: 
 :returncode: 
 :returnvalue: 
 :returns: 
 :returncode: 
 :returnvalue: 
 :returns: 
 :returncode: 
 :returnvalue: 
 :returns: 
 :returncode: 
 :returnvalue: 
 :returns: 
 :returncode: 
 :returnvalue: 
 :returns: 
 :return",'Add a stream handler to the default logger.'
"@click.command(name='snapshot') 
 @click.option('--repository', type=str, required=True, help='Snapshot   repository') 
 @click.option('--name', type=str, help='Snapshot   name', show_default=True, default='curator-%Y%m%d%H%M%S') 
 @click.option('--ignore_unavailable', is_flag=True, show_default=True, help='Ignore   unavailable   shards/indices.') 
 @click.option('--include_global_state', type=bool, show_default=True, default=True, expose_value=True, help='Store   cluster   global   state   with   snapshot.') 
 @click.option('--partial', is_flag=True, show_default=True, help='Do   not   fail   if   primary   shard   is   unavailable.') 
 @click.option('--wait_for_completion', type=bool, show_default=True, default=True, help='Wait   for   operation   to   complete') 
 @click.option('--skip_repo_fs_check', is_flag=True, expose_value=True, help='Skip   repository   filesystem   access   validation.') 
 @click.option('--ignore_empty_list', is_flag=True, help='Do   not   raise   exception   if   there   are   no   actionable   indices') 
 @click.option('--filter_list', callback=validate_filter_json, default='{""filtertype"":""none""}', help='JSON   string   representing   an   array   of   filters.') 
 @click.pass_context 
 def snapshot_singleton(ctx, repository, name, ignore_unavailable, include_global_state, partial, skip_repo_fs_check, wait_for_completion, ignore_empty_list, filter_list): 
    action = 'snapshot' 
   action_class = CLASS_MAP[action] 
   c_args = ctx.obj['config']['client'] 
   client = get_client(**c_args) 
   logger = logging.getLogger(__name__) 
   raw_options = {'repository': repository, 'name': name, 'ignore_unavailable': ignore_unavailable, 'include_global_state': include_global_state, 'partial': partial, 'skip_repo_fs_check': skip_repo_fs_check, 'wait_for_completion': wait_for_completion} 
   logger.debug('Validating   provided   options:   {0}'.format(raw_options)) 
   mykwargs = option_schema_check(action, raw_options) 
   logger.debug('Validating   provided   filters:   {0}'.format(filter_list)) 
   clean_filters = {'filters': filter_schema_check(action, filter_list)} 
   ilo = IndexList(client) 
   _do_filters(ilo, clean_filters, ignore_empty_list) 
   action_obj = action_class(ilo, **mykwargs) 
   _actionator(action, action_obj, dry_run=ctx.parent.params['dry_run'])"," 'Snapshots a repository. 
 :param repository: The name of the repository to snapshot. 
 :param name: The name of the snapshot. 
 :param ignore_unavailable: If true, ignore unavailable shards/indices. 
 :param include_global_state: If true, store cluster global state with snapshot. 
 :param partial: If true, do not fail if primary shard is unavailable. 
 :param wait_for_completion: If true, wait for operation to complete. 
 :param skip_repo_fs_check: Skip repository filesystem access validation. 
 :param filter_list: JSON string representing an array of filters. 
 :param ctx: The click context object'",'Snapshot indices'
"def info(request, message, extra_tags='', fail_silently=False): 
    add_message(request, constants.INFO, message, extra_tags=extra_tags, fail_silently=fail_silently)","'.info(request, message, extra_tags="""", fail_silently=False):'",'Adds a message with the ``INFO`` level.'
"def string_param(registry, xml_parent, data): 
    base_param(registry, xml_parent, data, True, 'hudson.model.StringParameterDefinition')"," 'Generate a StringParameterDefinition node. 
 :param registry: The registry. 
 :param xml_parent: The parent node. 
 :param data: The data to generate the node from. 
 :return: The generated node.'","'yaml: string 
 A string parameter. 
 :arg str name: the name of the parameter 
 :arg str default: the default value of the parameter (optional) 
 :arg str description: a description of the parameter (optional) 
 Example:: 
 parameters: 
 - string: 
 name: FOO 
 default: bar 
 description: ""A parameter named FOO, defaults to \'bar\'.""'"
"def parse_media_range(range): 
    (type, subtype, params) = parse_mime_type(range) 
   if ((not params.has_key('q')) or (not params['q']) or (not float(params['q'])) or (float(params['q']) > 1) or (float(params['q']) < 0)): 
      params['q'] = '1' 
   return (type, subtype, params)"," 'Parse a media range into a tuple. 
 :param range: A media range, e.g. ""image/jpeg,q=0.2"" 
 :returns: A tuple of (type, subtype, params) 
 :rtype: tuple'","'Parse a media-range into its component parts. 
 Carves up a media range and returns a tuple of the (type, subtype, 
 params) where \'params\' is a dictionary of all the parameters for the media 
 range.  For example, the media range \'application/*;q=0.5\' would get parsed 
 into: 
 (\'application\', \'*\', {\'q\', \'0.5\'}) 
 In addition this function also guarantees that there is a value for \'q\' 
 in the params dictionary, filling it in with a proper default if 
 necessary.'"
"def findImageFile(filename): 
    isfile = os.path.isfile 
   if isfile(filename): 
      return filename 
   orig = copy.copy(filename) 
   extensions = ('.jpg', '.png', '.tif', '.bmp', '.gif', '.jpeg', '.tiff') 
   def logCorrected(orig, actual): 
      logging.warn('Requested   image   {!r}   not   found   but   similar   filename   {!r}   exists.   This   will   be   used   instead   but   changing   the   filename   is   advised.'.format(orig, actual)) 
   if filename.endswith(extensions): 
      filename = os.path.splitext(orig)[0] 
   if isfile(filename): 
      logCorrected(orig, filename) 
      return filename 
   for ext in extensions: 
      if isfile((filename + ext)): 
         filename += ext 
         logCorrected(orig, filename) 
         return filename"," 'Finds a file with the same name as the requested filename, but with a 
 different extension. 
 :param filename: The name of the file to find. 
 :type filename: str 
 :return: The filename with the correct extension. 
 :rtype: str'","'Tests whether the filename is an image file. If not will try some common 
 alternatives (e.g. extensions .jpg .tif...)'"
"def rfft(x, n=None, axis=(-1), overwrite_x=False): 
    tmp = _asfarray(x) 
   if (not numpy.isrealobj(tmp)): 
      raise TypeError('1st   argument   must   be   real   sequence') 
   try: 
      work_function = _DTYPE_TO_RFFT[tmp.dtype] 
   except KeyError: 
      raise ValueError(('type   %s   is   not   supported' % tmp.dtype)) 
   overwrite_x = (overwrite_x or _datacopied(tmp, x)) 
   return _raw_fft(tmp, n, axis, 1, overwrite_x, work_function)"," 'Return the real-valued discrete Fourier transform of x. 
 Parameters 
 x : array_like 
 Input array. 
 n : int, optional 
 Length of the FFT. If None, the length of x is used. 
 axis : int, optional 
 Axis along which the FFT is computed. If None, the FFT is computed 
 along the last axis. 
 overwrite_x : bool, optional 
 If True, x is overwritten. 
 Returns 
 y : ndarray 
 The discrete Fourier transform of x. 
 Raises 
 ValueError 
 If the input array is not real. 
 See Also 
 fft 
 Notes 
 The input array must be real. 
 Examples 
 >>> x = np.array([1, 2, 3, 4]) 
 >>> rfft(x) 
 array([ 1,  2,  3,  4]) 
 >>> rfft(x, n=2) 
 array([ 1,  2,  3,  4]) 
 >>> rfft(x, n=2","'Discrete Fourier transform of a real sequence. 
 Parameters 
 x : array_like, real-valued 
 The data to transform. 
 n : int, optional 
 Defines the length of the Fourier transform.  If `n` is not specified 
 (the default) then ``n = x.shape[axis]``.  If ``n < x.shape[axis]``, 
 `x` is truncated, if ``n > x.shape[axis]``, `x` is zero-padded. 
 axis : int, optional 
 The axis along which the transform is applied.  The default is the 
 last axis. 
 overwrite_x : bool, optional 
 If set to true, the contents of `x` can be overwritten. Default is 
 False. 
 Returns 
 z : real ndarray 
 The returned real array contains:: 
 [y(0),Re(y(1)),Im(y(1)),...,Re(y(n/2))]              if n is even 
 [y(0),Re(y(1)),Im(y(1)),...,Re(y(n/2)),Im(y(n/2))]   if n is odd 
 where:: 
 y(j) = sum[k=0..n-1] x[k] * exp(-sqrt(-1)*j*k*2*pi/n) 
 j = 0..n-1 
 Note that ``y(-j) == y(n-j).conjugate()``. 
 See Also 
 fft, irfft, scipy.fftpack.basic 
 Notes 
 Within numerical accuracy, ``y == rfft(irfft(y))``. 
 Both single and double precision routines are implemented.  Half precision 
 inputs will be converted to single precision.  Non floating-point inputs 
 will be converted to double precision.  Long-double precision inputs are 
 not supported. 
 Examples 
 >>> from scipy.fftpack import fft, rfft 
 >>> a = [9, -9, 1, 3] 
 >>> fft(a) 
 array([  4. +0.j,   8.+12.j,  16. +0.j,   8.-12.j]) 
 >>> rfft(a) 
 array([  4.,   8.,  12.,  16.])'"
"def login_required(handler_method): 
    def check_login(self, *args, **kwargs): 
      if (self.request.method != 'GET'): 
         self.abort(400, detail='The   login_required   decorator   can   only   be   used   for   GET   requests.') 
      user = users.get_current_user() 
      if (not user): 
         return self.redirect(users.create_login_url(self.request.url)) 
      else: 
         handler_method(self, *args, **kwargs) 
   return check_login"," 'Decorator to check if the user is logged in. 
 If the user is not logged in, a 401 Unauthorized is returned.'","'A decorator to require that a user be logged in to access a handler. 
 To use it, decorate your get() method like this:: 
 @login_required 
 def get(self): 
 user = users.get_current_user(self) 
 self.response.out.write(\'Hello, \' + user.nickname()) 
 We will redirect to a login page if the user is not logged in. We always 
 redirect to the request URI, and Google Accounts only redirects back as 
 a GET request, so this should not be used for POSTs.'"
"def get_writer(extension): 
    global FORMAT_WRITERS 
   if (FORMAT_WRITERS is None): 
      _import_writers() 
   return FORMAT_WRITERS.get(extension, None)"," 'Return the writer for the given extension. 
 If the extension is not in the FORMAT_WRITERS dictionary, None is returned.'",'Returns None if no writer is found for extension.'
"def _compute_variable_length_solns(model, t0, k0, g, tol): 
    results = {} 
   for integrator in ['dopri5', 'dop853', 'vode', 'lsoda']: 
      discrete_soln = model.solve(t0, k0, h=1.0, g=g, tol=tol, integrator=integrator, atol=1e-14, rtol=1e-11) 
      results[integrator] = discrete_soln 
   return results"," 'Compute the discrete solutions to the model for each of the integrators. 
 Returns 
 results : dict 
 A dictionary of integrator names and the discrete solution to the model. 
 Notes 
 This computes the discrete solution to the model for each of the integrators. 
 It is assumed that the integrator is called with the same arguments each time. 
 This function is only used for testing.'",'Returns a dictionary of variable length solution trajectories.'
"def SortedConcatenate(sep=u','): 
    def step(ctxt, ndx, value): 
      if (value is not None): 
         ctxt[ndx] = value 
   def finalize(ctxt): 
      if (len(ctxt) == 0): 
         return None 
      return sep.join(map(ctxt.get, sorted(ctxt.iterkeys()))) 
   return ({}, step, finalize)"," 'Returns a generator which returns the sorted concatenation of the values 
 of the generator\'s items. 
 >>> sorted_concat = SortedConcatenate() 
 >>> sorted_concat([1, 2, 3, 4]) 
 \'1, 2, 3, 4\' 
 >>> sorted_concat([1, 2, 3, 4], \'\' +) 
 \'1, 2, 3, 4\' 
 >>> sorted_concat([1, 2, 3, 4], \'\' +) 
 \'1, 2, 3, 4\' 
 >>> sorted_concat([1, 2, 3, 4], \'\' +) 
 \'1, 2, 3, 4\' 
 >>> sorted_concat([1, 2, 3, 4], \',\') 
 \',1,2,3,4\''","'String concatenation aggregator for sqlite, sorted by supplied index'"
"def make_headers(keep_alive=None, accept_encoding=None, user_agent=None, basic_auth=None, proxy_basic_auth=None): 
    headers = {} 
   if accept_encoding: 
      if isinstance(accept_encoding, str): 
         pass 
      elif isinstance(accept_encoding, list): 
         accept_encoding = ','.join(accept_encoding) 
      else: 
         accept_encoding = 'gzip,deflate' 
      headers['accept-encoding'] = accept_encoding 
   if user_agent: 
      headers['user-agent'] = user_agent 
   if keep_alive: 
      headers['connection'] = 'keep-alive' 
   if basic_auth: 
      headers['authorization'] = ('Basic   ' + b64encode(six.b(basic_auth)).decode('utf-8')) 
   if proxy_basic_auth: 
      headers['proxy-authorization'] = ('Basic   ' + b64encode(six.b(proxy_basic_auth)).decode('utf-8')) 
   return headers"," 'Make headers for a request. 
 :param keep_alive: whether to use the keep-alive header 
 :type keep_alive: bool 
 :param accept_encoding: the encoding to use for the response 
 :type accept_encoding: str or list 
 :param user_agent: the user-agent to use 
 :type user_agent: str 
 :param basic_auth: the basic auth credentials to use 
 :type basic_auth: str 
 :param proxy_basic_auth: the proxy basic auth credentials to use 
 :type proxy_basic_auth: str'","'Shortcuts for generating request headers. 
 :param keep_alive: 
 If ``True``, adds \'connection: keep-alive\' header. 
 :param accept_encoding: 
 Can be a boolean, list, or string. 
 ``True`` translates to \'gzip,deflate\'. 
 List will get joined by comma. 
 String will be used as provided. 
 :param user_agent: 
 String representing the user-agent you want, such as 
 ""python-urllib3/0.6"" 
 :param basic_auth: 
 Colon-separated username:password string for \'authorization: basic ...\' 
 auth header. 
 :param proxy_basic_auth: 
 Colon-separated username:password string for \'proxy-authorization: basic ...\' 
 auth header. 
 Example: :: 
 >>> make_headers(keep_alive=True, user_agent=""Batman/1.0"") 
 {\'connection\': \'keep-alive\', \'user-agent\': \'Batman/1.0\'} 
 >>> make_headers(accept_encoding=True) 
 {\'accept-encoding\': \'gzip,deflate\'}'"
"def group(seq, size): 
    def take(seq, n): 
      for i in xrange(n): 
         (yield seq.next()) 
   if (not hasattr(seq, 'next')): 
      seq = iter(seq) 
   while True: 
      x = list(take(seq, size)) 
      if x: 
         (yield x) 
      else: 
         break"," 'Returns a generator that yields groups of size size. 
 >>> list(group(range(5), 2)) 
 [(0, 1), (2, 3)] 
 >>> list(group(range(5), 3)) 
 [(0, 1, 2), (3)] 
 >>> list(group(range(5), 4)) 
 [(0, 1, 2, 3)] 
 >>> list(group(range(5), 5)) 
 [(0, 1, 2, 3, 4)]'","'Returns an iterator over a series of lists of length size from iterable. 
 >>> list(group([1,2,3,4], 2)) 
 [[1, 2], [3, 4]] 
 >>> list(group([1,2,3,4,5], 2)) 
 [[1, 2], [3, 4], [5]]'"
"def safe_value(name, value): 
    if (name.lower() in LOGGER_SENSITIVE_HEADERS): 
      prefix_length = logger_settings.get('reveal_sensitive_prefix', 16) 
      prefix_length = int(min(prefix_length, ((len(value) ** 2) / 32), (len(value) / 2))) 
      redacted_value = value[0:prefix_length] 
      return (redacted_value + '...') 
   return value"," 'Redact sensitive headers from the request. 
 :param name: Header name 
 :type name: str 
 :param value: Header value 
 :type value: str 
 :return: Redacted header value'","'Only show up to logger_settings[\'reveal_sensitive_prefix\'] characters 
 from a sensitive header. 
 :param name: Header name 
 :param value: Header value 
 :return: Safe header value'"
"def quote(string, safe=u'/'): 
    if (sys.version_info.major < 3): 
      if isinstance(string, unicode): 
         string = string.encode(u'utf8') 
      string = urllib.quote(string, safe.encode(u'utf8')) 
   else: 
      string = urllib.parse.quote(str(string), safe) 
   return string"," 'Returns a url-encoded version of the string. 
 :param string: The string to encode. 
 :param safe: The characters to use as the safe characters in the url. 
 :return: The url-encoded version of the string.'",'Like urllib2.quote but handles unicode properly.'
"def delete_subnet_group(name, region=None, key=None, keyid=None, profile=None): 
    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) 
   if (not conn): 
      return False 
   try: 
      conn.delete_cache_subnet_group(name) 
      msg = 'Deleted   ElastiCache   subnet   group   {0}.'.format(name) 
      log.info(msg) 
      return True 
   except boto.exception.BotoServerError as e: 
      log.debug(e) 
      msg = 'Failed   to   delete   ElastiCache   subnet   group   {0}'.format(name) 
      log.error(msg) 
      return False", 'Delete a subnet group.',"'Delete an ElastiCache subnet group. 
 CLI example:: 
 salt myminion boto_elasticache.delete_subnet_group my-subnet-group                 region=us-east-1'"
"def _in_gce_environment(): 
    if (SETTINGS.env_name is not None): 
      return (SETTINGS.env_name == 'GCE_PRODUCTION') 
   if ((NO_GCE_CHECK != 'True') and _detect_gce_environment()): 
      SETTINGS.env_name = 'GCE_PRODUCTION' 
      return True 
   return False"," 'Checks if the environment is in GCE Production. 
 :return: True if the environment is in GCE Production, False otherwise'","'Detect if the code is running in the Compute Engine environment. 
 Returns: 
 True if running in the GCE environment, False otherwise.'"
"def useTest(vm, prompt=Prompt): 
    old = vm.logfile 
   if (old == stdout): 
      log('*   Temporarily   disabling   logging   to   stdout') 
      vm.logfile = None 
   log('*   Switching   to   interactive   use   -   press   control-]   to   exit') 
   vm.interact() 
   if (old == stdout): 
      log('*   Restoring   logging   to   stdout') 
      vm.logfile = stdout", 'Switches to interactive mode for testing','Use VM interactively - exit by pressing control-]'
"def dimension_mul(a, b): 
    if ((a == datashape.var) or (b == datashape.var)): 
      return datashape.var 
   if isinstance(a, Fixed): 
      a = int(a) 
   if isinstance(b, Fixed): 
      b = int(b) 
   return int((a * b))"," 'Multiply dimensions of two dimensions. 
 Parameters 
 a : int or datashape.Fixed 
 b : int or datashape.Fixed 
 Returns 
 int or datashape.Fixed 
 Dimensions of the product.'","'Given b number of a\'s how big is our dimension? 
 >>> dimension_mul(2, 5) 
 10 
 We round up 
 >>> dimension_mul(9, 3) 
 27 
 In the case of datashape.var, we resort to var 
 >>> from datashape import var 
 >>> dimension_mul(datashape.var, 5) 
 Var() 
 >>> dimension_mul(10, datashape.var) 
 Var()'"
"def mask_between_time(dts, start, end, include_start=True, include_end=True): 
    time_micros = dts._get_time_micros() 
   start_micros = _time_to_micros(start) 
   end_micros = _time_to_micros(end) 
   (left_op, right_op, join_op) = _opmap[(bool(include_start), bool(include_end), (start_micros <= end_micros))] 
   return join_op(left_op(start_micros, time_micros), right_op(time_micros, end_micros))"," 'Return the portion of the dts between start and end. 
 If include_start is True, the start time will be included. 
 If include_end is True, the end time will be included. 
 If include_start is False and include_end is False, the entire dts will be returned. 
 If include_start is True and include_end is False, the portion of the dts before the start time will be returned. 
 If include_start is False and include_end is True, the portion of the dts after the end time will be returned. 
 :param dts: the dts to mask 
 :param start: the start time to mask 
 :param end: the end time to mask 
 :param include_start: whether to include the start time 
 :param include_end: whether to include the end time 
 :return: the portion of the dts between start and end 
 :rtype: int'","'Return a mask of all of the datetimes in ``dts`` that are between 
 ``start`` and ``end``. 
 Parameters 
 dts : pd.DatetimeIndex 
 The index to mask. 
 start : time 
 Mask away times less than the start. 
 end : time 
 Mask away times greater than the end. 
 include_start : bool, optional 
 Inclusive on ``start``. 
 include_end : bool, optional 
 Inclusive on ``end``. 
 Returns 
 mask : np.ndarray[bool] 
 A bool array masking ``dts``. 
 See Also 
 :meth:`pandas.DatetimeIndex.indexer_between_time`'"
"def line_search_armijo(f, xk, pk, gfk, old_fval, args=(), c1=0.0001, alpha0=1): 
    xk = np.atleast_1d(xk) 
   fc = [0] 
   def phi(alpha1): 
      fc[0] += 1 
      return f((xk + (alpha1 * pk)), *args) 
   if (old_fval is None): 
      phi0 = phi(0.0) 
   else: 
      phi0 = old_fval 
   derphi0 = np.dot(gfk, pk) 
   (alpha, phi1) = scalar_search_armijo(phi, phi0, derphi0, c1=c1, alpha0=alpha0) 
   return (alpha, fc[0], phi1)"," 'Armijo line search for a single function. 
 The Armijo line search is a line search algorithm for finding the 
 root of a function. It is based on the Armijo rule for finding the root 
 of a polynomial. 
 Parameters 
 f : function 
 Function to be minimized. 
 xk : ndarray 
 Initial guess for the root. 
 pk : ndarray 
 Derivative of the function at the initial guess. 
 gfk : ndarray 
 Gradient of the function at the initial guess. 
 old_fval : float 
 The function value at the initial guess. 
 args : tuple 
 Additional arguments to the function. 
 c1 : float 
 The step size. 
 alpha0 : float 
 Initial guess for the step size. 
 Returns 
 alpha : float 
 The step size. 
 fc : ndarray 
 The function value at the step size. 
 phi1 : float 
 The function value at the step size. 
 Examples 
 >>> from sympy import symbols, Function, sin, cos, exp 
 >>>","'Minimize over alpha, the function ``f(xk+alpha pk)``. 
 Parameters 
 f : callable 
 Function to be minimized. 
 xk : array_like 
 Current point. 
 pk : array_like 
 Search direction. 
 gfk : array_like 
 Gradient of `f` at point `xk`. 
 old_fval : float 
 Value of `f` at point `xk`. 
 args : tuple, optional 
 Optional arguments. 
 c1 : float, optional 
 Value to control stopping criterion. 
 alpha0 : scalar, optional 
 Value of `alpha` at start of the optimization. 
 Returns 
 alpha 
 f_count 
 f_val_at_alpha 
 Notes 
 Uses the interpolation algorithm (Armijo backtracking) as suggested by 
 Wright and Nocedal in \'Numerical Optimization\', 1999, pg. 56-57'"
"def compile_and_install_client(project_client, extra_args='', install_client=True): 
    java_args = {} 
   java_args['compile_dir'] = _TMP_COMPILE_DIR 
   java_args['app_dir'] = _DEFAULT_APP_DIR 
   java_args['gwt_dir'] = find_gwt_dir() 
   java_args['extra_args'] = extra_args 
   java_args['project_client'] = project_client 
   cmd = (_COMPILE_LINE % java_args) 
   logging.info('Compiling   client   %s', project_client) 
   try: 
      utils.run(cmd, verbose=True) 
      if install_client: 
         return install_completed_client(java_args['compile_dir'], project_client) 
      return True 
   except error.CmdError: 
      logging.info('Error   compiling   %s,   leaving   old   client', project_client) 
   return False"," 'Compile and install a GWT client. 
 :param project_client: The name of the client project. 
 :param extra_args: Extra arguments to pass to the compiler. 
 :param install_client: Whether to install the compiled client. 
 :return: True if the client was compiled and installed successfully, False 
 otherwise. 
 :raises: CmdError if the compilation fails.'","'Compile the client into a temporary directory, if successful 
 call install_completed_client to install the new client. 
 :param project_client: project.client pair e.g. autotest.AfeClient 
 :param install_client: Boolean, if True install the clients 
 :return: True if install and compile was successful False if it failed'"
"def _replication_request(command, host=None, core_name=None, params=None): 
    params = ([] if (params is None) else params) 
   extra = (['command={0}'.format(command)] + params) 
   url = _format_url('replication', host=host, core_name=core_name, extra=extra) 
   return _http_request(url)"," 'Send a replication command to a host. 
 :param command: the command to send to the host 
 :param host: the host to send the command to 
 :param core_name: the core to send the command to 
 :param params: the parameters to send with the command 
 :return: the response from the host 
 :rtype: dict'","'PRIVATE METHOD 
 Performs the requested replication command and returns a dictionary with 
 success, errors and data as keys. The data object will contain the JSON 
 response. 
 command : str 
 The replication command to execute. 
 host : str (None) 
 The solr host to query. __opts__[\'host\'] is default 
 core_name: str (None) 
 The name of the solr core if using cores. Leave this blank if you are 
 not using cores or if you want to check all cores. 
 params : list<str> ([]) 
 Any additional parameters you want to send. Should be a lsit of 
 strings in name=value format. e.g. [\'name=value\'] 
 Return: dict<str, obj>:: 
 {\'success\':boolean, \'data\':dict, \'errors\':list, \'warnings\':list}'"
"def config_value(option): 
    return option_list[option]"," 'Returns the value of the option as a string. 
 :param option: The option to return the value for. 
 :type option: str'",'Return the current configuration value for the given option'
"def synchronize(*klasses): 
    if (threadingmodule is not None): 
      for klass in klasses: 
         for methodName in klass.synchronized: 
            sync = _sync(klass, klass.__dict__[methodName]) 
            setattr(klass, methodName, sync)"," 'Decorator that wraps the class method with a lock. 
 This decorator is a shortcut for wrapping the class method with a lock. 
 It works only if threading is available. 
 This decorator is not recommended for general use, because it is a 
 shortcut and not a general purpose locking mechanism. 
 If you need to lock a method, you should use the :class:`lock` decorator 
 instead. 
 :param klasses: A list of classes to decorate. 
 :type klasses: list(type)'","'Make all methods listed in each class\' synchronized attribute synchronized. 
 The synchronized attribute should be a list of strings, consisting of the 
 names of methods that must be synchronized. If we are running in threaded 
 mode these methods will be wrapped with a lock.'"
"def __virtual__(): 
    if (not salt.utils.is_windows()): 
      return (False, u'Module   PSGet:   Module   only   works   on   Windows   systems') 
   powershell_info = __salt__[u'cmd.shell_info'](u'powershell') 
   if ((not powershell_info[u'installed']) or (distutils.version.StrictVersion(powershell_info[u'version']) >= distutils.version.StrictVersion(u'5.0'))): 
      return (False, u'Module   DSC:   Module   only   works   with   PowerShell   5   or   newer.') 
   return __virtualname__"," 'Return the name of the module if it is available, else return False.'",'Set the system module of the kernel is Windows'
"def request_fingerprint(request, include_headers=None): 
    if include_headers: 
      include_headers = tuple((to_bytes(h.lower()) for h in sorted(include_headers))) 
   cache = _fingerprint_cache.setdefault(request, {}) 
   if (include_headers not in cache): 
      fp = hashlib.sha1() 
      fp.update(to_bytes(request.method)) 
      fp.update(to_bytes(canonicalize_url(request.url))) 
      fp.update((request.body or '')) 
      if include_headers: 
         for hdr in include_headers: 
            if (hdr in request.headers): 
               fp.update(hdr) 
               for v in request.headers.getlist(hdr): 
                  fp.update(v) 
      cache[include_headers] = fp.hexdigest() 
   return cache[include_headers]"," 'Returns the fingerprint of the request. 
 The fingerprint is a hexadecimal string that uniquely identifies the 
 request. 
 This is used for caching purposes. 
 :param request: The request to fingerprint. 
 :param include_headers: A list of headers to include in the fingerprint. 
 :return: The fingerprint of the request. 
 :rtype: str'","'Return the request fingerprint. 
 The request fingerprint is a hash that uniquely identifies the resource the 
 request points to. For example, take the following two urls: 
 http://www.example.com/query?id=111&cat=222 
 http://www.example.com/query?cat=222&id=111 
 Even though those are two different URLs both point to the same resource 
 and are equivalent (ie. they should return the same response). 
 Another example are cookies used to store session ids. Suppose the 
 following page is only accesible to authenticated users: 
 http://www.example.com/members/offers.html 
 Lot of sites use a cookie to store the session id, which adds a random 
 component to the HTTP Request and thus should be ignored when calculating 
 the fingerprint. 
 For this reason, request headers are ignored by default when calculating 
 the fingeprint. If you want to include specific headers use the 
 include_headers argument, which is a list of Request headers to include.'"
"def parse_argv(tokens, options, options_first=False): 
    parsed = [] 
   while (tokens.current() is not None): 
      if (tokens.current() == '--'): 
         return (parsed + [Argument(None, v) for v in tokens]) 
      elif tokens.current().startswith('--'): 
         parsed += parse_long(tokens, options) 
      elif (tokens.current().startswith('-') and (tokens.current() != '-')): 
         parsed += parse_shorts(tokens, options) 
      elif options_first: 
         return (parsed + [Argument(None, v) for v in tokens]) 
      else: 
         parsed.append(Argument(None, tokens.move())) 
   return parsed"," 'Parses the command line options. 
 :param tokens: The tokenizer 
 :param options: The options 
 :param options_first: If true, the options are parsed first. 
 :return: A list of parsed arguments'","'Parse command-line argument vector. 
 If options_first: 
 argv ::= [ long | shorts ]* [ argument ]* [ \'--\' [ argument ]* ] ; 
 else: 
 argv ::= [ long | shorts | argument ]* [ \'--\' [ argument ]* ] ;'"
"def _make_namespaced_xattr_key(key, namespace='user'): 
    namespaced_key = '.'.join([namespace, key]) 
   return namespaced_key"," 'Returns a namespaced key for a key. 
 :param key: Key name 
 :param namespace: Namespace to prepend to key name 
 :return: Namespaced key'","'Create a fully-qualified xattr-key by including the intended namespace. 
 Namespacing differs among OSes[1]: 
 FreeBSD: user, system 
 Linux: user, system, trusted, security 
 MacOS X: not needed 
 Mac OS X won\'t break if we include a namespace qualifier, so, for 
 simplicity, we always include it. 
 [1] http://en.wikipedia.org/wiki/Extended_file_attributes'"
"def main(): 
    args = _parse_args() 
   if (args.pkl is not None): 
      dataset = serial.load(args.pkl) 
   else: 
      dataset = NORB(args.which_norb, args.which_set) 
   grid_indices = ([0] * 5) 
   grid_to_short_label = _make_grid_to_short_label(dataset) 
   label_to_row_indices = _make_label_to_row_indices(dataset.y) 
   object_image_index = [0] 
   blank_image_index = [0] 
   blank_label = _get_blank_label(dataset) 
   grid_dimension = [0] 
   dataset_is_stereo = ('s' in dataset.view_converter.axes) 
   (figure, all_axes) = pyplot.subplots(1, (3 if dataset_is_stereo else 2), squeeze=True, figsize=(10, 3.5)) 
   set_name = (os.path.split(args.pkl)[1] if (args.which_set is None) else ('%sing   set' % args.which_set)) 
   figure.canvas.set_window_title(('NORB   dataset   (%s)' % set_name)) 
   label_text = figure.suptitle('Up/down   arrows   choose   label,   left/right   arrows   change   it', x=0.1, horizontalalignment='left') 
   for axes in all_axes: 
      axes.get_xaxis().set_visible(False) 
      axes.get_yaxis().set_visible(False) 
   (text_axes, image_axes) = (all_axes[0], all_axes[1:]) 
   image_captions = (('left', 'right') if dataset_is_stereo else ('mono   image',)) 
   if args.stereo_viewer: 
      image_captions = tuple(reversed(image_captions)) 
   for (image_ax, caption) in safe_zip(image_axes, image_captions): 
      image_ax.set_title(caption) 
   text_axes.set_frame_on(False) 
   def is_blank(grid_indices): 
      assert (len(grid_indices) == 5) 
      assert all(((x >= 0) for x in grid_indices)) 
      ci = dataset.label_name_to_index['category'] 
      category = grid_to_short_label[ci][grid_indices[ci]] 
      category_name = dataset.label_to_value_funcs[ci](category) 
      return (category_name == 'blank') 
   def get_short_label(grid_indices): 
      ""\n                        Returns   the   first   5   elements   of   the   label   vector   pointed   to   by\n                        grid_indices.   We   use   the   first   5,   since   they're   the   labels   used   by\n                        both   the   'big'   and   Small   NORB   datasets.\n                        "" 
      if is_blank(grid_indices): 
         return tuple(blank_label[:5]) 
      else: 
         return tuple((grid_to_short_label[i][g] for (i, g) in enumerate(grid_indices))) 
   def get_row_indices(grid_indices): 
      short_label = get_short_label(grid_indices) 
      return label_to_row_indices.get(short_label, None) 
   axes_to_pixels = {} 
   def redraw(redraw_text, redraw_images): 
      row_indices = get_row_indices(grid_indices) 
      if (row_indices is None): 
         row_index = None 
         image_index = 0 
         num_images = 0 
      else: 
         image_index = (blank_image_index if is_blank(grid_indices) else object_image_index)[0] 
         row_index = row_indices[image_index] 
         num_images = len(row_indices) 
      def draw_text(): 
         if (row_indices is None): 
            padding_length = (dataset.y.shape[1] - len(grid_indices)) 
            current_label = (tuple(get_short_label(grid_indices)) + ((0,) * padding_length)) 
         else: 
            current_label = dataset.y[row_index, :] 
         label_names = dataset.label_index_to_name 
         label_values = [label_to_value(label) for (label_to_value, label) in safe_zip(dataset.label_to_value_funcs, current_label)] 
         lines = [('%s:   %s' % (t, v)) for (t, v) in safe_zip(label_names, label_values)] 
         if (dataset.y.shape[1] > 5): 
            lines = ((lines[:5] + [('No   such   image' if (num_images == 0) else ('image:   %d   of   %d' % ((image_index + 1), num_images))), '\n']) + lines[5:]) 
         lines[grid_dimension[0]] = ('==>   ' + lines[grid_dimension[0]]) 
         text_axes.clear() 
         text_axes.text(0, 0.5, '\n'.join(lines), verticalalignment='center', transform=text_axes.transAxes) 
      def draw_images(): 
         if (row_indices is None): 
            for axis in image_axes: 
               axis.clear() 
         else: 
            data_row = dataset.X[row_index:(row_index + 1), :] 
            axes_names = dataset.view_converter.axes 
            assert (len(axes_names) in (4, 5)) 
            assert (axes_names[0] == 'b') 
            assert (axes_names[(-3)] == 0) 
            assert (axes_names[(-2)] == 1) 
            assert (axes_names[(-1)] == 'c') 
            def draw_image(image, axes): 
               assert (len(image.shape) == 2) 
               norm = (matplotlib.colors.NoNorm() if args.no_norm else None) 
               axes_to_pixels[axes] = image 
               axes.imshow(image, norm=norm, cmap='gray') 
            if ('s' in axes_names): 
               image_pair = dataset.get_topological_view(mat=data_row, single_tensor=True) 
               image_pair = tuple(image_pair[0, :, :, :, 0]) 
               if args.stereo_viewer: 
                  image_pair = tuple(reversed(image_pair)) 
               for (axis, image) in safe_zip(image_axes, image_pair): 
                  draw_image(image, axis) 
            else: 
               image = dataset.get_topological_view(mat=data_row) 
               image = image[0, :, :, 0] 
               draw_image(image, image_axes[0]) 
      if redraw_text: 
         draw_text() 
      if redraw_images: 
         draw_images() 
      figure.canvas.draw() 
   default_status_text = ('mouseover   image%s   for   pixel   values' % ('' if (len(image_axes) == 1) else 's')) 
   status_text = figure.text(0.5, 0.1, default_status_text) 
   def on_mouse_motion(event): 
      original_text = status_text.get_text() 
      if (event.inaxes not in image_axes): 
         status_text.set_text(default_status_text) 
      else: 
         pixels = axes_to_pixels[event.inaxes] 
         row = int((event.ydata + 0.5)) 
         col = int((event.xdata + 0.5)) 
         status_text.set_text(('Pixel   value:   %g' % pixels[(row, col)])) 
      if (status_text.get_text != original_text): 
         figure.canvas.draw() 
   def on_key_press(event): 
      def add_mod(arg, step, size): 
         return (((arg + size) + step) % size) 
      def incr_index_type(step): 
         num_dimensions = len(grid_indices) 
         if (dataset.y.shape[1] > 5): 
            num_dimensions += 1 
         grid_dimension[0] = add_mod(grid_dimension[0], step, num_dimensions) 
      def incr_index(step): 
         assert (step in (0, (-1), 1)), ('Step   was   %d' % step) 
         image_index = (blank_image_index if is_blank(grid_indices) else object_image_index) 
         if (grid_dimension[0] == 5): 
            row_indices = get_row_indices(grid_indices) 
            if (row_indices is None): 
               image_index[0] = 0 
            else: 
               image_index[0] = add_mod(image_index[0], step, len(row_indices)) 
         else: 
            gd = grid_dimension[0] 
            grid_indices[gd] = add_mod(grid_indices[gd], step, len(grid_to_short_label[gd])) 
            row_indices = get_row_indices(grid_indices) 
            if (row_indices is None): 
               image_index[0] = 0 
            else: 
               image_index[0] = min(image_index[0], len(row_indices)) 
      disable_left_right = (is_blank(grid_indices) and (not (grid_dimension[0] in (0, 5)))) 
      if (event.key == 'up'): 
         incr_index_type((-1)) 
         redraw(True, False) 
      elif (event.key == 'down'): 
         incr_index_type(1) 
         redraw(True, False) 
      elif (event.key == 'q'): 
         sys.exit(0) 
      elif (not disable_left_right): 
         if (event.key == 'left'): 
            incr_index((-1)) 
            redraw(True, True) 
         elif (event.key == 'right'): 
            incr_index(1) 
            redraw(True, True) 
   figure.canvas.mpl_connect('key_press_event', on_key_press) 
   figure.canvas.mpl_connect('motion_notify_event', on_mouse_motion) 
   redraw(True, True) 
   pyplot.show()"," 'Main function for the NORB viewer. 
 Parameters 
 args : NORBArgs 
 Arguments that were passed to the NORB viewer. 
 Returns 
 None'",'Top-level function.'
"@slow_test 
 @requires_sklearn_0_15 
 def test_generalization_across_time(): 
    from sklearn.svm import SVC 
   from sklearn.base import is_classifier 
   from sklearn.kernel_ridge import KernelRidge 
   from sklearn.preprocessing import LabelEncoder 
   from sklearn.metrics import roc_auc_score, mean_squared_error 
   epochs = make_epochs() 
   y_4classes = np.hstack((epochs.events[:7, 2], (epochs.events[7:, 2] + 1))) 
   if check_version('sklearn', '0.18'): 
      from sklearn.model_selection import KFold, StratifiedKFold, ShuffleSplit, LeaveOneGroupOut 
      cv = LeaveOneGroupOut() 
      cv_shuffle = ShuffleSplit() 
      cv_lolo = [(train, test) for (train, test) in cv.split(y_4classes, y_4classes, y_4classes)] 
      scorer_regress = None 
   else: 
      from sklearn.cross_validation import KFold, StratifiedKFold, ShuffleSplit, LeaveOneLabelOut 
      cv_shuffle = ShuffleSplit(len(epochs)) 
      cv_lolo = LeaveOneLabelOut(y_4classes) 
      scorer_regress = mean_squared_error 
   gat = GeneralizationAcrossTime(picks='foo') 
   assert_equal('<GAT   |   no   fit,   no   prediction,   no   score>', ('%s' % gat)) 
   assert_raises(ValueError, gat.fit, epochs) 
   with warnings.catch_warnings(record=True): 
      gat.picks = [0] 
      gat.fit(epochs) 
      gat.picks = None 
      gat.fit(epochs, y=epochs.events[:, 2]) 
      gat.fit(epochs, y=epochs.events[:, 2].tolist()) 
   assert_equal(len(gat.picks_), len(gat.ch_names), 1) 
   assert_equal('<GAT   |   fitted,   start   :   -0.200   (s),   stop   :   0.499   (s),   no   prediction,   no   score>', ('%s' % gat)) 
   assert_equal(gat.ch_names, epochs.ch_names) 
   gat = GeneralizationAcrossTime(predict_method='decision_function') 
   gat.fit(epochs) 
   assert_true((gat.cv_.__class__ == StratifiedKFold)) 
   gat.predict(epochs) 
   assert_array_equal(np.shape(gat.y_pred_), (15, 15, 14, 1)) 
   gat.predict_method = 'predict_proba' 
   gat.predict(epochs) 
   assert_array_equal(np.shape(gat.y_pred_), (15, 15, 14, 2)) 
   gat.predict_method = 'foo' 
   assert_raises(NotImplementedError, gat.predict, epochs) 
   gat.predict_method = 'predict' 
   gat.predict(epochs) 
   assert_array_equal(np.shape(gat.y_pred_), (15, 15, 14, 1)) 
   assert_equal('<GAT   |   fitted,   start   :   -0.200   (s),   stop   :   0.499   (s),   predicted   14   epochs,   no   score>', ('%s' % gat)) 
   gat.score(epochs) 
   assert_true((gat.scorer_.__name__ == 'accuracy_score')) 
   gat.scorer = None 
   gat.predict_method = 'decision_function' 
   assert_raises(ValueError, gat.score, epochs) 
   gat.predict_method = 'predict' 
   gat.score(epochs, y=epochs.events[:, 2]) 
   gat.score(epochs, y=epochs.events[:, 2].tolist()) 
   assert_equal('<GAT   |   fitted,   start   :   -0.200   (s),   stop   :   0.499   (s),   predicted   14   epochs,\n   scored   (accuracy_score)>', ('%s' % gat)) 
   with warnings.catch_warnings(record=True): 
      gat.fit(epochs, y=epochs.events[:, 2]) 
   old_mode = gat.predict_mode 
   gat.predict_mode = 'super-foo-mode' 
   assert_raises(ValueError, gat.predict, epochs) 
   gat.predict_mode = old_mode 
   gat.score(epochs, y=epochs.events[:, 2]) 
   assert_true(('accuracy_score' in ('%s' % gat.scorer_))) 
   epochs2 = epochs.copy() 
   assert_equal('<DecodingTime   |   start:   -0.200   (s),   stop:   0.499   (s),   step:   0.050   (s),   length:   0.050   (s),   n_time_windows:   15>', ('%s' % gat.train_times_)) 
   assert_equal('<DecodingTime   |   start:   -0.200   (s),   stop:   0.499   (s),   step:   0.050   (s),   length:   0.050   (s),   n_time_windows:   15   x   15>', ('%s' % gat.test_times_)) 
   gat.predict_mode = 'mean-prediction' 
   epochs2.events[:, 2] += 10 
   gat_ = copy.deepcopy(gat) 
   with use_log_level('error'): 
      assert_raises(ValueError, gat_.score, epochs2) 
   gat.predict_mode = 'cross-validation' 
   assert_true((gat.y_train_.shape[0] == gat.y_true_.shape[0] == len(gat.y_pred_[0][0]) == 14)) 
   assert_true((np.shape(gat.estimators_)[1] == gat.cv)) 
   assert_true((len(gat.train_times_['slices']) == 15 == np.shape(gat.estimators_)[0])) 
   assert_true((len(gat.test_times_['slices']) == 15 == np.shape(gat.scores_)[0])) 
   assert_true((len(gat.test_times_['slices'][0]) == 15 == np.shape(gat.scores_)[1])) 
   gat.score_mode = 'foo' 
   assert_raises(ValueError, gat.score, epochs) 
   gat.score_mode = 'fold-wise' 
   scores = gat.score(epochs) 
   assert_array_equal(np.shape(scores), [15, 15, 5]) 
   gat.score_mode = 'mean-sample-wise' 
   scores = gat.score(epochs) 
   assert_array_equal(np.shape(scores), [15, 15]) 
   gat.score_mode = 'mean-fold-wise' 
   scores = gat.score(epochs) 
   assert_array_equal(np.shape(scores), [15, 15]) 
   gat.predict_mode = 'mean-prediction' 
   with warnings.catch_warnings(record=True) as w: 
      gat.score(epochs) 
      assert_true(any((('score_mode   changed   from   ' in str(ww.message)) for ww in w))) 
   gat = GeneralizationAcrossTime(train_times={'length': 0.1}) 
   with warnings.catch_warnings(record=True): 
      gat2 = gat.fit(epochs) 
   assert_true((gat is gat2)) 
   assert_true(hasattr(gat2, 'cv_')) 
   assert_true((gat2.cv_ != gat.cv)) 
   with warnings.catch_warnings(record=True): 
      scores = gat.score(epochs) 
   assert_true(isinstance(scores, np.ndarray)) 
   assert_equal(len(scores[0]), len(scores)) 
   assert_equal(len(gat.test_times_['slices'][0][0]), 2) 
   gat = GeneralizationAcrossTime(train_times={'step': 0.1}) 
   with warnings.catch_warnings(record=True): 
      gat.fit(epochs) 
   gat.score(epochs) 
   assert_true((len(gat.scores_) == len(gat.estimators_) == 8)) 
   assert_equal(len(gat.scores_[0]), 15) 
   y_4classes = np.hstack((epochs.events[:7, 2], (epochs.events[7:, 2] + 1))) 
   train_times = dict(start=0.09, stop=0.25) 
   gat = GeneralizationAcrossTime(cv=cv_lolo, train_times=train_times) 
   assert_raises(RuntimeError, gat.predict, epochs) 
   with warnings.catch_warnings(record=True): 
      gat.fit(epochs, y=y_4classes) 
   gat.score(epochs) 
   assert_equal(len(gat.scores_), 4) 
   assert_equal(gat.train_times_['times'][0], epochs.times[6]) 
   assert_equal(gat.train_times_['times'][(-1)], epochs.times[9]) 
   gat = GeneralizationAcrossTime(test_times='diagonal') 
   with warnings.catch_warnings(record=True): 
      gat.fit(epochs) 
   assert_raises(RuntimeError, gat.score) 
   with warnings.catch_warnings(record=True): 
      gat.predict(epochs) 
   scores = gat.score() 
   assert_true((scores is gat.scores_)) 
   assert_equal(np.shape(gat.scores_), (15, 1)) 
   assert_array_equal([tim for ttime in gat.test_times_['times'] for tim in ttime], gat.train_times_['times']) 
   gat = GeneralizationAcrossTime(predict_mode='mean-prediction', cv=2) 
   with warnings.catch_warnings(record=True): 
      gat.fit(epochs[0:6]) 
   with warnings.catch_warnings(record=True): 
      gat.predict(epochs[7:]) 
      gat.score(epochs[7:]) 
   gat_ = copy.deepcopy(gat) 
   gat_.train_times = dict(start=(-999.0)) 
   with use_log_level('error'): 
      assert_raises(ValueError, gat_.fit, epochs) 
   gat_.train_times = dict(start=999.0) 
   assert_raises(ValueError, gat_.fit, epochs) 
   gat_.train_times = dict(step=1e-06) 
   assert_raises(ValueError, gat_.fit, epochs) 
   gat_.train_times = dict(length=1e-06) 
   assert_raises(ValueError, gat_.fit, epochs) 
   gat_.train_times = dict(length=999.0) 
   assert_raises(ValueError, gat_.fit, epochs) 
   gat.test_times = dict(start=(-999.0)) 
   with warnings.catch_warnings(record=True): 
      assert_raises(ValueError, gat.predict, epochs) 
   gat.test_times = dict(start=999.0) 
   with warnings.catch_warnings(record=True): 
      assert_raises(ValueError, gat.predict, epochs) 
   gat.test_times = dict(step=1e-06) 
   with warnings.catch_warnings(record=True): 
      assert_raises(ValueError, gat.predict, epochs) 
   gat_ = copy.deepcopy(gat) 
   gat_.train_times_['length'] = 1e-06 
   gat_.test_times = dict(length=1e-06) 
   with warnings.catch_warnings(record=True): 
      assert_raises(ValueError, gat_.predict, epochs) 
   gat.test_times = dict(step=0.15) 
   with warnings.catch_warnings(record=True): 
      gat.predict(epochs) 
   assert_array_equal(np.shape(gat.y_pred_), (15, 5, 14, 1)) 
   gat.test_times = 'foo' 
   with warnings.catch_warnings(record=True): 
      assert_raises(ValueError, gat.predict, epochs) 
   assert_raises(RuntimeError, gat.score) 
   gat.test_times = dict(length=0.15) 
   assert_raises(ValueError, gat.predict, epochs) 
   train_times = dict(slices=[[0, 1], [1]]) 
   test_times = dict(slices=[[[0, 1]], [[0], [1]]]) 
   gat = GeneralizationAcrossTime(train_times=train_times, test_times=test_times) 
   gat.fit(epochs) 
   with warnings.catch_warnings(record=True): 
      gat.score(epochs) 
   assert_array_equal(np.shape(gat.y_pred_[0]), [1, len(epochs), 1]) 
   assert_array_equal(np.shape(gat.y_pred_[1]), [2, len(epochs), 1]) 
   gat.test_times = None 
   assert_raises(ValueError, gat.predict, epochs) 
   svc = SVC(C=1, kernel='linear', probability=True) 
   gat = GeneralizationAcrossTime(clf=svc, predict_mode='mean-prediction') 
   with warnings.catch_warnings(record=True): 
      gat.fit(epochs) 
   with use_log_level('error'): 
      assert_raises(ValueError, gat.score, epochs2) 
      gat.score(epochs) 
   assert_true((0.0 <= np.min(scores) <= 1.0)) 
   assert_true((0.0 <= np.max(scores) <= 1.0)) 
   gat = GeneralizationAcrossTime(cv=cv_shuffle, predict_mode='cross-validation') 
   gat.fit(epochs) 
   assert_raises(ValueError, gat.predict, epochs) 
   gat = GeneralizationAcrossTime(cv=cv_shuffle, predict_mode='mean-prediction') 
   gat.fit(epochs) 
   gat.predict(epochs) 
   gat = GeneralizationAcrossTime() 
   gat.fit(epochs) 
   with warnings.catch_warnings(record=True): 
      gat.fit(epochs) 
   gat.predict(epochs) 
   assert_raises(ValueError, gat.predict, epochs[:10]) 
   gat._cv_splits[0] = [gat._cv_splits[0][0], np.empty(0)] 
   with warnings.catch_warnings(record=True) as w: 
      gat.predict(epochs) 
      assert_true((len(w) > 0)) 
      assert_true(any((('do   not   have   any   test   epochs' in str(ww.message)) for ww in w))) 
   gat = GeneralizationAcrossTime(cv=[([0], [1]), ([], [0])]) 
   assert_raises(ValueError, gat.fit, epochs[:2]) 
   if check_version('sklearn', '0.17'): 
      gat = GeneralizationAcrossTime(clf=KernelRidge(), cv=2) 
      epochs.crop(None, epochs.times[2]) 
      gat.fit(epochs) 
      assert_true((gat.cv_.__class__ == KFold)) 
      gat.score(epochs) 
      assert_true((gat.scorer_.__name__ == 'mean_squared_error')) 
   n_classes = [2, 4] 
   le = LabelEncoder() 
   y = le.fit_transform(epochs.events[:, 2]) 
   y[(len(y) // 2):] += 2 
   ys = (y, (y + 1000)) 
   svc = SVC(C=1, kernel='linear', probability=True) 
   reg = KernelRidge() 
   def scorer_proba(y_true, y_pred): 
      return roc_auc_score(y_true, y_pred[:, 0]) 
   scorers = [None, scorer_proba, scorer_regress] 
   predict_methods = [None, 'predict_proba', None] 
   clfs = [svc, svc, reg] 
   for (clf, predict_method, scorer) in zip(clfs, predict_methods, scorers): 
      for y in ys: 
         for n_class in n_classes: 
            for predict_mode in ['cross-validation', 'mean-prediction']: 
               if ((predict_method == 'predict_proba') and (n_class != 2)): 
                  continue 
               y_ = (y % n_class) 
               with warnings.catch_warnings(record=True): 
                  gat = GeneralizationAcrossTime(cv=2, clf=clf, scorer=scorer, predict_mode=predict_mode) 
                  gat.fit(epochs, y=y_) 
                  gat.score(epochs, y=y_) 
               scorer_name = gat.scorer_.__name__ 
               if (scorer is None): 
                  if is_classifier(clf): 
                     assert_equal(scorer_name, 'accuracy_score') 
                  else: 
                     assert_equal(scorer_name, 'mean_squared_error') 
               else: 
                  assert_equal(scorer_name, scorer.__name__)",             Below          Below          Below       Below                                                                                        Below          Below               Below               Below      Below         Below            Below           Below                 Below                   Below            Below         Below                                                                                                           Below    Below  Below       Below          Below    Below    Below    Below                      Below     Below    Below Below                                                         Below      Below    Below    Below    Below            Below          Below      Below    Below     Below      Below              Bel   Bel              Below  ,'Test time generalization decoding'
"def _zpklp2hp(z, p, k, wo=1.0): 
    z = atleast_1d(z) 
   p = atleast_1d(p) 
   wo = float(wo) 
   degree = _relative_degree(z, p) 
   z_hp = (wo / z) 
   p_hp = (wo / p) 
   z_hp = append(z_hp, zeros(degree)) 
   k_hp = (k * real((prod((- z)) / prod((- p))))) 
   return (z_hp, p_hp, k_hp)"," 'Compute the hypergeometric polynomials of ``z``, ``p``, ``k``. 
 The hypergeometric polynomials are defined as 
 .. math:: 
 h_n(z) := \frac{1}{z} \sum_{k=0}^{n} \frac{k!}{n! (k-n)!} \frac{p^k}{z^k} 
 where ``z`` is a complex variable, ``p`` is a polynomial and ``k`` is a 
 nonnegative integer. 
 Parameters 
 z : 1D array 
 The complex variable ``z``. 
 p : 1D array 
 The polynomial ``p``. 
 k : 1D array 
 The nonnegative integer ``k``. 
 wo : 1D array 
 The weight ``wo``. 
 Returns 
 z_hp : 1D array 
 The hypergeometric polynomials ``h_n(z)``. 
 p_hp : 1D array 
 The hypergeometric polynomials ``h_n(p)``. 
 k_hp : 1D array 
 The hypergeometric polynomials","'Transform a lowpass filter prototype to a highpass filter. 
 Return an analog high-pass filter with cutoff frequency `wo` 
 from an analog low-pass filter prototype with unity cutoff frequency, 
 using zeros, poles, and gain (\'zpk\') representation. 
 Parameters 
 z : array_like 
 Zeros of the analog IIR filter transfer function. 
 p : array_like 
 Poles of the analog IIR filter transfer function. 
 k : float 
 System gain of the analog IIR filter transfer function. 
 wo : float 
 Desired cutoff, as angular frequency (e.g. rad/s). 
 Defaults to no change. 
 Returns 
 z : ndarray 
 Zeros of the transformed high-pass filter transfer function. 
 p : ndarray 
 Poles of the transformed high-pass filter transfer function. 
 k : float 
 System gain of the transformed high-pass filter. 
 Notes 
 This is derived from the s-plane substitution 
 .. math:: s \rightarrow \frac{\omega_0}{s} 
 This maintains symmetry of the lowpass and highpass responses on a 
 logarithmic scale.'"
"def _getlabel(object_alias): 
    finder = _getfinder() 
   args = {} 
   attrs = {} 
   aeobj_00 = aetypes.ObjectSpecifier(want=aetypes.Type('cobj'), form='alis', seld=object_alias, fr=None) 
   aeobj_01 = aetypes.ObjectSpecifier(want=aetypes.Type('prop'), form='prop', seld=aetypes.Type('labi'), fr=aeobj_00) 
   args['----'] = aeobj_01 
   (_reply, args, attrs) = finder.send('core', 'getd', args, attrs) 
   if args.has_key('errn'): 
      raise Error, aetools.decodeerror(args) 
   if args.has_key('----'): 
      return args['----']"," 'Returns the label of an object 
 @param object_alias: the alias of the object 
 @return: the label of the object'",'label: Get the label for the object.'
"def snipmate_files_for(ft): 
    if (ft == 'all'): 
      ft = '_' 
   patterns = [('%s.snippets' % ft), os.path.join(ft, '*.snippets'), os.path.join(ft, '*.snippet'), os.path.join(ft, '*/*.snippet')] 
   ret = set() 
   for rtp in _vim.eval('&runtimepath').split(','): 
      path = os.path.realpath(os.path.expanduser(os.path.join(rtp, 'snippets'))) 
      for pattern in patterns: 
         for fn in glob.glob(os.path.join(path, pattern)): 
            ret.add(fn) 
   return ret"," 'Returns a list of files that match the given file type (e.g. \'all\', \'python\', \'vim\', etc).'",'Returns all snipMate files we need to look at for \'ft\'.'
"def dbsize(host=None, port=None, db=None, password=None): 
    server = _connect(host, port, db, password) 
   return server.dbsize()", 'Returns the size of the database in bytes.',"'Return the number of keys in the selected database 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' redis.dbsize'"
"def sign_entity_descriptor(edesc, ident, secc): 
    if (not ident): 
      ident = sid() 
   edesc.signature = pre_signature_part(ident, secc.my_cert, 1) 
   edesc.id = ident 
   xmldoc = secc.sign_statement(('%s' % edesc), class_name(edesc)) 
   edesc = md.entity_descriptor_from_string(xmldoc) 
   return (edesc, xmldoc)"," 'Generate an entity descriptor and its signature. 
 :param edesc: entity descriptor to be signed 
 :type edesc: L{EntityDescriptor} 
 :param ident: entity identifier 
 :type ident: str 
 :param secc: security context 
 :type secc: L{SecurityContext} 
 :return: entity descriptor and its signature 
 :rtype: (L{EntityDescriptor}, str)'","':param edesc: EntityDescriptor instance 
 :param ident: EntityDescriptor identifier 
 :param secc: Security context 
 :return: Tuple with EntityDescriptor instance and Signed XML document'"
"def multi_future(children, quiet_exceptions=()): 
    if isinstance(children, dict): 
      keys = list(children.keys()) 
      children = children.values() 
   else: 
      keys = None 
   children = list(map(convert_yielded, children)) 
   assert all((is_future(i) for i in children)) 
   unfinished_children = set(children) 
   future = Future() 
   if (not children): 
      future.set_result(({} if (keys is not None) else [])) 
   def callback(f): 
      unfinished_children.remove(f) 
      if (not unfinished_children): 
         result_list = [] 
         for f in children: 
            try: 
               result_list.append(f.result()) 
            except Exception as e: 
               if future.done(): 
                  if (not isinstance(e, quiet_exceptions)): 
                     app_log.error('Multiple   exceptions   in   yield   list', exc_info=True) 
               else: 
                  future.set_exc_info(sys.exc_info()) 
         if (not future.done()): 
            if (keys is not None): 
               future.set_result(dict(zip(keys, result_list))) 
            else: 
               future.set_result(result_list) 
   listening = set() 
   for f in children: 
      if (f not in listening): 
         listening.add(f) 
         f.add_done_callback(callback) 
   return future"," 'Creates a future that will wait for all children to complete. 
 If the children are not all futures, the future will be set to the 
 result of the last future to complete. 
 :param children: A list of futures or a dict of futures. 
 :param quiet_exceptions: A list of exceptions to ignore. 
 :return: A future.'","'Wait for multiple asynchronous futures in parallel. 
 This function is similar to `multi`, but does not support 
 `YieldPoints <YieldPoint>`. 
 .. versionadded:: 4.0 
 .. versionchanged:: 4.2 
 If multiple ``Futures`` fail, any exceptions after the first (which is 
 raised) will be logged. Added the ``quiet_exceptions`` 
 argument to suppress this logging for selected exception types. 
 .. deprecated:: 4.3 
 Use `multi` instead.'"
"def serve(request, path, document_root=None, insecure=False, **kwargs): 
    if ((not settings.DEBUG) and (not insecure)): 
      raise ImproperlyConfigured(""The   staticfiles   view   can   only   be   used   in   debug   mode   or   if   the   the   --insecure   option   of   'runserver'   is   used"") 
   normalized_path = posixpath.normpath(urllib.unquote(path)).lstrip('/') 
   absolute_path = finders.find(normalized_path) 
   if (not absolute_path): 
      if (path.endswith('/') or (path == '')): 
         raise Http404('Directory   indexes   are   not   allowed   here.') 
      raise Http404((""'%s'   could   not   be   found"" % path)) 
   (document_root, path) = os.path.split(absolute_path) 
   return static.serve(request, path, document_root=document_root, **kwargs)", 'Serve static files using the staticfiles view.',"'Serve static files below a given point in the directory structure or 
 from locations inferred from the staticfiles finders. 
 To use, put a URL pattern such as:: 
 (r\'^(?P<path>.*)$\', \'django.contrib.staticfiles.views.serve\') 
 in your URLconf. 
 It uses the django.views.static view to serve the found files.'"
"def ant(registry, xml_parent, data): 
    ant = XML.SubElement(xml_parent, 'hudson.tasks.Ant') 
   if (type(data) is str): 
      data = {'targets': data} 
   for (setting, value) in sorted(data.items()): 
      if (setting == 'targets'): 
         targets = XML.SubElement(ant, 'targets') 
         targets.text = value 
      if (setting == 'buildfile'): 
         buildfile = XML.SubElement(ant, 'buildFile') 
         buildfile.text = value 
      if (setting == 'properties'): 
         properties = data['properties'] 
         prop_string = '' 
         for (prop, val) in properties.items(): 
            prop_string += ('%s=%s\n' % (prop, val)) 
         prop_element = XML.SubElement(ant, 'properties') 
         prop_element.text = prop_string 
      if (setting == 'java-opts'): 
         javaopts = data['java-opts'] 
         jopt_string = '   '.join(javaopts) 
         jopt_element = XML.SubElement(ant, 'antOpts') 
         jopt_element.text = jopt_string 
   XML.SubElement(ant, 'antName').text = data.get('ant-name', 'default')"," 'Create an Ant task. 
 :param registry: The registry to register the task with. 
 :param xml_parent: The parent element to add the task to. 
 :param data: A dictionary of settings to apply to the task. 
 :returns: The created task.'","'yaml: ant 
 Execute an ant target. Requires the Jenkins :jenkins-wiki:`Ant Plugin 
 <Ant+Plugin>`. 
 To setup this builder you can either reference the list of targets 
 or use named parameters. Below is a description of both forms: 
 *1) Listing targets:* 
 After the ant directive, simply pass as argument a space separated list 
 of targets to build. 
 :Parameter: space separated list of Ant targets 
 Example to call two Ant targets: 
 .. literalinclude:: ../../tests/builders/fixtures/ant001.yaml 
 :language: yaml 
 The build file would be whatever the Jenkins Ant Plugin is set to use 
 per default (i.e build.xml in the workspace root). 
 *2) Using named parameters:* 
 :arg str targets: the space separated list of ANT targets. 
 :arg str buildfile: the path to the ANT build file. 
 :arg list properties: Passed to ant script using -Dkey=value (optional) 
 :arg str ant-name: the name of the ant installation, 
 (default \'default\') (optional) 
 :arg str java-opts: java options for ant, can have multiples, 
 must be in quotes (optional) 
 Example specifying the build file too and several targets: 
 .. literalinclude:: ../../tests/builders/fixtures/ant002.yaml 
 :language: yaml'"
"def _arcball(x, y, w, h): 
    r = ((w + h) / 2.0) 
   (x, y) = (((- ((2.0 * x) - w)) / r), ((- ((2.0 * y) - h)) / r)) 
   h = np.sqrt(((x * x) + (y * y))) 
   return ((0.0, (x / h), (y / h), 0.0) if (h > 1.0) else (0.0, x, y, np.sqrt((1.0 - (h * h)))))"," 'Calculate the arcball transform for the given window. 
 Parameters 
 x : float 
 x-coordinate of the center of the window 
 y : float 
 y-coordinate of the center of the window 
 w : float 
 width of the window 
 h : float 
 height of the window 
 Returns 
 transform : ndarray 
 4x4 transformation matrix 
 Examples 
 >>> from skimage import transform 
 >>> from skimage.transform import _arcball 
 >>> transform.warp(np.ones((5, 5)), _arcball(0, 0, 5, 5)) 
 array([[ 1.        ,  0.        ,  0.        ,  0.        ], 
 [ 0.        ,  1.        ,  0.        ,  0.        ], 
 [ 0.        ,  0.        ,  1.        ,  0.        ], 
 [ 0.        ,  0.        ,  0.        ,  1.        ], 
 [ 0.        ,  0.        ,  0.        ,","'Convert x,y coordinates to w,x,y,z Quaternion parameters 
 Adapted from: 
 linalg library 
 Copyright (c) 2010-2015, Renaud Blanch <rndblnch at gmail dot com> 
 Licence at your convenience: 
 GPLv3 or higher <http://www.gnu.org/licenses/gpl.html> 
 BSD new <http://opensource.org/licenses/BSD-3-Clause>'"
"def test_coordinate_vars(): 
    A = CoordSysCartesian('A') 
   assert (BaseScalar('A.x', 0, A, 'A_x', '\\mathbf{{x}_{A}}') == A.x) 
   assert (BaseScalar('A.y', 1, A, 'A_y', '\\mathbf{{y}_{A}}') == A.y) 
   assert (BaseScalar('A.z', 2, A, 'A_z', '\\mathbf{{z}_{A}}') == A.z) 
   assert (BaseScalar('A.x', 0, A, 'A_x', '\\mathbf{{x}_{A}}').__hash__() == A.x.__hash__()) 
   assert (isinstance(A.x, BaseScalar) and isinstance(A.y, BaseScalar) and isinstance(A.z, BaseScalar)) 
   assert ((A.x * A.y) == (A.y * A.x)) 
   assert (A.scalar_map(A) == {A.x: A.x, A.y: A.y, A.z: A.z}) 
   assert (A.x.system == A) 
   assert (A.x.diff(A.x) == 1) 
   B = A.orient_new_axis('B', q, A.k) 
   assert (B.scalar_map(A) == {B.z: A.z, B.y: (((- A.x) * sin(q)) + (A.y * cos(q))), B.x: ((A.x * cos(q)) + (A.y * sin(q)))}) 
   assert (A.scalar_map(B) == {A.x: ((B.x * cos(q)) - (B.y * sin(q))), A.y: ((B.x * sin(q)) + (B.y * cos(q))), A.z: B.z}) 
   assert (express(B.x, A, variables=True) == ((A.x * cos(q)) + (A.y * sin(q)))) 
   assert (express(B.y, A, variables=True) == (((- A.x) * sin(q)) + (A.y * cos(q)))) 
   assert (express(B.z, A, variables=True) == A.z) 
   assert (expand(express(((B.x * B.y) * B.z), A, variables=True)) == expand(((A.z * (((- A.x) * sin(q)) + (A.y * cos(q)))) * ((A.x * cos(q)) + (A.y * sin(q)))))) 
   assert (express((((B.x * B.i) + (B.y * B.j)) + (B.z * B.k)), A) == (((((B.x * cos(q)) - (B.y * sin(q))) * A.i) + (((B.x * sin(q)) + (B.y * cos(q))) * A.j)) + (B.z * A.k))) 
   assert (simplify(express((((B.x * B.i) + (B.y * B.j)) + (B.z * B.k)), A, variables=True)) == (((A.x * A.i) + (A.y * A.j)) + (A.z * A.k))) 
   assert (express((((A.x * A.i) + (A.y * A.j)) + (A.z * A.k)), B) == (((((A.x * cos(q)) + (A.y * sin(q))) * B.i) + ((((- A.x) * sin(q)) + (A.y * cos(q))) * B.j)) + (A.z * B.k))) 
   assert (simplify(express((((A.x * A.i) + (A.y * A.j)) + (A.z * A.k)), B, variables=True)) == (((B.x * B.i) + (B.y * B.j)) + (B.z * B.k))) 
   N = B.orient_new_axis('N', (- q), B.k) 
   assert (N.scalar_map(A) == {N.x: A.x, N.z: A.z, N.y: A.y}) 
   C = A.orient_new_axis('C', q, ((A.i + A.j) + A.k)) 
   mapping = A.scalar_map(C) 
   assert (mapping[A.x] == ((((C.x * ((2 * cos(q)) + 1)) / 3) + ((C.y * (((-2) * sin((q + (pi / 6)))) + 1)) / 3)) + ((C.z * (((-2) * cos((q + (pi / 3)))) + 1)) / 3))) 
   assert (mapping[A.y] == ((((C.x * (((-2) * cos((q + (pi / 3)))) + 1)) / 3) + ((C.y * ((2 * cos(q)) + 1)) / 3)) + ((C.z * (((-2) * sin((q + (pi / 6)))) + 1)) / 3))) 
   assert (mapping[A.z] == ((((C.x * (((-2) * sin((q + (pi / 6)))) + 1)) / 3) + ((C.y * (((-2) * cos((q + (pi / 3)))) + 1)) / 3)) + ((C.z * ((2 * cos(q)) + 1)) / 3))) 
   D = A.locate_new('D', (((a * A.i) + (b * A.j)) + (c * A.k))) 
   assert (D.scalar_map(A) == {D.z: (A.z - c), D.x: (A.x - a), D.y: (A.y - b)}) 
   E = A.orient_new_axis('E', a, A.k, (((a * A.i) + (b * A.j)) + (c * A.k))) 
   assert (A.scalar_map(E) == {A.z: (E.z + c), A.x: (((E.x * cos(a)) - (E.y * sin(a))) + a), A.y: (((E.x * sin(a)) + (E.y * cos(a))) + b)}) 
   assert (E.scalar_map(A) == {E.x: (((A.x - a) * cos(a)) + ((A.y - b) * sin(a))), E.y: ((((- A.x) + a) * sin(a)) + ((A.y - b) * cos(a))), E.z: (A.z - c)}) 
   F = A.locate_new('F', Vector.zero) 
   assert (A.scalar_map(F) == {A.z: F.z, A.x: F.x, A.y: F.y})", 'Test coordinate variables',"'Tests the coordinate variables functionality with respect to 
 reorientation of coordinate systems.'"
"def CheckBraces(filename, clean_lines, linenum, error): 
    line = clean_lines.elided[linenum] 
   if Match('\\s*{\\s*$', line): 
      prevline = GetPreviousNonBlankLine(clean_lines, linenum)[0] 
      if ((not Search('[,;:}{(]\\s*$', prevline)) and (not Match('\\s*#', prevline)) and (not ((GetLineWidth(prevline) > (_line_length - 2)) and ('[]' in prevline)))): 
         error(filename, linenum, 'whitespace/braces', 4, '{   should   almost   always   be   at   the   end   of   the   previous   line') 
   if Match('\\s*else\\b\\s*(?:if\\b|\\{|$)', line): 
      prevline = GetPreviousNonBlankLine(clean_lines, linenum)[0] 
      if Match('\\s*}\\s*$', prevline): 
         error(filename, linenum, 'whitespace/newline', 4, 'An   else   should   appear   on   the   same   line   as   the   preceding   }') 
   if Search('else   if\\s*\\(', line): 
      brace_on_left = bool(Search('}\\s*else   if\\s*\\(', line)) 
      pos = line.find('else   if') 
      pos = line.find('(', pos) 
      if (pos > 0): 
         (endline, _, endpos) = CloseExpression(clean_lines, linenum, pos) 
         brace_on_right = (endline[endpos:].find('{') != (-1)) 
         if (brace_on_left != brace_on_right): 
            error(filename, linenum, 'readability/braces', 5, 'If   an   else   has   a   brace   on   one   side,   it   should   have   it   on   both') 
   elif (Search('}\\s*else[^{]*$', line) or Match('[^}]*else\\s*{', line)): 
      error(filename, linenum, 'readability/braces', 5, 'If   an   else   has   a   brace   on   one   side,   it   should   have   it   on   both') 
   if (Search('\\belse   [^\\s{]', line) and (not Search('\\belse   if\\b', line))): 
      error(filename, linenum, 'whitespace/newline', 4, 'Else   clause   should   never   be   on   same   line   as   else   (use   2   lines)') 
   if Match('\\s*do   [^\\s{]', line): 
      error(filename, linenum, 'whitespace/newline', 4, 'do/while   clauses   should   not   be   on   a   single   line') 
   if_else_match = Search('\\b(if\\s*\\(|else\\b)', line) 
   if (if_else_match and (not Match('\\s*#', line))): 
      if_indent = GetIndentLevel(line) 
      (endline, endlinenum, endpos) = (line, linenum, if_else_match.end()) 
      if_match = Search('\\bif\\s*\\(', line) 
      if if_match: 
         pos = (if_match.end() - 1) 
         (endline, endlinenum, endpos) = CloseExpression(clean_lines, linenum, pos) 
      if ((not Match('\\s*{', endline[endpos:])) and (not (Match('\\s*$', endline[endpos:]) and (endlinenum < (len(clean_lines.elided) - 1)) and Match('\\s*{', clean_lines.elided[(endlinenum + 1)])))): 
         while ((endlinenum < len(clean_lines.elided)) and (';' not in clean_lines.elided[endlinenum][endpos:])): 
            endlinenum += 1 
            endpos = 0 
         if (endlinenum < len(clean_lines.elided)): 
            endline = clean_lines.elided[endlinenum] 
            endpos = endline.find(';') 
            if (not Match(';[\\s}]*(\\\\?)$', endline[endpos:])): 
               if (not Match('^[^{};]*\\[[^\\[\\]]*\\][^{}]*\\{[^{}]*\\}\\s*\\)*[;,]\\s*$', endline)): 
                  error(filename, linenum, 'readability/braces', 4, 'If/else   bodies   with   multiple   statements   require   braces') 
            elif (endlinenum < (len(clean_lines.elided) - 1)): 
               next_line = clean_lines.elided[(endlinenum + 1)] 
               next_indent = GetIndentLevel(next_line) 
               if (if_match and Match('\\s*else\\b', next_line) and (next_indent != if_indent)): 
                  error(filename, linenum, 'readability/braces', 4, 'Else   clause   should   be   indented   at   the   same   level   as   if.   Ambiguous   nested   if/else   chains   require   braces.') 
               elif (next_indent > if_indent): 
                  error(filename, linenum, 'readability/braces', 4, 'If/else   bodies   with   multiple   statements   require   braces')"," 'Checks for braces and brace-like structures. 
 See the readability/braces rule for more details.'","'Looks for misplaced braces (e.g. at the end of line). 
 Args: 
 filename: The name of the current file. 
 clean_lines: A CleansedLines instance containing the file. 
 linenum: The number of the line to check. 
 error: The function to call with any errors found.'"
"def full_restart(name): 
    restart(name)"," 'Restart a container with the given name. 
 :param name: Name of the container to restart. 
 :type name: str 
 :returns: None'","'Calls s6.restart() function 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' s6.full_restart <service name>'"
"def validate_auth_option(option, value): 
    (lower, value) = validate(option, value) 
   if (lower not in _AUTH_OPTIONS): 
      raise ConfigurationError(('Unknown   authentication   option:   %s' % (option,))) 
   return (lower, value)"," 'Validate the given authentication option and value. 
 This function validates the given option and value, and returns 
 the lowercase option and the value as a tuple. 
 :param option: the option to validate 
 :param value: the value to validate 
 :raises ConfigurationError: if the option is unknown'",'Validate optional authentication parameters.'
"def regen(it): 
    if isinstance(it, (list, tuple)): 
      return it 
   return _regen(it)"," 'Regenerate the iterator if it is a generator. 
 If the iterator is a generator, regenerate it. 
 This is done so that we can use iterators as arguments to 
 functions that do not support iterators. 
 :param it: 
 :type it: 
 :return: 
 :rtype:'","'Convert iterator to an object that can be consumed multiple times. 
 ``Regen`` takes any iterable, and if the object is an 
 generator it will cache the evaluated list on first access, 
 so that the generator can be ""consumed"" multiple times.'"
"@constructor 
 def round(a, mode='half_away_from_zero'): 
    if (mode == 'half_away_from_zero'): 
      return round_half_away_from_zero(a) 
   elif (mode == 'half_to_even'): 
      return round_half_to_even(a) 
   else: 
      raise Exception(('round   mode   %s   is   not   implemented.' % mode))"," 'Round a number to a specified mode. 
 Parameters 
 a : number 
 The number to round. 
 mode : str 
 The mode to round to. 
 Returns 
 round : number 
 The rounded number.'","'round_mode(a) with mode in [half_away_from_zero, half_to_even]'"
"def is_arity(n, func, sigspec=None): 
    (sigspec, rv) = _check_sigspec(sigspec, func, _sigs._is_arity, n, func) 
   if (sigspec is None): 
      return rv 
   num = num_required_args(func, sigspec) 
   if (num is not None): 
      num = (num == n) 
      if (not num): 
         return False 
   varargs = has_varargs(func, sigspec) 
   if varargs: 
      return False 
   keywords = has_keywords(func, sigspec) 
   if keywords: 
      return False 
   if ((num is None) or (varargs is None) or (keywords is None)): 
      return None 
   return True", 'Returns True if the function has the correct arity.',"'Does a function have only n positional arguments? 
 This function relies on introspection and does not call the function. 
 Returns None if validity can\'t be determined. 
 >>> def f(x): 
 ...     return x 
 >>> is_arity(1, f) 
 True 
 >>> def g(x, y=1): 
 ...     return x + y 
 >>> is_arity(1, g) 
 False'"
"def UINT(value): 
    if (value is None): 
      raise ValueError(u'None   is   not   a   valid   integer') 
   if (not value.isdigit()): 
      raise ValueError(u'Only   positive   numbers   are   allowed') 
   return int(value)"," 'Convert a string into a positive integer. 
 :param value: A string to convert into a positive integer. 
 :type value: str 
 :return: A positive integer. 
 :rtype: int'",'Converts a value that matches \d+ into an integer.'
"def _json_plays(drive, data): 
    plays = [] 
   seen_ids = set() 
   seen_desc = set() 
   for playid in map(str, sorted(map(int, data))): 
      p = data[playid] 
      desc = (p['desc'], p['time'], p['yrdln'], p['qtr']) 
      if ((playid in seen_ids) or (desc in seen_desc)): 
         continue 
      seen_ids.add(playid) 
      seen_desc.add(desc) 
      plays.append(Play(drive, playid, data[playid])) 
   return plays", 'Returns a list of plays from a json file',"'Takes a single JSON drive entry (data) and converts it to a list 
 of Play objects. This includes trying to resolve duplicate play 
 conflicts by only taking the first instance of a play.'"
"def add_trailing_slash(path): 
    if ((len(path) > 0) and (path[(-1)] == os.sep)): 
      return path 
   else: 
      return (path + os.sep)"," 'Add a trailing slash to the given path. 
 :param path: path to append a trailing slash to 
 :type path: str 
 :return: path with a trailing slash added 
 :rtype: str'","'If path does not end with /, add it and return.'"
"@api_versions.wraps('2.17') 
 @utils.arg('server', metavar='<server>', help=_('Name   or   ID   of   server.')) 
 def do_trigger_crash_dump(cs, args): 
    server = _find_server(cs, args.server) 
   server.trigger_crash_dump()", 'Trigger a crash dump on a server.','Trigger crash dump in an instance.'
"def libvlc_video_set_spu(p_mi, i_spu): 
    f = (_Cfunctions.get('libvlc_video_set_spu', None) or _Cfunction('libvlc_video_set_spu', ((1,), (1,)), None, ctypes.c_int, MediaPlayer, ctypes.c_int)) 
   return f(p_mi, i_spu)"," 'Set the SPU track. 
 @param p_mi: the Media Player. 
 @param i_spu: the SPU track. 
 @return the error code (0 if no error).'","'Set new video subtitle. 
 @param p_mi: the media player. 
 @param i_spu: video subtitle track to select (i_id from track description). 
 @return: 0 on success, -1 if out of range.'"
"def gf_eval(f, a, p, K): 
    result = K.zero 
   for c in f: 
      result *= a 
      result += c 
      result %= p 
   return result"," 'Evaluate a polynomial in ``GF(p)[x]``. 
 Examples 
 >>> from sympy.polys.galoistools import gf_eval 
 >>> gf_eval([x**2 + 1, x**3 + 1, x**2 + 1, x**2 + 1], 3, 5, 
 ... ZZ(7)) 
 [0, 0, 0, 0]'","'Evaluate ``f(a)`` in ``GF(p)`` using Horner scheme. 
 Examples 
 >>> from sympy.polys.domains import ZZ 
 >>> from sympy.polys.galoistools import gf_eval 
 >>> gf_eval([3, 2, 4], 2, 5, ZZ) 
 0'"
"def available(name): 
    return (name in _get_svc_list(name))"," 'Returns True if the service is available. 
 :param name: service name 
 :type name: str 
 :return: True if service is available 
 :rtype: bool'","'Returns ``True`` if the specified service is available, otherwise returns 
 ``False``. 
 name 
 the service\'s name 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' runit.available <service name>'"
"def TestH2a(sequenceLength, nTests, cellsPerColumn, numCols=100, nSequences=[2], pctShared=0.02, seqGenMode='shared   sequence', shouldFail=False): 
    print 'Test   H2a   -   second   repetition   of   the   same   sequence   should   not   add   synapses' 
   nFailed = 0 
   subsequenceStartPos = 10 
   assert (subsequenceStartPos < sequenceLength) 
   for numSequences in nSequences: 
      print 'Higher   order   test   with   sequenceLength=', sequenceLength, 
      print 'cellsPerColumn=', cellsPerColumn, 'nTests=', nTests, 'numCols=', numCols 
      print 'numSequences=', numSequences, 'pctShared=', pctShared, 
      print 'sharing   mode=', seqGenMode 
      for _ in range(nTests): 
         trainingSet = buildTrainingSet(numSequences=numSequences, sequenceLength=sequenceLength, pctShared=pctShared, seqGenMode=seqGenMode, subsequenceStartPos=subsequenceStartPos, numCols=numCols, minOnes=21, maxOnes=25) 
         print '==============   10   ======================' 
         (numFailures3, numStrictErrors3, numPerfect3, tp3) = testSequence(trainingSet, nTrainingReps=10, numberOfCols=numCols, cellsPerColumn=cellsPerColumn, initialPerm=0.4, connectedPerm=0.7, minThreshold=12, permanenceInc=0.1, permanenceDec=0.1, permanenceMax=1, globalDecay=0.0, newSynapseCount=15, activationThreshold=12, doPooling=False, shouldFail=shouldFail) 
         print '==============   2   ======================' 
         (numFailures, numStrictErrors, numPerfect, tp2) = testSequence(trainingSet, nTrainingReps=2, numberOfCols=numCols, cellsPerColumn=cellsPerColumn, initialPerm=0.8, connectedPerm=0.7, minThreshold=12, permanenceInc=0.1, permanenceDec=0, permanenceMax=1, globalDecay=0.0, newSynapseCount=15, activationThreshold=12, doPooling=False, shouldFail=shouldFail) 
         print '==============   1   ======================' 
         (numFailures1, numStrictErrors1, numPerfect1, tp1) = testSequence(trainingSet, nTrainingReps=1, numberOfCols=numCols, cellsPerColumn=cellsPerColumn, initialPerm=0.8, connectedPerm=0.7, minThreshold=12, permanenceInc=0.1, permanenceDec=0, permanenceMax=1, globalDecay=0.0, newSynapseCount=15, activationThreshold=12, doPooling=False, shouldFail=shouldFail) 
         segmentInfo1 = tp1.getSegmentInfo() 
         segmentInfo2 = tp2.getSegmentInfo() 
         if ((abs((segmentInfo1[0] - segmentInfo2[0])) > 3) or (abs((segmentInfo1[1] - segmentInfo2[1])) > (3 * 15))): 
            print 'Training   twice   incorrectly   resulted   in   too   many   segments   or   synapses' 
            print segmentInfo1 
            print segmentInfo2 
            print tp3.getSegmentInfo() 
            tp3.trimSegments() 
            print tp3.getSegmentInfo() 
            print 'Failures   for   1,   2,   and   N   reps' 
            print numFailures1, numStrictErrors1, numPerfect1 
            print numFailures, numStrictErrors, numPerfect 
            print numFailures3, numStrictErrors3, numPerfect3 
            numFailures += 1 
         if (((numFailures == 0) and (not shouldFail)) or ((numFailures > 0) and shouldFail)): 
            print 'Test   PASS', 
            if shouldFail: 
               print '(should   fail,   and   failed)' 
            else: 
               print 
         else: 
            print 'Test   FAILED' 
            nFailed = (nFailed + 1) 
            print 'numFailures=', numFailures 
            print 'numStrictErrors=', numStrictErrors 
            print 'numPerfect=', numPerfect 
   return nFailed"," 'Test that the second repetition of the same sequence does not add synapses 
 Parameters 
 sequenceLength : int 
 number of neurons in the sequence 
 nTests : int 
 number of tests to run 
 cellsPerColumn : int 
 number of neurons per column 
 numCols : int 
 number of columns 
 nSequences : list 
 list of sequences 
 pctShared : float 
 percentage of shared neurons 
 seqGenMode : string 
 ""shared   sequence"" or ""shared   neurons"" 
 shouldFail : boolean 
 if True, then the test will fail if the number of synapses is not 0 
 Returns 
 nFailed : int 
 number of tests that failed'","'Still need to test: 
 Two overlapping sequences. OK to get new segments but check that we can 
 get correct high order prediction after multiple reps.'"
"@block_user_agents 
 @require_GET 
 def top_level(request): 
    docs = Document.objects.filter_for_list(locale=request.LANGUAGE_CODE, toplevel=True) 
   paginated_docs = paginate(request, docs, per_page=DOCUMENTS_PER_PAGE) 
   context = {'documents': paginated_docs, 'count': docs.count(), 'toplevel': True} 
   return render(request, 'wiki/list/documents.html', context)", 'Displays a list of all top-level documents.','Lists documents directly under /docs/'
"def find_native_user_instrumentation_hook(cls): 
    return getattr(cls, INSTRUMENTATION_MANAGER, None)", 'Returns the native user instrumentation hook for the given class.','Find user-specified instrumentation management for a class.'
"def addFacesByConcaveLoop(faces, indexedLoop): 
    if (len(indexedLoop) < 3): 
      return 
   remainingLoop = indexedLoop[:] 
   while (len(remainingLoop) > 2): 
      remainingLoop = getRemainingLoopAddFace(faces, remainingLoop)"," 'Adds faces to the list of faces by concave loop. 
 Parameters 
 faces : list of faces 
 indexedLoop : list of indices 
 Returns 
 faces : list of faces 
 Examples 
 >>> faces = [] 
 >>> faces.append(Face(0, 0, 0)) 
 >>> faces.append(Face(0, 1, 0)) 
 >>> faces.append(Face(0, 2, 0)) 
 >>> faces.append(Face(1, 0, 0)) 
 >>> faces.append(Face(1, 1, 0)) 
 >>> faces.append(Face(1, 2, 0)) 
 >>> faces.append(Face(2, 0, 0)) 
 >>> faces.append(Face(2, 1, 0)) 
 >>> faces.append(Face(2, 2, 0)) 
 >>> faces.append(Face(0, 0, 1)) 
 >>> faces.append(Face(0, 1, 1)) 
 >>> faces.append(Face(0, 2, ",'Add faces from a polygon which is concave.'
"def init(mpstate): 
    return SerialModule(mpstate)", 'Initialize the serial module.','initialise module'
"def cache_project(cls): 
    projects = dict() 
   resources = dict() 
   def get_ctx(*args, **kwargs): 
      path = env.curbuf.name 
      if resources.get(path): 
         return resources.get(path) 
      project_path = env.var('g:pymode_rope_project_root') 
      if (not project_path): 
         project_path = env.curdir 
         env.debug('Look   ctx', project_path) 
         if env.var('g:pymode_rope_lookup_project', True): 
            project_path = look_ropeproject(project_path) 
      if (not os.path.exists(project_path)): 
         env.error(('Rope   project   root   not   exist:   %s' % project_path)) 
         ctx = None 
      else: 
         ctx = projects.get(project_path) 
      if (not ctx): 
         projects[project_path] = ctx = cls(path, project_path) 
      resources[path] = ctx 
      return ctx 
   return get_ctx", 'Caches the project class and its resources.',"'Cache projects. 
 :return func:'"
"@require_GET 
 def aggregated_metrics(request): 
    today = date.today() 
   locales = get_locales_by_visit((today - timedelta(days=30)), today) 
   product = _get_product(request) 
   return render(request, 'dashboards/aggregated_metrics.html', {'locales_json': json.dumps(settings.SUMO_LANGUAGES), 'locales': locales, 'product': product, 'products': Product.objects.filter(visible=True)})", 'Aggregated metrics for the last 30 days','The aggregated (all locales) kb metrics dashboard.'
"def RewriteResponse(response_file, response_rewriters=None, request_headers=None, env_dict=None): 
    if (response_rewriters is None): 
      response_rewriters = CreateResponseRewritersChain() 
   response = AppServerResponse(response_file) 
   for response_rewriter in response_rewriters: 
      if (response_rewriter.func_code.co_argcount == 1): 
         response_rewriter(response) 
      elif (response_rewriter.func_code.co_argcount == 2): 
         response_rewriter(response, request_headers) 
      else: 
         response_rewriter(response, request_headers, env_dict) 
   return response"," 'Rewrite the response file. 
 :param response_file: The file to rewrite. 
 :param response_rewriters: A list of response rewriters to use. 
 :param request_headers: A dictionary of request headers to use. 
 :param env_dict: A dictionary of environment variables to use. 
 :returns: The rewritten response. 
 :rtype: :class:`appserver.response.AppServerResponse`'","'Allows final rewrite of dev_appserver response. 
 This function receives the unparsed HTTP response from the application 
 or internal handler, parses out the basic structure and feeds that structure 
 in to a chain of response rewriters. 
 It also makes sure the final HTTP headers are properly terminated. 
 For more about response rewriters, please see documentation for 
 CreateResponeRewritersChain. 
 Args: 
 response_file: File-like object containing the full HTTP response including 
 the response code, all headers, and the request body. 
 response_rewriters: A list of response rewriters.  If none is provided it 
 will create a new chain using CreateResponseRewritersChain. 
 request_headers: Original request headers. 
 env_dict: Environment dictionary. 
 Returns: 
 An AppServerResponse instance configured with the rewritten response.'"
"def addBevelGear(derivation, extrudeDerivation, pitchRadius, positives, teeth, vector3GearProfile): 
    totalPitchRadius = (derivation.pitchRadiusComplement + derivation.pitchRadius) 
   totalTeeth = (derivation.teethPinion + derivation.teethComplement) 
   portionDirections = extrude.getSpacedPortionDirections(extrudeDerivation.interpolationDictionary) 
   loopLists = extrude.getLoopListsByPath(extrudeDerivation, None, vector3GearProfile[0], portionDirections) 
   firstLoopList = loopLists[0] 
   gearOverPinion = (float((totalTeeth - teeth)) / float(teeth)) 
   thirdLayerHeight = (0.33333333333 * setting.getLayerHeight(derivation.elementNode)) 
   pitchRadian = math.atan((math.sin(derivation.operatingRadian) / (gearOverPinion + math.cos(derivation.operatingRadian)))) 
   coneDistance = (pitchRadius / math.sin(pitchRadian)) 
   apex = Vector3(0.0, 0.0, math.sqrt(((coneDistance * coneDistance) - (pitchRadius * pitchRadius)))) 
   cosPitch = (apex.z / coneDistance) 
   sinPitch = math.sin(pitchRadian) 
   for loop in firstLoopList: 
      for point in loop: 
         alongWay = (point.z / coneDistance) 
         oneMinusAlongWay = (1.0 - alongWay) 
         pointComplex = point.dropAxis() 
         pointComplexLength = abs(pointComplex) 
         deltaRadius = (pointComplexLength - pitchRadius) 
         cosDeltaRadius = (cosPitch * deltaRadius) 
         sinDeltaRadius = (sinPitch * deltaRadius) 
         pointComplex *= ((cosDeltaRadius + pitchRadius) / pointComplexLength) 
         point.x = pointComplex.real 
         point.y = pointComplex.imag 
         point.z += sinDeltaRadius 
         point.x *= oneMinusAlongWay 
         point.y *= oneMinusAlongWay 
   addBottomLoop((- thirdLayerHeight), firstLoopList) 
   topLoop = firstLoopList[(-1)] 
   topAddition = [] 
   topZ = (euclidean.getTopPath(topLoop) + thirdLayerHeight) 
   oldIndex = topLoop[(-1)].index 
   for point in topLoop: 
      oldIndex += 1 
      topAddition.append(Vector3Index(oldIndex, (0.8 * point.x), (0.8 * point.y), topZ)) 
   firstLoopList.append(topAddition) 
   translation = Vector3(0.0, 0.0, (- euclidean.getBottomByPaths(firstLoopList))) 
   euclidean.translateVector3Paths(firstLoopList, translation) 
   geometryOutput = triangle_mesh.getPillarsOutput(loopLists) 
   positives.append(geometryOutput)"," 'Adds a bevel gear. 
 Parameters 
 derivation : Derivation 
 extrudeDerivation : ExtrudeDerivation 
 pitchRadius : float 
 teeth : int 
 positives : list 
 vector3GearProfile : Vector3 
 Returns 
 positives : list 
 Examples 
 >>> from sympy.geometry.euclidean import Derivation, ExtrudeDerivation, Vector3, Vector3Index, euclidean 
 >>> from sympy.geometry.geometric import addBevelGear 
 >>> from sympy.geometry.primitives import Point, Line, Plane, Circle, Segment, Angle, Vector3 
 >>> from sympy.geometry.transformations import Translation, Rotation 
 >>> from sympy.geometry.primitives import Point, Line, Plane, Circle, Segment, Angle, Vector3 
 >>> from sympy.geometry.transformations import Translation, Rotation 
 >>> from sympy.geometry.geometric import addBevelGear 
 >>> from sympy.geometry.transformations import Translation, Rotation 
 >>> from",'Get extrude output for a cylinder gear.'
"def t_LPAR(t): 
    t.lexer.paren_count += 1 
   return t", 'LPAR',''
"def cachefile(src, dst, api=None, logger=None): 
    lcache = os.path.join(os.path.dirname(os.path.dirname(dst)), '.link_cache') 
   if (not os.path.isdir(lcache)): 
      os.mkdir(lcache) 
   key = hashfile(src, lcache=lcache, logger=logger) 
   cachefile = os.path.join(lcache, key) 
   if (not os.path.exists(cachefile)): 
      logger.info(('trying   to   create   cache   file   %s' % cachefile)) 
      copyfile(src, cachefile, api=api, logger=logger) 
   logger.debug(('trying   cachelink   %s   ->   %s   ->   %s' % (src, cachefile, dst))) 
   os.link(cachefile, dst)"," 'Copy a file to another location, and create a link to it. 
 This function is used to copy files to the cache folder, and create a link to 
 the file. This is used to speed up the download of files.'","'Copy a file into a cache and link it into place. 
 Use this with caution, otherwise you could end up 
 copying data twice if the cache is not on the same device 
 as the destination'"
"def get_course_updates(location, provided_id, user_id): 
    try: 
      course_updates = modulestore().get_item(location) 
   except ItemNotFoundError: 
      course_updates = modulestore().create_item(user_id, location.course_key, location.block_type, location.block_id) 
   course_update_items = get_course_update_items(course_updates, _get_index(provided_id)) 
   return _get_visible_update(course_update_items)", 'Returns the most recent course update that is visible to the user.',"'Retrieve the relevant course_info updates and unpack into the model which the client expects: 
 [{id : index, date : string, content : html string}]'"
"def served_by_perl(url): 
    r = requests.get(url, allow_redirects=False) 
   status = (r.status_code == 302) 
   perl = (('x-perl-redirect' in r.headers) and (r.headers['x-perl-redirect'] == 'True')) 
   return all([status, perl])", 'Check if a URL is served by Perl.','Return True if url returns 200 and is served by Perl.'
"def func1(x): 
    return (((np.sin((x * 5)) / x) + (2.0 * x)) - (1.0 * (x ** 2)))", 'x^2 + 2x + sin(5x) + 1',"'made up example with sin, square'"
"def library_check_auth(content, renew=False): 
    user_library_info = get_user_library_info(openid) 
   if user_library_info: 
      cipher = AESCipher(app.config['PASSWORD_SECRET_KEY']) 
      librarypwd = cipher.decrypt(user_library_info['librarypwd']) 
      library.borrowing_record.delay(openid, user_library_info['libraryid'], librarypwd, renew=renew) 
      return wechat.response_text(content) 
   else: 
      url = ((app.config['HOST_URL'] + '/auth-library/') + openid) 
      content = (app.config['AUTH_LIBRARY_TEXT'] % url) 
      return wechat.response_text(content)"," 'check whether user\'s library is authorized 
 :param content: response content 
 :param renew: renew the library authorization 
 :return: response content'",''
"def b64c(b): 
    return string.translate(b, B64C_TRANSLATE, B64C_STRIP)"," 'Returns a Base64-encoded string containing ``b``. 
 This function is a Python 2-only version of the Base64 C algorithm. 
 The Base64 C algorithm is defined in RFC 3548, section 5. 
 >>> b64c(b""\x01\x02\x03\x04\x05\x06\x07\x08\x09\x0a\x0b\x0c\x0d\x0e\x0f"") 
 \'AQID\''","'Rewrite a base64 string: 
 - Remove LF and = characters 
 - Replace slashes by underscores 
 >>> b64c(""abc123456def"") 
 \'abc123456def\' 
 >>> b64c(""\na/=b=c/"") 
 \'a_bc_\' 
 >>> b64c(""a+b+c+123+"") 
 \'a+b+c+123+\''"
"@not_implemented_for('directed') 
 def common_neighbors(G, u, v): 
    if (u not in G): 
      raise nx.NetworkXError('u   is   not   in   the   graph.') 
   if (v not in G): 
      raise nx.NetworkXError('v   is   not   in   the   graph.') 
   return (w for w in G[u] if ((w in G[v]) and (w not in (u, v))))"," 'Return the common neighbors of u and v in G. 
 Parameters 
 G : NetworkX graph 
 The graph. 
 u : node 
 The first node. 
 v : node 
 The second node. 
 Returns 
 list 
 A list of common neighbors of u and v in G.'","'Return the common neighbors of two nodes in a graph. 
 Parameters 
 G : graph 
 A NetworkX undirected graph. 
 u, v : nodes 
 Nodes in the graph. 
 Returns 
 cnbors : iterator 
 Iterator of common neighbors of u and v in the graph. 
 Raises 
 NetworkXError 
 If u or v is not a node in the graph. 
 Examples 
 >>> G = nx.complete_graph(5) 
 >>> sorted(nx.common_neighbors(G, 0, 1)) 
 [2, 3, 4]'"
"def filename_match(filename, patterns, default=True): 
    if (not patterns): 
      return default 
   return any((fnmatch(filename, pattern) for pattern in patterns))"," 'Return True if the filename matches any of the patterns. 
 :param patterns: A list of patterns. 
 :param default: If True, return True if the filename matches any of the 
 patterns. If False, return False if the filename matches any of the 
 patterns.'","'Check if patterns contains a pattern that matches filename. 
 If patterns is unspecified, this always returns True.'"
"def connect_configservice(aws_access_key_id=None, aws_secret_access_key=None, **kwargs): 
    from boto.configservice.layer1 import ConfigServiceConnection 
   return ConfigServiceConnection(aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, **kwargs)"," 'Returns a ConfigServiceConnection object. 
 :param aws_access_key_id: AWS access key id. 
 :type aws_access_key_id: str 
 :param aws_secret_access_key: AWS secret access key. 
 :type aws_secret_access_key: str 
 :param region_name: Region name. 
 :type region_name: str 
 :param aws_session: AWS session object. 
 :type aws_session: boto.session.Session 
 :param kwargs: Additional keyword arguments to pass to the ConfigServiceConnection. 
 :type kwargs: dict 
 :return: ConfigServiceConnection object. 
 :rtype: ConfigServiceConnection'","'Connect to AWS Config 
 :type aws_access_key_id: string 
 :param aws_access_key_id: Your AWS Access Key ID 
 :type aws_secret_access_key: string 
 :param aws_secret_access_key: Your AWS Secret Access Key 
 rtype: :class:`boto.kms.layer1.ConfigServiceConnection` 
 :return: A connection to the AWS Config service'"
"@utils.arg('ip_range', metavar='<range>', help=_('Address   range   to   create.')) 
 @utils.arg('--pool', dest='pool', metavar='<pool>', default=None, help=_('Pool   for   new   Floating   IPs.')) 
 @utils.arg('--interface', metavar='<interface>', default=None, help=_('Interface   for   new   Floating   IPs.')) 
 @deprecated_network 
 def do_floating_ip_bulk_create(cs, args): 
    cs.floating_ips_bulk.create(args.ip_range, args.pool, args.interface)", 'Create a range of Floating IPs.','Bulk create floating IPs by range (nova-network only).'
"def _decode_preferred_encoding(s): 
    enc = locale.getpreferredencoding() 
   try: 
      try: 
         return s.decode(enc) 
      except LookupError: 
         enc = _DEFAULT_ENCODING 
      return s.decode(enc) 
   except UnicodeDecodeError: 
      return s.decode(u'latin-1')"," 'Decode a string, preferring the user\'s preferred encoding.'","'Decode the supplied byte string using the preferred encoding 
 for the locale (`locale.getpreferredencoding`) or, if the default encoding 
 is invalid, fall back first on utf-8, then on latin-1 if the message cannot 
 be decoded with utf-8.'"
"def run_migrations_offline(): 
    set_mysql_engine() 
   kwargs = dict() 
   if neutron_config.database.connection: 
      kwargs['url'] = neutron_config.database.connection 
   else: 
      kwargs['dialect_name'] = neutron_config.database.engine 
   kwargs['include_object'] = include_object 
   context.configure(**kwargs) 
   with context.begin_transaction(): 
      context.run_migrations()"," 'Run migrations in ``offline`` mode. 
 This configures the context with just a URL and not an Engine, 
 though an Engine is acceptable here as well.  By using context.run_migrations(), 
 we don\'t need to do anything further in this function.'","'Run migrations in \'offline\' mode. 
 This configures the context with either a URL 
 or an Engine. 
 Calls to context.execute() here emit the given string to the 
 script output.'"
"def create_resource(): 
    task_schema = get_task_schema() 
   partial_task_schema = _get_partial_task_schema() 
   deserializer = RequestDeserializer(task_schema) 
   serializer = ResponseSerializer(task_schema, partial_task_schema) 
   controller = TasksController() 
   return wsgi.Resource(controller, deserializer, serializer)", 'Create a resource object.','Task resource factory method'
"def conjuncts(expr): 
    return And.make_args(expr)"," 'Returns conjuncts of the given expression. 
 :param expr: An expression. 
 :type expr: :class:`~sympy.core.expr.Expr` 
 :returns: A :class:`~sympy.core.expr.And` object.'","'Return a list of the conjuncts in the expr s. 
 Examples 
 >>> from sympy.logic.boolalg import conjuncts 
 >>> from sympy.abc import A, B 
 >>> conjuncts(A & B) 
 frozenset({A, B}) 
 >>> conjuncts(A | B) 
 frozenset({Or(A, B)})'"
"def no_real_gs_credentials(): 
    if (parse_boolean_envvar(os.getenv('WALE_GS_INTEGRATION_TESTS')) is not True): 
      return True 
   if (os.getenv('GOOGLE_APPLICATION_CREDENTIALS') is None): 
      return True 
   return False"," 'Check if the credentials file is not present. 
 :return: True if the credentials file is not present, False otherwise.'","'Helps skip integration tests without live credentials. 
 Phrased in the negative to make it read better with \'skipif\'.'"
"def invert_docs_link_map(docs_links): 
    files_to_docs = defaultdict(list) 
   for (doc, files) in docs_links.iteritems(): 
      for file in files: 
         files_to_docs[file].append(doc) 
         files_to_docs[file] = list(set(files_to_docs[file])) 
   return files_to_docs"," 'Invert a dict of links to docs. 
 This is used to generate a list of docs that each file links to. 
 :param docs_links: dict of links to docs 
 :return: dict of files to docs'","'The docs links map is in this format: 
 ""doc_path"": [ 
 ""file_path"", 
 This transforms it to: 
 ""file_path"": [ 
 ""doc_path"",'"
"@require_global_staff 
 @require_POST 
 def generate_example_certificates(request, course_id=None): 
    course_key = CourseKey.from_string(course_id) 
   certs_api.generate_example_certificates(course_key) 
   return redirect(_instructor_dash_url(course_key, section='certificates'))",'./views.py:1482',"'Start generating a set of example certificates. 
 Example certificates are used to verify that certificates have 
 been configured correctly for the course. 
 Redirects back to the intructor dashboard once certificate 
 generation has begun.'"
"def start(): 
    try: 
      from . import saltnado 
   except ImportError as err: 
      logger.error('ImportError!   {0}'.format(str(err))) 
      return None 
   mod_opts = __opts__.get(__virtualname__, {}) 
   if ('num_processes' not in mod_opts): 
      mod_opts['num_processes'] = 1 
   if ((mod_opts['num_processes'] > 1) and (mod_opts.get('debug', False) is True)): 
      raise Exception(""Tornado's   debug   implementation   is   not   compatible   with   multiprocess.   Either   disable   debug,   or   set   num_processes   to   1."") 
   paths = [('/', saltnado.SaltAPIHandler), ('/login', saltnado.SaltAuthHandler), ('/minions/(.*)', saltnado.MinionSaltAPIHandler), ('/minions', saltnado.MinionSaltAPIHandler), ('/jobs/(.*)', saltnado.JobsSaltAPIHandler), ('/jobs', saltnado.JobsSaltAPIHandler), ('/run', saltnado.RunSaltAPIHandler), ('/events', saltnado.EventsSaltAPIHandler), ('/hook(/.*)?', saltnado.WebhookSaltAPIHandler)] 
   if mod_opts.get('websockets', False): 
      from . import saltnado_websockets 
      token_pattern = '([0-9A-Fa-f]{{{0}}})'.format(len(getattr(hashlib, __opts__.get('hash_type', 'md5'))().hexdigest())) 
      all_events_pattern = '/all_events/{0}'.format(token_pattern) 
      formatted_events_pattern = '/formatted_events/{0}'.format(token_pattern) 
      logger.debug('All   events   URL   pattern   is   {0}'.format(all_events_pattern)) 
      paths += [(all_events_pattern, saltnado_websockets.AllEventsHandler), (formatted_events_pattern, saltnado_websockets.FormattedEventsHandler)] 
   application = tornado.web.Application(paths, debug=mod_opts.get('debug', False)) 
   application.opts = __opts__ 
   application.mod_opts = mod_opts 
   application.auth = salt.auth.LoadAuth(__opts__) 
   kwargs = {} 
   if (not mod_opts.get('disable_ssl', False)): 
      if ('ssl_crt' not in mod_opts): 
         logger.error(""Not   starting   '%s'.   Options   'ssl_crt'   and   'ssl_key'   are   required   if   SSL   is   not   disabled."", __name__) 
         return None 
      ssl_opts = {'certfile': mod_opts['ssl_crt']} 
      if mod_opts.get('ssl_key', False): 
         ssl_opts.update({'keyfile': mod_opts['ssl_key']}) 
      kwargs['ssl_options'] = ssl_opts 
   http_server = tornado.httpserver.HTTPServer(application, **kwargs) 
   try: 
      http_server.bind(mod_opts['port'], address=mod_opts.get('address'), backlog=mod_opts.get('backlog', 128)) 
      http_server.start(mod_opts['num_processes']) 
   except: 
      logger.error('Rest_tornado   unable   to   bind   to   port   {0}'.format(mod_opts['port']), exc_info=True) 
      raise SystemExit(1) 
   try: 
      tornado.ioloop.IOLoop.instance().start() 
   except KeyboardInterrupt: 
      raise SystemExit(0)", 'Starts the Tornado server.','Start the saltnado!'
"def create_submission(conf, transform_valid, transform_test=None, features=None): 
    if (transform_test is None): 
      transform_test = transform_valid 
   kwargs = subdict(conf, ['dataset', 'normalize', 'normalize_on_the_fly', 'sparse']) 
   kwargs.update(randomize_valid=False, randomize_test=False) 
   (valid_set, test_set) = load_data(kwargs)[1:3] 
   if (not conf.get('sparse', False)): 
      valid_set = valid_set.get_value(borrow=True) 
      test_set = test_set.get_value(borrow=True) 
   if (features is not None): 
      valid_set = valid_set[:, features] 
      test_set = test_set[:, features] 
   valid_repr = transform_valid(valid_set) 
   test_repr = transform_test(test_set) 
   save_submission(conf, valid_repr, test_repr)"," 'Create a submission for the given dataset. 
 :param conf: Configuration dictionary 
 :param transform_valid: Validation transformation 
 :param transform_test: Test transformation 
 :param features: Features to use 
 :return: Submission file name'","'Create a submission file given a configuration dictionary and a 
 computation function. 
 Note that it always reload the datasets to ensure valid & test 
 are not permuted. 
 Parameters 
 conf : WRITEME 
 transform_valid : WRITEME 
 transform_test : WRITEME 
 features : WRITEME'"
"def translate(s, a, b=None, c=None): 
    from sympy.core.compatibility import maketrans 
   try: 
      ''.translate(None, '') 
      py3 = False 
   except TypeError: 
      py3 = True 
   mr = {} 
   if (a is None): 
      assert (c is None) 
      if (not b): 
         return s 
      c = b 
      a = b = '' 
   elif (type(a) is dict): 
      short = {} 
      for k in list(a.keys()): 
         if ((len(k) == 1) and (len(a[k]) == 1)): 
            short[k] = a.pop(k) 
      mr = a 
      c = b 
      if short: 
         (a, b) = [''.join(i) for i in list(zip(*short.items()))] 
      else: 
         a = b = '' 
   else: 
      assert (len(a) == len(b)) 
   if py3: 
      if c: 
         s = s.translate(maketrans('', '', c)) 
      s = replace(s, mr) 
      return s.translate(maketrans(a, b)) 
   else: 
      if c: 
         c = list(c) 
         rem = {} 
         for i in range((-1), ((-1) - len(c)), (-1)): 
            if (ord(c[i]) > 255): 
               rem[c[i]] = '' 
               c.pop(i) 
         s = s.translate(None, ''.join(c)) 
         s = replace(s, rem) 
         if a: 
            a = list(a) 
            b = list(b) 
            for i in range((-1), ((-1) - len(a)), (-1)): 
               if ((ord(a[i]) > 255) or (ord(b[i]) > 255)): 
                  mr[a.pop(i)] = b.pop(i) 
            a = ''.join(a) 
            b = ''.join(b) 
      s = replace(s, mr) 
      table = maketrans(a, b) 
      if ((type(table) is str) and (type(s) is str)): 
         s = s.translate(table) 
      else: 
         s = s.translate(dict([(i, ord(c)) for (i, c) in enumerate(table)])) 
      return s"," 'Translate a string from one character set to another. 
 If a is a string, it is assumed to be a dictionary of character 
 replacements.  If a is a list, it is assumed to be a list of 
 replacement characters.  If a is None, the character set is 
 assumed to be a string.  If b is a string, it is assumed to be a 
 list of replacement characters.  If b is a list, it is assumed to be 
 a list of replacement characters.  If b is None, the character set is 
 assumed to be a string.  If c is a string, it is assumed to be a 
 list of replacement characters.  If c is a list, it is assumed to be a 
 list of replacement characters.  If c is None, the character set is 
 assumed to be a string. 
 If the character sets are different, the string is first translated 
 using the first character set, then translated using the second 
 character set. 
 Examples 
 >>> translate(\'a\', \'b\', \'c\') 
 \'b\xc3\xbc\'","'Return ``s`` where characters have been replaced or deleted. 
 SYNTAX 
 translate(s, None, deletechars): 
 all characters in ``deletechars`` are deleted 
 translate(s, map [,deletechars]): 
 all characters in ``deletechars`` (if provided) are deleted 
 then the replacements defined by map are made; if the keys 
 of map are strings then the longer ones are handled first. 
 Multicharacter deletions should have a value of \'\'. 
 translate(s, oldchars, newchars, deletechars) 
 all characters in ``deletechars`` are deleted 
 then each character in ``oldchars`` is replaced with the 
 corresponding character in ``newchars`` 
 Examples 
 >>> from sympy.utilities.misc import translate 
 >>> from sympy.core.compatibility import unichr 
 >>> abc = \'abc\' 
 >>> translate(abc, None, \'a\') 
 \'bc\' 
 >>> translate(abc, {\'a\': \'x\'}, \'c\') 
 \'xb\' 
 >>> translate(abc, {\'abc\': \'x\', \'a\': \'y\'}) 
 \'x\' 
 >>> translate(\'abcd\', \'ac\', \'AC\', \'d\') 
 \'AbC\' 
 There is no guarantee that a unique answer will be 
 obtained if keys in a mapping overlap are the same 
 length and have some identical sequences at the 
 beginning/end: 
 >>> translate(abc, {\'ab\': \'x\', \'bc\': \'y\'}) in (\'xc\', \'ay\') 
 True'"
"@handle_response_format 
 @treeio_login_required 
 def sla_view(request, sla_id, response_format='html'): 
    sla = get_object_or_404(ServiceLevelAgreement, pk=sla_id) 
   if (not request.user.profile.has_permission(sla)): 
      return user_denied(request, message=""You   don't   have   access   to   this   Service   Level   Agreement"") 
   context = _get_default_context(request) 
   context.update({'sla': sla}) 
   return render_to_response('services/sla_view', context, context_instance=RequestContext(request), response_format=response_format)"," 'View a Service Level Agreement. 
 This view is only accessible to users with permission to view the SLA.'",'ServiceLevelAgreement view'
"def multi_replace(text, word_dic): 
    rc = re.compile('|'.join(map(re.escape, word_dic))) 
   def translate(match): 
      return word_dic[match.group(0)] 
   return rc.sub(translate, text)"," 'Replace a list of words in a string. 
 :param text: the string to be translated 
 :param word_dic: a list of words to be translated 
 :type word_dic: list 
 :return: the translated string 
 :rtype: str'","'Takes a string and replace words that match a key in a dictionary with the associated value, 
 then returns the changed text 
 :rtype str'"
"def get_azimuth_value(label): 
    _check_is_integral('azimuth', label) 
   if (label == (-1)): 
      return None 
   else: 
      if (((label % 2) != 0) or (label < 0) or (label > 34)): 
         raise ValueError(('Expected   azimuth   to   be   an   even   number   between   0   and   34   inclusive,   or   -1,   but   got   %s   instead.' % str(label))) 
      return (label * 10)"," 'Return the azimuth value from a label. 
 :param label: An integer label representing the azimuth value. 
 :type label: int 
 :return: The azimuth value represented by the label. 
 :rtype: int'","'Returns the angle in degrees represented by a azimuth label int. 
 Parameters 
 label: int 
 Azimuth label.'"
"def is_automated(): 
    is_automated = False 
   try: 
      dist_dir_i = sys.argv.index('--dist-dir') 
   except ValueError: 
      dist_dir_i = None 
   if (dist_dir_i is not None): 
      dist_dir = sys.argv[(dist_dir_i + 1)] 
      if ('egg-dist-tmp' in dist_dir): 
         is_automated = True 
   if ((sys.argv in [['-c', 'develop', '--no-deps'], ['--no-deps', '-c', 'develop'], ['-c', 'egg_info']]) or ('pip-egg-info' in sys.argv) or (sys.argv[:3] == ['-c', 'install', '--record']) or (sys.argv[:4] == ['-c', 'install', '--single-version-externally-managed', '--record'])): 
      is_automated = True 
   return is_automated"," 'Determine whether the current setup is automated (i.e. using pip). 
 This is done by checking for the presence of the --dist-dir option, 
 which is only used by pip. 
 :return: is_automated'",'Check for installation with easy_install or pip.'
"@pytest.mark.installed 
 def test_activate_does_not_leak_echo_setting(shell): 
    if ((not on_win) or (shell != u'cmd.exe')): 
      pytest.skip(u""echo   leaking   is   only   relevant   on   Window's   CMD.EXE"") 
   shell_vars = _format_vars(shell) 
   with TemporaryDirectory(prefix=u'envs', dir=os.path.dirname(__file__)) as envs: 
      (env_dirs, env_vars) = gen_test_env_paths(envs, shell) 
      scripts = [] 
      src_activate = shell_vars[u'source'].format(u'{syspath}{binpath}activate{suffix_executable}') 
      scripts += [dedent(u'                                    @ECHO   ON\n                                    {}   ""{{env_dirs[0]}}""\n                                    @ECHO\n                                    ')] 
      for script in scripts: 
         script = script.format(src_activate) 
         script = script.format(env_vars=env_vars, env_dirs=env_dirs, **shell_vars) 
         commands = (shell_vars[u'command_setup'] + script) 
         (stdout, stderr) = run_in(commands, shell) 
         print(u'commands:', commands) 
         print(u'stdout:', stdout) 
         print(u'stderr:', stderr) 
         assert_equals(stdout, u'ECHO   is   on.', stderr) 
         assert_equals(stderr, u'')", 'Test that activate does not leak echo settings','Test that activate\'s setting of echo to off does not disrupt later echo calls'
"def gf_berlekamp(f, p, K): 
    Q = gf_Qmatrix(f, p, K) 
   V = gf_Qbasis(Q, p, K) 
   for (i, v) in enumerate(V): 
      V[i] = gf_strip(list(reversed(v))) 
   factors = [f] 
   for k in range(1, len(V)): 
      for f in list(factors): 
         s = K.zero 
         while (s < p): 
            g = gf_sub_ground(V[k], s, p, K) 
            h = gf_gcd(f, g, p, K) 
            if ((h != [K.one]) and (h != f)): 
               factors.remove(f) 
               f = gf_quo(f, h, p, K) 
               factors.extend([f, h]) 
            if (len(factors) == len(V)): 
               return _sort_factors(factors, multiple=False) 
            s += K.one 
   return _sort_factors(factors, multiple=False)"," 'Berlekamp factorization algorithm for GF(p). 
 Returns the factors of f in GF(p) in increasing order. 
 Examples 
 >>> from sympy.polys.galoistools import gf_berlekamp 
 >>> gf_berlekamp([1, 2, 3, 4], 5, QQ) 
 [1, 2, 3, 4] 
 >>> gf_berlekamp([1, 2, 3, 4], 5, ZZ) 
 [2, 4, 3, 1]'","'Factor a square-free ``f`` in ``GF(p)[x]`` for small ``p``. 
 Examples 
 >>> from sympy.polys.domains import ZZ 
 >>> from sympy.polys.galoistools import gf_berlekamp 
 >>> gf_berlekamp([1, 0, 0, 0, 1], 5, ZZ) 
 [[1, 0, 2], [1, 0, 3]]'"
"def notify_status(doc, method=None): 
    party_type = None 
   for (key, doctypes) in status_depends_on.iteritems(): 
      if (doc.doctype in doctypes): 
         party_type = key 
         break 
   if (not party_type): 
      return 
   name = doc.get(party_type.lower()) 
   if (not name): 
      return 
   party = frappe.get_doc(party_type, name) 
   filters = get_filters_for(doc.doctype) 
   party.flags.ignore_mandatory = True 
   status = None 
   if filters: 
      if evaluate_filters(doc, filters): 
         status = u'Open' 
   if (status == u'Open'): 
      if (party.status != u'Open'): 
         party.status = u'Open' 
         party.save(ignore_permissions=True) 
   elif (party.status == u'Open'): 
      update_status(party) 
   party.update_modified() 
   party.notify_update()", 'Notify status update of the party based on the status of the document.',"'Notify status to customer, supplier'"
"def ajax_editable_boolean(attr, short_description): 
    def _fn(self, item): 
      return ajax_editable_boolean_cell(item, attr) 
   _fn.short_description = short_description 
   _fn.editable_boolean_field = attr 
   return _fn"," 'Decorator to make a boolean editable in the UI. 
 :param attr: The attribute to edit. 
 :param short_description: A short description of the attribute.'","'Convenience function: Assign the return value of this method to a variable 
 of your ModelAdmin class and put the variable name into list_display. 
 Example:: 
 class MyTreeEditor(TreeEditor): 
 list_display = (\'__str__\', \'active_toggle\') 
 active_toggle = ajax_editable_boolean(\'active\', _(\'is active\'))'"
"def from_text(textring): 
    keyring = {} 
   for keytext in textring: 
      keyname = dns.name.from_text(keytext) 
      secret = base64.decodestring(textring[keytext]) 
      keyring[keyname] = secret 
   return keyring"," 'Convert a text string into a dictionary of key names and their secrets. 
 :param textring: A text string containing the key names and their secrets 
 :type textring: str'","'Convert a dictionary containing (textual DNS name, base64 secret) pairs 
 into a binary keyring which has (dns.name.Name, binary secret) pairs. 
 @rtype: dict'"
"def _create_base_cipher(dict_parameters): 
    use_aesni = dict_parameters.pop('use_aesni', True) 
   try: 
      key = dict_parameters.pop('key') 
   except KeyError: 
      raise TypeError(""Missing   'key'   parameter"") 
   expect_byte_string(key) 
   if (len(key) not in key_size): 
      raise ValueError(('Incorrect   AES   key   length   (%d   bytes)' % len(key))) 
   if (use_aesni and _raw_aesni_lib): 
      start_operation = _raw_aesni_lib.AESNI_start_operation 
      stop_operation = _raw_aesni_lib.AESNI_stop_operation 
   else: 
      start_operation = _raw_aes_lib.AES_start_operation 
      stop_operation = _raw_aes_lib.AES_stop_operation 
   cipher = VoidPointer() 
   result = start_operation(key, c_size_t(len(key)), cipher.address_of()) 
   if result: 
      raise ValueError(('Error   %X   while   instantiating   the   AES   cipher' % result)) 
   return SmartPointer(cipher.get(), stop_operation)"," 'Create a base cipher object. 
 This function creates a cipher object using the AES algorithm. 
 Parameters 
 dict_parameters : dict 
 Dictionary with parameters. 
 key : byte string 
 AES key. 
 return : SmartPointer(AES cipher object) 
 Smart pointer to the AES cipher object.'","'This method instantiates and returns a handle to a low-level 
 base cipher. It will absorb named parameters in the process.'"
"def _lscmp(a, b): 
    return ((not sum(((0 if (x == y) else 1) for (x, y) in zip(a, b)))) and (len(a) == len(b)))"," 'Returns True if a and b are lexicographically equal. 
 If a and b are lists, then they must have the same length and 
 the elements must be equal. 
 >>> _lscmp([1, 2, 3], [1, 2, 3]) 
 True 
 >>> _lscmp([1, 2, 3], [1, 2, 4]) 
 False 
 >>> _lscmp([1, 2, 3], [1, 2, 3, 4]) 
 False 
 >>> _lscmp([1, 2, 3, 4], [1, 2, 3, 4]) 
 True 
 >>> _lscmp([1, 2, 3, 4], [1, 2, 3, 4, 5]) 
 False 
 >>> _lscmp([1, 2, 3, 4], [1, 2, 3, 4, 5, 6]) 
 False 
 >>> _lscmp([1, 2, 3, 4","'Compares two strings in a cryptographically safe way: 
 Runtime is not affected by length of common prefix.'"
"def get_minions(): 
    serv = _get_serv(ret=None) 
   sql = 'select   distinct(id)   from   returns' 
   data = serv.query(sql) 
   ret = [] 
   if data: 
      for jid in data[0]['points']: 
         ret.append(jid[1]) 
   return ret", 'Returns the list of minions currently in the game','Return a list of minions'
"def get_writer_names(): 
    return set([k for (k, v) in six.iteritems(REPORT_WRITERS_MAP) if v])", 'Returns a set of writer names that are supported by this module.','Get the registered writer names'
"def install(pkgs=None, requirements=None, bin_env=None, use_wheel=False, no_use_wheel=False, log=None, proxy=None, timeout=None, editable=None, find_links=None, index_url=None, extra_index_url=None, no_index=False, mirrors=None, build=None, target=None, download=None, download_cache=None, source=None, upgrade=False, force_reinstall=False, ignore_installed=False, exists_action=None, no_deps=False, no_install=False, no_download=False, global_options=None, install_options=None, user=None, no_chown=False, cwd=None, pre_releases=False, cert=None, allow_all_external=False, allow_external=None, allow_unverified=None, process_dependency_links=False, saltenv='base', env_vars=None, use_vt=False, trusted_host=None, no_cache_dir=False): 
    pip_bin = _get_pip_bin(bin_env) 
   cmd = [pip_bin, 'install'] 
   (cleanup_requirements, error) = _process_requirements(requirements=requirements, cmd=cmd, cwd=cwd, saltenv=saltenv, user=user) 
   if error: 
      return error 
   if use_wheel: 
      min_version = '1.4' 
      cur_version = __salt__['pip.version'](bin_env) 
      if (not salt.utils.compare_versions(ver1=cur_version, oper='>=', ver2=min_version)): 
         logger.error('The   --use-wheel   option   is   only   supported   in   pip   {0}   and   newer.   The   version   of   pip   detected   is   {1}.   This   option   will   be   ignored.'.format(min_version, cur_version)) 
      else: 
         cmd.append('--use-wheel') 
   if no_use_wheel: 
      min_version = '1.4' 
      cur_version = __salt__['pip.version'](bin_env) 
      if (not salt.utils.compare_versions(ver1=cur_version, oper='>=', ver2=min_version)): 
         logger.error('The   --no-use-wheel   option   is   only   supported   in   pip   {0}   and   newer.   The   version   of   pip   detected   is   {1}.   This   option   will   be   ignored.'.format(min_version, cur_version)) 
      else: 
         cmd.append('--no-use-wheel') 
   if log: 
      if os.path.isdir(log): 
         raise IOError(""'{0}'   is   a   directory.   Use   --log   path_to_file"".format(log)) 
      elif (not os.access(log, os.W_OK)): 
         raise IOError(""'{0}'   is   not   writeable"".format(log)) 
      cmd.extend(['--log', log]) 
   if proxy: 
      cmd.extend(['--proxy', proxy]) 
   if timeout: 
      try: 
         if isinstance(timeout, float): 
            raise ValueError('Timeout   cannot   be   a   float') 
         int(timeout) 
      except ValueError: 
         raise ValueError(""'{0}'   is   not   a   valid   timeout,   must   be   an   integer"".format(timeout)) 
      cmd.extend(['--timeout', timeout]) 
   if find_links: 
      if isinstance(find_links, string_types): 
         find_links = [l.strip() for l in find_links.split(',')] 
      for link in find_links: 
         if (not (salt.utils.url.validate(link, VALID_PROTOS) or os.path.exists(link))): 
            raise CommandExecutionError(""'{0}'   is   not   a   valid   URL   or   path"".format(link)) 
         cmd.extend(['--find-links', link]) 
   if (no_index and (index_url or extra_index_url)): 
      raise CommandExecutionError(""'no_index'   and   ('index_url'   or   'extra_index_url')   are   mutually   exclusive."") 
   if index_url: 
      if (not salt.utils.url.validate(index_url, VALID_PROTOS)): 
         raise CommandExecutionError(""'{0}'   is   not   a   valid   URL"".format(index_url)) 
      cmd.extend(['--index-url', index_url]) 
   if extra_index_url: 
      if (not salt.utils.url.validate(extra_index_url, VALID_PROTOS)): 
         raise CommandExecutionError(""'{0}'   is   not   a   valid   URL"".format(extra_index_url)) 
      cmd.extend(['--extra-index-url', extra_index_url]) 
   if no_index: 
      cmd.append('--no-index') 
   if mirrors: 
      pip_version = version(pip_bin) 
      if salt.utils.compare_versions(ver1=pip_version, oper='>=', ver2='7.0.0'): 
         raise CommandExecutionError('pip   >=   7.0.0   does   not   support   mirror   argument:   use   index_url   and/or   extra_index_url   instead') 
      if isinstance(mirrors, string_types): 
         mirrors = [m.strip() for m in mirrors.split(',')] 
      cmd.append('--use-mirrors') 
      for mirror in mirrors: 
         if (not mirror.startswith('http://')): 
            raise CommandExecutionError(""'{0}'   is   not   a   valid   URL"".format(mirror)) 
         cmd.extend(['--mirrors', mirror]) 
   if build: 
      cmd.extend(['--build', build]) 
   if target: 
      cmd.extend(['--target', target]) 
   if download: 
      cmd.extend(['--download', download]) 
   if download_cache: 
      cmd.extend(['--download-cache', download_cache]) 
   if source: 
      cmd.extend(['--source', source]) 
   if upgrade: 
      cmd.append('--upgrade') 
   if force_reinstall: 
      cmd.append('--force-reinstall') 
   if ignore_installed: 
      cmd.append('--ignore-installed') 
   if exists_action: 
      if (exists_action.lower() not in ('s', 'i', 'w', 'b')): 
         raise CommandExecutionError(""The   exists_action   pip   option   only   supports   the   values   s,   i,   w,   and   b.   '{0}'   is   not   valid."".format(exists_action)) 
      cmd.extend(['--exists-action', exists_action]) 
   if no_deps: 
      cmd.append('--no-deps') 
   if no_install: 
      cmd.append('--no-install') 
   if no_download: 
      cmd.append('--no-download') 
   if no_cache_dir: 
      cmd.append('--no-cache-dir') 
   if pre_releases: 
      pip_version = version(pip_bin) 
      if salt.utils.compare_versions(ver1=pip_version, oper='>=', ver2='1.4'): 
         cmd.append('--pre') 
   if cert: 
      cmd.extend(['--cert', cert]) 
   if global_options: 
      if isinstance(global_options, string_types): 
         global_options = [go.strip() for go in global_options.split(',')] 
      for opt in global_options: 
         cmd.extend(['--global-option', opt]) 
   if install_options: 
      if isinstance(install_options, string_types): 
         install_options = [io.strip() for io in install_options.split(',')] 
      for opt in install_options: 
         cmd.extend(['--install-option', opt]) 
   if pkgs: 
      if isinstance(pkgs, string_types): 
         pkgs = [p.strip() for p in pkgs.split(',')] 
      cmd.extend(['{0}'.format(p.replace(';', ',')) for p in pkgs]) 
   if editable: 
      egg_match = re.compile('(?:#|#.*?&)egg=([^&]*)') 
      if isinstance(editable, string_types): 
         editable = [e.strip() for e in editable.split(',')] 
      for entry in editable: 
         if (not ((entry == '.') or entry.startswith(('file://', '/')))): 
            match = egg_match.search(entry) 
            if ((not match) or (not match.group(1))): 
               raise CommandExecutionError('You   must   specify   an   egg   for   this   editable') 
         cmd.extend(['--editable', entry]) 
   if allow_all_external: 
      cmd.append('--allow-all-external') 
   if allow_external: 
      if isinstance(allow_external, string_types): 
         allow_external = [p.strip() for p in allow_external.split(',')] 
      for pkg in allow_external: 
         cmd.extend(['--allow-external', pkg]) 
   if allow_unverified: 
      if isinstance(allow_unverified, string_types): 
         allow_unverified = [p.strip() for p in allow_unverified.split(',')] 
      for pkg in allow_unverified: 
         cmd.extend(['--allow-unverified', pkg]) 
   if process_dependency_links: 
      cmd.append('--process-dependency-links') 
   if env_vars: 
      if isinstance(env_vars, dict): 
         for (k, v) in iteritems(env_vars): 
            if (not isinstance(v, string_types)): 
               env_vars[k] = str(v) 
         os.environ.update(env_vars) 
      else: 
         raise CommandExecutionError('env_vars   {0}   is   not   a   dictionary'.format(env_vars)) 
   if trusted_host: 
      cmd.extend(['--trusted-host', trusted_host]) 
   try: 
      cmd_kwargs = dict(saltenv=saltenv, use_vt=use_vt, runas=user) 
      if cwd: 
         cmd_kwargs['cwd'] = cwd 
      if (bin_env and os.path.isdir(bin_env)): 
         cmd_kwargs['env'] = {'VIRTUAL_ENV': bin_env} 
      logger.debug('TRY   BLOCK:   end   of   pip.install   --   cmd:   %s,   cmd_kwargs:   %s', str(cmd), str(cmd_kwargs)) 
      return __salt__['cmd.run_all'](cmd, python_shell=False, **cmd_kwargs) 
   finally: 
      for tempdir in [cr for cr in cleanup_requirements if (cr is not None)]: 
         if os.path.isdir(tempdir): 
            shutil.rmtree(tempdir)"," 'Install a Python package using pip. 
 .. versionchanged:: 2015.7.0 
 Added support for using pip 1.4. 
 .. versionchanged:: 2015.3.0 
 Added support for using pip 1.2. 
 .. versionchanged:: 2014.7.0 
 Added support for using pip 1.1. 
 .. versionchanged:: 2014.3.0 
 Added support for using pip 1.0. 
 .. versionchanged:: 2014.2.0 
 Added support for using pip 0.10. 
 .. versionchanged:: 2013.8.0 
 Added support for using pip 0.9. 
 .. versionchanged:: 2013.7.0 
 Added support for using pip 0.8. 
 .. versionchanged:: 2013.6.0 
 Added support for using pip 0.7. 
 .. versionchanged:: 2013.5.0 
 Added support for using pip 0.6. 
 .. version","'Install packages with pip 
 Install packages individually or from a pip requirements file. Install 
 packages globally or to a virtualenv. 
 pkgs 
 Comma separated list of packages to install 
 requirements 
 Path to requirements 
 bin_env 
 Path to pip bin or path to virtualenv. If doing a system install, 
 and want to use a specific pip bin (pip-2.7, pip-2.6, etc..) just 
 specify the pip bin you want. 
 .. note:: 
 If installing into a virtualenv, just use the path to the 
 virtualenv (e.g. ``/home/code/path/to/virtualenv/``) 
 use_wheel 
 Prefer wheel archives (requires pip>=1.4) 
 no_use_wheel 
 Force to not use wheel archives (requires pip>=1.4) 
 log 
 Log file where a complete (maximum verbosity) record will be kept 
 proxy 
 Specify a proxy in the form ``user:passwd@proxy.server:port``. Note 
 that the ``user:password@`` is optional and required only if you are 
 behind an authenticated proxy. If you provide 
 ``user@proxy.server:port`` then you will be prompted for a password. 
 timeout 
 Set the socket timeout (default 15 seconds) 
 editable 
 install something editable (e.g. 
 ``git+https://github.com/worldcompany/djangoembed.git#egg=djangoembed``) 
 find_links 
 URL to search for packages 
 index_url 
 Base URL of Python Package Index 
 extra_index_url 
 Extra URLs of package indexes to use in addition to ``index_url`` 
 no_index 
 Ignore package index 
 mirrors 
 Specific mirror URL(s) to query (automatically adds --use-mirrors) 
 .. warning:: 
 This option has been deprecated and removed in pip version 7.0.0. 
 Please use ``index_url`` and/or ``extra_index_url`` instead. 
 build 
 Unpack packages into ``build`` dir 
 target 
 Install packages into ``target`` dir 
 download 
 Download packages into ``download`` instead of installing them 
 download_cache 
 Cache downloaded packages in ``download_cache`` dir 
 source 
 Check out ``editable`` packages into ``source`` dir 
 upgrade 
 Upgrade all packages to the newest available version 
 force_reinstall 
 When upgrading, reinstall all packages even if they are already 
 up-to-date. 
 ignore_installed 
 Ignore the installed packages (reinstalling instead) 
 exists_action 
 Default action when a path already exists: (s)witch, (i)gnore, (w)ipe, 
 (b)ackup 
 no_deps 
 Ignore package dependencies 
 no_install 
 Download and unpack all packages, but don\'t actually install them 
 no_download 
 Don\'t download any packages, just install the ones already downloaded 
 (completes an install run with ``--no-install``) 
 install_options 
 Extra arguments to be supplied to the setup.py install command (e.g. 
 like ``--install-option=\'--install-scripts=/usr/local/bin\'``).  Use 
 multiple --install-option options to pass multiple options to setup.py 
 install. If you are using an option with a directory path, be sure to 
 use absolute path. 
 global_options 
 Extra global options to be supplied to the setup.py call before the 
 install command. 
 user 
 The user under which to run pip 
 no_chown 
 When user is given, do not attempt to copy and chown a requirements 
 file 
 cwd 
 Current working directory to run pip from 
 pre_releases 
 Include pre-releases in the available versions 
 cert 
 Provide a path to an alternate CA bundle 
 allow_all_external 
 Allow the installation of all externally hosted files 
 allow_external 
 Allow the installation of externally hosted files (comma separated 
 list) 
 allow_unverified 
 Allow the installation of insecure and unverifiable files (comma 
 separated list) 
 process_dependency_links 
 Enable the processing of dependency links 
 env_vars 
 Set environment variables that some builds will depend on. For example, 
 a Python C-module may have a Makefile that needs INCLUDE_PATH set to 
 pick up a header file while compiling.  This must be in the form of a 
 dictionary or a mapping. 
 Example: 
 .. code-block:: bash 
 salt \'*\' pip.install django_app env_vars=""{\'CUSTOM_PATH\': \'/opt/django_app\'}"" 
 trusted_host 
 Mark this host as trusted, even though it does not have valid or any 
 HTTPS. 
 use_vt 
 Use VT terminal emulation (see output while installing) 
 no_cache_dir 
 Disable the cache. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' pip.install <package name>,<package2 name> 
 salt \'*\' pip.install requirements=/path/to/requirements.txt 
 salt \'*\' pip.install <package name> bin_env=/path/to/virtualenv 
 salt \'*\' pip.install <package name> bin_env=/path/to/pip_bin 
 Complicated CLI example:: 
 salt \'*\' pip.install markdown,django                 editable=git+https://github.com/worldcompany/djangoembed.git#egg=djangoembed upgrade=True no_deps=True'"
"def _onPygletText(text, emulated=False): 
    global useText 
   if (not useText): 
      return 
   keyTime = psychopy.core.getTime() 
   if emulated: 
      keySource = 'EmulatedKey' 
   else: 
      keySource = 'KeyPress' 
   _keyBuffer.append((text, keyTime)) 
   logging.data(('%s:   %s' % (keySource, text)))"," 'Helper function to log key presses from the pyglet text 
 module. 
 Parameters 
 text : str 
 Text that was pressed. 
 emulated : bool 
 If True, this is a key press emulated by the user interface. 
 Returns 
 None'","'handler for on_text pyglet events, or call directly to emulate a text 
 event. 
 S Mathot 2012: This function only acts when the key that is pressed 
 corresponds to a non-ASCII text character (Greek, Arabic, Hebrew, etc.). 
 In that case the symbol that is passed to _onPygletKey() is translated 
 into a useless \'user_key()\' string. If this happens, _onPygletText takes 
 over the role of capturing the key. Unfortunately, _onPygletText() 
 cannot solely handle all input, because it does not respond to spacebar 
 presses, etc.'"
"@gen.coroutine 
 def _UploadWelcomePhotos(http_client, client, user, upload_request): 
    obj_store = ObjectStore.GetInstance(ObjectStore.PHOTO) 
   welcome_path = os.path.join(ResourcesManager.Instance().resources_path, 'welcome') 
   (yield _SetWelcomeIds(user, upload_request)) 
   upload_request = deepcopy(upload_request) 
   upload_request_copy = deepcopy(upload_request) 
   [ph_dict.pop('name') for ph_dict in upload_request_copy['photos']] 
   upload_response = (yield UploadEpisode(client, obj_store, user.user_id, user.webapp_dev_id, upload_request_copy)) 
   for (request_ph_dict, response_ph_dict) in zip(upload_request['photos'], upload_response['photos']): 
      for format in ('full', 'med', 'tn'): 
         f = open(os.path.join(welcome_path, ('%s_%s.jpg' % (request_ph_dict['name'], format))), 'r') 
         image_data = f.read() 
         f.close() 
         photo_url = response_ph_dict[(format + '_put_url')] 
         content_md5 = base64.b64encode(request_ph_dict[(format + '_md5')].decode('hex')) 
         headers = {'Content-Type': 'image/jpeg', 'Content-MD5': content_md5} 
         validate_cert = (not options.options.fileobjstore) 
         response = (yield gen.Task(http_client.fetch, photo_url, method='PUT', body=image_data, follow_redirects=False, validate_cert=validate_cert, headers=headers)) 
         if (response.code != 200): 
            raise Exception(('Cannot   upload   photo   ""%s"".   HTTP   error   code   %d.   Is   server   running   and   accessible?' % (request_ph_dict['photo_id'], response.code)))", 'Uploads welcome photos.',"'Uploads a set of photos that will be used in the new user welcome conversation. These 
 photos are uploaded to the given user account. ""upload_request"" is in the UPLOAD_EPISODE_REQUEST 
 format in json_schema.py, except: 
 1. Activity, episode, and photo ids are added by this method. 
 2. Each photo dict must contain an additional ""name"" field which gives the start of the 
 filename of a jpg file in the backend/resources/welcome directory. Three files must 
 exist there, in this format: <name>_full.jpg, <name>_med.jpg, <name>_tn.jpg.'"
"def _get_next_prev(generic_view, date, is_previous, period): 
    date_field = generic_view.get_date_field() 
   allow_empty = generic_view.get_allow_empty() 
   allow_future = generic_view.get_allow_future() 
   get_current = getattr(generic_view, (u'_get_current_%s' % period)) 
   get_next = getattr(generic_view, (u'_get_next_%s' % period)) 
   (start, end) = (get_current(date), get_next(date)) 
   if allow_empty: 
      if is_previous: 
         result = get_current((start - datetime.timedelta(days=1))) 
      else: 
         result = end 
      if (allow_future or (result <= timezone_today())): 
         return result 
      else: 
         return None 
   else: 
      if is_previous: 
         lookup = {(u'%s__lt' % date_field): generic_view._make_date_lookup_arg(start)} 
         ordering = (u'-%s' % date_field) 
      else: 
         lookup = {(u'%s__gte' % date_field): generic_view._make_date_lookup_arg(end)} 
         ordering = date_field 
      if (not allow_future): 
         if generic_view.uses_datetime_field: 
            now = timezone.now() 
         else: 
            now = timezone_today() 
         lookup[(u'%s__lte' % date_field)] = now 
      qs = generic_view.get_queryset().filter(**lookup).order_by(ordering) 
      try: 
         result = getattr(qs[0], date_field) 
      except IndexError: 
         return None 
      if generic_view.uses_datetime_field: 
         if settings.USE_TZ: 
            result = timezone.localtime(result) 
         result = result.date() 
      return get_current(result)"," 'Return the next or previous date for a given date. 
 If the given date is not in the past, then None is returned. 
 If the given date is in the past, then the next date is returned. 
 If the given date is in the future, then the previous date is returned. 
 If the given date is in the future and allow_future is False, then None 
 is returned. 
 If allow_empty is True, then the next date is returned if it is in the 
 past, and the previous date is returned if it is in the future. 
 If allow_empty is False, then the next date is returned if it is in the 
 past, and the previous date is returned if it is in the future. 
 If allow_empty is False and the given date is in the future, then None is 
 returned. 
 If allow_future is False, then the next date is returned if it is in the 
 past, and the previous date is returned if it is in the future. 
 If allow_future is False and the given date is in the future, then None is 
 returned. 
 If the given","'Helper: Get the next or the previous valid date. The idea is to allow 
 links on month/day views to never be 404s by never providing a date 
 that\'ll be invalid for the given view. 
 This is a bit complicated since it handles different intervals of time, 
 hence the coupling to generic_view. 
 However in essence the logic comes down to: 
 * If allow_empty and allow_future are both true, this is easy: just 
 return the naive result (just the next/previous day/week/month, 
 reguardless of object existence.) 
 * If allow_empty is true, allow_future is false, and the naive result 
 isn\'t in the future, then return it; otherwise return None. 
 * If allow_empty is false and allow_future is true, return the next 
 date *that contains a valid object*, even if it\'s in the future. If 
 there are no next objects, return None. 
 * If allow_empty is false and allow_future is false, return the next 
 date that contains a valid object. If that date is in the future, or 
 if there are no next objects, return None.'"
"@not_implemented_for('undirected') 
 def antichains(G): 
    TC = nx.transitive_closure(G) 
   antichains_stacks = [([], list(reversed(list(nx.topological_sort(G)))))] 
   while antichains_stacks: 
      (antichain, stack) = antichains_stacks.pop() 
      (yield antichain) 
      while stack: 
         x = stack.pop() 
         new_antichain = (antichain + [x]) 
         new_stack = [t for t in stack if (not ((t in TC[x]) or (x in TC[t])))] 
         antichains_stacks.append((new_antichain, new_stack))"," 'Returns the antichains of a graph. 
 The antichains of a graph are the minimal collections of vertices 
 such that there exists a path from any vertex in the collection to 
 every other vertex in the collection. 
 Parameters 
 G : NetworkX graph 
 The graph to return the antichains of. 
 Returns 
 antichains : list of lists 
 The antichains of `G`.'","'Generates antichains from a DAG. 
 An antichain is a subset of a partially ordered set such that any 
 two elements in the subset are incomparable. 
 Parameters 
 G : NetworkX DiGraph 
 Graph 
 Returns 
 antichain : generator object 
 Raises 
 NetworkXNotImplemented 
 If G is not directed 
 NetworkXUnfeasible 
 If G contains a cycle 
 Notes 
 This function was originally developed by Peter Jipsen and Franco Saliola 
 for the SAGE project. It\'s included in NetworkX with permission from the 
 authors. Original SAGE code at: 
 https://sage.informatik.uni-goettingen.de/src/combinat/posets/hasse_diagram.py 
 References 
 .. [1] Free Lattices, by R. Freese, J. Jezek and J. B. Nation, 
 AMS, Vol 42, 1995, p. 226.'"
"def get_build_version(): 
    prefix = 'MSC   v.' 
   i = string.find(sys.version, prefix) 
   if (i == (-1)): 
      return 6 
   i = (i + len(prefix)) 
   (s, rest) = sys.version[i:].split('   ', 1) 
   majorVersion = (int(s[:(-2)]) - 6) 
   minorVersion = (int(s[2:3]) / 10.0) 
   if (majorVersion == 6): 
      minorVersion = 0 
   if (majorVersion >= 6): 
      return (majorVersion + minorVersion) 
   return None"," 'Returns the build version of the Python interpreter. 
 The build version is the major and minor version numbers of the 
 Python interpreter plus the build number. 
 The build number is the second part of the string \'MSC v. 6.1\'. 
 For Python 2.6.0, the build number is 1. 
 For Python 2.6.1, the build number is 2. 
 For Python 2.6.2, the build number is 3. 
 For Python 2.6.3, the build number is 4. 
 For Python 2.6.4, the build number is 5. 
 For Python 2.6.5, the build number is 6. 
 For Python 2.6.6, the build number is 7. 
 For Python 2.6.7, the build number is 8. 
 For Python 2.6.8, the build number is 9. 
 For Python 2.6.9, the build number is 10. 
 For Python 2.6.10, the build number is ","'Return the version of MSVC that was used to build Python. 
 For Python 2.3 and up, the version number is included in 
 sys.version.  For earlier versions, assume the compiler is MSVC 6.'"
"def update_content_in_local_cache(url, content, method='GET'): 
    if (local_cache_enable and (method == 'GET') and cache.is_cached(url)): 
      info_dict = cache.get_info(url) 
      resp = cache.get_obj(url) 
      resp.set_data(content) 
      info_dict['without_content'] = False 
      if (verbose_level >= 4): 
         dbgprint('LocalCache_UpdateCache', url, content[:30], len(content)) 
      cache.put_obj(url, resp, obj_size=len(content), expires=get_expire_from_mime(parse.mime), last_modified=info_dict.get('last_modified'), info_dict=info_dict)"," 'Update the local cache for the given URL. 
 If the local cache is enabled and the method is GET, then the content is 
 updated in the local cache. 
 :param url: URL to update the local cache for 
 :param content: Content to update the local cache with 
 :param method: HTTP method to use 
 :type url: str 
 :type content: str 
 :type method: str 
 :rtype: None'","'æ´æ° local_cache ä¸­ç¼å­çèµæº, è¿½å content 
 å¨streamæ¨¡å¼ä¸­ä½¿ç¨'"
"def daemonize(pidfile=None, uid=None, umask=63): 
    if os.fork(): 
      return 1 
   os.setsid() 
   if os.fork(): 
      return 2 
   os.chdir('/') 
   null = os.open('/dev/null', os.O_RDWR) 
   for i in range(3): 
      try: 
         os.dup2(null, i) 
      except OSError as e: 
         if (e.errno != errno.EBADF): 
            raise 
   os.close(null) 
   warnings.filterwarnings('ignore') 
   if pidfile: 
      piddir = os.path.dirname(pidfile) 
      if (not os.path.exists(piddir)): 
         os.makedirs(piddir) 
      f = file(pidfile, 'w') 
      f.write(str(os.getpid())) 
      f.close() 
   if (umask is not None): 
      os.umask(umask) 
   if uid: 
      setugid(uid) 
   return None"," 'Sets the process up as a daemon. 
 This function does not actually daemonize the process, but rather sets 
 up the process so that it can be daemonized later. 
 If a pidfile is specified, the process will write its PID to the file. 
 If umask is specified, it will be used to set the umask for the process. 
 If uid is specified, it will be used to set the uid for the process. 
 If any of these arguments are not specified, they will be set to the 
 current process\'s values. 
 This function returns the pid of the child process. 
 :param pidfile: The name of the pidfile to write to. 
 :type pidfile: str 
 :param uid: The uid to set the process to. 
 :type uid: int 
 :param umask: The umask to set the process\'s umask to. 
 :type umask: int 
 :return: The pid of the child process. 
 :rtype: int'","'daemonize a Unix process. Set paranoid umask by default. 
 Return 1 in the original process, 2 in the first fork, and None for the 
 second fork (eg daemon process).'"
"def _blockdevice_id_to_dataset_id(blockdevice_id): 
    return UUID(blockdevice_id[len(_PREFIX):])"," 'Convert blockdevice_id to dataset_id. 
 :param blockdevice_id: blockdevice_id 
 :return: dataset_id'","'Computes a dataset_id from a blockdevice_id. 
 :param unicode blockdevice_id: The blockdevice_id to get the dataset_id 
 for. 
 :returns UUID: The corresponding dataset_id.'"
"def legitimize(text, os=platform.system()): 
    text = text.translate({0: None, ord('/'): '-', ord('|'): '-'}) 
   if (os == 'Windows'): 
      text = text.translate({ord(':'): '-', ord('*'): '-', ord('?'): '-', ord('\\'): '-', ord('""'): ""'"", ord('+'): '-', ord('<'): '-', ord('>'): '-', ord('['): '(', ord(']'): ')'}) 
   else: 
      if (os == 'Darwin'): 
         text = text.translate({ord(':'): '-'}) 
      if text.startswith('.'): 
         text = text[1:] 
   text = text[:82] 
   return text"," 'Returns a legitimized version of the text. 
 This function replaces the characters \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \', \',",'Converts a string to a valid filename.'
"def decrypt(stream, parameters): 
    encodedStream = '' 
   if ((parameters == None) or (parameters == {})): 
      return (0, stream) 
   elif ((not parameters.has_key('/Name')) or (parameters['/Name'] == None)): 
      return (0, stream) 
   else: 
      cryptFilterName = parameters['/Name'].getValue() 
      if (cryptFilterName == 'Identity'): 
         return (0, stream) 
      else: 
         return ((-1), 'Decrypt   not   supported   yet')"," 'Decrypts the stream using the parameters. 
 :param stream: The stream to decrypt. 
 :param parameters: The parameters to use. 
 :return: The decrypted stream, or (0, stream) if an error occurred.'","'Method to decrypt streams using a PDF security handler (NOT IMPLEMENTED YET) 
 @param stream: A PDF stream 
 @return: A tuple (status,statusContent), where statusContent is the decrypted PDF stream in case status = 0 or an error in case status = -1'"
"def UploadSeparatePatches(issue, rpc_server, patchset, data, options): 
    patches = SplitPatch(data) 
   rv = [] 
   for patch in patches: 
      set_status(('uploading   patch   for   ' + patch[0])) 
      if (len(patch[1]) > MAX_UPLOAD_SIZE): 
         print (('Not   uploading   the   patch   for   ' + patch[0]) + '   because   the   file   is   too   large.') 
         continue 
      form_fields = [('filename', patch[0])] 
      if (not options.download_base): 
         form_fields.append(('content_upload', '1')) 
      files = [('data', 'data.diff', patch[1])] 
      (ctype, body) = EncodeMultipartFormData(form_fields, files) 
      url = ('/%d/upload_patch/%d' % (int(issue), int(patchset))) 
      print ('Uploading   patch   for   ' + patch[0]) 
      response_body = rpc_server.Send(url, body, content_type=ctype) 
      lines = response_body.splitlines() 
      if ((not lines) or (lines[0] != 'OK')): 
         StatusUpdate(('      -->   %s' % response_body)) 
         sys.exit(1) 
      rv.append([lines[1], patch[0]]) 
   return rv"," 'Upload a separate patch to the server. 
 :param issue: The issue number. 
 :param rpc_server: The rpc server object. 
 :param patchset: The patchset number. 
 :param data: The data to upload. 
 :param options: The options dictionary.'","'Uploads a separate patch for each file in the diff output. 
 Returns a list of [patch_key, filename] for each file.'"
"@world.absorb 
 def css_html(css_selector, index=0): 
    assert is_css_present(css_selector) 
   return retry_on_exception((lambda : css_find(css_selector)[index].html))"," 'Return the HTML for the first element with the given CSS selector. 
 :param css_selector: CSS selector to match 
 :type css_selector: str 
 :param int index: 
 :type index: int 
 :return: 
 :rtype: str'",'Returns the HTML of a css_selector'
"def delete_multi(blob_keys, **options): 
    fut = delete_multi_async(blob_keys, **options) 
   fut.get_result()"," 'Delete multiple blobs. 
 :param blob_keys: List of blob keys to delete. 
 :type blob_keys: list 
 :param options: Options for the delete. 
 :type options: dict 
 :returns: Deleted blobs. 
 :rtype: list'","'Delete blobs from Blobstore. 
 Args: 
 blob_keys: A list of blob keys. 
 **options: Options for create_rpc().'"
"def show_check(request, name): 
    try: 
      check = CHECKS[name] 
   except KeyError: 
      raise Http404('No   check   matches   the   given   query.') 
   ignore = ('ignored' in request.GET) 
   url_params = {} 
   if ignore: 
      url_params['ignored'] = 'true' 
   checks = acl_checks(request.user).filter(check=name, ignore=ignore) 
   if ('language' in request.GET): 
      checks = checks.filter(language__code=request.GET['language']) 
      url_params['language'] = request.GET['language'] 
   if ('project' in request.GET): 
      return redirect_param('show_check_project', encode_optional(url_params), project=request.GET['project'], name=name) 
   checks = checks.values('project__slug').annotate(count=Count('id')) 
   return render(request, 'check.html', {'checks': checks, 'title': check.name, 'check': check, 'url_params': encode_optional(url_params)})"," 'Shows a check. 
 :param request: The request to render. 
 :param name: The name of the check to show. 
 :return: The rendered template. 
 :rtype: django.template.loader.render_to_string'",'Details about failing check.'
"def install(config, plugins): 
    try: 
      (installer, _) = plug_sel.choose_configurator_plugins(config, plugins, 'install') 
   except errors.PluginSelectionError as e: 
      return e.message 
   (domains, _) = _find_domains_or_certname(config, installer) 
   le_client = _init_le_client(config, authenticator=None, installer=installer) 
   assert (config.cert_path is not None) 
   le_client.deploy_certificate(domains, config.key_path, config.cert_path, config.chain_path, config.fullchain_path) 
   le_client.enhance_config(domains, config.chain_path)"," 'Installs a certificate for the given domains. 
 :param config: A :class:`~certbot.plugins.certbot.CertbotPlugin` object. 
 :param plugins: A :class:`~certbot.plugins.certbot.PluginManager` object. 
 :return: None'",'Install a previously obtained cert in a server.'
"def get_base_path(base): 
    base_parsed = urlparse(base) 
   if (not base_parsed.path): 
      sub_path = u'' 
   else: 
      sub_path = base_parsed.path 
   if sub_path.endswith(u'/'): 
      return sub_path 
   else: 
      return (sub_path + u'/')"," 'Return the base path for the given base URL. 
 :param base: The base URL. 
 :return: The base path. 
 :rtype: str'","'Return the path of a base URL if it contains one. 
 >>> get_base_path(\'http://some.site\') == \'/\' 
 True 
 >>> get_base_path(\'http://some.site/\') == \'/\' 
 True 
 >>> get_base_path(\'http://some.site/some/sub-path\') == \'/some/sub-path/\' 
 True 
 >>> get_base_path(\'http://some.site/some/sub-path/\') == \'/some/sub-path/\' 
 True'"
"def constant_time_compare(val1, val2): 
    if (len(val1) != len(val2)): 
      return False 
   result = 0 
   if (six.PY3 and isinstance(val1, bytes) and isinstance(val2, bytes)): 
      for (x, y) in zip(val1, val2): 
         result |= (x ^ y) 
   else: 
      for (x, y) in zip(val1, val2): 
         result |= (ord(x) ^ ord(y)) 
   return (result == 0)"," 'Returns true if val1 and val2 are equal, false otherwise. 
 This is a constant-time comparison, so it can be used to compare 
 values without worrying about the time taken. 
 The function assumes that val1 and val2 are the same length. 
 This is a Python 2/3 port of the C function of the same name. 
 :param val1: first value to compare 
 :param val2: second value to compare 
 :return: True if val1 and val2 are equal, False otherwise'","'Returns True if the two strings are equal, False otherwise. 
 The time taken is independent of the number of characters that match.'"
"def location_to_string(locationID): 
    loc = ['{}-'.format((locationID >> 24))] 
   while (locationID & 15728640): 
      if (len(loc) > 1): 
         loc.append('.') 
      loc.append('{}'.format(((locationID >> 20) & 15))) 
      locationID <<= 4 
   return ''.join(loc)"," 'Convert a location ID to a string representation. 
 Parameters 
 locationID : int 
 The location ID to convert. 
 Returns 
 string 
 The string representation of the location ID.'",'helper to calculate port and bus number from locationID'
"@login_check 
 def components_delete_layout(request): 
    try: 
      id = int(request.GET['id']) 
   except: 
      id = 0 
   if (not id): 
      return HttpResponseRedirect('/wc/components/show_layout') 
   ServerLayout.objects.filter(pk=id).delete() 
   return HttpResponseRedirect('/wc/components/show_layout')", 'Delete a layout component',''
"def _sphinx_version(): 
    (major, minor, micro, level, serial) = sys.version_info 
   release = ('%s%s' % (major, minor)) 
   if micro: 
      release += ('%s' % (micro,)) 
   if (level == 'candidate'): 
      release += ('rc%s' % (serial,)) 
   elif (level != 'final'): 
      release += ('%s%s' % (level[0], serial)) 
   return release", 'Return the version string of the Sphinx distribution.','Format sys.version_info to produce the Sphinx version string used to install the chm docs'
"def get_subordinate_users(user, site): 
    from cms.utils.page_permissions import get_change_permissions_id_list 
   try: 
      user_level = get_user_permission_level(user, site) 
   except NoPermissionsException: 
      qs = get_user_model().objects.distinct().filter(((Q(is_staff=True) & Q(pageuser__created_by=user)) & Q(pagepermission__page=None))) 
      qs = qs.exclude(pk=user.pk).exclude(groups__user__pk=user.pk) 
      return qs 
   if (user_level == ROOT_USER_LEVEL): 
      return get_user_model().objects.all() 
   page_id_allow_list = get_change_permissions_id_list(user, site, check_global=False) 
   qs = get_user_model().objects.distinct().filter(((Q(is_staff=True) & (Q(pagepermission__page__id__in=page_id_allow_list) & Q(pagepermission__page__depth__gte=user_level))) | (Q(pageuser__created_by=user) & Q(pagepermission__page=None)))) 
   qs = qs.exclude(pk=user.pk).exclude(groups__user__pk=user.pk) 
   return qs"," 'Return a queryset of users that are subordinate to the given user. 
 A user is considered subordinate to another user if they are a member of the 
 same group as the other user, or if the other user is a member of a group 
 that the first user is a member of. 
 :param user: The user to find subordinate users for. 
 :type user: User 
 :param site: The site to find users for. 
 :type site: Site 
 :return: A queryset of users that are subordinate to the given user. 
 :rtype: QuerySet[User]'","'Returns users queryset, containing all subordinate users to given user 
 including users created by given user and not assigned to any page. 
 Not assigned users must be returned, because they shouldn\'t get lost, and 
 user should still have possibility to see them. 
 Only users created_by given user which are on the same, or lover level are 
 returned. 
 If user haves global permissions or is a superuser, then he can see all the 
 users. 
 This function is currently used in PagePermissionInlineAdminForm for limit 
 users in permission combobox. 
 Example: 
 A,W                    level 0 
 /                              user    B,GroupE           level 1 
 Z       /                           C,X     D,Y,W                  level 2 
 Rules: W was created by user, Z was created by user, but is not assigned 
 to any page. 
 Will return [user, C, X, D, Y, Z]. W was created by user, but is also 
 assigned to higher level.'"
"def get_numpy_status(): 
    numpy_status = {} 
   try: 
      import numpy 
      numpy_version = numpy.__version__ 
      numpy_status['up_to_date'] = (parse_version(numpy_version) >= parse_version(NUMPY_MIN_VERSION)) 
      numpy_status['version'] = numpy_version 
   except ImportError: 
      traceback.print_exc() 
      numpy_status['up_to_date'] = False 
      numpy_status['version'] = '' 
   return numpy_status", 'Get the status of numpy.',"'Returns a dictionary containing a boolean specifying whether NumPy 
 is up-to-date, along with the version string (empty string if 
 not installed).'"
"def triangulate_point(x1, x2, P1, P2): 
    M = zeros((6, 6)) 
   M[:3, :4] = P1 
   M[3:, :4] = P2 
   M[:3, 4] = (- x1) 
   M[3:, 5] = (- x2) 
   (U, S, V) = linalg.svd(M) 
   X = V[(-1), :4] 
   return (X / X[3])"," 'Triangulate a point P1 and P2 using the line through P1 and P2. 
 Parameters 
 x1 : float 
 x1 coordinate of P1 
 x2 : float 
 x2 coordinate of P1 
 P1 : 2x1 array 
 coordinates of P1 
 P2 : 2x1 array 
 coordinates of P2 
 Returns 
 X : 1x4 array 
 coordinates of the point P1 and P2 
 Examples 
 >>> from sympy import Point, Triangulate 
 >>> P1 = Point(0, 0) 
 >>> P2 = Point(1, 0) 
 >>> T = Triangulate(P1, P2) 
 >>> T(P1, P2) 
 array([[ 0.0,  0.0,  1.0,  0.0], 
 [ 0.0,  0.0,  0.0,  1.0]])'","'Point pair triangulation from 
 least squares solution.'"
"def is_installed(pkg_name): 
    manager = MANAGER 
   with settings(hide('running', 'stdout', 'stderr', 'warnings'), warn_only=True): 
      res = run(('%(manager)s   -p   %(pkg_name)s' % locals())) 
   if (not res.succeeded): 
      return False 
   if pkg_name.startswith('='): 
      pkg_name = pkg_name[1:] 
   match = re.search(('\\n\\[ebuild   +(?P<code>\\w+)   *\\]   .*%(pkg_name)s.*' % locals()), res.stdout) 
   if (match and (match.groupdict()['code'] in ('U', 'R'))): 
      return True 
   else: 
      return False"," 'Check whether the package is installed. 
 :param pkg_name: name of the package 
 :type pkg_name: str 
 :return: True if the package is installed, False otherwise 
 :rtype: bool'",'Check if a Portage package is installed.'
"def knownfailureif(fail_condition, msg=None): 
    if (msg is None): 
      msg = 'Test   skipped   due   to   known   failure' 
   if callable(fail_condition): 
      fail_val = (lambda : fail_condition()) 
   else: 
      fail_val = (lambda : fail_condition) 
   def knownfail_decorator(f): 
      import nose 
      def knownfailer(*args, **kwargs): 
         if fail_val(): 
            raise KnownFailureTest(msg) 
         else: 
            return f(*args, **kwargs) 
      return nose.tools.make_decorator(f)(knownfailer) 
   return knownfail_decorator"," 'Decorator for tests that are known to fail. 
 The decorated test will be skipped and a KnownFailureTest will be raised. 
 If the decorated test is a function, it will be invoked with no arguments. 
 If the decorated test is a class, it will be invoked with no arguments and 
 no instance will be created. 
 If the decorated test is a method, it will be invoked with no arguments and 
 no instance will be created. 
 If the decorated test is a class or method, the decorator will be invoked with 
 the class or method name as the first argument. 
 If the decorated test is a function, the decorator will be invoked with the 
 function name as the first argument. 
 :param fail_condition: 
 A condition to check before the test is run. 
 :param msg: 
 A message to be displayed when the test is skipped. 
 :type fail_condition: 
 :type msg: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 Decorator for tests that are known to fail.'","'Make function raise KnownFailureTest exception if given condition is true. 
 If the condition is a callable, it is used at runtime to dynamically 
 make the decision. This is useful for tests that may require costly 
 imports, to delay the cost until the test suite is actually executed. 
 Parameters 
 fail_condition : bool or callable 
 Flag to determine whether to mark the decorated test as a known 
 failure (if True) or not (if False). 
 msg : str, optional 
 Message to give on raising a KnownFailureTest exception. 
 Default is None. 
 Returns 
 decorator : function 
 Decorator, which, when applied to a function, causes SkipTest 
 to be raised when `skip_condition` is True, and the function 
 to be called normally otherwise. 
 Notes 
 The decorator itself is decorated with the ``nose.tools.make_decorator`` 
 function in order to transmit function name, and various other metadata.'"
"@api_versions.wraps('2.10') 
 @utils.arg('name', metavar='<name>', help=_('Keypair   name   to   delete.')) 
 @utils.arg('--user', metavar='<user-id>', default=None, help=_('ID   of   key-pair   owner   (Admin   only).')) 
 def do_keypair_delete(cs, args): 
    cs.keypairs.delete(args.name, args.user)", 'Delete a keypair.','Delete keypair given by its name.'
"def _parse_date_rfc822(date): 
    daynames = set([u'mon', u'tue', u'wed', u'thu', u'fri', u'sat', u'sun']) 
   months = {u'jan': 1, u'feb': 2, u'mar': 3, u'apr': 4, u'may': 5, u'jun': 6, u'jul': 7, u'aug': 8, u'sep': 9, u'oct': 10, u'nov': 11, u'dec': 12} 
   parts = date.lower().split() 
   if (len(parts) < 5): 
      parts.extend((u'00:00:00', u'0000')) 
   if (parts[0][:3] in daynames): 
      parts = parts[1:] 
   if (len(parts) < 5): 
      return None 
   try: 
      day = int(parts[0]) 
   except ValueError: 
      if months.get(parts[0][:3]): 
         try: 
            day = int(parts[1]) 
         except ValueError: 
            return None 
         else: 
            parts[1] = parts[0] 
      else: 
         return None 
   month = months.get(parts[1][:3]) 
   if (not month): 
      return None 
   try: 
      year = int(parts[2]) 
   except ValueError: 
      return None 
   if (len(parts[2]) <= 2): 
      year += (1900, 2000)[(year < 90)] 
   timeparts = parts[3].split(u':') 
   timeparts = (timeparts + ([0] * (3 - len(timeparts)))) 
   try: 
      (hour, minute, second) = map(int, timeparts) 
   except ValueError: 
      return None 
   tzhour = 0 
   tzmin = 0 
   if parts[4].startswith(u'etc/'): 
      parts[4] = parts[4][4:] 
   if parts[4].startswith(u'gmt'): 
      parts[4] = (u''.join(parts[4][3:].split(u':')) or u'gmt') 
   if (parts[4] and (parts[4][0] in (u'-', u'+'))): 
      try: 
         tzhour = int(parts[4][1:3]) 
         tzmin = int(parts[4][3:]) 
      except ValueError: 
         return None 
      if parts[4].startswith(u'-'): 
         tzhour = (tzhour * (-1)) 
         tzmin = (tzmin * (-1)) 
   else: 
      tzhour = timezonenames.get(parts[4], 0) 
   try: 
      stamp = datetime.datetime(year, month, day, hour, minute, second) 
   except ValueError: 
      return None 
   delta = datetime.timedelta(0, 0, 0, 0, tzmin, tzhour) 
   try: 
      return (stamp - delta).utctimetuple() 
   except (OverflowError, ValueError): 
      return None"," 'Parse a date as RFC 822 format. 
 :param date: date as RFC 822 format 
 :return: date as datetime.datetime 
 :rtype: datetime.datetime'","'Parse RFC 822 dates and times 
 http://tools.ietf.org/html/rfc822#section-5 
 There are some formatting differences that are accounted for: 
 1. Years may be two or four digits. 
 2. The month and day can be swapped. 
 3. Additional timezone names are supported. 
 4. A default time and timezone are assumed if only a date is present.'"
"def Zero(dtype=None): 
    return Constant(0.0, dtype=dtype)"," 'Returns a constant with value 0.0 
 Parameters 
 dtype : data type 
 If None, the default data type is used. 
 Returns 
 A constant with value 0.0'","'Returns initializer that initializes array with the all-zero array. 
 Args: 
 dtype: Data type specifier. 
 Returns: 
 numpy.ndarray or cupy.ndarray: An initialized array.'"
"def griddata(points, values, xi, method='linear', fill_value=np.nan, rescale=False): 
    points = _ndim_coords_from_arrays(points) 
   if (points.ndim < 2): 
      ndim = points.ndim 
   else: 
      ndim = points.shape[(-1)] 
   if ((ndim == 1) and (method in ('nearest', 'linear', 'cubic'))): 
      from .interpolate import interp1d 
      points = points.ravel() 
      if isinstance(xi, tuple): 
         if (len(xi) != 1): 
            raise ValueError('invalid   number   of   dimensions   in   xi') 
         (xi,) = xi 
      idx = np.argsort(points) 
      points = points[idx] 
      values = values[idx] 
      if (method == 'nearest'): 
         fill_value = 'extrapolate' 
      ip = interp1d(points, values, kind=method, axis=0, bounds_error=False, fill_value=fill_value) 
      return ip(xi) 
   elif (method == 'nearest'): 
      ip = NearestNDInterpolator(points, values, rescale=rescale) 
      return ip(xi) 
   elif (method == 'linear'): 
      ip = LinearNDInterpolator(points, values, fill_value=fill_value, rescale=rescale) 
      return ip(xi) 
   elif ((method == 'cubic') and (ndim == 2)): 
      ip = CloughTocher2DInterpolator(points, values, fill_value=fill_value, rescale=rescale) 
      return ip(xi) 
   else: 
      raise ValueError(('Unknown   interpolation   method   %r   for   %d   dimensional   data' % (method, ndim)))"," 'Interpolate a 2D or 3D array of points and values. 
 Parameters 
 points : array 
 The coordinates of the points. 
 values : array 
 The values at the points. 
 xi : array, optional 
 The coordinates of the interpolation points. 
 method : str, optional 
 The interpolation method. 
 * `nearest` : Interpolate to nearest neighbor. 
 * `linear` : Interpolate using linear interpolation. 
 * `cubic` : Interpolate using cubic spline interpolation. 
 * `slinear` : Interpolate using smooth linear interpolation. 
 * `squad` : Interpolate using smooth quadratic interpolation. 
 * `sbiquad` : Interpolate using smooth bivariate quadratic interpolation. 
 * `sbiquad2` : Interpolate using smooth bivariate quadratic interpolation with 2D points. 
 * `spline` : Interpolate using cubic spline interpolation. 
 * `splrep` : Interpolate using cubic spline interpolation with a spline representation. 
 * `splprep` : Interpolate","'Interpolate unstructured D-dimensional data. 
 Parameters 
 points : ndarray of floats, shape (n, D) 
 Data point coordinates. Can either be an array of 
 shape (n, D), or a tuple of `ndim` arrays. 
 values : ndarray of float or complex, shape (n,) 
 Data values. 
 xi : ndarray of float, shape (M, D) 
 Points at which to interpolate data. 
 method : {\'linear\', \'nearest\', \'cubic\'}, optional 
 Method of interpolation. One of 
 ``nearest`` 
 return the value at the data point closest to 
 the point of interpolation.  See `NearestNDInterpolator` for 
 more details. 
 ``linear`` 
 tesselate the input point set to n-dimensional 
 simplices, and interpolate linearly on each simplex.  See 
 `LinearNDInterpolator` for more details. 
 ``cubic`` (1-D) 
 return the value determined from a cubic 
 spline. 
 ``cubic`` (2-D) 
 return the value determined from a 
 piecewise cubic, continuously differentiable (C1), and 
 approximately curvature-minimizing polynomial surface. See 
 `CloughTocher2DInterpolator` for more details. 
 fill_value : float, optional 
 Value used to fill in for requested points outside of the 
 convex hull of the input points.  If not provided, then the 
 default is ``nan``. This option has no effect for the 
 \'nearest\' method. 
 rescale : bool, optional 
 Rescale points to unit cube before performing interpolation. 
 This is useful if some of the input dimensions have 
 incommensurable units and differ by many orders of magnitude. 
 .. versionadded:: 0.14.0 
 Notes 
 .. versionadded:: 0.9 
 Examples 
 Suppose we want to interpolate the 2-D function 
 >>> def func(x, y): 
 ...     return x*(1-x)*np.cos(4*np.pi*x) * np.sin(4*np.pi*y**2)**2 
 on a grid in [0, 1]x[0, 1] 
 >>> grid_x, grid_y = np.mgrid[0:1:100j, 0:1:200j] 
 but we only know its values at 1000 data points: 
 >>> points = np.random.rand(1000, 2) 
 >>> values = func(points[:,0], points[:,1]) 
 This can be done with `griddata` -- below we try out all of the 
 interpolation methods: 
 >>> from scipy.interpolate import griddata 
 >>> grid_z0 = griddata(points, values, (grid_x, grid_y), method=\'nearest\') 
 >>> grid_z1 = griddata(points, values, (grid_x, grid_y), method=\'linear\') 
 >>> grid_z2 = griddata(points, values, (grid_x, grid_y), method=\'cubic\') 
 One can see that the exact result is reproduced by all of the 
 methods to some degree, but for this smooth function the piecewise 
 cubic interpolant gives the best results: 
 >>> import matplotlib.pyplot as plt 
 >>> plt.subplot(221) 
 >>> plt.imshow(func(grid_x, grid_y).T, extent=(0,1,0,1), origin=\'lower\') 
 >>> plt.plot(points[:,0], points[:,1], \'k.\', ms=1) 
 >>> plt.title(\'Original\') 
 >>> plt.subplot(222) 
 >>> plt.imshow(grid_z0.T, extent=(0,1,0,1), origin=\'lower\') 
 >>> plt.title(\'Nearest\') 
 >>> plt.subplot(223) 
 >>> plt.imshow(grid_z1.T, extent=(0,1,0,1), origin=\'lower\') 
 >>> plt.title(\'Linear\') 
 >>> plt.subplot(224) 
 >>> plt.imshow(grid_z2.T, extent=(0,1,0,1), origin=\'lower\') 
 >>> plt.title(\'Cubic\') 
 >>> plt.gcf().set_size_inches(6, 6) 
 >>> plt.show()'"
"def match(string, trie): 
    longest = None 
   for i in range(len(string)): 
      substr = string[:(i + 1)] 
      if (not trie.has_prefix(substr)): 
         break 
      if (substr in trie): 
         longest = substr 
   return longest"," 'Return the longest match in the trie. 
 Parameters 
 string : str 
 The string to search. 
 trie : Trie 
 The trie to search in. 
 Returns 
 str 
 The longest match in the trie.'","'match(string, trie) -> longest key or None 
 Find the longest key in the trie that matches the beginning of the 
 string.'"
"def _removeIfPresent(filename): 
    try: 
      os.unlink(filename) 
   except OSError as why: 
      if (why.errno == ENOENT): 
         return 0 
      else: 
         raise 
   else: 
      return 1", 'Remove a file if it exists',"'Attempt to remove a file, returning whether the file existed at 
 the time of the call. 
 str -> bool'"
"def coerce_kw_type(kw, key, type_, flexi_bool=True): 
    if ((key in kw) and (type(kw[key]) is not type_) and (kw[key] is not None)): 
      if ((type_ is bool) and flexi_bool): 
         kw[key] = asbool(kw[key]) 
      else: 
         kw[key] = type_(kw[key])", 'Coerce kwarg type to specified type.',"'If \'key\' is present in dict \'kw\', coerce its value to type \'type\_\' if 
 necessary.  If \'flexi_bool\' is True, the string \'0\' is considered false 
 when coercing to boolean.'"
"def test_validate_estimator_default(): 
    smt = SMOTETomek(random_state=RND_SEED) 
   (X_resampled, y_resampled) = smt.fit_sample(X, Y) 
   X_gt = np.array([[0.20622591, 0.0582794], [0.68481731, 0.51935141], [1.34192108, (-0.13367336)], [0.62366841, (-0.21312976)], [1.61091956, (-0.40283504)], [(-0.37162401), (-2.19400981)], [0.74680821, 1.63827342], [0.61472253, (-0.82309052)], [0.19893132, (-0.47761769)], [0.97407872, 0.44454207], [1.40301027, (-0.83648734)], [(-1.20515198), (-1.02689695)], [(-0.23374509), 0.18370049], [(-0.32635887), (-0.29299653)], [(-0.00288378), 0.84259929], [1.79580611, (-0.02219234)], [0.38307743, (-0.05670439)], [0.93976473, (-0.06570176)], [0.70319159, (-0.02571668)], [0.75052536, (-0.19246517)]]) 
   y_gt = np.array([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0]) 
   assert_allclose(X_resampled, X_gt, rtol=R_TOL) 
   assert_array_equal(y_resampled, y_gt)", 'Test the default estimator.','Test right processing while passing no object as initialization'
"def split_at_whitespace(string): 
    return re.split(__WHITESPACE_SPLIT, string)", 'Split a string at any whitespace character.',"'Like string.split(), but keeps empty words as empty words.'"
"def test_batch_normalized_mlp_mean_only_propagated_at_alloc(): 
    mlp = BatchNormalizedMLP([Tanh(), Tanh()], [5, 7, 9], mean_only=True) 
   assert mlp.mean_only 
   assert (not any((act.children[0].mean_only for act in mlp.activations))) 
   mlp.allocate() 
   assert all((act.children[0].mean_only for act in mlp.activations))", 'Test that the mean_only flag is propagated at allocation time','Test that setting mean_only on a BatchNormalizedMLP works.'
"def wrap_aws_conn(raw_conn): 
    def retry_if(ex): 
      'Retry   if   we   get   a   server   error   indicating   throttling.   Also\n                        handle   spurious   505s   that   are   thought   to   be   part   of   a   load\n                        balancer   issue   inside   AWS.' 
      return ((isinstance(ex, boto.exception.BotoServerError) and (('Throttling' in ex.body) or ('RequestExpired' in ex.body) or (ex.status == 505))) or (isinstance(ex, socket.error) and (ex.args in ((104, 'Connection   reset   by   peer'), (110, 'Connection   timed   out'))))) 
   return RetryWrapper(raw_conn, retry_if=retry_if, backoff=_EMR_BACKOFF, multiplier=_EMR_BACKOFF_MULTIPLIER, max_tries=_EMR_MAX_TRIES)"," 'Wrap a boto connection with a retry wrapper. 
 :param raw_conn: a boto connection object 
 :return: a boto connection object'","'Wrap a given boto Connection object so that it can retry when 
 throttled.'"
"def test_conversion_qtable_table(): 
    qt = QTable(MIXIN_COLS) 
   names = qt.colnames 
   for name in names: 
      qt[name].info.description = name 
   t = Table(qt) 
   for name in names: 
      assert (t[name].info.description == name) 
      if (name == 'quantity'): 
         assert np.all((t['quantity'] == qt['quantity'].value)) 
         assert np.all((t['quantity'].unit is qt['quantity'].unit)) 
         assert isinstance(t['quantity'], t.ColumnClass) 
      else: 
         assert_table_name_col_equal(t, name, qt[name]) 
   qt2 = QTable(qt) 
   for name in names: 
      assert (qt2[name].info.description == name) 
      assert_table_name_col_equal(qt2, name, qt[name])", 'Test that the conversion from QTable to Table works correctly.','Test that a table round trips from QTable => Table => QTable'
"@frame_transform_graph.transform(coord.StaticMatrixTransform, coord.Galactic, Sagittarius) 
 def galactic_to_sgr(): 
    return SGR_MATRIX"," 'Transform from Galactic to Sagittarius coordinates. 
 Parameters 
 frame : StaticMatrixTransform 
 The transform from Galactic to Sagittarius coordinates. 
 Returns 
 SGR_MATRIX : 4x4 numpy array 
 The transformation from Galactic to Sagittarius coordinates.'","'Compute the transformation matrix from Galactic spherical to 
 heliocentric Sgr coordinates.'"
"def _StructPackDecoder(wire_type, format): 
    value_size = struct.calcsize(format) 
   local_unpack = struct.unpack 
   def InnerDecode(buffer, pos): 
      new_pos = (pos + value_size) 
      result = local_unpack(format, buffer[pos:new_pos])[0] 
      return (result, new_pos) 
   return _SimpleDecoder(wire_type, InnerDecode)"," 'Returns a decoder that decodes data in the format specified by format. 
 The format string should be a string that can be passed to the struct 
 library. 
 The wire_type is the wire type to use when decoding this struct. 
 The format is the format string to use when decoding this struct.'","'Return a constructor for a decoder for a fixed-width field. 
 Args: 
 wire_type:  The field\'s wire type. 
 format:  The format string to pass to struct.unpack().'"
"def dt_row_cnt(reporter, check=(), quiet=True, utObj=None): 
    config = current.test_config 
   browser = config.browser 
   elem = browser.find_element_by_id('datatable_info') 
   details = elem.text 
   if (not quiet): 
      reporter(details) 
   words = details.split() 
   start = int(words[1]) 
   end = int(words[3]) 
   length = int(words[5]) 
   filtered = None 
   if (len(words) > 10): 
      filtered = int(words[9]) 
   if (check != ()): 
      if (len(check) == 3): 
         expected = ('Showing   %d   to   %d   of   %d   entries' % check) 
         actual = ('Showing   %d   to   %d   of   %d   entries' % (start, end, length)) 
         msg = (""Expected   result   of   '%s'   doesn't   equal   '%s'"" % (expected, actual)) 
         if (utObj != None): 
            utObj.assertEqual(((start, end, length) == check), msg) 
         else: 
            assert ((start, end, length) == check), msg 
      elif (len(check) == 4): 
         expected = ('Showing   %d   to   %d   of   %d   entries   (filtered   from   %d   total   entries)' % check) 
         if filtered: 
            actual = ('Showing   %d   to   %d   of   %d   entries   (filtered   from   %d   total   entries)' % (start, end, length, filtered)) 
         else: 
            actual = ('Showing   %d   to   %d   of   %d   entries' % (start, end, length)) 
         msg = (""Expected   result   of   '%s'   doesn't   equal   '%s'"" % (expected, actual)) 
         if (utObj != None): 
            utObj.assertEqual(((start, end, length) == check), msg) 
         else: 
            assert ((start, end, length, filtered) == check), msg 
   if (len(words) > 10): 
      return (start, end, length, filtered) 
   else: 
      return (start, end, length)", 'Returns the current row count of the table.','return the rows that are being displayed and the total rows in the dataTable'
"def pagerank(matrix, d_factor=0.85): 
    size = len(matrix) 
   epsilon = 0.0001 
   matrix = matrix.copy() 
   for i in xrange(0, size): 
      col_sum = matrix[:, i].sum() 
      if col_sum: 
         matrix[:, i] /= col_sum 
   e = (((1.0 - d_factor) / size) * numpy.ones((size, size))) 
   matrix = ((d_factor * matrix) + e) 
   result = (numpy.ones(size) / size) 
   prev = (numpy.ones(size) / size) 
   iteration = 0 
   while True: 
      result = numpy.dot(matrix, result) 
      result /= result.sum() 
      diff = numpy.abs((result - prev)).sum() 
      print ('Iteration   %d,   change   %f' % (iteration, diff)) 
      if (diff < epsilon): 
         break 
      prev = result 
      iteration += 1 
   return result"," 'Compute PageRank using the method described in the paper 
 ""PageRank: Bringing Order to the Web"" by L. Page and S. Brin 
 (http://www.google.com/intl/en/us/technology/pdfs/page-rank-patent.pdf). 
 This is the method used by the Google PageRank Toolbar. 
 Parameters 
 matrix : numpy.ndarray 
 A symmetric matrix of values. 
 d_factor : float, optional 
 The decay factor used to calculate the PageRank. 
 Returns 
 result : numpy.ndarray 
 The PageRank values. 
 Notes 
 The algorithm is iterative, so the result will not be unique. 
 The initial PageRank values are not necessarily 1.0, but 
 they will be close to 1.0 after a few iterations. 
 References 
 http://en.wikipedia.org/wiki/PageRank 
 http://www.google.com/intl/en/us/technology/pdfs/page-rank-patent.pdf 
 http://en.wikipedia.","'Calculate the pagerank vector of a given adjacency matrix (using 
 the power method). 
 :param matrix: an adjacency matrix 
 :param d_factor: the damping factor'"
"def login(studentid, studentpwd, url, session, proxy): 
    if (not proxy): 
      pre_login = session.get(url, allow_redirects=False, timeout=5) 
   else: 
      pre_login = session.get(url, allow_redirects=False, timeout=5, proxies=app.config['SCHOOL_LAN_PROXIES']) 
   pre_login.raise_for_status() 
   pre_login_soup = BeautifulSoup(pre_login.text, 'html.parser', parse_only=SoupStrainer('input')) 
   login_view_state = pre_login_soup.find(attrs={'name': '__VIEWSTATE'})['value'] 
   payload = {'__VIEWSTATE': login_view_state, 'TextBox1': studentid, 'TextBox2': studentpwd, 'RadioButtonList1': u'\u5b66\u751f', 'Button1': u'   \u767b   \u5f55   '} 
   if (not proxy): 
      res = session.post(url, data=payload, allow_redirects=False, timeout=5) 
   else: 
      res = session.post(url, data=payload, allow_redirects=False, timeout=5, proxies=app.config['SCHOOL_LAN_PROXIES']) 
   return res", 'Login to the school portal.','ç»å½è·å cookie'
"def main(global_config, **settings): 
    config = Configurator(root_factory=root_factory, settings=settings) 
   config.include('pyramid_chameleon') 
   config.add_static_view('static', 'static', cache_max_age=3600) 
   config.scan() 
   return config.make_wsgi_app()", 'Create the WSGI application.','This function returns a Pyramid WSGI application.'
"def isPointInsideLoop(loop, point): 
    return ((getNumberOfIntersectionsToLeft(loop, point) % 2) == 1)"," 'Returns True if point is inside the loop. 
 :param loop: the loop 
 :param point: the point to check 
 :returns: True if point is inside the loop, False otherwise 
 :rtype: bool'",'Determine if a point is inside another loop.'
"@frappe.whitelist() 
 def enroll_student(source_name): 
    student = get_mapped_doc(u'Student   Applicant', source_name, {u'Student   Applicant': {u'doctype': u'Student', u'field_map': {u'name': u'student_applicant'}}}, ignore_permissions=True) 
   student.save() 
   program_enrollment = frappe.new_doc(u'Program   Enrollment') 
   program_enrollment.student = student.name 
   program_enrollment.student_name = student.title 
   program_enrollment.program = frappe.db.get_value(u'Student   Applicant', source_name, u'program') 
   return program_enrollment"," 'Enrolls a student in a program. 
 :param source_name: Name of the student applicant 
 :returns: Program enrollment'","'Creates a Student Record and returns a Program Enrollment. 
 :param source_name: Student Applicant.'"
"def fill_gaps(*args, **kwargs): 
    generator = fill_gaps_generator(*args, **kwargs) 
   return list(generator)"," 'Returns a generator that returns the missing values in the input 
 generator. 
 Parameters 
 *args : iterable 
 The generator to fill gaps from. 
 **kwargs : dict 
 Keyword arguments to pass to the fill_gaps_generator function. 
 Examples 
 >>> from itertools import islice 
 >>> from itertools import chain 
 >>> from itertools import count 
 >>> from itertools import repeat 
 >>> from itertools import fill_gaps 
 >>> a = count(0) 
 >>> b = fill_gaps(a) 
 >>> list(b) 
 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31",'Listify the generator returned by fill_gaps_generator for `memoize`.'
"@verbose 
 def tweets_by_user_demo(user='NLTK_org', count=200): 
    oauth = credsfromfile() 
   client = Query(**oauth) 
   client.register(TweetWriter()) 
   client.user_tweets(user, count)", 'Demo of the :class:`~twitter.Twitter` API.','Use the REST API to search for past tweets by a given user.'
"def grad_clip(x, lower_bound, upper_bound): 
    return GradClip(lower_bound, upper_bound)(x)"," 'Applies gradient clipping to a tensor. 
 Parameters 
 x : tensor 
 Tensor to clip. 
 lower_bound : scalar 
 Lower bound of the gradient clipping. 
 upper_bound : scalar 
 Upper bound of the gradient clipping. 
 Returns 
 out : tensor 
 Gradient-clipped tensor. 
 Examples 
 >>> x = Tensor(\'x\', [3, 4, 5]) 
 >>> grad_clip(x, 0, 10) 
 Tensor([[0, 0, 0], 
 [0, 0, 0], 
 [0, 0, 0]])'","'This op do a view in the forward, but clip the gradient. 
 This is an elemwise operation. 
 :param x: the variable we want its gradient inputs clipped 
 :param lower_bound: The lower bound of the gradient value 
 :param upper_bound: The upper bound of the gradient value. 
 :examples: 
 x = theano.tensor.scalar() 
 z = theano.tensor.grad(grad_clip(x, -1, 1)**2, x) 
 z2 = theano.tensor.grad(x**2, x) 
 f = theano.function([x], outputs = [z, z2]) 
 print(f(2.0))  # output (1.0, 4.0) 
 :note: We register an opt in tensor/opt.py that remove the GradClip. 
 So it have 0 cost in the forward and only do work in the grad.'"
"def merge(file, names, config, coord): 
    inputs = get_tiles(names, config, coord) 
   output = {'type': 'Topology', 'transform': inputs[0]['transform'], 'objects': dict(), 'arcs': list()} 
   for (name, input) in zip(names, inputs): 
      for (index, object) in enumerate(input['objects'].values()): 
         if (len(input['objects']) > 1): 
            output['objects'][('%(name)s-%(index)d' % locals())] = object 
         else: 
            output['objects'][name] = object 
         for geometry in object['geometries']: 
            update_arc_indexes(geometry, output['arcs'], input['arcs']) 
   file.write(json.dumps(output, separators=(',', ':')).encode('utf8'))"," 'Merge topology files. 
 :param file: file to write to 
 :param names: list of names to write 
 :param config: config object 
 :param coord: coord object 
 :return: None'","'Retrieve a list of TopoJSON tile responses and merge them into one. 
 get_tiles() retrieves data and performs basic integrity checks.'"
"def script(vm_): 
    return salt.utils.cloud.os_script(config.get_cloud_config_value('script', vm_, __opts__), vm_, __opts__, salt.utils.cloud.salt_config_to_yaml(salt.utils.cloud.minion_config(__opts__, vm_)))", 'Runs a script on the VM.','Return the script deployment object'
"def _do_surface_dots_subset(intrad, rsurf, rmags, rref, refl, lsurf, rlens, this_nn, cosmags, ws, volume, lut, n_fact, ch_type, idx): 
    products = _fast_sphere_dot_r0(intrad, rsurf, rmags, lsurf, rlens, this_nn, cosmags, None, ws, volume, lut, n_fact, ch_type).T 
   if (rref is not None): 
      raise NotImplementedError 
   return products"," 'Subset of the surface dots. 
 Parameters 
 intrad : ndarray 
 The intrinsic parameters of the reference image. 
 rsurf : ndarray 
 The surface radii of the reference image. 
 rmags : ndarray 
 The magnitudes of the reference image. 
 rref : ndarray 
 The reference image. 
 refl : ndarray 
 The reference image. 
 lsurf : ndarray 
 The surface radii of the source image. 
 rlens : ndarray 
 The radii of the lensing shear. 
 this_nn : ndarray 
 The normal vectors of the source image. 
 cosmags : ndarray 
 The cosmological parameters of the source image. 
 ws : ndarray 
 The weights of the source image. 
 volume : ndarray 
 The volume of the source image. 
 lut : ndarray 
 The look-up table. 
 n_fact : int 
 The number of iterations. 
 ch_type : int 
 The type of the surface dots.","'Helper for parallelization. 
 Parameters 
 refl : array | None 
 If ch_type is \'eeg\', the magnitude of position vector of the 
 virtual reference (never used). 
 lsurf : array 
 Magnitude of position vector of the surface points. 
 rlens : list of arrays of length n_coils 
 Magnitude of position vector. 
 this_nn : array, shape (n_vertices, 3) 
 Surface normals. 
 cosmags : list of array. 
 Direction of the integration points in the coils. 
 ws : list of array 
 Integration weights of the coils. 
 volume : bool 
 If True, compute volume integral. 
 lut : callable 
 Look-up table for evaluating Legendre polynomials. 
 n_fact : array 
 Coefficients in the integration sum. 
 ch_type : str 
 \'meg\' or \'eeg\' 
 idx : array, shape (n_coils x 1) 
 Index of coil. 
 Returns 
 products : array, shape (n_coils, n_coils) 
 The integration products.'"
"def gf_pow_mod(f, n, g, p, K): 
    if (not n): 
      return [K.one] 
   elif (n == 1): 
      return gf_rem(f, g, p, K) 
   elif (n == 2): 
      return gf_rem(gf_sqr(f, p, K), g, p, K) 
   h = [K.one] 
   while True: 
      if (n & 1): 
         h = gf_mul(h, f, p, K) 
         h = gf_rem(h, g, p, K) 
         n -= 1 
      n >>= 1 
      if (not n): 
         break 
      f = gf_sqr(f, p, K) 
      f = gf_rem(f, g, p, K) 
   return h"," 'Compute ``f**n`` mod ``g`` in ``GF(p)``. 
 Examples 
 >>> from sympy.polys.galoistools import gf_pow_mod 
 >>> gf_pow_mod(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_sqr(gf_","'Compute ``f**n`` in ``GF(p)[x]/(g)`` using repeated squaring. 
 Given polynomials ``f`` and ``g`` in ``GF(p)[x]`` and a non-negative 
 integer ``n``, efficiently computes ``f**n (mod g)`` i.e. the remainder 
 of ``f**n`` from division by ``g``, using the repeated squaring algorithm. 
 Examples 
 >>> from sympy.polys.domains import ZZ 
 >>> from sympy.polys.galoistools import gf_pow_mod 
 >>> gf_pow_mod(ZZ.map([3, 2, 4]), 3, ZZ.map([1, 1]), 5, ZZ) 
 References 
 1. [Gathen99]_'"
"def computeOverlap(x, y): 
    return (x & y).sum()"," 'Computes the overlap between two sets. 
 Parameters 
 x : set 
 The first set 
 y : set 
 The second set 
 Returns 
 The overlap between the two sets. 
 Examples 
 >>> from sympy.combinatorics.permutations import Permutation 
 >>> from sympy.combinatorics.permutations import computeOverlap 
 >>> Permutation(2, 3).computeOverlap(Permutation(2, 3)) 
 1 
 >>> Permutation(2, 3).computeOverlap(Permutation(2, 4)) 
 0'","'Given two binary arrays, compute their overlap. The overlap is the number 
 of bits where x[i] and y[i] are both 1'"
"def list_job(jid, ext_source=None, display_progress=False): 
    ret = {'jid': jid} 
   mminion = salt.minion.MasterMinion(__opts__) 
   returner = _get_returner((__opts__['ext_job_cache'], ext_source, __opts__['master_job_cache'])) 
   if display_progress: 
      __jid_event__.fire_event({'message': 'Querying   returner:   {0}'.format(returner)}, 'progress') 
   job = mminion.returners['{0}.get_load'.format(returner)](jid) 
   ret.update(_format_jid_instance(jid, job)) 
   ret['Result'] = mminion.returners['{0}.get_jid'.format(returner)](jid) 
   fstr = '{0}.get_endtime'.format(__opts__['master_job_cache']) 
   if (__opts__.get('job_cache_store_endtime') and (fstr in mminion.returners)): 
      endtime = mminion.returners[fstr](jid) 
      if endtime: 
         ret['EndTime'] = endtime 
   return ret"," 'Retrieve a job instance by jid. 
 :param jid: job id 
 :param ext_source: source of job data 
 :param display_progress: display progress 
 :returns: dictionary of job information'","'List a specific job given by its jid 
 ext_source 
 If provided, specifies which external job cache to use. 
 display_progress : False 
 If ``True``, fire progress events. 
 .. versionadded:: 2015.8.8 
 CLI Example: 
 .. code-block:: bash 
 salt-run jobs.list_job 20130916125524463507 
 salt-run jobs.list_job 20130916125524463507 --out=pprint'"
"def DEFINE_choice(name, default, choices, help): 
    CONFIG.AddOption(type_info.Choice(name=name, default=default, choices=choices, description=help))"," 'Define a choice in the config file. 
 :param name: name of the option 
 :param default: default value of the option 
 :param choices: choices of the option 
 :param help: help message for the option'",'A helper for defining choice string options.'
"def _fulfills_version_spec(versions, oper, desired_version, ignore_epoch=False): 
    cmp_func = __salt__.get('pkg.version_cmp') 
   if salt.utils.is_freebsd(): 
      if (isinstance(versions, dict) and ('version' in versions)): 
         versions = versions['version'] 
   for ver in versions: 
      if salt.utils.compare_versions(ver1=ver, oper=oper, ver2=desired_version, cmp_func=cmp_func, ignore_epoch=ignore_epoch): 
         return True 
   return False"," 'Checks if a version fulfills the given version spec. 
 If the version spec is a dict, the keys are the version components and 
 the values are the version components to compare to. 
 If the version spec is a string, the string is assumed to be the version 
 of the package to check. 
 If the version spec is a list, the list is assumed to be a list of 
 version components to compare to. 
 If the version spec is a tuple, the tuple is assumed to be a tuple of 
 version components to compare to. 
 If the version spec is a dict, the dict is assumed to be a dict of 
 version components to compare to. 
 If the version spec is a list, the list is assumed to be a list of 
 version components to compare to. 
 If the version spec is a tuple, the tuple is assumed to be a tuple of 
 version components to compare to. 
 If the version spec is a dict, the dict is assumed to be a dict of 
 version components to compare to. 
 If the version spec is a list, the list is assumed to be a list of 
 version components to compare","'Returns True if any of the installed versions match the specified version, 
 otherwise returns False'"
"def list_exports(exports='/etc/exports'): 
    ret = {} 
   with salt.utils.fopen(exports, 'r') as efl: 
      for line in efl.read().splitlines(): 
         if (not line): 
            continue 
         if line.startswith('#'): 
            continue 
         comps = line.split() 
         ret[comps[0]] = [] 
         newshares = [] 
         for perm in comps[1:]: 
            if perm.startswith('/'): 
               newshares.append(perm) 
               continue 
            permcomps = perm.split('(') 
            permcomps[1] = permcomps[1].replace(')', '') 
            hosts = permcomps[0].split(',') 
            options = permcomps[1].split(',') 
            ret[comps[0]].append({'hosts': hosts, 'options': options}) 
         for share in newshares: 
            ret[share] = ret[comps[0]] 
   return ret"," 'Get list of exports from /etc/exports. 
 Returns: 
 dict: 
 The exports file contents as a dictionary.'","'List configured exports 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' nfs.list_exports'"
"def mkdir(path, owner=None, grant_perms=None, deny_perms=None, inheritance=True): 
    drive = os.path.splitdrive(path)[0] 
   if (not os.path.isdir(drive)): 
      raise CommandExecutionError('Drive   {0}   is   not   mapped'.format(drive)) 
   path = os.path.expanduser(path) 
   path = os.path.expandvars(path) 
   if (not os.path.isdir(path)): 
      os.mkdir(path) 
      if owner: 
         salt.utils.win_dacl.set_owner(path, owner) 
      set_perms(path, grant_perms, deny_perms, inheritance) 
   return True"," 'Create a directory on the local file system. 
 If the directory already exists, it is not deleted and the function 
 returns True. 
 :param path: Path to the directory 
 :param owner: The owner of the directory 
 :param grant_perms: The permissions to grant 
 :param deny_perms: The permissions to deny 
 :param inheritance: If True, inheritance is enabled for the directory. 
 If False, inheritance is disabled for the directory. 
 :return: True if the directory is created, False otherwise.'","'Ensure that the directory is available and permissions are set. 
 Args: 
 path (str): The full path to the directory. 
 owner (str): The owner of the directory. If not passed, it will be the 
 account that created the directory, likely SYSTEM 
 grant_perms (dict): A dictionary containing the user/group and the basic 
 permissions to grant, ie: ``{\'user\': {\'perms\': \'basic_permission\'}}``. 
 You can also set the ``applies_to`` setting here. The default is 
 ``this_folder_subfolders_files``. Specify another ``applies_to`` setting 
 like this: 
 .. code-block:: yaml 
 {\'user\': {\'perms\': \'full_control\', \'applies_to\': \'this_folder\'}} 
 To set advanced permissions use a list for the ``perms`` parameter, ie: 
 .. code-block:: yaml 
 {\'user\': {\'perms\': [\'read_attributes\', \'read_ea\'], \'applies_to\': \'this_folder\'}} 
 deny_perms (dict): A dictionary containing the user/group and 
 permissions to deny along with the ``applies_to`` setting. Use the same 
 format used for the ``grant_perms`` parameter. Remember, deny 
 permissions supersede grant permissions. 
 inheritance (bool): If True the object will inherit permissions from the 
 parent, if False, inheritance will be disabled. Inheritance setting will 
 not apply to parent directories if they must be created 
 Returns: 
 bool: True if successful, otherwise raise an error 
 CLI Example: 
 .. code-block:: bash 
 # To grant the \'Users\' group \'read & execute\' permissions. 
 salt \'*\' file.mkdir C:\Temp\ Administrators ""{\'Users\': {\'perms\': \'read_execute\'}}"" 
 # Locally using salt call 
 salt-call file.mkdir C:\Temp\ Administrators ""{\'Users\': {\'perms\': \'read_execute\', \'applies_to\': \'this_folder_only\'}}"" 
 # Specify advanced attributes with a list 
 salt \'*\' file.mkdir C:\Temp\ Administrators ""{\'jsnuffy\': {\'perms\': [\'read_attributes\', \'read_ea\'], \'applies_to\': \'this_folder_only\'}}""'"
"@snippet 
 def client_list_subscriptions(client, to_delete): 
    def do_something_with(sub): 
      pass 
   for subscription in client.list_subscriptions(): 
      do_something_with(subscription)", 'List all subscriptions for a client.','List all subscriptions for a project.'
"def article(word, function=INDEFINITE): 
    return (((function == DEFINITE) and definite_article(word)) or indefinite_article(word))"," 'Returns the article for the given word. 
 :param word: the word to check. 
 :param function: the function to use for the article. 
 :return: the article for the word. 
 :rtype: str'",'Returns the indefinite or definite article for the given word.'
"@csrf_exempt 
 def notify_url_handler(request): 
    logger1.info('>>notify   url   handler   start...') 
   if (request.method == 'POST'): 
      if notify_verify(request.POST): 
         logger1.info('pass   verification...') 
         tn = request.POST.get('out_trade_no') 
         logger1.info(('Change   the   status   of   bill   %s' % tn)) 
         bill = Bill.objects.get(pk=tn) 
         trade_status = request.POST.get('trade_status') 
         logger1.info(('the   status   of   bill   %s   changed   to   %s' % (tn, trade_status))) 
         bill.trade_status = trade_status 
         bill.save() 
         trade_no = request.POST.get('trade_no') 
         if (trade_status == 'WAIT_SELLER_SEND_GOODS'): 
            logger1.info('It   is   WAIT_SELLER_SEND_GOODS,   so   upgrade   bill') 
            upgrade_bill(bill, ((6 * 30) + 7)) 
            url = send_goods_confirm_by_platform(trade_no) 
            logger1.info(('send   goods   confirmation.   %s' % url)) 
            req = urllib.urlopen(url) 
            return HttpResponse('success') 
         else: 
            logger1.info(('##info:   Status   of   %s' % trade_status)) 
            return HttpResponse('success') 
   return HttpResponse('fail')"," 'Handler for notify url. 
 :param request: 
 :return: 
 :rtype: 
 :raises: 
 :example: 
 >>> url = \'http://www.example.com/notify_url_handler\' 
 >>> request = HttpRequest() 
 >>> request.POST = \'out_trade_no=123456&trade_status=WAIT_SELLER_SEND_GOODS&trade_no=1234567890123456\' 
 >>> request.method = \'POST\' 
 >>> request.get_full_path() 
 \'http://www.example.com/notify_url_handler?out_trade_no=123456&trade_status=WAIT_SELLER_SEND_GOODS&trade_no=1234567890123456\' 
 >>> request.get_full_url() 
 \'http://www.example.com/notify_url_handler?out_trade_","'Handler for notify_url for asynchronous updating billing information. 
 Logging the information.'"
"def validate_bool_maybe_none(b): 
    if (type(b) is str): 
      b = b.lower() 
   if (b == 'none'): 
      return None 
   if (b in ('t', 'y', 'yes', 'on', 'true', '1', 1, True)): 
      return True 
   elif (b in ('f', 'n', 'no', 'off', 'false', '0', 0, False)): 
      return False 
   else: 
      raise ValueError(('Could   not   convert   ""%s""   to   boolean' % b))"," 'Validates a boolean value. 
 :param b: the value to validate 
 :return: the validated boolean value 
 :raise ValueError: if the value cannot be converted to boolean'",'Convert b to a boolean or raise'
"def test_frame_init(): 
    sc = SkyCoord(RA, DEC, frame=u'icrs') 
   assert (sc.frame.name == u'icrs') 
   sc = SkyCoord(RA, DEC, frame=ICRS) 
   assert (sc.frame.name == u'icrs') 
   with catch_warnings(AstropyDeprecationWarning) as w: 
      sc = SkyCoord(RA, DEC, u'icrs') 
   assert (sc.frame.name == u'icrs') 
   assert (len(w) == 1) 
   assert (str(w[0].message) == FRAME_DEPRECATION_WARNING) 
   with catch_warnings(AstropyDeprecationWarning) as w: 
      sc = SkyCoord(RA, DEC, ICRS) 
   assert (sc.frame.name == u'icrs') 
   assert (len(w) == 1) 
   assert (str(w[0].message) == FRAME_DEPRECATION_WARNING) 
   with catch_warnings(AstropyDeprecationWarning) as w: 
      sc = SkyCoord(u'icrs', RA, DEC) 
   assert (sc.frame.name == u'icrs') 
   assert (len(w) == 1) 
   assert (str(w[0].message) == FRAME_DEPRECATION_WARNING) 
   with catch_warnings(AstropyDeprecationWarning) as w: 
      sc = SkyCoord(ICRS, RA, DEC) 
   assert (sc.frame.name == u'icrs') 
   assert (len(w) == 1) 
   assert (str(w[0].message) == FRAME_DEPRECATION_WARNING) 
   sc = SkyCoord(sc) 
   assert (sc.frame.name == u'icrs') 
   sc = SkyCoord(C_ICRS) 
   assert (sc.frame.name == u'icrs') 
   SkyCoord(C_ICRS, frame=u'icrs') 
   assert (sc.frame.name == u'icrs') 
   with pytest.raises(ValueError) as err: 
      SkyCoord(C_ICRS, frame=u'galactic') 
   assert (u'Cannot   override   frame=' in str(err))", 'Test SkyCoord initialization with frame keyword','Different ways of providing the frame.'
"def random_bytes(n): 
    return os.urandom(n)", 'Returns n random bytes.','Returns n bytes of strong random data.'
"def has_player(accessing_obj, accessed_obj, *args, **kwargs): 
    return (hasattr(accessing_obj, 'has_player') and accessing_obj.has_player)", 'Return True if accessing_obj has a player attribute.',"'Only returns true if accessing_obj has_player is true, that is, 
 this is a player-controlled object. It fails on actual players! 
 This is a useful lock for traverse-locking Exits to restrain NPC 
 mobiles from moving outside their areas.'"
"def CalculateGeneratorInputInfo(params): 
    toplevel = params['options'].toplevel_dir 
   qualified_out_dir = os.path.normpath(os.path.join(toplevel, ComputeOutputDir(params), 'gypfiles')) 
   global generator_filelist_paths 
   generator_filelist_paths = {'toplevel': toplevel, 'qualified_out_dir': qualified_out_dir}", 'Returns generator input info.','Called by __init__ to initialize generator values based on params.'
"@testing.requires_testing_data 
 def test_add_reference(): 
    raw = read_raw_fif(fif_fname, preload=True) 
   picks_eeg = pick_types(raw.info, meg=False, eeg=True) 
   assert_raises(ValueError, add_reference_channels, raw, raw.info['ch_names'][0]) 
   raw_ref = add_reference_channels(raw, 'Ref', copy=True) 
   assert_equal(raw_ref._data.shape[0], (raw._data.shape[0] + 1)) 
   assert_array_equal(raw._data[picks_eeg, :], raw_ref._data[picks_eeg, :]) 
   _check_channel_names(raw_ref, 'Ref') 
   orig_nchan = raw.info['nchan'] 
   raw = add_reference_channels(raw, 'Ref', copy=False) 
   assert_array_equal(raw._data, raw_ref._data) 
   assert_equal(raw.info['nchan'], (orig_nchan + 1)) 
   _check_channel_names(raw, 'Ref') 
   assert_allclose(raw.info['chs'][(-1)]['loc'][:3], raw.info['chs'][picks_eeg[0]]['loc'][3:6], 1e-06) 
   ref_idx = raw.ch_names.index('Ref') 
   (ref_data, _) = raw[ref_idx] 
   assert_array_equal(ref_data, 0) 
   raw = read_raw_fif(fif_fname).crop(0, 1).load_data() 
   picks_eeg = pick_types(raw.info, meg=False, eeg=True) 
   del raw.info['dig'] 
   raw_ref = add_reference_channels(raw, 'Ref', copy=True) 
   assert_equal(raw_ref._data.shape[0], (raw._data.shape[0] + 1)) 
   assert_array_equal(raw._data[picks_eeg, :], raw_ref._data[picks_eeg, :]) 
   _check_channel_names(raw_ref, 'Ref') 
   orig_nchan = raw.info['nchan'] 
   raw = add_reference_channels(raw, 'Ref', copy=False) 
   assert_array_equal(raw._data, raw_ref._data) 
   assert_equal(raw.info['nchan'], (orig_nchan + 1)) 
   _check_channel_names(raw, 'Ref') 
   assert_raises(ValueError, add_reference_channels, raw, raw.info['ch_names'][0]) 
   raw_ref = add_reference_channels(raw, ['M1', 'M2'], copy=True) 
   _check_channel_names(raw_ref, ['M1', 'M2']) 
   assert_equal(raw_ref._data.shape[0], (raw._data.shape[0] + 2)) 
   assert_array_equal(raw._data[picks_eeg, :], raw_ref._data[picks_eeg, :]) 
   assert_array_equal(raw_ref._data[(-2):, :], 0) 
   raw = add_reference_channels(raw, ['M1', 'M2'], copy=False) 
   _check_channel_names(raw, ['M1', 'M2']) 
   ref_idx = raw.ch_names.index('M1') 
   ref_idy = raw.ch_names.index('M2') 
   (ref_data, _) = raw[[ref_idx, ref_idy]] 
   assert_array_equal(ref_data, 0) 
   raw = read_raw_fif(fif_fname, preload=True) 
   events = read_events(eve_fname) 
   picks_eeg = pick_types(raw.info, meg=False, eeg=True) 
   epochs = Epochs(raw, events=events, event_id=1, tmin=(-0.2), tmax=0.5, picks=picks_eeg, preload=True) 
   assert_raises(RuntimeError, add_reference_channels, epochs, 'Ref') 
   epochs = Epochs(raw, events=events, event_id=1, tmin=(-0.2), tmax=0.5, picks=picks_eeg, preload=True, proj='delayed') 
   epochs_ref = add_reference_channels(epochs, 'Ref', copy=True) 
   assert_equal(epochs_ref._data.shape[1], (epochs._data.shape[1] + 1)) 
   _check_channel_names(epochs_ref, 'Ref') 
   ref_idx = epochs_ref.ch_names.index('Ref') 
   ref_data = epochs_ref.get_data()[:, ref_idx, :] 
   assert_array_equal(ref_data, 0) 
   picks_eeg = pick_types(epochs.info, meg=False, eeg=True) 
   assert_array_equal(epochs.get_data()[:, picks_eeg, :], epochs_ref.get_data()[:, picks_eeg, :]) 
   raw = read_raw_fif(fif_fname, preload=True) 
   events = read_events(eve_fname) 
   picks_eeg = pick_types(raw.info, meg=False, eeg=True) 
   epochs = Epochs(raw, events=events, event_id=1, tmin=(-0.2), tmax=0.5, picks=picks_eeg, preload=True, proj='delayed') 
   with warnings.catch_warnings(record=True): 
      epochs_ref = add_reference_channels(epochs, ['M1', 'M2'], copy=True) 
   assert_equal(epochs_ref._data.shape[1], (epochs._data.shape[1] + 2)) 
   _check_channel_names(epochs_ref, ['M1', 'M2']) 
   ref_idx = epochs_ref.ch_names.index('M1') 
   ref_idy = epochs_ref.ch_names.index('M2') 
   assert_equal(epochs_ref.info['chs'][ref_idx]['ch_name'], 'M1') 
   assert_equal(epochs_ref.info['chs'][ref_idy]['ch_name'], 'M2') 
   ref_data = epochs_ref.get_data()[:, [ref_idx, ref_idy], :] 
   assert_array_equal(ref_data, 0) 
   picks_eeg = pick_types(epochs.info, meg=False, eeg=True) 
   assert_array_equal(epochs.get_data()[:, picks_eeg, :], epochs_ref.get_data()[:, picks_eeg, :]) 
   raw = read_raw_fif(fif_fname, preload=True) 
   events = read_events(eve_fname) 
   picks_eeg = pick_types(raw.info, meg=False, eeg=True) 
   epochs = Epochs(raw, events=events, event_id=1, tmin=(-0.2), tmax=0.5, picks=picks_eeg, preload=True, proj='delayed') 
   evoked = epochs.average() 
   evoked_ref = add_reference_channels(evoked, 'Ref', copy=True) 
   assert_equal(evoked_ref.data.shape[0], (evoked.data.shape[0] + 1)) 
   _check_channel_names(evoked_ref, 'Ref') 
   ref_idx = evoked_ref.ch_names.index('Ref') 
   ref_data = evoked_ref.data[ref_idx, :] 
   assert_array_equal(ref_data, 0) 
   picks_eeg = pick_types(evoked.info, meg=False, eeg=True) 
   assert_array_equal(evoked.data[picks_eeg, :], evoked_ref.data[picks_eeg, :]) 
   raw = read_raw_fif(fif_fname, preload=True) 
   events = read_events(eve_fname) 
   picks_eeg = pick_types(raw.info, meg=False, eeg=True) 
   epochs = Epochs(raw, events=events, event_id=1, tmin=(-0.2), tmax=0.5, picks=picks_eeg, preload=True, proj='delayed') 
   evoked = epochs.average() 
   with warnings.catch_warnings(record=True): 
      evoked_ref = add_reference_channels(evoked, ['M1', 'M2'], copy=True) 
   assert_equal(evoked_ref.data.shape[0], (evoked.data.shape[0] + 2)) 
   _check_channel_names(evoked_ref, ['M1', 'M2']) 
   ref_idx = evoked_ref.ch_names.index('M1') 
   ref_idy = evoked_ref.ch_names.index('M2') 
   ref_data = evoked_ref.data[[ref_idx, ref_idy], :] 
   assert_array_equal(ref_data, 0) 
   picks_eeg = pick_types(evoked.info, meg=False, eeg=True) 
   assert_array_equal(evoked.data[picks_eeg, :], evoked_ref.data[picks_eeg, :]) 
   raw_np = read_raw_fif(fif_fname, preload=False) 
   assert_raises(RuntimeError, add_reference_channels, raw_np, ['Ref']) 
   assert_raises(ValueError, add_reference_channels, raw, 1)", 'Test adding reference channels','Test adding a reference.'
"def module_s1(nfm, first=False): 
    sidepath = (Conv(**conv_params(1, (nfm * 4), 1, False, False)) if first else SkipNode()) 
   mainpath = ([] if first else [BatchNorm(), Activation(Rectlin())]) 
   mainpath.append(Conv(**conv_params(1, nfm))) 
   mainpath.append(Conv(**conv_params(3, nfm))) 
   mainpath.append(Conv(**conv_params(1, (nfm * 4), relu=False, batch_norm=False))) 
   return MergeSum([sidepath, mainpath])"," 'S1: 
 Parameters 
 nfm : int 
 The number of feature maps in the input. 
 Returns 
 A :class:`Node` instance. 
 Examples 
 >>> module = module_s1(128) 
 >>> module.outputs[0].shape 
 (3, 32, 32, 32) 
 >>> module.outputs[1].shape 
 (3, 32, 32, 32)'",'non-strided'
"def _mocked_presets(*args, **kwargs): 
    return [MockPreset('1')]"," 'Mock the preset manager. 
 :param args: 
 :param kwargs: 
 :return: 
 :rtype: 
 :raises: 
 :example: 
 >>> from nipype.interfaces.base import MockPreset 
 >>> from nipype.interfaces.fsl import FSLCommandLine 
 >>> cl = FSLCommandLine() 
 >>> cl.inputs.preset_manager = MockPreset 
 >>> cl.inputs.preset = \'1\' 
 >>> cl.cmdline 
 ""--preset=1"" 
 >>> cl.cmdline 
 ""--preset=1"" 
 >>> cl.cmdline 
 ""--preset=1"" 
 >>> cl.cmdline 
 ""--preset=1"" 
 >>> cl.cmdline 
 ""--preset=1""'",'Return a list of mocked presets.'
"def runSome(): 
    tests = [] 
   names = ['testParseHostname', 'testExtractMastersSingle', 'testExtractMastersMultiple'] 
   tests.extend(list(list(map(BasicTestCase, names)))) 
   suite = unittest.TestSuite(tests) 
   unittest.TextTestRunner(verbosity=2).run(suite)", 'Run some of the tests.','Unittest runner'
"def pportD5(state): 
    global dataReg 
   if (state == 0): 
      dataReg = (dataReg & (~ 32)) 
   else: 
      dataReg = (dataReg | 32) 
   port.DlPortWritePortUchar(baseAddress, dataReg)"," 'Sets the data register to the value of state 
 state: 0 or 1'",'toggle data register D5 bit'
"def is_executable(exe): 
    return os.access(exe, os.X_OK)", 'Check if the given file is executable.','Checks a file is executable'
"def option_present(name, value, reload=False): 
    ret = {'name': 'testing   mode', 'changes': {}, 'result': True, 'comment': 'Option   already   present.'} 
   option = name 
   current_option = __salt__['csf.get_option'](option) 
   if current_option: 
      l = __salt__['csf.split_option'](current_option) 
      option_value = l[1] 
      if ('""{0}""'.format(value) == option_value): 
         return ret 
      else: 
         result = __salt__['csf.set_option'](option, value) 
         ret['comment'] = 'Option   modified.' 
         ret['changes']['Option'] = 'Changed' 
   else: 
      result = __salt__['file.append']('/etc/csf/csf.conf', args='{0}   =   ""{1}""'.format(option, value)) 
      ret['comment'] = 'Option   not   present.   Appended   to   csf.conf' 
      ret['changes']['Option'] = 'Changed.' 
   if reload: 
      if __salt__['csf.reload'](): 
         ret['comment'] += '.   Csf   reloaded.' 
      else: 
         ret['comment'] += '.   Csf   failed   to   reload.' 
         ret['result'] = False 
   return ret"," 'Checks if an option is present in the csf.conf file. 
 If the option is present, it returns a dict with the option name and value. 
 If the option is not present, it appends the option to the csf.conf file 
 and returns a dict with the option name and value.'","'Ensure the state of a particular option/setting in csf. 
 name 
 The option name in csf.conf 
 value 
 The value it should be set to. 
 reload 
 Boolean. If set to true, csf will be reloaded after.'"
"@validator 
 def app(environ, start_response): 
    if (environ['REQUEST_METHOD'].upper() != 'POST'): 
      data = 'Hello,   World!\n' 
   else: 
      data = environ['wsgi.input'].read() 
   status = '200   OK' 
   response_headers = [('Content-type', 'text/plain'), ('Content-Length', str(len(data))), ('X-Gunicorn-Version', __version__), ('Test', 'test   \xd1\x82\xd0\xb5\xd1\x81\xd1\x82')] 
   start_response(status, response_headers) 
   return iter([data])"," 'Gunicorn WSGI middleware. 
 This middleware is used to serve the HTTP requests. 
 It is registered with Gunicorn::Application::middleware. 
 :param environ: The WSGI environment. 
 :param start_response: The start_response function from the WSGI 
 environment. 
 :return: An iterable of the data.'",'Simplest possible application object'
"def attach_total_points(queryset, as_field='total_points_attr'): 
    model = queryset.model 
   sql = 'SELECT   SUM(projects_points.value)\n                                                            FROM   userstories_rolepoints\n                                                            INNER   JOIN   userstories_userstory   ON   userstories_userstory.id   =   userstories_rolepoints.user_story_id\n                                                            INNER   JOIN   projects_points   ON   userstories_rolepoints.points_id   =   projects_points.id\n                                                            WHERE   userstories_userstory.milestone_id   =   {tbl}.id' 
   sql = sql.format(tbl=model._meta.db_table) 
   queryset = queryset.extra(select={as_field: sql}) 
   return queryset"," 'Add a total points field to a queryset. 
 This method attaches a field to the queryset that contains the total points 
 for a user story. 
 :param queryset: The queryset to attach the field to. 
 :param as_field: The field to attach to the queryset. 
 :type as_field: str 
 :return: The queryset with the field attached.'","'Attach total of point values to each object of the queryset. 
 :param queryset: A Django milestones queryset object. 
 :param as_field: Attach the points as an attribute with this name. 
 :return: Queryset object with the additional `as_field` field.'"
"def create_vdir(name, site, sourcepath, app='/'): 
    ret = {'name': name, 'changes': {}, 'comment': str(), 'result': None} 
   current_vdirs = __salt__['win_iis.list_vdirs'](site, app) 
   if (name in current_vdirs): 
      ret['comment'] = 'Virtual   directory   already   present:   {0}'.format(name) 
      ret['result'] = True 
   elif __opts__['test']: 
      ret['comment'] = 'Virtual   directory   will   be   created:   {0}'.format(name) 
      ret['changes'] = {'old': None, 'new': name} 
   else: 
      ret['comment'] = 'Created   virtual   directory:   {0}'.format(name) 
      ret['changes'] = {'old': None, 'new': name} 
      ret['result'] = __salt__['win_iis.create_vdir'](name, site, sourcepath, app) 
   return ret"," 'Create a virtual directory on IIS. 
 name: name of the virtual directory 
 site: name of the site 
 sourcepath: path to the physical directory 
 app: application to use 
 Returns: 
 {name: \'virtual directory name\', changes: \'virtual directory name\' -> \'virtual directory name\', comment: \'virtual directory comment\', result: \'virtual directory result\'}'","'Create an IIS virtual directory. 
 .. note: 
 This function only validates against the virtual directory name, and will return 
 True even if the virtual directory already exists with a different configuration. 
 It will not modify the configuration of an existing virtual directory. 
 :param str name: The virtual directory name. 
 :param str site: The IIS site name. 
 :param str sourcepath: The physical path. 
 :param str app: The IIS application. 
 Example of usage with only the required arguments: 
 .. code-block:: yaml 
 site0-foo-vdir: 
 win_iis.create_vdir: 
 - name: foo 
 - site: site0 
 - sourcepath: C:\inetpub\vdirs\foo 
 Example of usage specifying all available arguments: 
 .. code-block:: yaml 
 site0-foo-vdir: 
 win_iis.create_vdir: 
 - name: foo 
 - site: site0 
 - sourcepath: C:\inetpub\vdirs\foo 
 - app: v1'"
"def _test_args(): 
    import pandas as pd 
   return {'start': pd.Timestamp('2004', tz='utc'), 'end': pd.Timestamp('2008', tz='utc')}", 'Test for args.','Extra arguments to use when zipline\'s automated tests run this example.'
"def run_tests_in_emulator(package): 
    env_vars = PACKAGE_INFO[package] 
   start_command = get_start_command(package) 
   proc_start = subprocess.Popen(start_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE) 
   try: 
      wait_ready(package, proc_start) 
      env_init_command = get_env_init_command(package) 
      proc_env = subprocess.Popen(env_init_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE) 
      env_status = proc_env.wait() 
      if (env_status != 0): 
         raise RuntimeError(env_status, proc_env.stderr.read()) 
      env_lines = proc_env.stdout.read().strip().split('\n') 
      for env_var in env_vars: 
         line_prefix = (('export   ' + env_var) + '=') 
         (value,) = [line.split(line_prefix, 1)[1] for line in env_lines if line.startswith(line_prefix)] 
         os.environ[env_var] = value 
      run_module_tests(package, ignore_requirements=True) 
   finally: 
      cleanup(proc_start.pid)", 'Run tests for a package in an emulator.',"'Spawn an emulator instance and run the system tests. 
 :type package: str 
 :param package: The package to run system tests against.'"
"def test_hsl_to_rgb_part_4(): 
    assert (hsl_to_rgb(0, 100, 50) == (255, 0, 0)) 
   assert (hsl_to_rgb(12, 100, 50) == (255, 51, 0)) 
   assert (hsl_to_rgb(24, 100, 50) == (255, 102, 0)) 
   assert (hsl_to_rgb(36, 100, 50) == (255, 153, 0)) 
   assert (hsl_to_rgb(48, 100, 50) == (255, 204, 0)) 
   assert (hsl_to_rgb(60, 100, 50) == (255, 255, 0)) 
   assert (hsl_to_rgb(72, 100, 50) == (204, 255, 0)) 
   assert (hsl_to_rgb(84, 100, 50) == (153, 255, 0)) 
   assert (hsl_to_rgb(96, 100, 50) == (102, 255, 0)) 
   assert (hsl_to_rgb(108, 100, 50) == (51, 255, 0)) 
   assert (hsl_to_rgb(120, 100, 50) == (0, 255, 0))"," 'Test hsl to rgb for 100, 50'",'Test hsl to rgb color function'
"def from_agraph(A, create_using=None): 
    if (create_using is None): 
      if A.is_directed(): 
         if A.is_strict(): 
            create_using = nx.DiGraph() 
         else: 
            create_using = nx.MultiDiGraph() 
      elif A.is_strict(): 
         create_using = nx.Graph() 
      else: 
         create_using = nx.MultiGraph() 
   N = nx.empty_graph(0, create_using) 
   N.name = '' 
   if (A.name is not None): 
      N.name = A.name 
   for n in A.nodes(): 
      str_attr = dict(((str(k), v) for (k, v) in n.attr.items())) 
      N.add_node(str(n), **str_attr) 
   for e in A.edges(): 
      (u, v) = (str(e[0]), str(e[1])) 
      attr = dict(e.attr) 
      str_attr = dict(((str(k), v) for (k, v) in attr.items())) 
      if (not N.is_multigraph()): 
         if (e.name is not None): 
            str_attr['key'] = e.name 
         N.add_edge(u, v, **str_attr) 
      else: 
         N.add_edge(u, v, key=e.name, **str_attr) 
   N.graph['graph'] = dict(A.graph_attr) 
   N.graph['node'] = dict(A.node_attr) 
   N.graph['edge'] = dict(A.edge_attr) 
   return N"," 'Create a networkx graph from an acyclic graph. 
 :param A: A graph object. 
 :param create_using: The networkx graph to create the graph in. 
 :return: A networkx graph. 
 :rtype: NetworkX graph'","'Return a NetworkX Graph or DiGraph from a PyGraphviz graph. 
 Parameters 
 A : PyGraphviz AGraph 
 A graph created with PyGraphviz 
 create_using : NetworkX graph class instance 
 The output is created using the given graph class instance 
 Examples 
 >>> K5 = nx.complete_graph(5) 
 >>> A = nx.nx_agraph.to_agraph(K5) 
 >>> G = nx.nx_agraph.from_agraph(A) 
 >>> G = nx.nx_agraph.from_agraph(A) 
 Notes 
 The Graph G will have a dictionary G.graph_attr containing 
 the default graphviz attributes for graphs, nodes and edges. 
 Default node attributes will be in the dictionary G.node_attr 
 which is keyed by node. 
 Edge attributes will be returned as edge data in G.  With 
 edge_attr=False the edge data will be the Graphviz edge weight 
 attribute or the value 1 if no edge weight attribute is found.'"
"def filter_tool(context, tool): 
    return False"," 'Filter a tool for a given context. 
 :param context: 
 :param tool: 
 :return: 
 :rtype: 
 :raises: 
 :example: 
 >>> from invenio.base.i18n import _ 
 >>> from invenio.search.filters import filter_tool 
 >>> filter_tool(_(""Indexing""), ""indexing"") 
 True 
 >>> filter_tool(_(""Indexing""), ""indexing_and_publishing"") 
 False'",'Test Filter Tool'
"def toggle(device, partition, flag): 
    _validate_device(device) 
   try: 
      int(partition) 
   except Exception: 
      raise CommandExecutionError('Invalid   partition   number   passed   to   partition.toggle') 
   if (flag not in set(['bios_grub', 'legacy_boot', 'boot', 'lba', 'root', 'swap', 'hidden', 'raid', 'LVM', 'PALO', 'PREP', 'DIAG'])): 
      raise CommandExecutionError('Invalid   flag   passed   to   partition.toggle') 
   cmd = 'parted   -m   -s   {0}   toggle   {1}   {2}'.format(device, partition, flag) 
   out = __salt__['cmd.run'](cmd).splitlines() 
   return out"," 'Toggle the flag of a partition. 
 This will toggle the flag of a partition. 
 .. code-block:: bash 
 salt-call partition.toggle /dev/sda3 bios_grub 
 .. versionadded:: 2014.7.0'","'Toggle the state of <flag> on <partition>. Valid flags are the same as 
 the set command. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' partition.toggle /dev/sda 1 boot'"
"def parse_editable(editable_req, default_vcs=None): 
    url = editable_req 
   if (os.path.isdir(url) and os.path.exists(os.path.join(url, 'setup.py'))): 
      url = path_to_url(url) 
   if url.lower().startswith('file:'): 
      return (None, url) 
   for version_control in vcs: 
      if url.lower().startswith(('%s:' % version_control)): 
         url = ('%s+%s' % (version_control, url)) 
   if ('+' not in url): 
      if default_vcs: 
         url = ((default_vcs + '+') + url) 
      else: 
         raise InstallationError(('--editable=%s   should   be   formatted   with   svn+URL,   git+URL,   hg+URL   or   bzr+URL' % editable_req)) 
   vc_type = url.split('+', 1)[0].lower() 
   if (not vcs.get_backend(vc_type)): 
      raise InstallationError(('For   --editable=%s   only   svn   (svn+URL),   Git   (git+URL),   Mercurial   (hg+URL)   and   Bazaar   (bzr+URL)   is   currently   supported' % editable_req)) 
   match = re.search('(?:#|#.*?&)egg=([^&]*)', editable_req) 
   if (((not match) or (not match.group(1))) and vcs.get_backend(vc_type)): 
      parts = [p for p in editable_req.split('#', 1)[0].split('/') if p] 
      if (parts[(-2)] in ('tags', 'branches', 'tag', 'branch')): 
         req = parts[(-3)] 
      elif (parts[(-1)] == 'trunk'): 
         req = parts[(-2)] 
      else: 
         raise InstallationError(('--editable=%s   is   not   the   right   format;   it   must   have   #egg=Package' % editable_req)) 
   else: 
      req = match.group(1) 
   match = re.search('^(.*?)(?:-dev|-\\d.*)$', req) 
   if match: 
      req = match.group(1) 
   return (req, url)"," 'Parse an editable URL into a package name and a URL. 
 This function parses an editable URL, and returns a tuple of 
 (package name, editable URL). 
 :param editable_req: editable URL 
 :type editable_req: str 
 :param default_vcs: default backend to use if not specified 
 :type default_vcs: str 
 :return: (package name, editable URL) 
 :rtype: tuple'","'Parses svn+http://blahblah@rev#egg=Foobar into a requirement 
 (Foobar) and a URL'"
"def processElementNodeByDerivation(derivation, elementNode): 
    if (derivation == None): 
      derivation = SolidDerivation(elementNode) 
   elementAttributesCopy = elementNode.attributes.copy() 
   for target in derivation.targets: 
      targetAttributesCopy = target.attributes.copy() 
      target.attributes = elementAttributesCopy 
      processTarget(target) 
      target.attributes = targetAttributesCopy"," 'Processes the element node by the derivation. 
 :param derivation: Derivation to be processed. 
 :param elementNode: Element node to be processed. 
 :return: None.'",'Process the xml element by derivation.'
"@contextfilter 
 def do_map(*args, **kwargs): 
    context = args[0] 
   seq = args[1] 
   if ((len(args) == 2) and ('attribute' in kwargs)): 
      attribute = kwargs.pop('attribute') 
      if kwargs: 
         raise FilterArgumentError(('Unexpected   keyword   argument   %r' % next(iter(kwargs)))) 
      func = make_attrgetter(context.environment, attribute) 
   else: 
      try: 
         name = args[2] 
         args = args[3:] 
      except LookupError: 
         raise FilterArgumentError('map   requires   a   filter   argument') 
      func = (lambda item: context.environment.call_filter(name, item, args, kwargs, context=context)) 
   if seq: 
      for item in seq: 
         (yield func(item))"," 'Filter items in a sequence, mapping them to a new sequence.'","'Applies a filter on a sequence of objects or looks up an attribute. 
 This is useful when dealing with lists of objects but you are really 
 only interested in a certain value of it. 
 The basic usage is mapping on an attribute.  Imagine you have a list 
 of users but you are only interested in a list of usernames: 
 .. sourcecode:: jinja 
 Users on this page: {{ users|map(attribute=\'username\')|join(\', \') }} 
 Alternatively you can let it invoke a filter by passing the name of the 
 filter and the arguments afterwards.  A good example would be applying a 
 text conversion filter on a sequence: 
 .. sourcecode:: jinja 
 Users on this page: {{ titles|map(\'lower\')|join(\', \') }} 
 .. versionadded:: 2.7'"
"def test_parser_without_subparser_recieves_root_entry(complete_parser): 
    result = convert(complete_parser) 
   assert ('primary' in result['widgets'])", 'Test that a parser without subparsers receives the root entry','Non-subparser setups should receive a default root key called \'primary\''
"def test_table_with_no_newline(): 
    table = BytesIO() 
   with pytest.raises(ascii.InconsistentTableError): 
      ascii.read(table) 
   table = BytesIO() 
   with pytest.raises(ValueError) as err: 
      ascii.read(table, guess=False, fast_reader=False, format='basic') 
   assert ('No   header   line   found' in str(err.value)) 
   table = BytesIO() 
   with pytest.raises(ValueError) as err: 
      ascii.read(table, guess=False, fast_reader=True, format='fast_basic') 
   assert ('Inconsistent   data   column   lengths' in str(err.value)) 
   for kwargs in [dict(), dict(guess=False, fast_reader=False, format='basic'), dict(guess=False, fast_reader=True, format='fast_basic')]: 
      table = BytesIO() 
      table.write('a   b') 
      t = ascii.read(table, **kwargs) 
      assert (t.colnames == ['a', 'b']) 
      assert (len(t) == 0)", 'Test that the table reader can handle no newline at the end of the header',"'Test that an input file which is completely empty fails in the expected way. 
 Test that an input file with one line but no newline succeeds.'"
"def gauge(): 
    def prep(r): 
      if r.interactive: 
         pass 
      elif (r.representation == 'plain'): 
         r.table.image_url.readable = False 
      return True 
   s3.prep = prep 
   def postp(r, output): 
      if r.interactive: 
         pass 
      elif (r.representation == 'plain'): 
         image_url = r.record.image_url 
         if image_url: 
            output['item'].append(IMG(_src=image_url, _width=400, _height=310)) 
      return output 
   s3.postp = postp 
   output = s3_rest_controller() 
   return output", 'Gauge the response time of a request',"'Flood Gauges, RESTful controller'"
"def initiate_deletion(req, location_data, id): 
    store_utils.delete_image_location_from_backend(req.context, id, location_data)"," 'Initiate deletion of an image location from the backend. 
 :param req: The request object. 
 :param location_data: The image location data. 
 :param id: The image location ID. 
 :returns: The image location data.'","'Deletes image data from the location of backend store. 
 :param req: The WSGI/Webob Request object 
 :param location_data: Location to the image data in a data store 
 :param id: Opaque image identifier'"
"def timedelta_to_integral_minutes(delta): 
    return (timedelta_to_integral_seconds(delta) // 60)", 'Convert a timedelta to the integral minutes.','Convert a pd.Timedelta to a number of minutes as an int.'
"def purge(name=None, pkgs=None, **kwargs): 
    return _uninstall(action='purge', name=name, pkgs=pkgs, **kwargs)"," 'Remove a package and its dependencies. 
 This is the most aggressive uninstallation method. 
 It will remove all files that were installed by the package, 
 including those that are not used by any other packages. 
 :param name: name of package to uninstall 
 :param pkgs: list of packages to uninstall 
 :param action: action to perform, can be one of ``install``, ``upgrade``, 
 ``dist-upgrade``, ``remove`` or ``purge`` 
 :param force: force uninstallation 
 :param keep_cache: keep cached files 
 :param keep_logs: keep logs files 
 :param keep_tmp: keep temporary files 
 :param keep_state: keep package states 
 :param keep_db: keep database files 
 :param keep_locks: keep locks files 
 :param keep_caches: keep caches files 
 :param keep_history: keep history files 
 :param keep_config: keep configuration files 
 :param keep_files: keep files 
 :param keep_dirs: keep directories 
 :param keep_symlinks: keep","'.. versionchanged:: 2015.8.12,2016.3.3,2016.11.0 
 On minions running systemd>=205, `systemd-run(1)`_ is now used to 
 isolate commands which modify installed packages from the 
 ``salt-minion`` daemon\'s control group. This is done to keep systemd 
 from killing any apt-get/dpkg commands spawned by Salt when the 
 ``salt-minion`` service is restarted. (see ``KillMode`` in the 
 `systemd.kill(5)`_ manpage for more information). If desired, usage of 
 `systemd-run(1)`_ can be suppressed by setting a :mod:`config option 
 <salt.modules.config.get>` called ``systemd.scope``, with a value of 
 ``False`` (no quotes). 
 .. _`systemd-run(1)`: https://www.freedesktop.org/software/systemd/man/systemd-run.html 
 .. _`systemd.kill(5)`: https://www.freedesktop.org/software/systemd/man/systemd.kill.html 
 Remove packages via ``apt-get purge`` along with all configuration files. 
 name 
 The name of the package to be deleted. 
 Multiple Package Options: 
 pkgs 
 A list of packages to delete. Must be passed as a python list. The 
 ``name`` parameter will be ignored if this option is passed. 
 .. versionadded:: 0.16.0 
 Returns a dict containing the changes. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' pkg.purge <package name> 
 salt \'*\' pkg.purge <package1>,<package2>,<package3> 
 salt \'*\' pkg.purge pkgs=\'[""foo"", ""bar""]\''"
"def matchOnlyAtCol(n): 
    def verifyCol(strg, locn, toks): 
      if (col(locn, strg) != n): 
         raise ParseException(strg, locn, ('matched   token   not   at   column   %d' % n)) 
   return verifyCol"," 'A matcher that only matches at a specific column 
 :param n: the column to match at 
 :type n: int 
 :return: the matcher 
 :rtype: Callable[str, int, str]'","'Helper method for defining parse actions that require matching at a specific 
 column in the input text.'"
"def test_odd(value): 
    return ((value % 2) == 1)", 'Returns true if the value is odd.','Return true if the variable is odd.'
"def show_key(kwargs=None, call=None): 
    if (call != 'function'): 
      log.error('The   list_keys   function   must   be   called   with   -f   or   --function.') 
      return False 
   if (not kwargs): 
      kwargs = {} 
   if ('keyname' not in kwargs): 
      log.error('A   keyname   is   required.') 
      return False 
   (rcode, data) = query(command='my/keys/{0}'.format(kwargs['keyname']), method='GET') 
   return {'keys': {data['name']: data['key']}}"," 'Show a key\'s value. 
 .. versionadded:: 0.3.0 
 :param call: The call to use. 
 :param kwargs: The keyword arguments to pass to the call. 
 :returns: A dictionary with the key\'s value.'",'List the keys available'
"def adjacency(graph, directed=False, reversed=False, stochastic=False, heuristic=None): 
    if ((graph._adjacency is not None) and (graph._adjacency[1:] == (directed, reversed, stochastic, (heuristic and heuristic.func_code)))): 
      return graph._adjacency[0] 
   map = {} 
   for n in graph.nodes: 
      map[n.id] = {} 
   for e in graph.edges: 
      (id1, id2) = (((not reversed) and (e.node1.id, e.node2.id)) or (e.node2.id, e.node1.id)) 
      map[id1][id2] = (1.0 - (0.5 * e.weight)) 
      if heuristic: 
         map[id1][id2] += heuristic(id1, id2) 
      if (not directed): 
         map[id2][id1] = map[id1][id2] 
   if stochastic: 
      for id1 in map: 
         n = sum(map[id1].values()) 
         for id2 in map[id1]: 
            map[id1][id2] /= n 
   graph._adjacency = (map, directed, reversed, stochastic, (heuristic and heuristic.func_code)) 
   return map"," 'Creates the adjacency matrix of a graph. 
 If the graph has already been created with the adjacency 
 property, it will not be recreated. 
 Parameters 
 graph : NetworkX graph 
 The graph to create the adjacency matrix for. 
 directed : boolean 
 Whether the graph is directed or not. 
 reversed : boolean 
 Whether the graph is directed or not. 
 stochastic : boolean 
 Whether the graph is stochastic or not. 
 heuristic : function 
 A function that takes two nodes and returns a value. 
 Returns 
 map : dictionary 
 The adjacency matrix. 
 Examples 
 >>> G = nx.Graph() 
 >>> G.add_edge(0, 1, weight=1) 
 >>> G.add_edge(0, 2, weight=2) 
 >>> G.add_edge(1, 2, weight=3) 
 >>> adjacency(G) 
 {0: {1: 0.5, 2: 0.5}, 1: {0: 0.5, 2","'Returns a dictionary indexed by node id1\'s, 
 in which each value is a dictionary of connected node id2\'s linking to the edge weight. 
 If directed=True, edges go from id1 to id2, but not the other way. 
 If stochastic=True, all the weights for the neighbors of a given node sum to 1. 
 A heuristic function can be given that takes two node id\'s and returns 
 an additional cost for movement between the two nodes.'"
"def get_name_levels(node): 
    visitor = _NodeNameCollector() 
   ast.walk(node, visitor) 
   return visitor.names"," 'Returns a list of names that are found in the given AST node. 
 This function is used to collect names that are used in the AST node. 
 This is used to find the name of the class that the given AST node 
 represents. 
 :param node: The AST node to get the names from. 
 :type node: ast.AST 
 :return: A list of names found in the given AST node. 
 :rtype: list'","'Return a list of ``(name, level)`` tuples for assigned names 
 The `level` is `None` for simple assignments and is a list of 
 numbers for tuple assignments for example in:: 
 a, (b, c) = x 
 The levels for for `a` is ``[0]``, for `b` is ``[1, 0]`` and for 
 `c` is ``[1, 1]``.'"
"def condentropy(px, py, pxpy=None, logbase=2): 
    if ((not _isproperdist(px)) or (not _isproperdist(py))): 
      raise ValueError('px   or   py   is   not   a   proper   probability   distribution') 
   if ((pxpy != None) and (not _isproperdist(pxpy))): 
      raise ValueError('pxpy   is   not   a   proper   joint   distribtion') 
   if (pxpy == None): 
      pxpy = np.outer(py, px) 
   condent = np.sum((pxpy * np.nan_to_num(np.log2((py / pxpy))))) 
   if (logbase == 2): 
      return condent 
   else: 
      return (logbasechange(2, logbase) * condent)"," 'Compute the conditional entropy of the joint distribution p(x, y) 
 given the marginal distribution p(x). 
 Parameters 
 px : ndarray 
 Probability distribution of x 
 py : ndarray 
 Probability distribution of y 
 pxpy : ndarray 
 Joint distribution of x and y 
 logbase : int 
 Base of logarithms 
 Returns 
 condent : float 
 Conditional entropy of x given y 
 Notes 
 Conditional entropy is the average logarithm of the logarithm of the 
 joint distribution. 
 References 
 .. [1] ""A new measure of information: Shannon's entropy"", 
 John Wiley & Sons, 1948'","'Return the conditional entropy of X given Y. 
 Parameters 
 px : array-like 
 py : array-like 
 pxpy : array-like, optional 
 If pxpy is None, the distributions are assumed to be independent 
 and conendtropy(px,py) = shannonentropy(px) 
 logbase : int or np.e 
 Returns 
 sum_{kj}log(q_{j}/w_{kj} 
 where q_{j} = Y[j] 
 and w_kj = X[k,j]'"
"def task(*args, **kwargs): 
    kwargs.setdefault('accept_magic_kwargs', False) 
   return app_or_default().task(*args, **kwargs)"," 'Decorator for tasks. 
 :param args: 
 :param kwargs: 
 :return: 
 :rtype: 
 :raises: 
 :returns: 
 :example: 
 .. code-block:: python 
 @task 
 def some_task(name, *args, **kwargs): 
 :param name: 
 :type name: str 
 :param args: 
 :type args: list 
 :param kwargs: 
 :type kwargs: dict 
 :return: 
 :rtype: 
 :raises: 
 :example: 
 .. code-block:: python 
 @task 
 def some_task(name, *args, **kwargs): 
 :param name: 
 :type name: str 
 :param args: 
 :type args: list 
 :param kwargs: 
 :type kwargs: dict 
 :return: 
 :rtype: 
 :raises: 
 :example: 
 .. code-block:: python 
 @task 
 def some_task(name, *args, **kwargs): 
","'Decorator to create a task class out of any callable. 
 **Examples** 
 .. code-block:: python 
 @task 
 def refresh_feed(url): 
 return Feed.objects.get(url=url).refresh() 
 With setting extra options and using retry. 
 .. code-block:: python 
 @task(max_retries=10) 
 def refresh_feed(url): 
 try: 
 return Feed.objects.get(url=url).refresh() 
 except socket.error, exc: 
 refresh_feed.retry(exc=exc) 
 Calling the resulting task: 
 >>> refresh_feed(""http://example.com/rss"") # Regular 
 <Feed: http://example.com/rss> 
 >>> refresh_feed.delay(""http://example.com/rss"") # Async 
 <AsyncResult: 8998d0f4-da0b-4669-ba03-d5ab5ac6ad5d>'"
"def _is_image_available(context, image): 
    if (hasattr(context, 'auth_token') and context.auth_token): 
      return True 
   def _is_image_public(image): 
      if hasattr(image, 'visibility'): 
         return (str(image.visibility).lower() == 'public') 
      else: 
         return image.is_public 
   if (context.is_admin or _is_image_public(image)): 
      return True 
   properties = image.properties 
   if (context.project_id and ('owner_id' in properties)): 
      return (str(properties['owner_id']) == str(context.project_id)) 
   if (context.project_id and ('project_id' in properties)): 
      return (str(properties['project_id']) == str(context.project_id)) 
   try: 
      user_id = properties['user_id'] 
   except KeyError: 
      return False 
   return (str(user_id) == str(context.user_id))", 'Checks if the image is available for the current context.',"'Check image availability. 
 This check is needed in case Nova and Glance are deployed 
 without authentication turned on.'"
"def publish_exploration_and_update_user_profiles(committer_id, exp_id): 
    rights_manager.publish_exploration(committer_id, exp_id) 
   contribution_time_msec = utils.get_current_time_in_millisecs() 
   contributor_ids = get_exploration_summary_by_id(exp_id).contributor_ids 
   for contributor in contributor_ids: 
      user_services.update_first_contribution_msec_if_not_set(contributor, contribution_time_msec)", 'Publishes the exploration and updates user profiles.',"'Publishes the exploration with publish_exploration() function in 
 rights_manager.py, as well as updates first_contribution_msec. 
 It is the responsibility of the caller to check that the exploration is 
 valid prior to publication.'"
"def subsample(inputs, factor, scope=None): 
    if (factor == 1): 
      return inputs 
   else: 
      return slim.max_pool2d(inputs, [1, 1], stride=factor, scope=scope)"," 'Subsample the inputs by a factor. 
 Arguments: 
 inputs: a tensor of shape [batch, height, width, channels] 
 factor: a positive integer, the factor to subsample by. 
 scope: Optional scope for the operation. 
 Returns: 
 A tensor of shape [batch, height, width, channels] 
 Examples: 
 >>> inputs = tf.placeholder(tf.float32, shape=[None, 100, 100, 3]) 
 >>> subsampled = slim.max_pool2d(inputs, [1, 1], stride=2, scope=\'subsampled\') 
 >>> print(subsampled.get_shape().as_list()) 
 [100, 100, 3]'","'Subsamples the input along the spatial dimensions. 
 Args: 
 inputs: A `Tensor` of size [batch, height_in, width_in, channels]. 
 factor: The subsampling factor. 
 scope: Optional variable_scope. 
 Returns: 
 output: A `Tensor` of size [batch, height_out, width_out, channels] with the 
 input, either intact (if factor == 1) or subsampled (if factor > 1).'"
"@membership_required 
 def group_invite(request, slug, template_name='groups/group_invite.html'): 
    group = get_object_or_404(Group, slug=slug, is_active=True) 
   form = GroupInviteForm(initial={'group': group.pk, 'user': request.user.pk}) 
   return render(request, template_name, {'group': group, 'form': form})", 'Display a form to invite a user to a group.',"'Returns an invite form. 
 Templates: ``groups/group_invite.html`` 
 Context: 
 form 
 GroupInviteForm object'"
"def strip_files(files, argv_max=(256 * 1024)): 
    while files: 
      cmd = list(STRIPCMD) 
      pathlen = sum(((len(s) + 1) for s in cmd)) 
      while ((pathlen < argv_max) and files): 
         f = files.pop() 
         cmd.append(f) 
         pathlen += (len(f) + 1) 
      if (len(cmd) > len(STRIPCMD)): 
         all_files = cmd[len(STRIPCMD):] 
         unwritable_files = tuple(filter(None, ((None if os.access(x, os.W_OK) else (x, os.stat(x).st_mode)) for x in all_files))) 
         [os.chmod(x, (stat.S_IWRITE | old_mode)) for (x, old_mode) in unwritable_files] 
         subprocess.check_call(cmd) 
         [os.chmod(x, old_mode) for (x, old_mode) in unwritable_files]"," 'Strip the files in the given list of filenames. 
 The files are stripped by using the strip command, which is a 
 shell script that uses the ``find`` command to find all files 
 and then strips them. 
 :param files: List of filenames 
 :param argv_max: Maximum size of the command line, in bytes 
 :returns: None'",'Strip a list of files'
"def riemann_cyclic(t2): 
    if isinstance(t2, (TensMul, Tensor)): 
      args = [t2] 
   else: 
      args = t2.args 
   a1 = [x.split() for x in args] 
   a2 = [[riemann_cyclic_replace(tx) for tx in y] for y in a1] 
   a3 = [tensor_mul(*v) for v in a2] 
   t3 = TensAdd(*a3) 
   if (not t3): 
      return t3 
   else: 
      return canon_bp(t3)"," 'Returns the canonical form of the Riemann cyclic tensor 
 The Riemann cyclic tensor is defined as: 
 .. math:: 
 R_{12} = \frac{1}{2} (R_{13} R_{23} + R_{14} R_{24}) 
 where 
 R_{12} = \frac{1}{2} (R_{13} + R_{14}) 
 R_{12} = \frac{1}{2} (R_{23} + R_{24}) 
 R_{12} = \frac{1}{2} (R_{13} - R_{14}) 
 R_{12} = \frac{1}{2} (R_{23} - R_{24}) 
 where 
 R_{13} = \frac{1}{2} (R_{12} + R_{14}) 
 R_{13} = \frac{1}{2} (R_{12} - R_{14}) 
 R_{13} = \frac{1}{2} (R_{23} + R","'replace each Riemann tensor with an equivalent expression 
 satisfying the cyclic identity. 
 This trick is discussed in the reference guide to Cadabra. 
 Examples 
 >>> from sympy.tensor.tensor import TensorIndexType, tensor_indices, tensorhead, riemann_cyclic 
 >>> Lorentz = TensorIndexType(\'Lorentz\', dummy_fmt=\'L\') 
 >>> i, j, k, l = tensor_indices(\'i,j,k,l\', Lorentz) 
 >>> R = tensorhead(\'R\', [Lorentz]*4, [[2, 2]]) 
 >>> t = R(i,j,k,l)*(R(-i,-j,-k,-l) - 2*R(-i,-k,-j,-l)) 
 >>> riemann_cyclic(t) 
 0'"
"def sanitize_html(html_code): 
    attributes = bleach.ALLOWED_ATTRIBUTES.copy() 
   if (u'data' not in bleach.BleachSanitizer.allowed_protocols): 
      bleach.BleachSanitizer.allowed_protocols.append(u'data') 
   attributes.update({'*': ['class', 'style', 'id'], 'audio': ['controls', 'autobuffer', 'autoplay', 'src'], 'img': ['src', 'width', 'height', 'class']}) 
   output = bleach.clean(html_code, tags=(bleach.ALLOWED_TAGS + ['div', 'p', 'audio', 'pre', 'img', 'span']), styles=['white-space'], attributes=attributes) 
   return output"," 'Sanitize html code for security reasons. 
 :param html_code: html code to be sanitized 
 :return: sanitized html code'","'Sanitize html_code for safe embed on LMS pages. 
 Used to sanitize XQueue responses from Matlab.'"
"def read_local(tex_root, name): 
    cache_path = _local_cache_path(tex_root) 
   _validate_life_span(cache_path) 
   return _read(cache_path, name)"," 'Reads the local file cache. 
 Parameters 
 tex_root : str 
 Root directory of the TeX root 
 name : str 
 Name of the file to read 
 Returns 
 file : str 
 File contents 
 Raises 
 ValueError 
 If the file does not exist'","'Reads the object from the local cache using pickle. 
 The local cache is per tex document and the path will extracted 
 from the tex root 
 Arguments: 
 tex_root -- the root of the tex file (for the folder of the cache) 
 name -- the relative file name to read the object 
 Returns: 
 The object at the location with the name'"
"def cut_threshold(labels, rag, thresh, in_place=True): 
    if (not in_place): 
      rag = rag.copy() 
   to_remove = [(x, y) for (x, y, d) in rag.edges_iter(data=True) if (d['weight'] >= thresh)] 
   rag.remove_edges_from(to_remove) 
   comps = nx.connected_components(rag) 
   map_array = np.arange((labels.max() + 1), dtype=labels.dtype) 
   for (i, nodes) in enumerate(comps): 
      for node in nodes: 
         for label in rag.node[node]['labels']: 
            map_array[label] = i 
   return map_array[labels]"," 'Cut the threshold into components of the graph. 
 Parameters 
 labels : array 
 labels of the nodes 
 rag : networkx.DiGraph 
 the graph 
 thresh : float 
 the threshold 
 in_place : bool 
 if True, the graph is modified in place 
 Returns 
 labels : array 
 the labels of the nodes after the cut 
 Examples 
 >>> from networkx.algorithms.community import cut_threshold 
 >>> from networkx.algorithms.community.functions import rag 
 >>> from networkx.algorithms.community.functions import labels_from_edges 
 >>> G = rag(range(10), labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 
 >>> labels = labels_from_edges(G) 
 >>> cut_threshold(labels, G, 0.5, in_place=True) 
 array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])'","'Combine regions separated by weight less than threshold. 
 Given an image\'s labels and its RAG, output new labels by 
 combining regions whose nodes are separated by a weight less 
 than the given threshold. 
 Parameters 
 labels : ndarray 
 The array of labels. 
 rag : RAG 
 The region adjacency graph. 
 thresh : float 
 The threshold. Regions connected by edges with smaller weights are 
 combined. 
 in_place : bool 
 If set, modifies `rag` in place. The function will remove the edges 
 with weights less that `thresh`. If set to `False` the function 
 makes a copy of `rag` before proceeding. 
 Returns 
 out : ndarray 
 The new labelled array. 
 Examples 
 >>> from skimage import data, segmentation 
 >>> from skimage.future import graph 
 >>> img = data.astronaut() 
 >>> labels = segmentation.slic(img) 
 >>> rag = graph.rag_mean_color(img, labels) 
 >>> new_labels = graph.cut_threshold(labels, rag, 10) 
 References 
 .. [1] Alain Tremeau and Philippe Colantoni 
 ""Regions Adjacency Graph Applied To Color Image Segmentation"" 
 http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.11.5274'"
"@login_required 
 @require_POST 
 def add_leader(request, group_slug): 
    prof = get_object_or_404(GroupProfile, slug=group_slug) 
   if (not _user_can_manage_leaders(request.user, prof)): 
      raise PermissionDenied 
   form = AddUserForm(request.POST) 
   if form.is_valid(): 
      for user in form.cleaned_data['users']: 
         if (prof.group not in user.groups.all()): 
            user.groups.add(prof.group) 
         prof.leaders.add(user) 
      msg = _('{users}   added   to   the   group   leaders   successfully!').format(users=request.POST.get('users')) 
      messages.add_message(request, messages.SUCCESS, msg) 
      return HttpResponseRedirect(prof.get_absolute_url()) 
   msg = _('There   were   errors   adding   leaders   to   the   group,   see   below.') 
   messages.add_message(request, messages.ERROR, msg) 
   return profile(request, group_slug, leader_form=form)"," 'Adds a user to the group leaders. 
 :param request: The request object. 
 :param group_slug: The slug of the group to add the leader to. 
 :return: A HTTP response.'",'Add a leader to the group.'
"@register.inclusion_tag('addons/impala/contribution.html') 
 @jinja2.contextfunction 
 def impala_contribution(context, addon, text=None, src='', show_install=False, show_help=True, large=False, contribution_src=None): 
    if (not contribution_src): 
      contribution_src = src 
   has_suggested = bool(addon.suggested_amount) 
   comment_limit = PAYPAL_MAX_COMMENT_LENGTH 
   return new_context(**locals())"," 'Display a contribution form for the given addon. 
 :param context: The jinja2 context to add to the template. 
 :param addon: The addon to display the contribution form for. 
 :param text: The text to display in the contribution form. 
 :param src: The source of the contribution form. 
 :param show_install: Whether or not to show the install button. 
 :param show_help: Whether or not to show the help link. 
 :param large: Whether or not to show the large contribution form. 
 :param contribution_src: The source of the contribution form. 
 :returns: A jinja2 context with the contribution form.'","'Show a contribution box. 
 Parameters: 
 addon 
 text: The begging text at the top of the box. 
 src: The page where the contribution link is coming from. 
 show_install: Whether or not to show the install button. 
 show_help: Show ""What\'s this?"" link? 
 contribution_src: The source for the contribution src, 
 will use src if not provided.'"
"def kinetic_energy(vel): 
    return (0.5 * (vel ** 2).sum(axis=1))"," 'Kinetic energy of a vector field. 
 Parameters 
 vel : VectorField 
 The vector field. 
 Returns 
 kinetic_energy : scalar 
 The kinetic energy of the vector field.'","'Returns the kinetic energy associated with the given velocity 
 and mass of 1. 
 Parameters 
 vel: theano matrix 
 Symbolic matrix whose rows are velocity vectors. 
 Returns 
 return: theano vector 
 Vector whose i-th entry is the kinetic entry associated with vel[i].'"
"def ldata(): 
    try: 
      _id = request.args[0] 
   except: 
      raise HTTP(400) 
   translate = settings.get_L10n_translate_gis_location() 
   if translate: 
      language = session.s3.language 
      if (language == settings.get_L10n_default_language()): 
         translate = False 
   table = s3db.gis_location 
   query = ((((table.deleted == False) & (table.level != None)) & (table.end_date == None)) & ((table.parent == _id) | (table.id == _id))) 
   fields = [table.id, table.name, table.level, table.parent, table.lon_min, table.lat_min, table.lon_max, table.lat_max] 
   if translate: 
      ntable = s3db.gis_location_name 
      fields.append(ntable.name_l10n) 
      left = ntable.on((((ntable.deleted == False) & (ntable.language == language)) & (ntable.location_id == table.id))) 
   else: 
      left = None 
   locations = db(query).select(left=left, *fields) 
   if translate: 
      try: 
         id_level = int(locations.as_dict(key='gis_location.id')[int(_id)]['gis_location']['level'][1:]) 
      except: 
         return '' 
   else: 
      try: 
         id_level = int(locations.as_dict()[int(_id)]['level'][1:]) 
      except: 
         return '' 
   output_level = (id_level + 1) 
   search_level = ('L%s' % output_level) 
   location_dict = {} 
   if translate: 
      for location in locations: 
         l = location['gis_location'] 
         if (l.level == search_level): 
            this_level = output_level 
            f = int(l.parent) 
         else: 
            this_level = int(l.level[1:]) 
            parent = l.parent 
            if parent: 
               f = int(parent) 
            else: 
               f = None 
         name = (location['gis_location_name.name_l10n'] or l.name) 
         if (l.lon_min is not None): 
            location_dict[int(l.id)] = dict(n=name, l=this_level, f=f, b=[l.lon_min, l.lat_min, l.lon_max, l.lat_max]) 
         else: 
            location_dict[int(l.id)] = dict(n=name, l=this_level, f=f) 
   else: 
      for l in locations: 
         if (l.level == search_level): 
            this_level = output_level 
            f = int(l.parent) 
         else: 
            this_level = int(l.level[1:]) 
            parent = l.parent 
            if parent: 
               f = int(parent) 
            else: 
               f = None 
         if (l.lon_min is not None): 
            location_dict[int(l.id)] = dict(n=l.name, l=this_level, f=f, b=[l.lon_min, l.lat_min, l.lon_max, l.lat_max]) 
         else: 
            location_dict[int(l.id)] = dict(n=l.name, l=this_level, f=f) 
   script = ('n=%s\n' % json.dumps(location_dict, separators=SEPARATORS)) 
   response.headers['Content-Type'] = 'application/json' 
   return script"," 'JSON representation of a GIS Location hierarchy. 
 The response is a JSON string with a single property, \'n\' which is the name 
 of the location at the top level. The value of this property is the name of the 
 location at the top level, the name of the location at the next level, and so on. 
 The value of the property \'l\' is the level of the location in the hierarchy. 
 The value of the property \'f\' is the parent location of the location at the 
 current level. The value of the property \'b\' is the bounding box of the 
 location at the current level. 
 The response is: 
 \'n=%s\n\' % json.dumps(location_dict, separators=SEPARATORS) 
 where \'location_dict\' is a dictionary of dictionaries, where the keys are 
 the IDs of the locations and the values are dictionaries with the following 
 properties: 
 \'n\' is the name of the location at the top level. 
 \'l\' is the level of the location in the hierarchy. 
 \'f\'","'Return JSON of location hierarchy suitable for use by S3LocationSelector 
 \'/eden/gis/ldata/\' + id 
 n = {id : {\'n\' : name, 
 \'l\' : level, 
 \'f\' : parent, 
 \'b\' : [lon_min, lat_min, lon_max, lat_max]'"
"def flavor_destroy(context, flavor_id): 
    return IMPL.flavor_destroy(context, flavor_id)", 'Destroy a flavor.','Delete an instance type.'
"def generate_module(src): 
    if (not src): 
      return UNKNOWN_MODULE 
   (filename, ext) = splitext(urlsplit(src).path) 
   if (ext not in ('.js', '.jsx', '.coffee')): 
      return UNKNOWN_MODULE 
   if filename.endswith('.min'): 
      filename = filename[:(-4)] 
   tokens = filename.split('/') 
   for (idx, token) in enumerate(tokens): 
      if VERSION_RE.match(token): 
         return '/'.join(tokens[(idx + 1):]) 
   return (CLEAN_MODULE_RE.sub('', filename) or UNKNOWN_MODULE)"," 'Given a URL to a module, return the module name. 
 If the module is a commonjs module, return the name of the module 
 as the first argument. 
 If the module is an ES6 module, return the name of the module as the 
 first argument, and the name of the corresponding CommonJS module as 
 the second argument. 
 If the module is a CommonJS module, return the name of the module as the 
 first argument. 
 If the module is unknown, return UNKNOWN_MODULE. 
 If the module is a URL to a file, return the filename as the first 
 argument. 
 If the module is a URL to a directory, return the name of the directory 
 as the first argument. 
 If the module is a URL to a directory that contains a file, return the 
 name of the file as the first argument. 
 :param src: the URL to a module'","'Converts a url into a made-up module name by doing the following: 
 * Extract just the path name ignoring querystrings 
 * Trimming off the initial / 
 * Trimming off the file extension 
 * Removes off useless folder prefixes 
 e.g. http://google.com/js/v1.0/foo/bar/baz.js -> foo/bar/baz'"
"@contextmanager 
 def set_trap(app): 
    trap = Trap() 
   prev_tls = _state._tls 
   _state.set_default_app(trap) 
   class NonTLS(object, ): 
      current_app = trap 
   _state._tls = NonTLS() 
   (yield) 
   _state._tls = prev_tls"," 'Context manager to temporarily set the current app to the given `app`. 
 This is useful when you want to temporarily use a different app for 
 testing purposes. 
 :param app: The new app to use. 
 :type app: :class:`flask.Flask` 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return:","'Contextmanager that installs the trap app. 
 The trap means that anything trying to use the current or default app 
 will raise an exception.'"
"@functools.lru_cache() 
 def get_languages(): 
    return OrderedDict(settings.LANGUAGES)"," 'Returns the list of languages supported by the site. 
 :return: A list of dictionaries, each representing a language.'",'Cache of settings.LANGUAGES in an OrderedDict for easy lookups by key.'
"def is_clean_uri(uri): 
    return (not bool(BAD_URI_CHARS_RE.search(uri)))", 'Check if a URI is clean (i.e. it does not contain any reserved characters).',"'>>> is_clean_uri(""ABC!"") 
 True 
 >>> is_clean_uri(u""ABC!"") 
 True 
 >>> is_clean_uri(""ABC|"") 
 False 
 >>> is_clean_uri(u""ABC|"") 
 False 
 >>> is_clean_uri(""http://example.com/0"") 
 True 
 >>> is_clean_uri(u""http://example.com/0"") 
 True'"
"def build_model(): 
    net = {} 
   net['input'] = InputLayer((None, 3, 16, 112, 112)) 
   net['conv1a'] = Conv3DDNNLayer(net['input'], 64, (3, 3, 3), pad=1, nonlinearity=lasagne.nonlinearities.rectify, flip_filters=False) 
   net['pool1'] = MaxPool3DDNNLayer(net['conv1a'], pool_size=(1, 2, 2), stride=(1, 2, 2)) 
   net['conv2a'] = Conv3DDNNLayer(net['pool1'], 128, (3, 3, 3), pad=1, nonlinearity=lasagne.nonlinearities.rectify) 
   net['pool2'] = MaxPool3DDNNLayer(net['conv2a'], pool_size=(2, 2, 2), stride=(2, 2, 2)) 
   net['conv3a'] = Conv3DDNNLayer(net['pool2'], 256, (3, 3, 3), pad=1, nonlinearity=lasagne.nonlinearities.rectify) 
   net['conv3b'] = Conv3DDNNLayer(net['conv3a'], 256, (3, 3, 3), pad=1, nonlinearity=lasagne.nonlinearities.rectify) 
   net['pool3'] = MaxPool3DDNNLayer(net['conv3b'], pool_size=(2, 2, 2), stride=(2, 2, 2)) 
   net['conv4a'] = Conv3DDNNLayer(net['pool3'], 512, (3, 3, 3), pad=1, nonlinearity=lasagne.nonlinearities.rectify) 
   net['conv4b'] = Conv3DDNNLayer(net['conv4a'], 512, (3, 3, 3), pad=1, nonlinearity=lasagne.nonlinearities.rectify) 
   net['pool4'] = MaxPool3DDNNLayer(net['conv4b'], pool_size=(2, 2, 2), stride=(2, 2, 2)) 
   net['conv5a'] = Conv3DDNNLayer(net['pool4'], 512, (3, 3, 3), pad=1, nonlinearity=lasagne.nonlinearities.rectify) 
   net['conv5b'] = Conv3DDNNLayer(net['conv5a'], 512, (3, 3, 3), pad=1, nonlinearity=lasagne.nonlinearities.rectify) 
   net['pad'] = PadLayer(net['conv5b'], width=[(0, 1), (0, 1)], batch_ndim=3) 
   net['pool5'] = MaxPool3DDNNLayer(net['pad'], pool_size=(2, 2, 2), pad=(0, 0, 0), stride=(2, 2, 2)) 
   net['fc6-1'] = DenseLayer(net['pool5'], num_units=4096, nonlinearity=lasagne.nonlinearities.rectify) 
   net['fc7-1'] = DenseLayer(net['fc6-1'], num_units=4096, nonlinearity=lasagne.nonlinearities.rectify) 
   net['fc8-1'] = DenseLayer(net['fc7-1'], num_units=487, nonlinearity=None) 
   net['prob'] = NonlinearityLayer(net['fc8-1'], softmax) 
   return net"," 'Builds a model to classify images. 
 Returns 
 net : dict 
 Dictionary with the network structure. 
 Examples 
 >>> net = build_model()'","'Builds C3D model 
 Returns 
 dict 
 A dictionary containing the network layers, where the output layer is at key \'prob\''"
"def _get_ssh_interface(vm_): 
    return config.get_cloud_config_value('ssh_interface', vm_, __opts__, default='public_ips', search_global=False)", 'Get the SSH interface of the VM.',"'Return the ssh_interface type to connect to. Either \'public_ips\' (default) 
 or \'private_ips\'.'"
"def get_tenancy(vm_): 
    return config.get_cloud_config_value('tenancy', vm_, __opts__, search_global=False)", 'Get the tenancy from the vm config.',"'Returns the Tenancy to use. 
 Can be ""dedicated"" or ""default"". Cannot be present for spot instances.'"
"def true_dot(x, y, grad_preserves_dense=True): 
    if hasattr(x, 'getnnz'): 
      x = as_sparse_variable(x) 
      assert (x.format in ['csr', 'csc']) 
   if hasattr(y, 'getnnz'): 
      y = as_sparse_variable(y) 
      assert (y.format in ['csr', 'csc']) 
   x_is_sparse_variable = _is_sparse_variable(x) 
   y_is_sparse_variable = _is_sparse_variable(y) 
   if ((not x_is_sparse_variable) and (not y_is_sparse_variable)): 
      raise TypeError() 
   if x_is_sparse_variable: 
      return TrueDot(grad_preserves_dense)(x, y) 
   else: 
      assert y_is_sparse_variable 
      return transpose(TrueDot(grad_preserves_dense)(y.T, x.T))"," 'Returns the dot product of two sparse matrices. 
 Parameters 
 x : sparse matrix 
 y : sparse matrix 
 Returns 
 output : sparse matrix 
 Examples 
 >>> x = sp.rand(3, 3, format = \'csc\') 
 >>> y = sp.rand(3, 3, format = \'csc\') 
 >>> x.todense() 
 array([[ 0.,  0.,  0.], 
 [ 0.,  0.,  0.], 
 [ 0.,  0.,  0.]]) 
 >>> y.todense() 
 array([[ 0.,  0.,  0.], 
 [ 0.,  0.,  0.], 
 [ 0.,  0.,  0.]]) 
 >>> true_dot(x, y) 
 array([[ 0.,  0.,  0.], 
 [ 0.,  0.,  0.], 
 [ 0.,  0.,  0.]]) 
 >>> x = sp.rand(3, 3, format = \'csc\')","'Operation for efficiently calculating the dot product when 
 one or all operands are sparse. Supported formats are CSC and CSR. 
 The output of the operation is sparse. 
 Parameters 
 x 
 Sparse matrix. 
 y 
 Sparse matrix or 2d tensor variable. 
 grad_preserves_dense : bool 
 If True (default), makes the grad of dense inputs dense. 
 Otherwise the grad is always sparse. 
 Returns 
 The dot product `x`.`y` in a sparse format. 
 Notex 
 The grad implemented is regular, i.e. not structured.'"
"def parse_owner_mappings(type, options, fatal): 
    opt_name = ('--map-' + type) 
   value_rx = '^([^=]+)=([^=]*)$' 
   if (type in ('uid', 'gid')): 
      value_rx = '^(-?[0-9]+)=(-?[0-9]+)$' 
   owner_map = {} 
   for flag in options: 
      (option, parameter) = flag 
      if (option != opt_name): 
         continue 
      match = re.match(value_rx, parameter) 
      if (not match): 
         raise fatal((""couldn't   parse   %s   as   %s   mapping"" % (parameter, type))) 
      (old_id, new_id) = match.groups() 
      if (type in ('uid', 'gid')): 
         old_id = int(old_id) 
         new_id = int(new_id) 
      owner_map[old_id] = new_id 
   return owner_map"," 'Parse the owner mapping options. 
 :type type: str 
 :param type: The type of mapping to parse. 
 :type options: list 
 :param options: The options to parse. 
 :type fatal: bool 
 :param fatal: Whether to raise a fatal error if the mapping can\'t be 
 parsed. 
 :rtype: dict 
 :return: The parsed owner mapping.'","'Traverse the options and parse all --map-TYPEs, or call Option.fatal().'"
"def parse_xmlrpc(xml_string): 
    handler = XmlRpcReadHandler() 
   xml.sax.parseString(xml_string, handler) 
   return handler", 'Parse an XML-RPC request and return a :class:`XmlRpcRequest` object.',"'The user should call these functions: parse_xmlrpc and build_xmlrpc. 
 :param xml_string: The original XML string that we got from the browser. 
 :return: A handler that can then be used to access the result information 
 from: 
 - handler.fuzzable_parameters 
 - handler.all_parameters 
 - handler.get_data_container'"
"def distrib_release(): 
    with settings(hide('running', 'stdout')): 
      kernel = run('uname   -s') 
      if (kernel == 'Linux'): 
         return run('lsb_release   -r   --short') 
      elif (kernel == 'SunOS'): 
         return run('uname   -v')", 'Return the distribution release number.',"'Get the release number of the distribution. 
 Example:: 
 from fabtools.system import distrib_id, distrib_release 
 if distrib_id() == \'CentOS\' and distrib_release() == \'6.1\': 
 print(u""CentOS 6.2 has been released. Please upgrade."")'"
"def get_total_project_memberships(project): 
    return project.memberships.count()"," 'Returns the number of project memberships for the project. 
 :param project: The project object 
 :type project: :class:`~github3.models.Project` 
 :return: The number of project memberships 
 :rtype: int'","'Return tha total of memberships of a project (members and unaccepted invitations). 
 :param project: A project object. 
 :return: a number.'"
"def fit(function, x, y): 
    p0 = [guess_plateau(x, y), 4.0, guess_lag(x, y), 0.1, min(y)] 
   (params, pcov) = curve_fit(function, x, y, p0=p0) 
   return (params, pcov)"," 'Fit the function to the data. 
 Parameters 
 function : function 
 The function to fit. 
 x : array 
 The independent variable. 
 y : array 
 The dependent variable. 
 Returns 
 params : array 
 The fitted parameters. 
 pcov : array 
 The covariance matrix. 
 Examples 
 >>> from scipy.optimize import curve_fit 
 >>> def f(x, a, b, c, d): 
 ...     return a * x + b * x**2 + c * x**3 + d 
 >>> x = np.linspace(0, 10, 100) 
 >>> y = f(x, 1, 2, 3, 4) 
 >>> fit(f, x, y) 
 (array([1., 2., 3., 4.]), array([[ 1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  1.00","'Fit the provided functrion to the x and y values. 
 The function parameters and the parameters covariance.'"
"def indexable(*iterables): 
    result = [] 
   for X in iterables: 
      if sp.issparse(X): 
         result.append(X.tocsr()) 
      elif (hasattr(X, '__getitem__') or hasattr(X, 'iloc')): 
         result.append(X) 
      elif (X is None): 
         result.append(X) 
      else: 
         result.append(np.array(X)) 
   check_consistent_length(*result) 
   return result"," 'Return a list of indexable objects. 
 Parameters 
 iterables : iterables 
 Iterables of indexable objects. 
 Returns 
 list 
 A list of indexable objects. 
 Notes 
 If a single iterable is provided, it is returned as is. 
 Examples 
 >>> from numpy.testing import assert_array_equal 
 >>> from numpy.testing import assert_almost_equal 
 >>> assert_array_equal(indexable([[1, 2, 3], [4, 5, 6]]), [[1, 2, 3], [4, 5, 6]]) 
 >>> assert_array_equal(indexable([[1, 2, 3], [4, 5, 6]]), [[1, 2, 3], [4, 5, 6]]) 
 >>> assert_array_equal(indexable([[1, 2, 3], [4, 5, 6]]), [[1, 2, 3], [4, 5, 6]]) 
 >>> assert_array_equal(indexable([[1,","'Make arrays indexable for cross-validation. 
 Checks consistent length, passes through None, and ensures that everything 
 can be indexed by converting sparse matrices to csr and converting 
 non-interable objects to arrays. 
 Parameters 
 *iterables : lists, dataframes, arrays, sparse matrices 
 List of objects to ensure sliceability.'"
"@decorators.api_view(['GET']) 
 @decorators.permission_classes((permissions.AllowAny,)) 
 @decorators.renderer_classes((JSONRenderer,)) 
 def section_search(request): 
    query = request.GET.get('q', None) 
   if (not query): 
      return Response({'error': 'Search   term   required.   Use   the   ""q""   GET   arg   to   search.   '}, status=status.HTTP_400_BAD_REQUEST) 
   project_slug = request.GET.get('project', None) 
   version_slug = request.GET.get('version', LATEST) 
   path = request.GET.get('path', None) 
   log.debug('(API   Section   Search)   [%s:%s]   %s', project_slug, version_slug, query) 
   results = search_section(request=request, query=query, project_slug=project_slug, version_slug=version_slug, path=path) 
   return Response({'results': results})"," 'Returns a list of Section objects matching the given query. 
 :param request: The current request 
 :type request: django.http.HttpRequest 
 :param query: The search term 
 :type query: str 
 :param project_slug: The project slug 
 :type project_slug: str 
 :param version_slug: The version slug 
 :type version_slug: str 
 :param path: The path to search 
 :type path: str 
 :return: A JSON response containing the search results 
 :rtype: dict'","'Section search 
 Queries with query ``q`` across all documents and projects. Queries can be 
 limited to a single project or version by using the ``project`` and 
 ``version`` GET arguments in your request. 
 When you search, you will have a ``project`` facet, which includes the 
 number of matching sections per project. When you search inside a project, 
 the ``path`` facet will show the number of matching sections per page. 
 Possible GET args 
 q **(required)** 
 The query string **Required** 
 project 
 A project slug 
 version 
 A version slug 
 path 
 A file path slug 
 Example:: 
 GET /api/v2/search/section/?q=virtualenv&project=django'"
"def remove_non_release_groups(name): 
    if (not name): 
      return name 
   removeWordsList = {u'\\[rartv\\]$': u'searchre', u'\\[rarbg\\]$': u'searchre', u'\\[eztv\\]$': u'searchre', u'\\[ettv\\]$': u'searchre', u'\\[cttv\\]$': u'searchre', u'\\[vtv\\]$': u'searchre', u'\\[EtHD\\]$': u'searchre', u'\\[GloDLS\\]$': u'searchre', u'\\[silv4\\]$': u'searchre', u'\\[Seedbox\\]$': u'searchre', u'\\[PublicHD\\]$': u'searchre', u'\\[AndroidTwoU\\]$': u'searchre', u'\\.\\[BT\\]$': u'searchre', u'   \\[1044\\]$': u'searchre', u'\\.RiPSaLoT$': u'searchre', u'\\.GiuseppeTnT$': u'searchre', u'\\.Renc$': u'searchre', u'-NZBGEEK$': u'searchre', u'-Siklopentan$': u'searchre', u'-\\[SpastikusTV\\]$': u'searchre', u'-RP$': u'searchre', u'-20-40$': u'searchre', u'\\.\\[www\\.usabit\\.com\\]$': u'searchre', u'^\\[www\\.Cpasbien\\.pe\\]   ': u'searchre', u'^\\[www\\.Cpasbien\\.com\\]   ': u'searchre', u'^\\[   www\\.Cpasbien\\.pw   \\]   ': u'searchre', u'^\\.www\\.Cpasbien\\.pw': u'searchre', u'^\\[www\\.newpct1\\.com\\]': u'searchre', u'^\\[   www\\.Cpasbien\\.com   \\]   ': u'searchre', u'-   \\{   www\\.SceneTime\\.com   \\}$': u'searchre', u'^\\{   www\\.SceneTime\\.com   \\}   -   ': u'searchre', u'^\\]\\.\\[www\\.tensiontorrent.com\\]   -   ': u'searchre', u'^\\]\\.\\[   www\\.tensiontorrent.com   \\]   -   ': u'searchre', u'-   \\[   www\\.torrentday\\.com   \\]$': u'searchre', u'^\\[   www\\.TorrentDay\\.com   \\]   -   ': u'searchre', u'\\[NO-RAR\\]   -   \\[   www\\.torrentday\\.com   \\]$': u'searchre'} 
   _name = name 
   for (remove_string, remove_type) in removeWordsList.items(): 
      if (remove_type == u'search'): 
         _name = _name.replace(remove_string, u'') 
      elif (remove_type == u'searchre'): 
         _name = re.sub((u'(?i)' + remove_string), u'', _name) 
   return _name.strip(u'.-   []{}')"," 'Remove non-release groups from the name. 
 Example: \'[NZB]-NZBGEEK$\' -> \'[NZB]\' 
 :param name: the name to remove non-release groups from 
 :type name: str 
 :return: the name with non-release groups removed'",'Remove non release groups from name'
"def is_filelike(obj): 
    if (isinstance(obj, str) or isinstance(obj, unicode)): 
      return False 
   res = True 
   for a in ('read', 'tell', 'seek'): 
      res = (res and hasattr(obj, a)) 
   if (not res): 
      raise ValueError('Invalid   object   passed   as   file') 
   return True"," 'Check if an object is a file-like object. 
 This is a low level check for a file-like object, so it can be used 
 to check if an object is a file or a file-like object. 
 This is a low level check, so it does not check if the object 
 is a readable file. 
 :param obj: The object to check. 
 :returns: True if the object is a file-like object, False otherwise.'",'Filename or file object?'
"def get_netrc_auth(url): 
    try: 
      locations = (os.path.expanduser('~/{0}'.format(f)) for f in NETRC_FILES) 
      netrc_path = None 
      for loc in locations: 
         if (os.path.exists(loc) and (not netrc_path)): 
            netrc_path = loc 
      if (netrc_path is None): 
         return netrc_path 
      ri = urlparse(url) 
      host = ri.netloc.split(':')[0] 
      try: 
         _netrc = netrc(netrc_path).authenticators(host) 
         if _netrc: 
            login_i = (0 if _netrc[0] else 1) 
            return (_netrc[login_i], _netrc[2]) 
      except (NetrcParseError, IOError): 
         pass 
   except (ImportError, AttributeError): 
      pass"," 'Get the auth information from netrc file 
 :param url: The url to be authenticated 
 :return: (user, password) or None'",'Returns the Requests tuple auth for a given url from netrc.'
"def _tmp_name(input): 
    if (input is not None): 
      input = os.path.dirname(input) 
   (f, fn) = tempfile.mkstemp(dir=input) 
   os.close(f) 
   return fn"," 'Return a temporary file name in the directory specified by input. 
 This is a workaround for a bug in the tempfile module that causes it to 
 not work on Windows.'","'Create a temporary file name which should not already exist.  Use the 
 directory of the input file as the base name of the mkstemp() output.'"
"@register.filter(is_safe=True, needs_autoescape=True) 
 @stringfilter 
 def linenumbers(value, autoescape=True): 
    lines = value.split('\n') 
   width = str(len(str(len(lines)))) 
   if ((not autoescape) or isinstance(value, SafeData)): 
      for (i, line) in enumerate(lines): 
         lines[i] = ((('%0' + width) + 'd.   %s') % ((i + 1), line)) 
   else: 
      for (i, line) in enumerate(lines): 
         lines[i] = ((('%0' + width) + 'd.   %s') % ((i + 1), escape(line))) 
   return mark_safe('\n'.join(lines))"," 'Render a list of lines with line numbers. 
 :param value: The list of lines to render 
 :type value: list 
 :param autoescape: If True, the value will be autoescaped. 
 :type autoescape: bool 
 :return: The rendered lines'",'Displays text with line numbers.'
"def name(pretty=False): 
    return _distro.name(pretty)"," 'Returns the name of the distribution. 
 If pretty is True, the name is displayed with the version. 
 If pretty is False, the name is displayed without the version.'","'Return the name of the current Linux distribution, as a human-readable 
 string. 
 If *pretty* is false, the name is returned without version or codename. 
 (e.g. ""CentOS Linux"") 
 If *pretty* is true, the version and codename are appended. 
 (e.g. ""CentOS Linux 7.1.1503 (Core)"") 
 **Lookup hierarchy:** 
 The name is obtained from the following sources, in the specified order. 
 The first available and non-empty value is used: 
 * If *pretty* is false: 
 - the value of the ""NAME"" attribute of the os-release file, 
 - the value of the ""Distributor ID"" attribute returned by the lsb_release 
 command, 
 - the value of the ""<name>"" field of the distro release file. 
 * If *pretty* is true: 
 - the value of the ""PRETTY_NAME"" attribute of the os-release file, 
 - the value of the ""Description"" attribute returned by the lsb_release 
 command, 
 - the value of the ""<name>"" field of the distro release file, appended 
 with the value of the pretty version (""<version_id>"" and ""<codename>"" 
 fields) of the distro release file, if available.'"
"def get_logger(name): 
    old_class = logging.getLoggerClass() 
   logging.setLoggerClass(logging.Logger) 
   logger = logging.getLogger(name) 
   logging.setLoggerClass(old_class) 
   return logger"," 'Create a logger for a given name. 
 :param name: the name of the logger to create. 
 :type name: str 
 :return: the logger.'","'Always use logging.Logger class. 
 The user code may change the loggerClass (e.g. pyinotify), 
 and will cause exception when format log message.'"
"def prepare_database(db, coord, projection): 
    db.execute('CREATE   TEMPORARY   TABLE   box_node_list   (id   bigint   PRIMARY   KEY)   ON   COMMIT   DROP') 
   db.execute('CREATE   TEMPORARY   TABLE   box_way_list   (id   bigint   PRIMARY   KEY)   ON   COMMIT   DROP') 
   db.execute('CREATE   TEMPORARY   TABLE   box_relation_list   (id   bigint   PRIMARY   KEY)   ON   COMMIT   DROP') 
   (n, s, e, w) = coordinate_bbox(coord, projection) 
   bbox = ('ST_SetSRID(ST_MakeBox2D(ST_MakePoint(%.7f,   %.7f),   ST_MakePoint(%.7f,   %.7f)),   4326)' % (w, s, e, n)) 
   db.execute(('INSERT   INTO   box_node_list\n                                                      SELECT   id\n                                                      FROM   nodes\n                                                      WHERE   (geom   &&   %(bbox)s)' % locals())) 
   db.execute('INSERT   INTO   box_way_list\n                                                      SELECT   wn.way_id\n                                                      FROM   way_nodes   wn\n                                                      INNER   JOIN   box_node_list   n\n                                                      ON   wn.node_id   =   n.id\n                                                      GROUP   BY   wn.way_id') 
   db.execute(""INSERT   INTO   box_relation_list\n                                                      (\n                                                            SELECT   rm.relation_id   AS   relation_id\n                                                            FROM   relation_members   rm\n                                                            INNER   JOIN   box_node_list   n\n                                                            ON   rm.member_id   =   n.id\n                                                            WHERE   rm.member_type   =   'N'\n                                                      UNION\n                                                            SELECT   rm.relation_id   AS   relation_id\n                                                            FROM   relation_members   rm\n                                                            INNER   JOIN   box_way_list   w\n                                                            ON   rm.member_id   =   w.id\n                                                            WHERE   rm.member_type   =   'W'\n                                                      )"") 
   db.execute(""INSERT   INTO   box_relation_list\n                                                      SELECT   rm.relation_id   AS   relation_id\n                                                      FROM   relation_members   rm\n                                                      INNER   JOIN   box_relation_list   r\n                                                      ON   rm.member_id   =   r.id\n                                                      WHERE   rm.member_type   =   'R'\n                                                      EXCEPT\n                                                      SELECT   id   AS   relation_id\n                                                      FROM   box_relation_list"") 
   db.execute('ANALYZE   box_node_list') 
   db.execute('ANALYZE   box_way_list') 
   db.execute('ANALYZE   box_relation_list')"," 'Prepare database for box coordinates. 
 :param db: database connection 
 :param coord: box coordinates 
 :param projection: projection'",''
"def getTricomplexTimesColumn(firstTricomplex, otherColumn): 
    dotProductX = ((firstTricomplex[0].real * otherColumn.real) + (firstTricomplex[1].real * otherColumn.imag)) 
   dotProductY = ((firstTricomplex[0].imag * otherColumn.real) + (firstTricomplex[1].imag * otherColumn.imag)) 
   return complex(dotProductX, dotProductY)", 'Return the dot product of the first tricomplex and the other column','Get this matrix multiplied by the otherColumn.'
"def getFabmetheusUtilitiesPath(subName=''): 
    return getJoinedPath(getFabmetheusPath('fabmetheus_utilities'), subName)"," 'Get the path to the fabmetheus utilities. 
 This function will return the path to the fabmetheus utilities. 
 The path will be relative to the Fabmetheus folder. 
 Parameters 
 subName : str 
 The sub name of the path. 
 Returns 
 The path to the fabmetheus utilities. 
 Notes 
 The fabmetheus utilities folder is located in the Fabmetheus folder. 
 This function will return the path to the fabmetheus utilities folder 
 relative to the Fabmetheus folder. 
 Examples 
 >>> getFabmetheusUtilitiesPath() 
 \'fabmetheus_utilities\' 
 >>> getFabmetheusUtilitiesPath(\'math\') 
 \'fabmetheus_utilities/math\' 
 >>> getFabmetheusUtilitiesPath(\'math/geometry\') 
 \'fabmetheus_utilities/math/geometry\' 
 >>> getFabmetheusUtilitiesPath(\'math/geometry/geometry.py\') 
 \'fabmethe",'Get the fabmetheus utilities directory path.'
"def file_upload_echo(request): 
    r = {k: f.name for (k, f) in request.FILES.items()} 
   return HttpResponse(json.dumps(r))"," 'Echoes the uploaded files. 
 :param request: 
 :return: 
 :rtype: 
 :raises: 
 :example: 
 .. sourcecode:: python 
 :linenos: 
 >>> response = file_upload_echo(request) 
 >>> print(response.content.decode(\'utf-8\')) 
 {""file_1"": \'uploaded_file_1.jpg\', ""file_2"": ""uploaded_file_2.jpg""}'",'Simple view to echo back info about uploaded files for tests.'
"def _setSocketPreConnect(): 
    if conf.disablePrecon: 
      return 
   def _(): 
      while (kb.get('threadContinue') and (not conf.get('disablePrecon'))): 
         try: 
            for key in socket._ready: 
               if (len(socket._ready[key]) < SOCKET_PRE_CONNECT_QUEUE_SIZE): 
                  (family, type, proto, address) = key 
                  s = socket.socket(family, type, proto) 
                  s._connect(address) 
                  with kb.locks.socket: 
                     socket._ready[key].append((s._sock, time.time())) 
         except KeyboardInterrupt: 
            break 
         except: 
            pass 
         finally: 
            time.sleep(0.01) 
   def connect(self, address): 
      found = False 
      key = (self.family, self.type, self.proto, address) 
      with kb.locks.socket: 
         if (key not in socket._ready): 
            socket._ready[key] = [] 
         while (len(socket._ready[key]) > 0): 
            (candidate, created) = socket._ready[key].pop(0) 
            if ((time.time() - created) < PRECONNECT_CANDIDATE_TIMEOUT): 
               self._sock = candidate 
               found = True 
               break 
            else: 
               try: 
                  candidate.close() 
               except socket.error: 
                  pass 
      if (not found): 
         self._connect(address) 
   if (not hasattr(socket.socket, '_connect')): 
      socket._ready = {} 
      socket.socket._connect = socket.socket.connect 
      socket.socket.connect = connect 
      thread = threading.Thread(target=_) 
      setDaemon(thread) 
      thread.start()"," 'Preconnect socket, if possible. 
 This is a hack to improve performance of the socket API. 
 It will try to preconnect sockets and avoid the costly 
 socket.socket.connect() call. 
 The socket will be preconnected only if the socket is not 
 already connected. 
 The socket will be preconnected only if the socket is not 
 already connected. 
 If the socket is already connected, it will not be preconnected. 
 If the socket is already connected, it will not be preconnected. 
 If the socket is already connected, it will not be preconnected. 
 If the socket is already connected, it will not be preconnected. 
 If the socket is already connected, it will not be preconnected. 
 If the socket is already connected, it will not be preconnected. 
 If the socket is already connected, it will not be preconnected. 
 If the socket is already connected, it will not be preconnected. 
 If the socket is already connected, it will not be preconnected. 
 If the socket is already connected, it will not be preconnected. 
 If the socket is already connected, it will",'Makes a pre-connect version of socket.connect'
"def urlquote_plus(url, safe=''): 
    return force_unicode(urllib.quote_plus(smart_str(url), safe))"," 'Quote a URL, using the \'plus\' encoding (RFC 3986, section 2.1). 
 :param url: The URL to quote. 
 :param safe: Safe characters to include in the output. 
 :return: The quoted URL.'","'A version of Python\'s urllib.quote_plus() function that can operate on 
 unicode strings. The url is first UTF-8 encoded before quoting. The 
 returned string can safely be used as part of an argument to a subsequent 
 iri_to_uri() call without double-quoting occurring.'"
"def mock_open(mock=None, read_data=''): 
    def _readlines_side_effect(*args, **kwargs): 
      if (handle.readlines.return_value is not None): 
         return handle.readlines.return_value 
      return list(_data) 
   def _read_side_effect(*args, **kwargs): 
      if (handle.read.return_value is not None): 
         return handle.read.return_value 
      return ''.join(_data) 
   def _readline_side_effect(): 
      if (handle.readline.return_value is not None): 
         while True: 
            (yield handle.readline.return_value) 
      for line in _data: 
         (yield line) 
   global file_spec 
   if (file_spec is None): 
      import _io 
      file_spec = list(set(dir(_io.TextIOWrapper)).union(set(dir(_io.BytesIO)))) 
   if (mock is None): 
      mock = MagicMock(name='open', spec=open) 
   handle = MagicMock(spec=file_spec) 
   handle.__enter__.return_value = handle 
   _data = _iterate_read_data(read_data) 
   handle.write.return_value = None 
   handle.read.return_value = None 
   handle.readline.return_value = None 
   handle.readlines.return_value = None 
   handle.read.side_effect = _read_side_effect 
   handle.readline.side_effect = _readline_side_effect() 
   handle.readlines.side_effect = _readlines_side_effect 
   mock.return_value = handle 
   return mock"," 'A helper function to mock open() and return a mocked object. 
 :param mock: The mock object to return. 
 :param read_data: The data to read. 
 :return: A mocked open object. 
 :rtype: MagicMock'","'A helper function to create a mock to replace the use of `open`. It works 
 for `open` called directly or used as a context manager. 
 The `mock` argument is the mock object to configure. If `None` (the 
 default) then a `MagicMock` will be created for you, with the API limited 
 to methods or attributes available on standard file handles. 
 `read_data` is a string for the `read` methoddline`, and `readlines` of the 
 file handle to return.  This is an empty string by default.'"
"def parse_only_date(raw, assume_utc=True, as_utc=True): 
    f = (utcnow if assume_utc else now) 
   default = f().replace(hour=0, minute=0, second=0, microsecond=0, day=15) 
   return fix_only_date(parse_date(raw, default=default, assume_utc=assume_utc, as_utc=as_utc))"," 'Parse only the date part of a raw string. 
 :param raw: raw string 
 :param assume_utc: whether to assume the string is UTC 
 :param as_utc: whether to return the date as UTC 
 :return: datetime object'","'Parse a date string that contains no time information in a manner that 
 guarantees that the month and year are always correct in all timezones, and 
 the day is at most one day wrong.'"
"@jit(nopython=True, cache=True) 
 def get_mixed_actions(tableaux, bases): 
    nums_actions = (tableaux[1].shape[0], tableaux[0].shape[0]) 
   num = (nums_actions[0] + nums_actions[1]) 
   out = np.zeros(num) 
   for (pl, (start, stop)) in enumerate(zip((0, nums_actions[0]), (nums_actions[0], num))): 
      sum_ = 0.0 
      for i in range(nums_actions[(1 - pl)]): 
         k = bases[pl][i] 
         if (start <= k < stop): 
            out[k] = tableaux[pl][(i, (-1))] 
            sum_ += tableaux[pl][(i, (-1))] 
      if (sum_ != 0): 
         out[start:stop] /= sum_ 
   return (out[:nums_actions[0]], out[nums_actions[0]:])"," 'Returns the mixed actions for the tableaux in bases. 
 This function returns the mixed actions for the tableaux in bases. 
 The mixed actions are given by: 
 out[start:stop] = tableaux[pl][(i, (-1))] / sum_{i=0}^{nums_actions[1]} 
 tableaux[pl][(i, (-1))] 
 where sum_{i=0}^{nums_actions[1]} is the sum of the mixed actions for the 
 tableaux in bases. 
 The mixed actions are computed in two steps: 
 1. Compute the mixed actions for the tableaux in bases. 
 2. Compute the sum of the mixed actions for the tableaux in bases. 
 The mixed actions for the tableaux in bases are computed by calling 
 get_mixed_actions_tableaux() on each tableaux in bases. 
 The sum of the mixed actions for the tableaux in bases is computed by 
 calling get_mixed_actions_tableaux() on each tableaux in bases. 
 The sum of the mixed actions for the tableaux in bases is computed by 
 calling get","'From `tableaux` and `bases`, extract non-slack basic variables and 
 return a tuple of the corresponding, normalized mixed actions. 
 Parameters 
 tableaux : tuple(ndarray(float, ndim=2)) 
 Tuple of two arrays containing the tableaux, of shape (n, m+n+1) 
 and (m, m+n+1), respectively. 
 bases : tuple(ndarray(int, ndim=1)) 
 Tuple of two arrays containing the bases, of shape (n,) and 
 (m,), respectively. 
 Returns 
 tuple(ndarray(float, ndim=1)) 
 Tuple of mixed actions as given by the non-slack basic variables 
 in the tableaux.'"
"def make_pkgng_aware(jname): 
    ret = {'changes': {}} 
   cdir = _config_dir() 
   if (not os.path.isdir(cdir)): 
      os.makedirs(cdir) 
      if os.path.isdir(cdir): 
         ret['changes'] = 'Created   poudriere   make   file   dir   {0}'.format(cdir) 
      else: 
         return 'Could   not   create   or   find   required   directory   {0}'.format(cdir) 
   __salt__['file.write']('{0}-make.conf'.format(os.path.join(cdir, jname)), 'WITH_PKGNG=yes') 
   if os.path.isfile((os.path.join(cdir, jname) + '-make.conf')): 
      ret['changes'] = 'Created   {0}'.format(os.path.join(cdir, '{0}-make.conf'.format(jname))) 
      return ret 
   else: 
      return 'Looks   like   file   {0}   could   not   be   created'.format(os.path.join(cdir, (jname + '-make.conf')))"," 'Create a makefile for a given job in poudriere. 
 :param jname: the name of the job to create the makefile for 
 :returns: the string of the changes made 
 :rtype: str'","'Make jail ``jname`` pkgng aware 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' poudriere.make_pkgng_aware <jail name>'"
"@csrf_exempt 
 @gzip_page 
 @require_sync_session 
 @api_handle_error_with_json 
 def device_download(data, session): 
    zone = session.client_device.get_zone() 
   devicezones = list(DeviceZone.all_objects.filter(zone=zone, device__in=data['devices'])) 
   devices = [devicezone.device for devicezone in devicezones] 
   session.models_downloaded += (len(devices) + len(devicezones)) 
   return JsonResponse({'devices': serialize((devices + devicezones), dest_version=session.client_version, ensure_ascii=False)})"," 'Download a list of devices and their zones. 
 This function is used by the client to download a list of devices and their zones. 
 This is useful for the client to cache the list of devices and their zones. 
 :param data: a dictionary with the device ids to download 
 :type data: dict 
 :param session: the session to download the devices for 
 :type session: Session 
 :return: a JSON response with the devices and their zones'",'This device is having its own devices downloaded'
"def load_auth_tokens(user=None): 
    if (user is None): 
      user = users.get_current_user() 
   if (user is None): 
      return {} 
   pickled_tokens = memcache.get(('gdata_pickled_tokens:%s' % user)) 
   if pickled_tokens: 
      return pickle.loads(pickled_tokens) 
   user_tokens = TokenCollection.all().filter('user   =', user).get() 
   if user_tokens: 
      memcache.set(('gdata_pickled_tokens:%s' % user), user_tokens.pickled_tokens) 
      return pickle.loads(user_tokens.pickled_tokens) 
   return {}"," 'Loads the auth tokens for the user. 
 :param user: The user object to load the auth tokens for. 
 :returns: A dictionary of auth tokens. 
 :rtype: dict'","'Reads a dictionary of the current user\'s tokens from the datastore. 
 If there is no current user (a user is not signed in to the app) or the user 
 does not have any tokens, an empty dictionary is returned.'"
"def modify_monitor(hostname, username, password, monitor_type, name, **kwargs): 
    ret = {'name': name, 'changes': {}, 'result': False, 'comment': ''} 
   if __opts__['test']: 
      params = {'hostname': hostname, 'username': username, 'password': password, 'monitor_type': monitor_type, 'name': name} 
      for (key, value) in six.iteritems(kwargs): 
         params[key] = value 
      return _test_output(ret, 'modify', params) 
   existing = __salt__['bigip.list_monitor'](hostname, username, password, monitor_type, name) 
   if (existing['code'] == 200): 
      modified = __salt__['bigip.modify_monitor'](hostname, username, password, monitor_type, name, **kwargs) 
      if (modified['code'] == 200): 
         del existing['content']['selfLink'] 
         del modified['content']['selfLink'] 
         ret = _check_for_changes('Monitor', ret, existing, modified) 
      else: 
         ret = _load_result(modified, ret) 
   elif (existing['code'] == 404): 
      ret['comment'] = 'A   Monitor   with   this   name   was   not   found.' 
   else: 
      ret = _load_result(existing, ret) 
   return ret"," 'Modify a Monitor. 
 .. versionadded:: 2015.7.0 
 .. versionchanged:: 2015.7.0 
 Use ``modify`` method instead of ``update``. 
 .. versionchanged:: 2015.7.0 
 Add ``name`` parameter. 
 .. versionchanged:: 2015.7.0 
 Add ``monitor_type`` parameter. 
 .. versionchanged:: 2015.7.0 
 Add ``username`` and ``password`` parameters. 
 .. versionchanged:: 2015.7.0 
 Add ``changes`` parameter. 
 .. versionchanged:: 2015.7.0 
 Add ``comment`` parameter. 
 .. versionchanged:: 2015.7.0 
 Add ``test`` parameter. 
 .. versionchanged:: 2015.7.0 
 Add ``monitor_type`` parameter. 
 .. versionchanged:: 2015.7.0 
 Add ``name`` parameter. 
 .. versionchanged:: 2015","'Modify an existing monitor.  If it does exists, only 
 the parameters specified will be enforced. 
 hostname 
 The host/address of the bigip device 
 username 
 The iControl REST username 
 password 
 The iControl REST password 
 monitor_type 
 The type of monitor to create 
 name 
 The name of the monitor to create 
 kwargs 
 [ arg=val ] ... 
 Consult F5 BIGIP user guide for specific options for each monitor type. 
 Typically, tmsh arg names are used.'"
"def sample_from_model(hps, logdir, traindir): 
    hps.batch_size = 100 
   with tf.Graph().as_default(): 
      with tf.device('/cpu:0'): 
         with tf.variable_scope('model') as var_scope: 
            eval_model = RealNVP(hps, sampling=True) 
            summary_writer = tf.summary.FileWriter(logdir) 
            var_scope.reuse_variables() 
            summary_op = tf.summary.merge_all() 
         saver = tf.train.Saver() 
         sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) 
         coord = tf.train.Coordinator() 
         threads = tf.train.start_queue_runners(sess=sess, coord=coord) 
         previous_global_step = 0 
         initialized = False 
         with sess.as_default(): 
            while True: 
               ckpt_state = tf.train.get_checkpoint_state(traindir) 
               if (not (ckpt_state and ckpt_state.model_checkpoint_path)): 
                  if (not initialized): 
                     print ('No   model   to   eval   yet   at   %s' % traindir) 
                     time.sleep(30) 
                     continue 
               else: 
                  print ('Loading   file   %s' % ckpt_state.model_checkpoint_path) 
                  saver.restore(sess, ckpt_state.model_checkpoint_path) 
               current_step = tf.train.global_step(sess, eval_model.step) 
               if (current_step == previous_global_step): 
                  print 'Waiting   for   the   checkpoint   to   be   updated.' 
                  time.sleep(30) 
                  continue 
               previous_global_step = current_step 
               fetches = [summary_op] 
               outputs = sess.run(fetches) 
               summary_writer.add_summary(outputs[0], current_step) 
         coord.request_stop() 
         coord.join(threads)"," 'Sample from the model with a fixed batch size. 
 This is used for evaluating the model on the test set.'",'Sampling.'
"@pytest.mark.parametrize('fast_reader', [True, False, 'force']) 
 def test_read_rdb_wrong_type(fast_reader): 
    table = 'col1 DCTB col2\nN DCTB N\n1 DCTB Hello' 
   with pytest.raises(ValueError): 
      ascii.read(table, Reader=ascii.Rdb, fast_reader=fast_reader)", 'Test the read method with a wrong type of Reader','Read RDB data with inconstent data type (except failure)'
"def get_arg_text(ob): 
    arg_text = '' 
   if (ob is not None): 
      arg_offset = 0 
      if (type(ob) in (types.ClassType, types.TypeType)): 
         fob = _find_constructor(ob) 
         if (fob is None): 
            fob = (lambda : None) 
         else: 
            arg_offset = 1 
      elif (type(ob) == types.MethodType): 
         fob = ob.im_func 
         arg_offset = 1 
      else: 
         fob = ob 
      if (type(fob) in [types.FunctionType, types.LambdaType]): 
         argcount = fob.func_code.co_argcount 
         real_args = fob.func_code.co_varnames[arg_offset:argcount] 
         defaults = (fob.func_defaults or []) 
         defaults = list(map((lambda name: ('=%s' % repr(name))), defaults)) 
         defaults = (([''] * (len(real_args) - len(defaults))) + defaults) 
         items = map((lambda arg, dflt: (arg + dflt)), real_args, defaults) 
         if (fob.func_code.co_flags & 4): 
            items.append('...') 
         if (fob.func_code.co_flags & 8): 
            items.append('***') 
         arg_text = ',   '.join(items) 
         arg_text = ('(%s)' % re.sub('\\.\\d+', '<tuple>', arg_text)) 
      doc = getattr(ob, '__doc__', '') 
      if doc: 
         doc = doc.lstrip() 
         pos = doc.find('\n') 
         if ((pos < 0) or (pos > 70)): 
            pos = 70 
         if arg_text: 
            arg_text += '\n' 
         arg_text += doc[:pos] 
   return arg_text"," 'Returns the text that describes the arguments of the given object. 
 If the object is a method or a function, the text is the text that 
 describes the arguments of the function or method. 
 If the object is a class, the text is the text that describes the 
 arguments of the class\'s constructor. 
 If the object is a tuple, the text is the text that describes the 
 arguments of the tuple. 
 If the object is None, the text is \'None\' 
 If the object is not a tuple, a function, or a method, the text is 
 \'None\'. 
 :param obj: the object whose arguments are to be described 
 :type obj: object'",'Get a string describing the arguments for the given object'
"@sopel.module.commands(u'reload') 
 @sopel.module.priority(u'low') 
 @sopel.module.thread(False) 
 def pm_f_reload(bot, trigger): 
    if trigger.is_privmsg: 
      f_reload(bot, trigger)", 'Reload all plugins','Wrapper for allowing delivery of .reload command via PM'
"def _safeFormat(fmtString, fmtDict): 
    try: 
      text = (fmtString % fmtDict) 
   except KeyboardInterrupt: 
      raise 
   except: 
      try: 
         text = ('Invalid   format   string   or   unformattable   object   in   log   message:   %r,   %s' % (fmtString, fmtDict)) 
      except: 
         try: 
            text = ('UNFORMATTABLE   OBJECT   WRITTEN   TO   LOG   with   fmt   %r,   MESSAGE   LOST' % (fmtString,)) 
         except: 
            text = 'PATHOLOGICAL   ERROR   IN   BOTH   FORMAT   STRING   AND   MESSAGE   DETAILS,   MESSAGE   LOST' 
   return text"," 'Format a log message with a format string and a dict of format arguments. 
 This is a helper function that is used to format log messages. 
 It will raise a ValueError if the format string is invalid or if the 
 format arguments are unformattable objects. 
 :param fmtString: The format string to use for the log message. 
 :param fmtDict: The dict of format arguments to use for the log message. 
 :return: The formatted string. 
 :raise ValueError: If the format string is invalid or if the format 
 arguments are unformattable objects. 
 :raise KeyboardInterrupt: If the user interrupts the logging process. 
 :raise RuntimeError: If the logging process is interrupted by an 
 unhandled exception. 
 :raise ValueError: If the format string is invalid or if the format 
 arguments are unformattable objects. 
 :raise KeyboardInterrupt: If the user interrupts the logging process. 
 :raise RuntimeError: If the logging process is interrupted by an 
 unhandled exception. 
 :raise ValueError: If the format string is invalid or if the format ","'Try to format the string C{fmtString} using C{fmtDict} arguments, 
 swallowing all errors to always return a string.'"
"def getInsetLoopsFromLoop(loop, radius, thresholdRatio=0.9): 
    if (radius == 0.0): 
      return [loop] 
   isInset = (radius > 0) 
   insetLoops = [] 
   isLoopWiddershins = euclidean.isWiddershins(loop) 
   arounds = getAroundsFromLoop(loop, radius, thresholdRatio) 
   for around in arounds: 
      leftPoint = euclidean.getLeftPoint(around) 
      shouldBeWithin = (isInset == isLoopWiddershins) 
      if (euclidean.isPointInsideLoop(loop, leftPoint) == shouldBeWithin): 
         if (isLoopWiddershins != euclidean.isWiddershins(around)): 
            around.reverse() 
         insetLoops.append(around) 
   return insetLoops"," 'Given a loop, return a list of all loops that are inset into the loop. 
 If the radius is 0, the function returns only the given loop. 
 If the radius is not 0, the function returns all loops that are inset into 
 the given loop. 
 :param loop: the loop to be inset 
 :param radius: the radius of the inset loops 
 :param thresholdRatio: the threshold ratio for the euclidean distance 
 between two points. 
 :return: a list of inset loops'","'Get the inset loops, which might overlap.'"
"def testInterface(algo): 
    emptyalgo = algo() 
   try: 
      emptyalgo.learn(0) 
      return 'Failed   to   throw   missing   evaluator   error?' 
   except AssertionError: 
      pass 
   emptyalgo.setEvaluator(sf, xa1) 
   emptyalgo.learn(0) 
   algo(sf, xa1) 
   algo(sf, xa100) 
   algo(sf, numParameters=2) 
   try: 
      algo(sf) 
      return 'Failed   to   throw   unknown   dimension   error' 
   except ValueError: 
      pass 
   algo(ife1) 
   algo(ife2, pc2) 
   return True", 'Test that the interface is correct for the given algorithm.',"'Tests whether the algorithm is properly implementing the 
 correct Blackbox-optimization interface.'"
"def build_api_error(message, **kwargs): 
    return {'developer_message': message.format(**kwargs), 'user_message': _(message).format(**kwargs)}"," 'Builds an APIError object with the given message and extra data. 
 :param message: The message to include in the error. 
 :type message: str 
 :param kwargs: Additional data to include in the message. 
 :type kwargs: dict 
 :return: An APIError object.'","'Build an error dict corresponding to edX API conventions. 
 Args: 
 message (string): The string to use for developer and user messages. 
 The user message will be translated, but for this to work message 
 must have already been scraped. ugettext_noop is useful for this. 
 **kwargs: format parameters for message'"
"def p_definition_token(p): 
    for i in p[3]: 
      if (i[0] not in '\'""'): 
         tokenlist.append(i) 
   if (p[1] == '%left'): 
      preclist.append((('left',) + tuple(p[3]))) 
   elif (p[1] == '%right'): 
      preclist.append((('right',) + tuple(p[3]))) 
   elif (p[1] == '%nonassoc'): 
      preclist.append((('nonassoc',) + tuple(p[3])))"," 'Parse a definition token. 
 p[0] = \'%%definition\' 
 p[1] = \'left\' 
 p[2] = \'a\' 
 p[3] = \'b\' 
 p[4] = \'c\' 
 p[5] = \'d\' 
 p[6] = \'e\' 
 p[7] = \'f\' 
 p[8] = \'g\' 
 p[9] = \'h\' 
 p[10] = \'i\' 
 p[11] = \'j\' 
 p[12] = \'k\' 
 p[13] = \'l\' 
 p[14] = \'m\' 
 p[15] = \'n\' 
 p[16] = \'o\' 
 p[17] = \'p\' 
 p[18] = \'q\' 
 p[19] = \'r\' 
 p[20] = \'s\' 
 p[21]",'definition : toktype opttype idlist optsemi'
"def win32_clipboard_get(): 
    try: 
      import win32clipboard 
   except ImportError: 
      raise TryNext('Getting   text   from   the   clipboard   requires   the   pywin32   extensions:   http://sourceforge.net/projects/pywin32/') 
   win32clipboard.OpenClipboard() 
   try: 
      text = win32clipboard.GetClipboardData(win32clipboard.CF_UNICODETEXT) 
   except (TypeError, win32clipboard.error): 
      try: 
         text = win32clipboard.GetClipboardData(win32clipboard.CF_TEXT) 
         text = py3compat.cast_unicode(text, py3compat.DEFAULT_ENCODING) 
      except (TypeError, win32clipboard.error): 
         raise ClipboardEmpty 
   finally: 
      win32clipboard.CloseClipboard() 
   return text"," 'Get the text from the clipboard. 
 This will work on Windows XP and later, but requires pywin32. 
 :return: The text from the clipboard. 
 :rtype: unicode'","'Get the current clipboard\'s text on Windows. 
 Requires Mark Hammond\'s pywin32 extensions.'"
"def _setup_fixtures(doctest_item): 
    def func(): 
      pass 
   doctest_item.funcargs = {} 
   fm = doctest_item.session._fixturemanager 
   doctest_item._fixtureinfo = fm.getfixtureinfo(node=doctest_item, func=func, cls=None, funcargs=False) 
   fixture_request = FixtureRequest(doctest_item) 
   fixture_request._fillfixtures() 
   return fixture_request"," 'Setup the fixture manager and fixture information for a doctest item. 
 This is called by the doctest module when a doctest item is about to 
 be run. 
 :param doctest_item: The doctest item to be run. 
 :return: The fixture request for the doctest item. 
 :rtype: FixtureRequest'",'Used by DoctestTextfile and DoctestItem to setup fixture information.'
"def uniq_stable(elems): 
    seen = set() 
   return [x for x in elems if ((x not in seen) and (not seen.add(x)))]"," 'Return a list of unique elements in a sequence, in a stable order. 
 This function is based on the algorithm described in 
 http://en.wikipedia.org/wiki/Stable_sort 
 and implemented in the Python stdlib (sorted). 
 >>> list(uniq_stable([1, 1, 2, 2, 3, 3, 4, 4, 5])) 
 [1, 2, 3, 4, 5] 
 >>> list(uniq_stable([1, 1, 2, 2, 3, 3, 4, 4, 5, 5])) 
 [1, 2, 3, 4, 5] 
 >>> list(uniq_stable([1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 5])) 
 [1, 2, 3, 4, 5] 
 >>> list(uniq_stable([1, 1, 2, 2, 3, 3, 4, 4,","'uniq_stable(elems) -> list 
 Return from an iterable, a list of all the unique elements in the input, 
 but maintaining the order in which they first appear. 
 Note: All elements in the input must be hashable for this routine 
 to work, as it internally uses a set for efficiency reasons.'"
"def migrate_cohort_settings(course): 
    (cohort_settings, created) = CourseCohortsSettings.objects.get_or_create(course_id=course.id, defaults={'is_cohorted': course.is_cohorted, 'cohorted_discussions': list(course.cohorted_discussions), 'always_cohort_inline_discussions': course.always_cohort_inline_discussions}) 
   if created: 
      manual_cohorts = CourseUserGroup.objects.filter(course_id=course.id, group_type=CourseUserGroup.COHORT).exclude(name__in=course.auto_cohort_groups) 
      for cohort in manual_cohorts: 
         CourseCohort.create(course_user_group=cohort) 
      for group_name in course.auto_cohort_groups: 
         CourseCohort.create(cohort_name=group_name, course_id=course.id, assignment_type=CourseCohort.RANDOM) 
   return cohort_settings"," 'Migrate course cohort settings to CourseCohortsSettings. 
 This function will migrate the course cohort settings from 
 CourseCohorts to CourseCohortsSettings. 
 This will be run for all courses that have been migrated to 
 Django 1.9, and should be run once. 
 :param course: The course to migrate. 
 :type course: Course'","'Migrate all the cohort settings associated with this course from modulestore to mysql. 
 After that we will never touch modulestore for any cohort related settings.'"
"@register.tag 
 def app_reverse(parser, token): 
    bits = token.split_contents() 
   if (len(bits) < 3): 
      raise TemplateSyntaxError((u""'%s'   takes   at   least   two   arguments   (path   to   a   view   and   a   urlconf)"" % bits[0])) 
   viewname = parser.compile_filter(bits[1]) 
   urlconf = parser.compile_filter(bits[2]) 
   args = [] 
   kwargs = {} 
   asvar = None 
   bits = bits[3:] 
   if ((len(bits) >= 2) and (bits[(-2)] == u'as')): 
      asvar = bits[(-1)] 
      bits = bits[:(-2)] 
   if len(bits): 
      for bit in bits: 
         match = kwarg_re.match(bit) 
         if (not match): 
            raise TemplateSyntaxError(u'Malformed   arguments   to   app_reverse   tag') 
         (name, value) = match.groups() 
         if name: 
            kwargs[name] = parser.compile_filter(value) 
         else: 
            args.append(parser.compile_filter(value)) 
   return AppReverseNode(viewname, urlconf, args, kwargs, asvar)"," 'Reverse a URL to a view. 
 The template tag ``{% app_reverse %}`` takes three arguments: 
 - The name of the view to reverse to. 
 - The URLconf to use to reverse to the view. 
 - An optional dictionary of keyword arguments to pass to the view. 
 - An optional variable to use to store the URL that is generated. 
 If the view accepts arguments, you can also pass a sequence of 
 arguments to the view. 
 If the view accepts keyword arguments, you can also pass a dictionary 
 of keyword arguments to the view. 
 If the view accepts both arguments and keyword arguments, you can 
 pass both a sequence of arguments and a dictionary of keyword arguments 
 to the view. 
 If the view accepts both arguments and keyword arguments, you can 
 pass both a sequence of arguments and a dictionary of keyword arguments 
 to the view. 
 If the view accepts both arguments and keyword arguments, you can 
 pass both a sequence of arguments and a dictionary of keyword arguments 
 to the view. 
 If the view accepts both arguments and keyword arguments, you can 
 pass both a sequence of arguments and a dictionary","'Returns an absolute URL for applications integrated with ApplicationContent 
 The tag mostly works the same way as Django\'s own {% url %} tag:: 
 {% load applicationcontent_tags %} 
 {% app_reverse ""mymodel_detail"" ""myapp.urls"" arg1 arg2 %} 
 or 
 {% load applicationcontent_tags %} 
 {% app_reverse ""mymodel_detail"" ""myapp.urls"" name1=value1 %} 
 The first argument is a path to a view. The second argument is the URLconf 
 under which this app is known to the ApplicationContent. The second 
 argument may also be a request object if you want to reverse an URL 
 belonging to the current application content. 
 Other arguments are space-separated values that will be filled in place of 
 positional and keyword arguments in the URL. Don\'t mix positional and 
 keyword arguments. 
 If you want to store the URL in a variable instead of showing it right away 
 you can do so too:: 
 {% app_reverse ""mymodel_detail"" ""myapp.urls"" arg1 arg2 as url %}'"
"@register.filter 
 def break_long_headers(header): 
    if ((len(header) > 160) and (u',' in header)): 
      header = mark_safe((u'<br>   ' + u',   <br>'.join(header.split(u',')))) 
   return header"," 'Break long headers into lines of 160 characters or less. 
 The header is split on commas and joined with a line break. 
 :param header: The header to be broken. 
 :type header: str 
 :return: The header with any long lines broken.'","'Breaks headers longer than 160 characters (~page length) 
 when possible (are comma separated)'"
"def _ensure_exists(name, path=None): 
    if (not exists(name, path=path)): 
      raise CommandExecutionError(""Container   '{0}'   does   not   exist"".format(name))"," 'Ensure that a container exists. 
 :param name: Name of the container. 
 :param path: Path to the container. 
 :raises: CommandExecutionError'",'Raise an exception if the container does not exist'
"def _convert_java_pattern_to_python(pattern): 
    s = list(pattern) 
   i = 0 
   while (i < (len(s) - 1)): 
      c = s[i] 
      if ((c == '$') and (s[(i + 1)] in '0123456789')): 
         s[i] = '\\' 
      elif ((c == '\\') and (s[(i + 1)] == '$')): 
         s[i] = '' 
         i += 1 
      i += 1 
   return pattern[:0].join(s)", 'Convert a Java pattern into a Python pattern.','Convert a replacement pattern from the Java-style `$5` to the Python-style `\5`.'
"def rgConv(inpedfilepath, outhtmlname, outfilepath): 
    recode = {'A': '1', 'C': '2', 'G': '3', 'T': '4', 'N': '0', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4'} 
   basename = os.path.split(inpedfilepath)[(-1)] 
   inmap = ('%s.map' % inpedfilepath) 
   inped = ('%s.ped' % inpedfilepath) 
   outf = ('%s.ped' % basename) 
   outfpath = os.path.join(outfilepath, outf) 
   try: 
      mf = open(inmap, 'r') 
   except: 
      sys.stderr.write(('%s   cannot   open   inmap   file   %s   -   do   you   have   permission?\n' % (prog, inmap))) 
      sys.exit(1) 
   try: 
      rsl = [x.split()[1] for x in mf] 
   except: 
      sys.stderr.write(('##   cannot   parse   %s' % inmap)) 
      sys.exit(1) 
   try: 
      os.makedirs(outfilepath) 
   except: 
      pass 
   head = '   '.join(rsl) 
   pedf = open(inped, 'r') 
   o = open(outfpath, 'w', (2 ** 20)) 
   o.write(head) 
   o.write('\n') 
   for (i, row) in enumerate(pedf): 
      if (i == 0): 
         lrow = row.split() 
         try: 
            [int(x) for x in lrow[10:50]] 
         except: 
            dorecode = 1 
      if dorecode: 
         lrow = row.strip().split() 
         p = lrow[:6] 
         g = lrow[6:] 
         gc = [recode.get(z, '0') for z in g] 
         lrow = (p + gc) 
         row = ('%s\n' % '   '.join(lrow)) 
      o.write(row) 
   o.close()"," 'Convert a ped file into a html file. 
 :param inpedfilepath: path to the ped file 
 :param outhtmlname: name of the html file 
 :param outfilepath: path to the output file'",'convert linkage ped/map to fbat'
"def format_correlation_info(corr_coeff, param_p_val, nonparam_p_val, conf_interval, num_permutations, header=''): 
    result = '' 
   if (header != ''): 
      result += (header + '\n') 
   result += ('Correlation   coefficient DCTB Parametric   p-value DCTB ' + 'Nonparametric   p-value DCTB CI   (lower) DCTB CI   (upper)\n') 
   if (num_permutations > 0): 
      nonparam_p_val_str = format_p_value_for_num_iters(nonparam_p_val, num_permutations) 
   else: 
      nonparam_p_val_str = 'N/A' 
   if (conf_interval == (None, None)): 
      conf_interval_str = 'N/A DCTB N/A' 
   else: 
      conf_interval_str = ('%.4f DCTB %.4f' % conf_interval) 
   result += ('%.4f DCTB %.4f DCTB %s DCTB %s\n' % (corr_coeff, param_p_val, nonparam_p_val_str, conf_interval_str)) 
   return result", 'Format the correlation info as a table for the DCTB.',"'Formats correlation information to be suitable for writing to a file. 
 Returns a string containing a header and a single line (with a newline at 
 the end) that has the input correlation information in tab-separated 
 format, with nonparametric p-value formatted according to the number of 
 permutations. 
 If the confidence interval is not valid for this dataset (i.e. the 
 input CI is (None, None)), the confidence interval will be formatted as 
 \'N/A\' for both lower and upper endpoints. 
 Arguments: 
 corr_coeff - the correlation coefficient (a float) 
 param_p_val - the parametric p-value (a float) 
 nonparam_p_val - the nonparametric p-value (a float) 
 conf_interval - a tuple containing the lower and upper bounds of the 
 confidence interval 
 num_permutations - the number of permutations that were used to 
 calculate the nonparametric p-value. Will be used to format the 
 correct number of digits for this p-value. If less than 1, the 
 p-value will be \'N/A\' 
 header - if provided, this string will be inserted at the beginning of 
 the returned string. For example, might be useful to add a comment 
 describing what correlation coefficient was used. This string does 
 not need to contain a newline at the end'"
"def test_mark_done(pg_xlog): 
    seg = make_segment(1, explicit=False) 
   pg_xlog.touch(seg.name, '.ready') 
   seg.mark_done()", 'Test mark_done()','Check non-explicit segments can be .mark_done\'d.'
"def dict_union(*dicts, **kwargs): 
    dicts = list(dicts) 
   if (dicts and isinstance(dicts[0], OrderedDict)): 
      result = OrderedDict() 
   else: 
      result = {} 
   for d in (list(dicts) + [kwargs]): 
      duplicate_keys = (set(result.keys()) & set(d.keys())) 
      if duplicate_keys: 
         raise ValueError('The   following   keys   have   duplicate   entries:   {}'.format(',   '.join((str(key) for key in duplicate_keys)))) 
      result.update(d) 
   return result"," 'Union of dictionaries. 
 Parameters 
 dicts : dict 
 A sequence of dictionaries to union. 
 Returns 
 dict 
 The union of the dictionaries. 
 Raises 
 ValueError 
 If there are duplicate keys in the dictionaries. 
 Example 
 >>> dict_union({\'a\' : 1}, {\'a\' : 2}) 
 {\'a\' : 2}'","'Return union of a sequence of disjoint dictionaries. 
 Parameters 
 dicts : dicts 
 A set of dictionaries with no keys in common. If the first 
 dictionary in the sequence is an instance of `OrderedDict`, the 
 result will be OrderedDict. 
 \*\*kwargs 
 Keywords and values to add to the resulting dictionary. 
 Raises 
 ValueError 
 If a key appears twice in the dictionaries or keyword arguments.'"
"def disabled(name, **kwargs): 
    return (not enabled(name, **kwargs))", 'Return True if the given name is disabled.',"'Return True if the named service is enabled, false otherwise 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' service.disabled <service name>'"
"def make_path_searcher(path_generator, target_predicate, target_normalizer, extra_paths, **kwargs): 
    def path_searcher(target, extra_dirs=extra_paths): 
      matches = itertools.ifilter(target_predicate, path_generator(target, extra_dirs, **kwargs)) 
      paths = itertools.imap(target_normalizer, matches) 
      return next(paths, '') 
   return path_searcher"," 'Create a searcher for paths in a given directory tree. 
 :param path_generator: A generator that yields paths. 
 :param target_predicate: A predicate that returns True for paths that should be 
 searched. 
 :param target_normalizer: A function that normalizes the path. 
 :param extra_paths: Extra paths to search. 
 :param kwargs: Keyword arguments to pass to the generator. 
 :returns: A searcher function that yields paths. 
 :rtype: callable'","'Universal search function generator using lazy evaluation. 
 Generate a function that will iterate over all the paths from path_generator using 
 target_predicate to filter matching paths.  Each matching path is then noramlized by target_predicate. 
 Only the first match is returned. 
 :param path_generator: all paths to test with target_predicate 
 :type path_generator: iterator 
 :param target_predicate: boolean function that tests a given path 
 :type target_predicate: function 
 :param target_normalizer: function that transforms a matching path to some noramlized form 
 :type target_normalizer: function 
 :param extra_paths: extra paths to pass to the path_generator 
 :type extra_paths: iterator 
 :return: the path searching function 
 :rtype:  function'"
"def validate(): 
    cmd = 'monit   validate' 
   return (not __salt__['cmd.retcode'](cmd, python_shell=False))"," 'Check if the monit service is running and valid 
 This will run the monit validate command to check if the 
 monit service is running and valid. 
 This is used to check if the monit service is running and 
 valid before running the monit start command. 
 This function will return True if the monit service is valid 
 and False otherwise. 
 Example: 
 .. code-block:: bash 
 salt-call monit.validate'","'.. versionadded:: 2016.3.0 
 Check all services 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' monit.validate'"
"def update_transferability(site_id=None): 
    db = current.db 
   s3db = current.s3db 
   now = current.request.utcnow 
   from dateutil.relativedelta import relativedelta 
   TODAY = now.date() 
   ONE_YEAR_AGO = (now - relativedelta(years=1)).date() 
   ptable = s3db.pr_person 
   ctable = s3db.dvr_case 
   stable = s3db.dvr_case_status 
   ftable = s3db.dvr_case_flag 
   cftable = s3db.dvr_case_flag_case 
   utable = s3db.cr_shelter_unit 
   rtable = s3db.cr_shelter_registration 
   ttable = s3db.dvr_case_appointment_type 
   atable = s3db.dvr_case_appointment 
   COMPLETED = 4 
   NOT_REQUIRED = 7 
   query = (ctable.deleted != True) 
   db(query).update(transferable=False, household_transferable=False) 
   query = (((ttable.name == 'Reported   Transferable') | (ttable.name == 'Transfer')) & (ttable.deleted != True)) 
   rows = db(query).select(ttable.id, limitby=(0, 2)) 
   if rows: 
      transferability_complete = set((row.id for row in rows)) 
   else: 
      transferability_complete = None 
   query = (((stable.is_closed == False) | (stable.is_closed == None)) & (stable.deleted != True)) 
   rows = db(query).select(stable.id) 
   if rows: 
      OPEN = set((row.id for row in rows)) 
   else: 
      OPEN = None 
   query = ((ftable.is_not_transferable == True) & (ftable.deleted != True)) 
   rows = db(query).select(ftable.id) 
   if rows: 
      NOT_TRANSFERABLE = set((row.id for row in rows)) 
   else: 
      NOT_TRANSFERABLE = None 
   age_groups = {'children': (None, 15, 'mandatory_children', None), 'adolescents': (15, 18, 'mandatory_adolescents', None), 'adults': (18, None, 'mandatory_adults', 4)} 
   left = [stable.on((stable.id == ctable.status_id)), ptable.on((ptable.id == ctable.person_id)), rtable.on(((rtable.person_id == ptable.id) & (rtable.deleted != True))), utable.on((utable.id == rtable.shelter_unit_id))] 
   if transferability_complete: 
      tctable = atable.with_alias('transferability_complete') 
      tcjoin = tctable.on((((((((tctable.person_id == ctable.person_id) & tctable.type_id.belongs(transferability_complete)) & (tctable.deleted != True)) & (tctable.date != None)) & (tctable.date >= ONE_YEAR_AGO)) & (tctable.date <= TODAY)) & (tctable.status == COMPLETED))) 
      left.append(tcjoin) 
   if NOT_TRANSFERABLE: 
      cfjoin = cftable.on((((cftable.person_id == ctable.person_id) & cftable.flag_id.belongs(NOT_TRANSFERABLE)) & (cftable.deleted != True))) 
      left.append(cfjoin) 
   result = 0 
   for age_group in age_groups: 
      (min_age, max_age, appointment_flag, maximum_absence) = age_groups[age_group] 
      dob_query = (ptable.date_of_birth != None) 
      if max_age: 
         dob_min = (now - relativedelta(years=max_age)) 
         dob_query &= (ptable.date_of_birth > dob_min) 
      if min_age: 
         dob_max = (now - relativedelta(years=min_age)) 
         dob_query &= (ptable.date_of_birth <= dob_max) 
      case_query = ((ctable.deleted != True) & ((ctable.archived == False) | (ctable.archived == None))) 
      if OPEN: 
         case_query &= ctable.status_id.belongs(OPEN) 
      if site_id: 
         case_query &= (ctable.site_id == site_id) 
      case_query &= ((stable.is_not_transferable == False) | (stable.is_not_transferable == None)) 
      if NOT_TRANSFERABLE: 
         case_query &= (cftable.id == None) 
      case_query &= ((utable.id != None) & ((utable.transitory == False) | (utable.transitory == None))) 
      case_query &= dob_query 
      if transferability_complete: 
         case_query &= (tctable.id == None) 
      if (maximum_absence is not None): 
         if maximum_absence: 
            earliest_check_out_date = (now - relativedelta(days=maximum_absence)) 
            presence_query = ((rtable.registration_status == 2) | ((rtable.registration_status == 3) & (rtable.check_out_date > earliest_check_out_date))) 
         else: 
            presence_query = rtable.registration_status.belongs(2, 3) 
         case_query &= presence_query 
      cases = db(case_query).select(ctable.id, left=left) 
      case_ids = set((case.id for case in cases)) 
      if case_ids: 
         query = ctable.id.belongs(case_ids) 
         aleft = [] 
         if appointment_flag: 
            tquery = ((ttable[appointment_flag] == True) & (ttable.deleted != True)) 
            rows = db(tquery).select(ttable.id) 
            mandatory_appointments = [row.id for row in rows] 
         else: 
            mandatory_appointments = None 
         if mandatory_appointments: 
            for appointment_type_id in mandatory_appointments: 
               alias = ('appointments_%s' % appointment_type_id) 
               atable_ = atable.with_alias(alias) 
               join = atable_.on(((((atable_.person_id == ctable.person_id) & (atable_.type_id == appointment_type_id)) & (atable_.deleted != True)) & (((((atable_.status == COMPLETED) & (atable_.date != None)) & (atable_.date >= ONE_YEAR_AGO)) & (atable_.date <= TODAY)) | (atable_.status == NOT_REQUIRED)))) 
               aleft.append(join) 
               query &= (atable_.id != None) 
            cases = db(query).select(ctable.id, left=aleft) 
            case_ids = set((case.id for case in cases)) 
         success = db(ctable.id.belongs(case_ids)).update(transferable=True) 
         if success: 
            result += success 
   gtable = s3db.pr_group 
   mtable = s3db.pr_group_membership 
   query = (((gtable.group_type == 7) & (gtable.deleted != True)) & (ctable.id != None)) 
   left = [mtable.on(((mtable.group_id == gtable.id) & (mtable.deleted != True))), ctable.on(((ctable.person_id == mtable.person_id) & (ctable.transferable == True)))] 
   members = ctable.id.count() 
   rows = db(query).select(gtable.id, groupby=gtable.id, having=(members == 0), left=left) 
   group_ids = set((row.id for row in rows)) 
   open_case = ((ctable.archived != True) & (ctable.deleted != True)) 
   if OPEN: 
      open_case = (ctable.status_id.belongs(OPEN) & open_case) 
   if group_ids: 
      query &= (~ gtable.id.belongs(group_ids)) 
   left = [mtable.on(((mtable.group_id == gtable.id) & (mtable.deleted != True))), ctable.on((((ctable.person_id == mtable.person_id) & open_case) & ((ctable.transferable == False) | (ctable.transferable == None))))] 
   if transferability_complete: 
      left.append(tcjoin) 
      query &= (tctable.id == None) 
   rows = db(query).select(gtable.id, groupby=gtable.id, left=left) 
   group_ids |= set((row.id for row in rows)) 
   ftable = mtable.with_alias('family') 
   left = [mtable.on((((mtable.person_id == ctable.person_id) & mtable.group_id.belongs(group_ids)) & (mtable.deleted != True))), gtable.on(((((ftable.person_id == ctable.person_id) & (ftable.deleted != True)) & (gtable.id == ftable.group_id)) & (gtable.group_type == 7)))] 
   query = ((mtable.id == None) & (ctable.deleted != True)) 
   families = gtable.id.count() 
   required = ((families > 0) | (ctable.transferable == True)) 
   rows = db(query).select(ctable.id, groupby=ctable.id, having=required, left=left) 
   case_ids = set((row.id for row in rows)) 
   if case_ids: 
      db(ctable.id.belongs(case_ids)).update(household_transferable=True) 
   return result"," 'Update transferability 
 @param site_id: site id for site-specific transferability 
 @return: number of cases updated'","'Update transferability status of all cases, to be called either 
 from scheduler task or manually through custom controller. 
 @param site_id: the site to check for transferability of cases'"
"def project_indicator_progress_report(r, **attr): 
    if ((r.representation == 'html') and (r.name == 'project')): 
      T = current.T 
      db = current.db 
      s3db = current.s3db 
      project_id = r.id 
      item = TABLE(_class='project_indicator_progress_report') 
      output = dict(item=item) 
      output['title'] = T('Monthly   Progress   by   Indicator') 
      output['subtitle'] = ('%s:   %s' % (T('Project'), r.record.name)) 
      if ('rheader' in attr): 
         rheader = attr['rheader'](r) 
         if rheader: 
            output['rheader'] = rheader 
      current.response.view = 'simple.html' 
      return output 
   else: 
      raise HTTP(405, current.ERROR.BAD_METHOD)", 'Project Indicator Progress Report','@ToDo: Display the Progress of a Project'
"def find_module(module, paths=None): 
    parts = module.split('.') 
   while parts: 
      part = parts.pop(0) 
      (f, path, (suffix, mode, kind)) = info = imp.find_module(part, paths) 
      if (kind == PKG_DIRECTORY): 
         parts = (parts or ['__init__']) 
         paths = [path] 
      elif parts: 
         raise ImportError((""Can't   find   %r   in   %s"" % (parts, module))) 
   return info"," 'Find a module in the given paths. 
 This is a wrapper around the ``imp.find_module`` function. 
 :param module: The module to find. 
 :param paths: A list of paths to search. 
 :returns: The module info tuple. 
 :rtype: tuple'","'Just like \'imp.find_module()\', but with package support'"
"def make_layout(doc, meta, format_data=None): 
    (layout, page) = ([], []) 
   layout.append(page) 
   if format_data: 
      if (format_data[0].get(u'fieldname') == u'print_heading_template'): 
         doc.print_heading_template = format_data[0].get(u'options') 
         format_data = format_data[1:] 
   def get_new_section(): 
      return {u'columns': [], u'has_data': False} 
   def append_empty_field_dict_to_page_column(page): 
      u'   append   empty   columns   dict   to   page   layout   ' 
      if (not page[(-1)][u'columns']): 
         page[(-1)][u'columns'].append({u'fields': []}) 
   for df in (format_data or meta.fields): 
      if format_data: 
         df = frappe._dict(df) 
         if df.fieldname: 
            original = meta.get_field(df.fieldname) 
            if original: 
               newdf = original.as_dict() 
               newdf.update(df) 
               df = newdf 
         df.print_hide = 0 
      if ((df.fieldtype == u'Section   Break') or (page == [])): 
         if (len(page) > 1): 
            if (page[(-1)][u'has_data'] == False): 
               del page[(-1)] 
         section = get_new_section() 
         if ((df.fieldtype == u'Section   Break') and df.label): 
            section[u'label'] = df.label 
         page.append(section) 
      elif (df.fieldtype == u'Column   Break'): 
         page[(-1)][u'columns'].append({u'fields': []}) 
      else: 
         append_empty_field_dict_to_page_column(page) 
      if ((df.fieldtype == u'HTML') and df.options): 
         doc.set(df.fieldname, True) 
      if (is_visible(df, doc) and has_value(df, doc)): 
         append_empty_field_dict_to_page_column(page) 
         page[(-1)][u'columns'][(-1)][u'fields'].append(df) 
         page[(-1)][u'has_data'] = True 
         if (df.fieldtype == u'Table'): 
            df.rows = [] 
            df.start = 0 
            df.end = None 
            for (i, row) in enumerate(doc.get(df.fieldname)): 
               if row.get(u'page_break'): 
                  df.end = i 
                  page = [get_new_section()] 
                  layout.append(page) 
                  append_empty_field_dict_to_page_column(page) 
                  df = copy.copy(df) 
                  df.start = i 
                  df.end = None 
                  page[(-1)][u'columns'][(-1)][u'fields'].append(df) 
   return layout"," 'Builds layout for docx document. 
 @param doc: docx document 
 @param meta: meta data for doc 
 @param format_data: list of fields to be formatted, first item is 
 print_heading_template options 
 @return: layout and page'","'Builds a hierarchical layout object from the fields list to be rendered 
 by `standard.html` 
 :param doc: Document to be rendered. 
 :param meta: Document meta object (doctype). 
 :param format_data: Fields sequence and properties defined by Print Format Builder.'"
"def create_rpc(deadline=None, callback=None): 
    if (deadline is None): 
      deadline = get_default_fetch_deadline() 
   return apiproxy_stub_map.UserRPC('urlfetch', deadline, callback)"," 'Create a new UserRPC object. 
 The UserRPC object can be used to make HTTP requests from JavaScript. 
 It can also be used to make HTTP requests from Python. 
 For example, to make an HTTP request from JavaScript, use: 
 | call: UserRPC.fetch(url, \'GET\', \'callback\') 
 | args: url, method, callback 
 | returns: The response object from the server.'","'Creates an RPC object for use with the urlfetch API. 
 Args: 
 deadline: Optional deadline in seconds for the operation; the default 
 is a system-specific deadline (typically 5 seconds). 
 callback: Optional callable to invoke on completion. 
 Returns: 
 An apiproxy_stub_map.UserRPC object specialized for this service.'"
"def get_default_access_key_id(): 
    access_key_id_script = AWS_ACCOUNTS['default'].ACCESS_KEY_ID_SCRIPT.get() 
   return (access_key_id_script or get_s3a_access_key())", 'Return the default access key id for the S3A client.',"'Attempt to set AWS access key ID from script, else core-site, else None'"
"def cache_relation(descriptor, timeout=None): 
    rel = descriptor.related 
   related_name = ('%s_cache' % rel.field.related_query_name()) 
   @property 
   def get(self): 
      '\n                        Returns   the   cached   value   of   the   related   model   if   found\n                        in   the   cache.   Otherwise   gets   and   caches   the   related   model.\n                        ' 
      try: 
         return getattr(self, descriptor.cache_name) 
      except AttributeError: 
         pass 
      try: 
         return getattr(self, ('_%s_cache' % related_name)) 
      except AttributeError: 
         pass 
      instance = get_instance(rel.model, self.pk, timeout) 
      setattr(self, ('_%s_cache' % related_name), instance) 
      return instance 
   setattr(rel.parent_model, related_name, get) 
   def clear(self): 
      '\n                        Clears   the   cache   of   all   related   models   of   self.\n                        ' 
      delete_instance(rel.model, self) 
   @classmethod 
   def clear_pk(cls, *instances_or_pk): 
      '\n                        Clears   the   cache   of   all   related   models   of\n                        the   provided   instances_or_pk.\n                        ' 
      delete_instance(rel.model, *instances_or_pk) 
   def clear_cache(sender, instance, *args, **kwargs): 
      '\n                        Clears   the   cache   of   all   related   models   of   the\n                        given   instance.\n                        ' 
      delete_instance(rel.model, instance) 
   setattr(rel.parent_model, ('%s_clear' % related_name), clear) 
   setattr(rel.parent_model, ('%s_clear_pk' % related_name), clear_pk) 
   post_save.connect(clear_cache, sender=rel.model, weak=False) 
   post_delete.connect(clear_cache, sender=rel.model, weak=False)"," 'Decorate a related field with a cache. 
 The cache is stored in the cache attribute of the related model. 
 If the related model is not found in the cache, it is loaded from the 
 database and cached. 
 If the related model is already in the cache, the cache is used instead of 
 loading it from the database. 
 The cache is cleared when the related model is saved or deleted. 
 The cache timeout is set to the same as the one for the related model. 
 If the related model does not have a cache, the cache is not used. 
 If the related model has a cache, but no cache timeout is set, the cache 
 timeout is set to the same as the one for the related model. 
 The cache timeout is set to None to disable the cache. 
 The cache is cleared when the related model is saved or deleted. 
 If the related model does not have a cache, the cache is not used. 
 If the related model has a cache, but no cache timeout is set, the cache 
 timeout is set to the same as the one for the related model. 
 The cache timeout is set to None to disable the","'Adds utility methods to a model to obtain related 
 model instances via a cache.'"
"def tuplize(seq): 
    if isinstance(seq, (list, tuple)): 
      return tuple((tuplize(i) for i in seq)) 
   return seq"," 'Tuples are immutable, so we need to make a copy of a sequence 
 of tuples, if it is a list or tuple.'",'Turn all nested sequences to tuples in given sequence.'
"@conf.commands.register 
 def tshark(*args, **kargs): 
    sniff(prn=(lambda x: x.display()), *args, **kargs)"," 'Run a TCP/IP packet sniffer. 
 .. versionadded:: 0.9.1 
 :param args: 
 :param kargs: 
 :return: 
 .. seealso:: 
 :ref:`tshark-man` 
 :ref:`tshark-usage` 
 :ref:`tshark-examples`'","'Sniff packets and print them calling pkt.show(), a bit like text wireshark'"
"def make_sparse_random_conv2D(num_nonzero, input_space, output_space, kernel_shape, pad=0, kernel_stride=(1, 1), border_mode='valid', message='', rng=None, partial_sum=None): 
    rng = make_np_rng(rng, default_sparse_seed, which_method=['randn', 'randint']) 
   W = np.zeros((input_space.num_channels, kernel_shape[0], kernel_shape[1], output_space.num_channels)) 
   def random_coord(): 
      return [rng.randint(dim) for dim in W.shape[0:3]] 
   for o in xrange(output_space.num_channels): 
      for i in xrange(num_nonzero): 
         (ch, r, c) = random_coord() 
         while (W[(ch, r, c, o)] != 0): 
            (ch, r, c) = random_coord() 
         W[(ch, r, c, o)] = rng.randn() 
   W = sharedX(W) 
   return Conv2D(filters=W, input_axes=input_space.axes, output_axes=output_space.axes, kernel_stride=kernel_stride, pad=pad, message=message, partial_sum=partial_sum)"," 'Create a sparse random convolution layer. 
 Parameters 
 num_nonzero : int 
 Number of nonzero values in the kernel. 
 input_space : keras.layers.Layer 
 Input layer. 
 output_space : keras.layers.Layer 
 Output layer. 
 kernel_shape : list 
 Shape of the kernel. 
 pad : int 
 Padding. 
 kernel_stride : list 
 Stride of the kernel. 
 border_mode : \'valid\' or \'same\' 
 Border mode. 
 message : string 
 Message to be printed. 
 rng : RandomState 
 Random state. 
 partial_sum : boolean 
 If True, the layer will be a partial sum layer. 
 Returns 
 layer : keras.layers.Layer 
 The convolution layer. 
 Examples 
 >>> from keras.layers import Input, Dense, Dropout, Flatten, Conv2D 
 >>> from keras.models import Model 
 >>> from keras.utils import to_categorical 
 >>> input_shape = (10, 10","'.. todo:: 
 WRITEME properly 
 Creates a Conv2D with random kernels, where the randomly initialized 
 values are sparse'"
"def infixNotation(baseExpr, opList, lpar=Suppress('('), rpar=Suppress(')')): 
    ret = Forward() 
   lastExpr = (baseExpr | ((lpar + ret) + rpar)) 
   for (i, operDef) in enumerate(opList): 
      (opExpr, arity, rightLeftAssoc, pa) = (operDef + (None,))[:4] 
      if (arity == 3): 
         if ((opExpr is None) or (len(opExpr) != 2)): 
            raise ValueError('if   numterms=3,   opExpr   must   be   a   tuple   or   list   of   two   expressions') 
         (opExpr1, opExpr2) = opExpr 
      thisExpr = Forward() 
      if (rightLeftAssoc == opAssoc.LEFT): 
         if (arity == 1): 
            matchExpr = (FollowedBy((lastExpr + opExpr)) + Group((lastExpr + OneOrMore(opExpr)))) 
         elif (arity == 2): 
            if (opExpr is not None): 
               matchExpr = (FollowedBy(((lastExpr + opExpr) + lastExpr)) + Group((lastExpr + OneOrMore((opExpr + lastExpr))))) 
            else: 
               matchExpr = (FollowedBy((lastExpr + lastExpr)) + Group((lastExpr + OneOrMore(lastExpr)))) 
         elif (arity == 3): 
            matchExpr = (FollowedBy(((((lastExpr + opExpr1) + lastExpr) + opExpr2) + lastExpr)) + Group(((((lastExpr + opExpr1) + lastExpr) + opExpr2) + lastExpr))) 
         else: 
            raise ValueError('operator   must   be   unary   (1),   binary   (2),   or   ternary   (3)') 
      elif (rightLeftAssoc == opAssoc.RIGHT): 
         if (arity == 1): 
            if (not isinstance(opExpr, Optional)): 
               opExpr = Optional(opExpr) 
            matchExpr = (FollowedBy((opExpr.expr + thisExpr)) + Group((opExpr + thisExpr))) 
         elif (arity == 2): 
            if (opExpr is not None): 
               matchExpr = (FollowedBy(((lastExpr + opExpr) + thisExpr)) + Group((lastExpr + OneOrMore((opExpr + thisExpr))))) 
            else: 
               matchExpr = (FollowedBy((lastExpr + thisExpr)) + Group((lastExpr + OneOrMore(thisExpr)))) 
         elif (arity == 3): 
            matchExpr = (FollowedBy(((((lastExpr + opExpr1) + thisExpr) + opExpr2) + thisExpr)) + Group(((((lastExpr + opExpr1) + thisExpr) + opExpr2) + thisExpr))) 
         else: 
            raise ValueError('operator   must   be   unary   (1),   binary   (2),   or   ternary   (3)') 
      else: 
         raise ValueError('operator   must   indicate   right   or   left   associativity') 
      if pa: 
         matchExpr.setParseAction(pa) 
      thisExpr <<= (matchExpr | lastExpr) 
      lastExpr = thisExpr 
   ret <<= lastExpr 
   return ret"," 'Construct a parse tree for an expression using infix notation. 
 This function takes an expression using infix notation and constructs a 
 parse tree. 
 Parameters 
 baseExpr : Expr 
 The expression to be parsed. 
 opList : List[Tuple[opDef, int, opAssoc, ParseAction]] 
 The list of operators to be used in the expression. 
 lpar : Expr 
 The opening parenthesis for the expression. 
 rpar : Expr 
 The closing parenthesis for the expression. 
 Returns 
 Expr 
 A parse tree for the expression. 
 Raises 
 ValueError 
 If the expression does not have the correct number of arguments. 
 Examples 
 >>> from sympy.parsing.exprtools import infixNotation 
 >>> from sympy import S, Symbol, sin, cos, exp, log, sqrt, I, oo, pi 
 >>> infixNotation(S(1), [(S(1), 2, opAssoc.LEFT, None)], S(1)) 
 S(1) 
 >>> infix","'Helper method for constructing grammars of expressions made up of 
 operators working in a precedence hierarchy.  Operators may be unary or 
 binary, left- or right-associative.  Parse actions can also be attached 
 to operator expressions. 
 Parameters: 
 - baseExpr - expression representing the most basic element for the nested 
 - opList - list of tuples, one for each operator precedence level in the 
 expression grammar; each tuple is of the form 
 (opExpr, numTerms, rightLeftAssoc, parseAction), where: 
 - opExpr is the pyparsing expression for the operator; 
 may also be a string, which will be converted to a Literal; 
 if numTerms is 3, opExpr is a tuple of two expressions, for the 
 two operators separating the 3 terms 
 - numTerms is the number of terms for this operator (must 
 be 1, 2, or 3) 
 - rightLeftAssoc is the indicator whether the operator is 
 right or left associative, using the pyparsing-defined 
 constants C{opAssoc.RIGHT} and C{opAssoc.LEFT}. 
 - parseAction is the parse action to be associated with 
 expressions matching this operator expression (the 
 parse action tuple member may be omitted) 
 - lpar - expression for matching left-parentheses (default=Suppress(\'(\')) 
 - rpar - expression for matching right-parentheses (default=Suppress(\')\'))'"
"def get_account_created(name): 
    ret = _get_account_policy_data_value(name, 'creationTime') 
   unix_timestamp = salt.utils.mac_utils.parse_return(ret) 
   date_text = _convert_to_datetime(unix_timestamp) 
   return date_text"," 'Get the date when the account was created 
 :param name: the name of the account to retrieve 
 :type name: str 
 :return: the date when the account was created as a string 
 :rtype: str'","'Get the date/time the account was created 
 :param str name: the username of the account 
 :return: the date/time the account was created (yyyy-mm-dd hh:mm:ss) 
 :rtype: str 
 :raises: CommandExecutionError on user not found or any other unknown error 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' shadow.get_account_created admin'"
"@commands(u'iplookup', u'ip') 
 @example(u'.ip   8.8.8.8', u'[IP/Host   Lookup]   Hostname:   google-public-dns-a.google.com   |   Location:   United   States   |   Region:   CA   |   ISP:   AS15169   Google   Inc.', re=True, ignore=u'Downloading   GeoIP   database,   please   wait...') 
 def ip(bot, trigger): 
    if (not trigger.group(2)): 
      return bot.reply(u'No   search   term.') 
   query = trigger.group(2) 
   db_path = _find_geoip_db(bot) 
   if (db_path is False): 
      LOGGER.error(u""Can't   find   (or   download)   usable   GeoIP   database"") 
      bot.say(u""Sorry,   I   don't   have   a   GeoIP   database   to   use   for   this   lookup"") 
      return False 
   geolite_city_filepath = os.path.join(_find_geoip_db(bot), u'GeoLiteCity.dat') 
   geolite_ASN_filepath = os.path.join(_find_geoip_db(bot), u'GeoIPASNum.dat') 
   gi_city = pygeoip.GeoIP(geolite_city_filepath) 
   gi_org = pygeoip.GeoIP(geolite_ASN_filepath) 
   host = socket.getfqdn(query) 
   response = (u'[IP/Host   Lookup]   Hostname:   %s' % host) 
   try: 
      response += (u'   |   Location:   %s' % gi_city.country_name_by_name(query)) 
   except AttributeError: 
      response += u'   |   Location:   Unknown' 
   except socket.gaierror: 
      return bot.say(u'[IP/Host   Lookup]   Unable   to   resolve   IP/Hostname') 
   region_data = gi_city.region_by_name(query) 
   try: 
      region = region_data[u'region_code'] 
   except KeyError: 
      region = region_data[u'region_name'] 
   if region: 
      response += (u'   |   Region:   %s' % region) 
   isp = gi_org.org_by_name(query) 
   response += (u'   |   ISP:   %s' % isp) 
   bot.say(response)"," 'Lookup IP address or hostname. 
 Example: 
 .. code-block:: 
 .. code-block:: 
 :examples: 
 :related: geoip'",'IP Lookup tool'
"def layer_gpx(): 
    tablename = ('%s_%s' % (module, resourcename)) 
   s3db.table(tablename) 
   type = 'GPX' 
   LAYERS = T((TYPE_LAYERS_FMT % type)) 
   ADD_NEW_LAYER = T((ADD_NEW_TYPE_LAYER_FMT % type)) 
   EDIT_LAYER = T((EDIT_TYPE_LAYER_FMT % type)) 
   LIST_LAYERS = T((LIST_TYPE_LAYERS_FMT % type)) 
   NO_LAYERS = T((NO_TYPE_LAYERS_FMT % type)) 
   s3.crud_strings[tablename] = Storage(label_create=ADD_LAYER, title_display=LAYER_DETAILS, title_list=LAYERS, title_update=EDIT_LAYER, label_list_button=LIST_LAYERS, label_delete_button=DELETE_LAYER, msg_record_created=LAYER_ADDED, msg_record_modified=LAYER_UPDATED, msg_record_deleted=LAYER_DELETED, msg_list_empty=NO_LAYERS) 
   def prep(r): 
      if r.interactive: 
         if (r.component_name == 'config'): 
            ltable = s3db.gis_layer_config 
            ltable.base.writable = ltable.base.readable = False 
            if (r.method != 'update'): 
               table = r.table 
               query = ((ltable.layer_id == table.layer_id) & (table.id == r.id)) 
               rows = db(query).select(ltable.config_id) 
               ltable.config_id.requires = IS_ONE_OF(db, 'gis_config.id', '%(name)s', not_filterby='config_id', not_filter_opts=[row.config_id for row in rows]) 
      return True 
   s3.prep = prep 
   def postp(r, output): 
      if (r.interactive and (r.method != 'import')): 
         if (not r.component): 
            inject_enable(output) 
      return output 
   s3.postp = postp 
   output = s3_rest_controller(rheader=s3db.gis_rheader) 
   return output", 'Layer GPX handler','RESTful CRUD controller'
"def _stub_islink(path): 
    return False", 'Returns False if the path is a symlink.',"'Always return \'false\' if the operating system does not support symlinks. 
 @param path: a path string. 
 @type path: L{str} 
 @return: false'"
"def generate_py(bits, randfunc, progress_func=None, e=65537): 
    obj = RSAobj() 
   obj.e = long(e) 
   if progress_func: 
      progress_func('p,q\n') 
   p = q = 1L 
   while (number.size((p * q)) < bits): 
      p = pubkey.getStrongPrime((bits >> 1), obj.e, 1e-12, randfunc) 
      q = pubkey.getStrongPrime((bits - (bits >> 1)), obj.e, 1e-12, randfunc) 
   if (p > q): 
      (p, q) = (q, p) 
   obj.p = p 
   obj.q = q 
   if progress_func: 
      progress_func('u\n') 
   obj.u = pubkey.inverse(obj.p, obj.q) 
   obj.n = (obj.p * obj.q) 
   if progress_func: 
      progress_func('d\n') 
   obj.d = pubkey.inverse(obj.e, ((obj.p - 1) * (obj.q - 1))) 
   assert (bits <= (1 + obj.size())), 'Generated   key   is   too   small' 
   return obj"," 'Generates a RSA key. 
 :param bits: The number of bits in the key. 
 :param randfunc: A function that takes no arguments and returns a 
 random number. 
 :param progress_func: A function that takes no arguments and prints 
 progress messages. 
 :return: A RSAKey object.'","'generate(bits:int, randfunc:callable, progress_func:callable, e:int) 
 Generate an RSA key of length \'bits\', public exponent \'e\'(which must be 
 odd), using \'randfunc\' to get random data and \'progress_func\', 
 if present, to display the progress of the key generation.'"
"@register.tag(u'filter') 
 def do_filter(parser, token): 
    (_, rest) = token.contents.split(None, 1) 
   filter_expr = parser.compile_filter((u'var|%s' % rest)) 
   for (func, unused) in filter_expr.filters: 
      filter_name = getattr(func, u'_filter_name', None) 
      if (filter_name in (u'escape', u'safe')): 
         raise TemplateSyntaxError((u'""filter   %s""   is   not   permitted.      Use   the   ""autoescape""   tag   instead.' % filter_name)) 
   nodelist = parser.parse((u'endfilter',)) 
   parser.delete_first_token() 
   return FilterNode(filter_expr, nodelist)"," 'Renders a filter. 
 .. versionchanged:: 1.4 
 The ``endfilter`` token is now required.'","'Filters the contents of the block through variable filters. 
 Filters can also be piped through each other, and they can have 
 arguments -- just like in variable syntax. 
 Sample usage:: 
 {% filter force_escape|lower %} 
 This text will be HTML-escaped, and will appear in lowercase. 
 {% endfilter %} 
 Note that the ``escape`` and ``safe`` filters are not acceptable arguments. 
 Instead, use the ``autoescape`` tag to manage autoescaping for blocks of 
 template code.'"
"def reshape_text(buffer, from_row, to_row): 
    lines = buffer.text.splitlines(True) 
   lines_before = lines[:from_row] 
   lines_after = lines[(to_row + 1):] 
   lines_to_reformat = lines[from_row:(to_row + 1)] 
   if lines_to_reformat: 
      length = re.search(u'^\\s*', lines_to_reformat[0]).end() 
      indent = lines_to_reformat[0][:length].replace(u'\n', u'') 
      words = u''.join(lines_to_reformat).split() 
      width = ((buffer.text_width or 80) - len(indent)) 
      reshaped_text = [indent] 
      current_width = 0 
      for w in words: 
         if current_width: 
            if (((len(w) + current_width) + 1) > width): 
               reshaped_text.append(u'\n') 
               reshaped_text.append(indent) 
               current_width = 0 
            else: 
               reshaped_text.append(u'   ') 
               current_width += 1 
         reshaped_text.append(w) 
         current_width += len(w) 
      if (reshaped_text[(-1)] != u'\n'): 
         reshaped_text.append(u'\n') 
      buffer.document = Document(text=u''.join(((lines_before + reshaped_text) + lines_after)), cursor_position=len(u''.join((lines_before + reshaped_text))))"," 'Reshape the text in a buffer to fit in the given range. 
 :param buffer: The buffer to reshape. 
 :param from_row: The row to start reshaping from. 
 :param to_row: The row to stop reshaping at. 
 :return: The buffer after reshaping.'","'Reformat text, taking the width into account. 
 `to_row` is included. 
 (Vi \'gq\' operator.)'"
"def _not_a_knot(x, k): 
    x = np.asarray(x) 
   if ((k % 2) != 1): 
      raise ValueError(('Odd   degree   for   now   only.   Got   %s.' % k)) 
   m = ((k - 1) // 2) 
   t = x[(m + 1):((- m) - 1)] 
   t = np.r_[(((x[0],) * (k + 1)), t, ((x[(-1)],) * (k + 1)))] 
   return t"," 'Returns a knot sequence of length k-1, such that the knot is not a knot.'","'Given data x, construct the knot vector w/ not-a-knot BC. 
 cf de Boor, XIII(12).'"
"def _defaultFetcher(url): 
    try: 
      r = urlfetch.fetch(url, method=urlfetch.GET) 
   except urlfetch.Error as e: 
      log.warn((u'Error   opening   url=%r:   %s' % (url, e)), error=IOError) 
   else: 
      if (r.status_code == 200): 
         mimetype = 'application/octet-stream' 
         try: 
            (mimetype, params) = cgi.parse_header(r.headers['content-type']) 
            encoding = params['charset'] 
         except KeyError: 
            encoding = None 
         if (mimetype != u'text/css'): 
            log.error((u'Expected   ""text/css""   mime   type   for   url   %r   but   found:   %r' % (url, mimetype)), error=ValueError) 
         return (encoding, r.content) 
      else: 
         log.warn((u'Error   opening   url=%r:   HTTP   status   %s' % (url, r.status_code)), error=IOError)"," 'Fetches the url and returns a tuple of (encoding, content). 
 If the content is not text/css, a ValueError is raised. 
 :param url: The URL to fetch. 
 :returns: A tuple of (encoding, content).'","'uses GoogleAppEngine (GAE) 
 fetch(url, payload=None, method=GET, headers={}, allow_truncated=False) 
 Response 
 content 
 The body content of the response. 
 content_was_truncated 
 True if the allow_truncated parameter to fetch() was True and 
 the response exceeded the maximum response size. In this case, 
 the content attribute contains the truncated response. 
 status_code 
 The HTTP status code. 
 headers 
 The HTTP response headers, as a mapping of names to values. 
 Exceptions 
 exception InvalidURLError() 
 The URL of the request was not a valid URL, or it used an 
 unsupported method. Only http and https URLs are supported. 
 exception DownloadError() 
 There was an error retrieving the data. 
 This exception is not raised if the server returns an HTTP 
 error code: In that case, the response data comes back intact, 
 including the error code. 
 exception ResponseTooLargeError() 
 The response data exceeded the maximum allowed size, and the 
 allow_truncated parameter passed to fetch() was False.'"
"def newer(source, target): 
    if (not os.path.exists(source)): 
      raise DistutilsFileError((""file   '%s'   does   not   exist"" % os.path.abspath(source))) 
   if (not os.path.exists(target)): 
      return 1 
   from stat import ST_MTIME 
   mtime1 = os.stat(source)[ST_MTIME] 
   mtime2 = os.stat(target)[ST_MTIME] 
   return (mtime1 > mtime2)", 'Return 1 if the source file is newer than the target file.',"'Return true if \'source\' exists and is more recently modified than 
 \'target\', or if \'source\' exists and \'target\' doesn\'t.  Return false if 
 both exist and \'target\' is the same age or younger than \'source\'. 
 Raise DistutilsFileError if \'source\' does not exist.'"
"def restore_asset_from_trashcan(location): 
    trash = contentstore('trashcan') 
   store = contentstore() 
   loc = StaticContent.get_location_from_path(location) 
   content = trash.find(loc) 
   store.save(content) 
   if (content.thumbnail_location is not None): 
      try: 
         thumbnail_content = trash.find(content.thumbnail_location) 
         store.save(thumbnail_content) 
      except Exception: 
         pass"," 'Restores an asset from the trashcan. 
 :param str location: The path to the asset to restore from the trashcan. 
 :return: The restored asset. 
 :rtype: StaticContent'",'This method will restore an asset which got soft deleted and put back in the original course'
"def ToCanonicalJSON(dict, indent=False): 
    return json.dumps(dict, sort_keys=True, indent=indent)", 'Returns a canonical representation of a dict in JSON format.',"'Convert ""dict"" to a canonical JSON string. Sort keys so that output 
 ordering is always the same.'"
"def url_decode_stream(stream, charset='utf-8', decode_keys=False, include_empty=True, errors='replace', separator='&', cls=None, limit=None, return_iterator=False): 
    from werkzeug.wsgi import make_chunk_iter 
   if return_iterator: 
      cls = (lambda x: x) 
   elif (cls is None): 
      cls = MultiDict 
   pair_iter = make_chunk_iter(stream, separator, limit) 
   return cls(_url_decode_impl(pair_iter, charset, decode_keys, include_empty, errors))"," 'Decodes a URL encoded string into a dictionary. 
 :param stream: A string to decode. 
 :param charset: The charset to decode the string to. 
 :param decode_keys: Whether to decode the keys in the dictionary. 
 :param include_empty: Whether to include the empty values in the dictionary. 
 :param errors: The error handling strategy to use. 
 :param separator: The separator between the key and value. 
 :param cls: The class to use for the returned dictionary. 
 :param limit: The maximum number of items to decode. 
 :param return_iterator: Whether to return a generator or an iterator. 
 :returns: A dictionary.'","'Works like :func:`url_decode` but decodes a stream.  The behavior 
 of stream and limit follows functions like 
 :func:`~werkzeug.wsgi.make_line_iter`.  The generator of pairs is 
 directly fed to the `cls` so you can consume the data while it\'s 
 parsed. 
 .. versionadded:: 0.8 
 :param stream: a stream with the encoded querystring 
 :param charset: the charset of the query string.  If set to `None` 
 no unicode decoding will take place. 
 :param decode_keys: Used on Python 2.x to control whether keys should 
 be forced to be unicode objects.  If set to `True`, 
 keys will be unicode in all cases. Otherwise, they 
 remain `str` if they fit into ASCII. 
 :param include_empty: Set to `False` if you don\'t want empty values to 
 appear in the dict. 
 :param errors: the decoding error behavior. 
 :param separator: the pair separator to be used, defaults to ``&`` 
 :param cls: an optional dict class to use.  If this is not specified 
 or `None` the default :class:`MultiDict` is used. 
 :param limit: the content length of the URL data.  Not necessary if 
 a limited stream is provided. 
 :param return_iterator: if set to `True` the `cls` argument is ignored 
 and an iterator over all decoded pairs is 
 returned'"
"def delete_subnet(subnet, profile=None): 
    conn = _auth(profile) 
   return conn.delete_subnet(subnet)"," 'Delete a subnet. 
 :param subnet: Subnet to delete. 
 :type subnet: dict 
 :param profile: Auth profile to use. 
 :type profile: str 
 :returns: None'","'Deletes the specified subnet 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' neutron.delete_subnet subnet-name 
 salt \'*\' neutron.delete_subnet subnet-name profile=openstack1 
 :param subnet: ID or name of subnet to delete 
 :param profile: Profile to build on (Optional) 
 :return: True(Succeed) or False'"
"@_ensure_exists 
 def pid(name): 
    try: 
      return int(info(name).get('PID')) 
   except (TypeError, ValueError) as exc: 
      raise CommandExecutionError(""Unable   to   get   PID   for   container   '{0}':   {1}"".format(name, exc))"," 'Get the PID of a container. 
 This function is intended to be used by the Docker client to retrieve the 
 PID of a container. 
 :param name: The name of the container 
 :return: The PID of the container 
 :raises: CommandExecutionError'","'Returns the PID of a container 
 name 
 Container name 
 CLI Example: 
 .. code-block:: bash 
 salt myminion nspawn.pid arch1'"
"def _get_used_lun_id_counter(mapping): 
    used_luns = _get_used_lun_ids_for_mappings(mapping) 
   used_lun_id_counter = collections.Counter(used_luns) 
   return used_lun_id_counter", 'Get the used LUN IDs for a mapping.','Returns used LUN IDs with count as a dictionary.'
"def _adapt_mismatch(original, matchee): 
    marker = object() 
   if (getattr(original, 'mismatched', marker) is marker): 
      return mismatch(matchee, original.describe(), original.get_details()) 
   return original"," 'Returns a new Matcher that will adapt the original matcher to the 
 given matchee. 
 The original matcher is adapted by replacing all of its \'mismatch\' 
 methods with a new method that calls the original \'mismatch\' method, 
 and then returns a Mismatch object. 
 :param original: The original matcher to adapt. 
 :param matchee: The matchee to adapt to. 
 :return: A new Matcher that adapts the original to the given matchee. 
 :rtype: Matcher'","'If ``original`` doesn\'t already store ``matchee`` then return a new 
 one that has it stored.'"
"def top(**kwargs): 
    if ('id' not in kwargs['opts']): 
      return {} 
   cmd = '{0}   {1}'.format(__opts__['master_tops']['ext_nodes'], kwargs['opts']['id']) 
   ndata = yaml.safe_load(subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).communicate()[0]) 
   if (not ndata): 
      log.info('master_tops   ext_nodes   call   did   not   return   any   data') 
   ret = {} 
   if ('environment' in ndata): 
      env = ndata['environment'] 
   else: 
      env = 'base' 
   if ('classes' in ndata): 
      if isinstance(ndata['classes'], dict): 
         ret[env] = list(ndata['classes']) 
      elif isinstance(ndata['classes'], list): 
         ret[env] = ndata['classes'] 
      else: 
         return ret 
   else: 
      log.info('master_tops   ext_nodes   call   did   not   have   a   dictionary   with   a   ""classes""   key.') 
   return ret", 'Returns a dictionary of the top classes by environment.','Run the command configured'
"def test_attributes(mixin_cols): 
    m = mixin_cols['m'] 
   m.info.name = 'a' 
   assert (m.info.name == 'a') 
   m.info.description = 'a' 
   assert (m.info.description == 'a') 
   if isinstance(m, (u.Quantity, coordinates.SkyCoord, time.Time)): 
      with pytest.raises(AttributeError): 
         m.info.unit = u.m 
   else: 
      m.info.unit = u.m 
      assert (m.info.unit is u.m) 
   m.info.format = 'a' 
   assert (m.info.format == 'a') 
   m.info.meta = {'a': 1} 
   assert (m.info.meta == {'a': 1}) 
   with pytest.raises(AttributeError): 
      m.info.bad_attr = 1 
   with pytest.raises(AttributeError): 
      m.info.bad_attr", 'Test that we can set attributes on the info dictionary.','Required attributes for a column can be set.'
"def reinitialize_command(self, command, reinit_subcommands): 
    cmd_obj = _DISTUTILS_REINIT(self, command, reinit_subcommands) 
   options = self.command_options.get(command) 
   if options: 
      self._set_command_options(cmd_obj, options) 
   return cmd_obj"," 'Reinitialize a command object. 
 :param command: The command to reinitialize. 
 :param reinit_subcommands: A list of subcommands that will be reinitialized. 
 :type reinit_subcommands: list 
 :return: A command object.'","'Monkeypatch the original version from distutils. 
 It\'s supposed to match the behavior of Distribution.get_command_obj() 
 This fixes issues with \'pip install -e\' and \'./setup.py test\' not 
 respecting the setup.cfg configuration directives for the build_ext 
 command.'"
"def _zpkbilinear(z, p, k, fs): 
    z = atleast_1d(z) 
   p = atleast_1d(p) 
   degree = _relative_degree(z, p) 
   fs2 = (2 * fs) 
   z_z = ((fs2 + z) / (fs2 - z)) 
   p_z = ((fs2 + p) / (fs2 - p)) 
   z_z = append(z_z, (- ones(degree))) 
   k_z = (k * real((prod((fs2 - z)) / prod((fs2 - p))))) 
   return (z_z, p_z, k_z)"," 'Compute the bilinear transform of a polynomial and a complex number 
 in the ZPK domain. 
 Parameters 
 z : array 
 Complex number. 
 p : array 
 Polynomial. 
 k : array 
 Constant. 
 Returns 
 z_z : array 
 Bilinear transform of z. 
 p_z : array 
 Bilinear transform of p. 
 k_z : array 
 Bilinear transform of k. 
 Examples 
 >>> from sympy.polys.zpk import _zpkbilinear 
 >>> from sympy import Symbol, I 
 >>> z = Symbol(\'z\') 
 >>> p = Symbol(\'p\') 
 >>> k = Symbol(\'k\') 
 >>> _zpkbilinear(z, p, k, 44100) 
 (array([ 1.00000000e+00, -2.00000000e+00,  0.00000000e+00,  0.0000","'Return a digital filter from an analog one using a bilinear transform. 
 Transform a set of poles and zeros from the analog s-plane to the digital 
 z-plane using Tustin\'s method, which substitutes ``(z-1) / (z+1)`` for 
 ``s``, maintaining the shape of the frequency response. 
 Parameters 
 z : array_like 
 Zeros of the analog IIR filter transfer function. 
 p : array_like 
 Poles of the analog IIR filter transfer function. 
 k : float 
 System gain of the analog IIR filter transfer function. 
 fs : float 
 Sample rate, as ordinary frequency (e.g. hertz). No prewarping is 
 done in this function. 
 Returns 
 z : ndarray 
 Zeros of the transformed digital filter transfer function. 
 p : ndarray 
 Poles of the transformed digital filter transfer function. 
 k : float 
 System gain of the transformed digital filter.'"
"def pytest_configure(config): 
    if (config.getoption('gae_sdk') is not None): 
      set_up_gae_environment(config.getoption('gae_sdk'))", 'Set up the GAE environment for the test runner.','Configures the App Engine SDK imports on py.test startup.'
"def _date_year(release): 
    try: 
      date = release['ReleaseDate'] 
   except TypeError: 
      date = '' 
   if (date is not None): 
      year = date[:4] 
   else: 
      year = '' 
   return (date, year)"," 'Return the release date and year of a release. 
 :param release: The release to inspect. 
 :return: A tuple of the release date and year.'",'Extract release date and year from database row'
"def FindEndOfExpressionInLine(line, startpos, depth, startchar, endchar): 
    for i in xrange(startpos, len(line)): 
      if (line[i] == startchar): 
         depth += 1 
      elif (line[i] == endchar): 
         depth -= 1 
         if (depth == 0): 
            return ((i + 1), 0) 
   return ((-1), depth)"," 'Find the end of an expression in a line. 
 This is a recursive function that searches for the end of an expression 
 in a line of text. 
 Arguments: 
 line: The line of text to search. 
 startpos: The start position of the search. 
 depth: The depth of the current expression. 
 startchar: The character that starts the current expression. 
 endchar: The character that ends the current expression. 
 Returns: 
 (i, j): The position of the end of the expression in the line. 
 depth: The depth of the current expression. 
 Examples: 
 >>> FindEndOfExpressionInLine(\'a + b\', 0, 0, \'+\', \'+\') 
 (5, 2, 0) 
 >>> FindEndOfExpressionInLine(\'a + b\', 0, 0, \'+\', \'+\') 
 (5, 2, 0) 
 >>> FindEndOfExpressionInLine(\'a + b\', 0, 0, \'+\', \'+\') 
 (5, ","'Find the position just after the matching endchar. 
 Args: 
 line: a CleansedLines line. 
 startpos: start searching at this position. 
 depth: nesting level at startpos. 
 startchar: expression opening character. 
 endchar: expression closing character. 
 Returns: 
 On finding matching endchar: (index just after matching endchar, 0) 
 Otherwise: (-1, new depth at end of this line)'"
"def expand_login_view(login_view): 
    if login_view.startswith(('https://', 'http://', '/')): 
      return login_view 
   else: 
      return url_for(login_view)"," 'Expand login view URL. 
 If login view URL starts with \'https://\', \'http://\' or \'/\', 
 return it as is. Otherwise, return URL for login view. 
 :param login_view: Login view URL 
 :type login_view: str 
 :return: Login view URL'","'Returns the url for the login view, expanding the view name to a url if 
 needed. 
 :param login_view: The name of the login view or a URL for the login view. 
 :type login_view: str'"
"def varOr(population, toolbox, lambda_, cxpb, mutpb): 
    assert ((cxpb + mutpb) <= 1.0), 'The   sum   of   the   crossover   and   mutation   probabilities   must   be   smaller   or   equal   to   1.0.' 
   offspring = [] 
   for _ in xrange(lambda_): 
      op_choice = random.random() 
      if (op_choice < cxpb): 
         (ind1, ind2) = map(toolbox.clone, random.sample(population, 2)) 
         (ind1, ind2) = toolbox.mate(ind1, ind2) 
         del ind1.fitness.values 
         offspring.append(ind1) 
      elif (op_choice < (cxpb + mutpb)): 
         ind = toolbox.clone(random.choice(population)) 
         (ind,) = toolbox.mutate(ind) 
         del ind.fitness.values 
         offspring.append(ind) 
      else: 
         offspring.append(random.choice(population)) 
   return offspring"," 'Generates offspring by crossover and mutation. 
 Parameters 
 population : list 
 A list of individuals. 
 toolbox : population.Population 
 The population toolbox. 
 lambda_ : float 
 The probability of crossover. 
 cxpb : float 
 The probability of crossover. 
 mutpb : float 
 The probability of mutation. 
 Returns 
 offspring : list 
 A list of individuals. 
 Examples 
 >>> from nltk.genetic import Population, Chromosome, crossover, mutate, varOr 
 >>> from nltk.util import random 
 >>> from nltk.util import Tree 
 >>> from nltk.util import string 
 >>> from nltk.util import random as rand 
 >>> from nltk.util import Prob 
 >>> from nltk.genetic import Population 
 >>> from nltk.genetic import Chromosome 
 >>> from nltk.genetic import varOr 
 >>> pop = Population(50) 
 >>> pop.addChrom","'Part of an evolutionary algorithm applying only the variation part 
 (crossover, mutation **or** reproduction). The modified individuals have 
 their fitness invalidated. The individuals are cloned so returned 
 population is independent of the input population. 
 :param population: A list of individuals to vary. 
 :param toolbox: A :class:`~deap.base.Toolbox` that contains the evolution 
 operators. 
 :param lambda\_: The number of children to produce 
 :param cxpb: The probability of mating two individuals. 
 :param mutpb: The probability of mutating an individual. 
 :returns: The final population 
 :returns: A class:`~deap.tools.Logbook` with the statistics of the 
 evolution 
 The variation goes as follow. On each of the *lambda_* iteration, it 
 selects one of the three operations; crossover, mutation or reproduction. 
 In the case of a crossover, two individuals are selected at random from 
 the parental population :math:`P_\mathrm{p}`, those individuals are cloned 
 using the :meth:`toolbox.clone` method and then mated using the 
 :meth:`toolbox.mate` method. Only the first child is appended to the 
 offspring population :math:`P_\mathrm{o}`, the second child is discarded. 
 In the case of a mutation, one individual is selected at random from 
 :math:`P_\mathrm{p}`, it is cloned and then mutated using using the 
 :meth:`toolbox.mutate` method. The resulting mutant is appended to 
 :math:`P_\mathrm{o}`. In the case of a reproduction, one individual is 
 selected at random from :math:`P_\mathrm{p}`, cloned and appended to 
 :math:`P_\mathrm{o}`. 
 This variation is named *Or* beceause an offspring will never result from 
 both operations crossover and mutation. The sum of both probabilities 
 shall be in :math:`[0, 1]`, the reproduction probability is 
 1 - *cxpb* - *mutpb*.'"
"def _escape(value): 
    if isinstance(value, (list, tuple)): 
      value = u','.join(value) 
   elif isinstance(value, (date, datetime)): 
      value = value.isoformat() 
   elif isinstance(value, bool): 
      value = str(value).lower() 
   if isinstance(value, string_types): 
      try: 
         return value.encode(u'utf-8') 
      except UnicodeDecodeError: 
         pass 
   return str(value)"," 'Escape a value to be used in a URL. 
 This method is used to escape values that are going to be used in a URL. 
 This includes values that are used in the query string and values that are 
 used in the path. 
 :param value: The value to escape. 
 :return: The escaped value. 
 :rtype: str'","'Escape a single value of a URL string or a query parameter. If it is a list 
 or tuple, turn it into a comma-separated string first.'"
"def api_validate(response_type=None, add_api_type_doc=False): 
    def wrap(response_function): 
      def _api_validate(*simple_vals, **param_vals): 
         def val(fn): 
            @wraps(fn) 
            def newfn(self, *a, **env): 
               renderstyle = request.params.get('renderstyle') 
               if renderstyle: 
                  c.render_style = api_type(renderstyle) 
               elif (not c.extension): 
                  c.render_style = api_type(response_type) 
               if ((response_type == 'html') and (not (request.params.get('api_type') == 'json'))): 
                  responder = JQueryResponse() 
               else: 
                  responder = JsonResponse() 
               response.content_type = responder.content_type 
               try: 
                  kw = _make_validated_kw(fn, simple_vals, param_vals, env) 
                  return response_function(self, fn, responder, simple_vals, param_vals, *a, **kw) 
               except UserRequiredException: 
                  responder.send_failure(errors.USER_REQUIRED) 
                  return self.api_wrapper(responder.make_response()) 
               except VerifiedUserRequiredException: 
                  responder.send_failure(errors.VERIFIED_USER_REQUIRED) 
                  return self.api_wrapper(responder.make_response()) 
            extra_param_vals = {} 
            if add_api_type_doc: 
               extra_param_vals = {'api_type': 'the   string   `json`'} 
            set_api_docs(newfn, simple_vals, param_vals, extra_param_vals) 
            newfn.handles_csrf = _validators_handle_csrf(simple_vals, param_vals) 
            return newfn 
         return val 
      return _api_validate 
   return wrap"," 'Wraps the response function to validate the request. 
 The response function is called with the request object, and 
 the response object is returned. 
 :param response_type: The response type to validate against. 
 :param add_api_type_doc: If true, add a docstring to the response 
 function that describes the api_type parameter. 
 :return: A wrapped version of the response function.'","'Factory for making validators for API calls, since API calls come 
 in two flavors: responsive and unresponsive.  The machinary 
 associated with both is similar, and the error handling identical, 
 so this function abstracts away the kw validation and creation of 
 a Json-y responder object.'"
"def build_lcms_70(compiler): 
    if (compiler['platform'] == 'x64'): 
      return '' 
   'Build   LCMS   on   VC2008.   This   version   is   only   32bit/Win32' 
   return ('\nrem   Build   lcms2\nsetlocal\nrd   /S   /Q   %%LCMS%%\\Lib\nrd   /S   /Q   %%LCMS%%\\Projects\\VC%(vc_version)s\\Release\n%%MSBUILD%%   %%LCMS%%\\Projects\\VC%(vc_version)s\\lcms2.sln      /t:Clean   /p:Configuration=""Release""   /p:Platform=Win32   /m\n%%MSBUILD%%   %%LCMS%%\\Projects\\VC%(vc_version)s\\lcms2.sln   /t:lcms2_static   /p:Configuration=""Release""   /p:Platform=Win32   /m\nxcopy   /Y   /E   /Q   %%LCMS%%\\include   %%INCLIB%%\ncopy   /Y   /B   %%LCMS%%\\Projects\\VC%(vc_version)s\\Release\\*.lib   %%INCLIB%%\nendlocal\n' % compiler)", 'Build   LCMS   on   VC2008.   This   version   is   only   32bit/Win32','Link error here on x64'
"def _parse_date_rfc822(dt): 
    try: 
      m = _rfc822_match(dt.lower()).groupdict(0) 
   except AttributeError: 
      return None 
   return _parse_date_group_rfc822(m)"," 'Parse a date in RFC 822 format. 
 :param dt: The string to parse. 
 :type dt: str 
 :return: The parsed date or None if the string is not in RFC 822 format.'","'Parse RFC 822 dates and times, with one minor 
 difference: years may be 4DIGIT or 2DIGIT. 
 http://tools.ietf.org/html/rfc822#section-5'"
"def create_image(ami_name, instance_id=None, instance_name=None, tags=None, region=None, key=None, keyid=None, profile=None, description=None, no_reboot=False, dry_run=False, filters=None): 
    instances = find_instances(instance_id=instance_id, name=instance_name, tags=tags, region=region, key=key, keyid=keyid, profile=profile, return_objs=True, filters=filters) 
   if (not instances): 
      log.error('Source   instance   not   found') 
      return False 
   if (len(instances) > 1): 
      log.error('Multiple   instances   found,   must   match   exactly   only   one   instance   to   create   an   image   from') 
      return False 
   instance = instances[0] 
   try: 
      return instance.create_image(ami_name, description=description, no_reboot=no_reboot, dry_run=dry_run) 
   except boto.exception.BotoServerError as exc: 
      log.error(exc) 
      return False"," 'Create an image from an instance 
 :param ami_name: Name of the image 
 :param instance_id: ID of the instance to create the image from 
 :param instance_name: Name of the instance to create the image from 
 :param tags: Tags to apply to the image 
 :param region: Region of the instance to create the image from 
 :param key: Access key to use to create the image 
 :param keyid: Access key ID to use to create the image 
 :param profile: Profile to use to create the image 
 :param description: Description of the image 
 :param no_reboot: Do not reboot the instance after the image creation 
 :param dry_run: Dry run 
 :param filters: Filters to use to find the instance'","'Given instance properties that define exactly one instance, create AMI and return AMI-id. 
 CLI Examples: 
 .. code-block:: bash 
 salt myminion boto_ec2.create_instance ami_name instance_name=myinstance 
 salt myminion boto_ec2.create_instance another_ami_name tags=\'{""mytag"": ""value""}\' description=\'this is my ami\''"
"def sentence_chrf(reference, hypothesis, min_len=1, max_len=6, beta=3.0): 
    return corpus_chrf([reference], [hypothesis], min_len, max_len, beta=beta)"," 'Computes the Chain Rule of Rationalized Hypotheses (CRH) 
 for a sentence pair. 
 Parameters 
 reference : list of strings 
 The reference sentence. 
 hypothesis : list of strings 
 The hypothesis sentence. 
 min_len : int 
 The minimum length of the reference and hypothesis. 
 max_len : int 
 The maximum length of the reference and hypothesis. 
 beta : float 
 The value of the beta parameter. 
 Returns 
 score : float 
 The Chain Rule of Rationalized Hypotheses (CRH) score. 
 Examples 
 >>> reference = [""The"", ""dog"", ""is"", ""sleeping"", ""on"", ""the"", ""bed""] 
 >>> hypothesis = [""The"", ""cat"", ""is"", ""sleeping"", ""on"", ""the"", ""bed""] 
 >>> sentence_chrf(reference, hypothesis) 
 0.0000000000000000000000'","'Calculates the sentence level CHRF (Character n-gram F-score) described in 
 - Maja Popovic. 2015. CHRF: Character n-gram F-score for Automatic MT Evaluation. 
 In Proceedings of the 10th Workshop on Machine Translation. 
 http://www.statmt.org/wmt15/pdf/WMT49.pdf 
 - Maja Popovic. 2016. CHRF Deconstructed: Î² Parameters and n-gram Weights. 
 In Proceedings of the 1st Conference on Machine Translation. 
 http://www.statmt.org/wmt16/pdf/W16-2341.pdf 
 Unlike multi-reference BLEU, CHRF only supports a single reference. 
 An example from the original BLEU paper 
 http://www.aclweb.org/anthology/P02-1040.pdf 
 >>> ref1 = str(\'It is a guide to action that ensures that the military \' 
 ...            \'will forever heed Party commands\').split() 
 >>> hyp1 = str(\'It is a guide to action which ensures that the military \' 
 ...            \'always obeys the commands of the party\').split() 
 >>> hyp2 = str(\'It is to insure the troops forever hearing the activity \' 
 ...            \'guidebook that party direct\').split() 
 >>> sentence_chrf(ref1, hyp1) # doctest: +ELLIPSIS 
 0.6768... 
 >>> sentence_chrf(ref1, hyp2) # doctest: +ELLIPSIS 
 0.4201... 
 The infamous ""the the the ... "" example 
 >>> ref = \'the cat is on the mat\'.split() 
 >>> hyp = \'the the the the the the the\'.split() 
 >>> sentence_chrf(ref, hyp)  # doctest: +ELLIPSIS 
 0.2530... 
 An example to show that this function allows users to use strings instead of 
 tokens, i.e. list(str) as inputs. 
 >>> ref1 = str(\'It is a guide to action that ensures that the military \' 
 ...            \'will forever heed Party commands\') 
 >>> hyp1 = str(\'It is a guide to action which ensures that the military \' 
 ...            \'always obeys the commands of the party\') 
 >>> sentence_chrf(ref1, hyp1) # doctest: +ELLIPSIS 
 0.6768... 
 >>> type(ref1) == type(hyp1) == str 
 True 
 >>> sentence_chrf(ref1.split(), hyp1.split()) # doctest: +ELLIPSIS 
 0.6768... 
 To skip the unigrams and only use 2- to 3-grams: 
 >>> sentence_chrf(ref1, hyp1, min_len=2, max_len=3) # doctest: +ELLIPSIS 
 0.7018... 
 :param references: reference sentence 
 :type references: list(str) / str 
 :param hypothesis: a hypothesis sentence 
 :type hypothesis: list(str) / str 
 :param min_len: The minimum order of n-gram this function should extract. 
 :type min_len: int 
 :param max_len: The maximum order of n-gram this function should extract. 
 :type max_len: int 
 :param beta: the parameter to assign more importance to recall over precision 
 :type beta: float 
 :return: the sentence level CHRF score. 
 :rtype: float'"
"def check_print_compat(): 
    return (not ((os.name == 'nt') and version_check('5.3.0', operator.lt)))", 'Check if the Python version is compatible with print.','Check if printing should work in the given Qt version.'
"def run_script(scriptfile): 
    try: 
      f = open(scriptfile, mode='r') 
   except Exception: 
      return 
   mpstate.console.writeln(('Running   script   %s' % scriptfile)) 
   for line in f: 
      line = line.strip() 
      if ((line == '') or line.startswith('#')): 
         continue 
      if line.startswith('@'): 
         line = line[1:] 
      else: 
         mpstate.console.writeln(('->   %s' % line)) 
      process_stdin(line) 
   f.close()"," 'Run a script in the interpreter. 
 The script is read from the file ``scriptfile``, which is opened in 
 read-only mode. 
 The script is run in the interpreter, which means that it will have 
 access to the global namespace. 
 Parameters 
 scriptfile : str 
 The script file to run. 
 Returns 
 None 
 Notes 
 The script is run in the interpreter, so it will have access to the 
 global namespace. 
 Examples 
 >>> import os 
 >>> import sys 
 >>> os.environ[""PYTHONPATH""] = ""."" 
 >>> import numpy 
 >>> import matplotlib.pyplot as plt 
 >>> import IPython 
 >>> IPython.run_script(""script.py"") 
 >>> import matplotlib 
 >>> import numpy 
 >>> import matplotlib.pyplot as plt 
 >>> matplotlib.use(""TkAgg"") 
 >>> import matplotlib.pyplot as plt 
 >>> import numpy 
 >>> import IPython 
 >>> IPython.run_script(""script.py"") 
 >>> matplotlib.use(""Agg"") 
 >>> import matplotlib.pyplot as",'run a script file'
"def add_cohort(course_key, name, assignment_type): 
    log.debug('Adding   cohort   %s   to   %s', name, course_key) 
   if is_cohort_exists(course_key, name): 
      raise ValueError(_('You   cannot   create   two   cohorts   with   the   same   name')) 
   try: 
      course = courses.get_course_by_id(course_key) 
   except Http404: 
      raise ValueError('Invalid   course_key') 
   cohort = CourseCohort.create(cohort_name=name, course_id=course.id, assignment_type=assignment_type).course_user_group 
   tracker.emit('edx.cohort.creation_requested', {'cohort_name': cohort.name, 'cohort_id': cohort.id}) 
   return cohort"," 'Adds a cohort to a course. 
 If the cohort with the given name already exists, an exception is raised. 
 Args: 
 course_key (CourseKey): The course key of the course to which the cohort 
 will be added. 
 name (str): The name of the cohort. 
 assignment_type (str): The assignment type for the cohort. 
 Returns: 
 The CourseCohort object that was created.'","'Add a cohort to a course.  Raises ValueError if a cohort of the same name already 
 exists.'"
"def delete_affinity_group(kwargs=None, conn=None, call=None): 
    if (call != 'function'): 
      raise SaltCloudSystemExit('The   delete_affinity_group   function   must   be   called   with   -f   or   --function.') 
   if (kwargs is None): 
      kwargs = {} 
   if ('name' not in kwargs): 
      raise SaltCloudSystemExit('A   name   must   be   specified   as   ""name""') 
   if (not conn): 
      conn = get_conn() 
   try: 
      conn.delete_affinity_group(kwargs['name']) 
      return {'Success': 'The   affinity   group   was   successfully   deleted'} 
   except AzureMissingResourceHttpError as exc: 
      raise SaltCloudSystemExit('{0}:   {1}'.format(kwargs['name'], exc.message))", 'Delete an affinity group.',"'.. versionadded:: 2015.8.0 
 Delete a specific affinity group associated with the account 
 CLI Examples: 
 .. code-block:: bash 
 salt-cloud -f delete_affinity_group my-azure name=my_affinity_group'"
"def _sqrtdenest_rec(expr): 
    from sympy.simplify.radsimp import radsimp, rad_rationalize, split_surds 
   if (not expr.is_Pow): 
      return sqrtdenest(expr) 
   if (expr.base < 0): 
      return (sqrt((-1)) * _sqrtdenest_rec(sqrt((- expr.base)))) 
   (g, a, b) = split_surds(expr.base) 
   a = (a * sqrt(g)) 
   if (a < b): 
      (a, b) = (b, a) 
   c2 = _mexpand(((a ** 2) - (b ** 2))) 
   if (len(c2.args) > 2): 
      (g, a1, b1) = split_surds(c2) 
      a1 = (a1 * sqrt(g)) 
      if (a1 < b1): 
         (a1, b1) = (b1, a1) 
      c2_1 = _mexpand(((a1 ** 2) - (b1 ** 2))) 
      c_1 = _sqrtdenest_rec(sqrt(c2_1)) 
      d_1 = _sqrtdenest_rec(sqrt((a1 + c_1))) 
      (num, den) = rad_rationalize(b1, d_1) 
      c = _mexpand(((d_1 / sqrt(2)) + (num / (den * sqrt(2))))) 
   else: 
      c = _sqrtdenest1(sqrt(c2)) 
   if (sqrt_depth(c) > 1): 
      raise SqrtdenestStopIteration 
   ac = (a + c) 
   if (len(ac.args) >= len(expr.args)): 
      if (count_ops(ac) >= count_ops(expr.base)): 
         raise SqrtdenestStopIteration 
   d = sqrtdenest(sqrt(ac)) 
   if (sqrt_depth(d) > 1): 
      raise SqrtdenestStopIteration 
   (num, den) = rad_rationalize(b, d) 
   r = ((d / sqrt(2)) + (num / (den * sqrt(2)))) 
   r = radsimp(r) 
   return _mexpand(r)"," 'Recursive version of sqrtdenest 
 Parameters 
 expr : Expr 
 The expression to be denested 
 Returns 
 Expr 
 The denested expression'","'Helper that denests the square root of three or more surds. 
 It returns the denested expression; if it cannot be denested it 
 throws SqrtdenestStopIteration 
 Algorithm: expr.base is in the extension Q_m = Q(sqrt(r_1),..,sqrt(r_k)); 
 split expr.base = a + b*sqrt(r_k), where `a` and `b` are on 
 Q_(m-1) = Q(sqrt(r_1),..,sqrt(r_(k-1))); then a**2 - b**2*r_k is 
 on Q_(m-1); denest sqrt(a**2 - b**2*r_k) and so on. 
 See [1], section 6. 
 Examples 
 >>> from sympy import sqrt 
 >>> from sympy.simplify.sqrtdenest import _sqrtdenest_rec 
 >>> _sqrtdenest_rec(sqrt(-72*sqrt(2) + 158*sqrt(5) + 498)) 
 -sqrt(10) + sqrt(2) + 9 + 9*sqrt(5) 
 >>> w=-6*sqrt(55)-6*sqrt(35)-2*sqrt(22)-2*sqrt(14)+2*sqrt(77)+6*sqrt(10)+65 
 >>> _sqrtdenest_rec(sqrt(w)) 
 -sqrt(11) - sqrt(7) + sqrt(2) + 3*sqrt(5)'"
"def user_pre_save(sender, instance, **kw): 
    if instance.id: 
      user = User.objects.get(id=instance.id) 
      if (user.username != instance.username): 
         questions = Question.objects.filter((Q(creator=instance) | Q(answers__creator=instance))).only('id').distinct() 
         for q in questions: 
            q.index_later()", 'Sets the username on the user object before save.',"'When a user\'s username is changed, we must reindex the questions 
 they participated in.'"
"def peakDetection(mX, t): 
    thresh = np.where((mX[1:(-1)] > t), mX[1:(-1)], 0) 
   next_minor = np.where((mX[1:(-1)] > mX[2:]), mX[1:(-1)], 0) 
   prev_minor = np.where((mX[1:(-1)] > mX[:(-2)]), mX[1:(-1)], 0) 
   ploc = ((thresh * next_minor) * prev_minor) 
   ploc = (ploc.nonzero()[0] + 1) 
   return ploc", 'peak detection',"'Detect spectral peak locations 
 mX: magnitude spectrum, t: threshold 
 returns ploc: peak locations'"
"def parse_encoding(fp): 
    pos = fp.tell() 
   fp.seek(0) 
   try: 
      line1 = fp.readline() 
      has_bom = line1.startswith(codecs.BOM_UTF8) 
      if has_bom: 
         line1 = line1[len(codecs.BOM_UTF8):] 
      m = PYTHON_MAGIC_COMMENT_re.match(line1) 
      if (not m): 
         try: 
            import parser 
            parser.suite(line1) 
         except (ImportError, SyntaxError): 
            pass 
         else: 
            line2 = fp.readline() 
            m = PYTHON_MAGIC_COMMENT_re.match(line2) 
      if has_bom: 
         if m: 
            raise SyntaxError('python   refuses   to   compile   code   with   both   a   UTF8   byte-order-mark   and   a   magic   encoding   comment') 
         return 'utf_8' 
      elif m: 
         return m.group(1) 
      else: 
         return None 
   finally: 
      fp.seek(pos)"," 'Returns the encoding of the file, or None if the file is not a Python 
 file. 
 The encoding is determined by the magic comment at the top of the file. 
 If the file has a UTF-8 byte-order-mark, then it is assumed to be UTF-8. 
 Otherwise, the magic comment is used. 
 The magic comment is a string of the form 
 # coding: <encoding> 
 where <encoding> is one of the following: 
 * latin1 
 * utf_8 
 * utf_16 
 * utf_16be 
 * utf_16le 
 * utf_16_be 
 * utf_16_le 
 * utf_16_bom 
 * utf_16_bom_be 
 * utf_16_bom_le 
 * utf_32 
 * utf_32_be 
 * utf_32_le 
 * utf_32_bom_be 
 * utf_32_bom_le 
 * utf_32_bom 
 * utf_","'Deduce the encoding of a source file from magic comment. 
 It does this in the same way as the `Python interpreter`__ 
 .. __: http://docs.python.org/ref/encodings.html 
 The ``fp`` argument should be a seekable file object. 
 (From Jeff Dairiki)'"
"def _allow_CTRL_C_posix(): 
    signal.signal(signal.SIGINT, signal.default_int_handler)", 'Posix only: Allow CTRL-C to be caught.','Take CTRL+C into account (SIGINT).'
"def dir(suffix='', prefix='tmp', parent=None): 
    return tempfile.mkdtemp(suffix, prefix, parent)"," 'Create a temporary directory. 
 :param suffix: (str) The suffix to be appended to the directory name. 
 :param prefix: (str) The prefix to be prepended to the directory name. 
 :param parent: (str) The parent directory to be used. 
 :return: (str) The directory name.'","'Create a temporary directory 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' temp.dir 
 salt \'*\' temp.dir prefix=\'mytemp-\' parent=\'/var/run/\''"
"def backup_dir(dir, ext='.bak'): 
    n = 1 
   extension = ext 
   while os.path.exists((dir + extension)): 
      n += 1 
      extension = (ext + str(n)) 
   return (dir + extension)"," 'Return a backup directory name. 
 :param dir: The directory to back up to. 
 :type dir: str 
 :param ext: The extension to use for the backup. 
 :type ext: str 
 :returns: The backup directory name.'","'Figure out the name of a directory to back up the given dir to 
 (adding .bak, .bak2, etc)'"
"@memoized 
 def cyclic_pattern(size=None, start=None, charset_type=None): 
    charset = config.Option.get('p_charset') 
   if (not charset): 
      charset = cyclic_pattern_charset(charset) 
   else: 
      charset = ''.join(set(charset)) 
   if (start is None): 
      start = 0 
   if (size is None): 
      size = 65536 
   size += start 
   pattern = de_bruijn(charset, 3, size) 
   return pattern[start:size].encode('utf-8')"," 'Returns a cyclic pattern of size ``size`` from ``start`` in the 
 given charset. 
 ``start`` is the offset from the beginning of the pattern. 
 ``size`` is the length of the pattern. 
 If ``charset`` is not specified, the default charset is used. 
 If ``charset`` is a string, it is used as the charset. 
 If ``charset`` is a callable, it is called with the size of the pattern 
 and the result is used as the charset. 
 If ``charset`` is a dict, it is used as the charset. 
 If ``charset`` is a tuple, it is used as the charset. 
 If ``charset`` is a list, it is used as the charset. 
 If ``charset`` is a set, it is used as the charset. 
 If ``charset`` is a range, it is used as the charset. 
 If ``charset`` is a range of length 2, it is used as the charset. 
 If ``charset`` is None, the default charset is used. 
 If ``charset_type`` is not","'Generate a cyclic pattern 
 Args: 
 - size: size of generated pattern (Int) 
 - start: the start offset of the generated pattern (Int) 
 - charset_type: charset type 
 0: basic (0-9A-za-z) 
 1: extended (default) 
 2: maximum (almost printable chars) 
 Returns: 
 - pattern text (byte string) (str in Python 2; bytes in Python 3)'"
"def Bar(xs, ys, **options): 
    options = _UnderrideColor(options) 
   options = _Underride(options, linewidth=0, alpha=0.6) 
   pyplot.bar(xs, ys, **options)"," 'Draw a bar chart. 
 Parameters 
 xs : array-like 
 The x-values. 
 ys : array-like 
 The y-values. 
 **options : dict 
 Keyword arguments are passed to :py:meth:`matplotlib.pyplot.bar`. 
 See :py:meth:`matplotlib.pyplot.bar` for more details. 
 Returns 
 None 
 Examples 
 >>> import numpy as np 
 >>> from matplotlib import pyplot as plt 
 >>> xs = np.arange(10) 
 >>> ys = np.arange(10) 
 >>> plt.bar(xs, ys) 
 >>> plt.show() 
 >>> xs = np.arange(10) 
 >>> ys = np.arange(10) 
 >>> plt.bar(xs, ys, color=""blue"") 
 >>> plt.bar(xs, ys, color=""blue"", linewidth=2) 
 >>> plt.bar(xs, ys, color=""blue"", alpha=0.5) 
 >>> plt.bar(xs,","'Plots a line. 
 Args: 
 xs: sequence of x values 
 ys: sequence of y values 
 options: keyword args passed to pyplot.bar'"
"def dump(obj, fp, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, cls=None, indent=None, separators=None, encoding='utf-8', default=None, use_decimal=True, namedtuple_as_object=True, tuple_as_array=True, bigint_as_string=False, sort_keys=False, item_sort_key=None, for_json=False, ignore_nan=False, int_as_string_bitcount=None, iterable_as_array=False, **kw): 
    if ((not skipkeys) and ensure_ascii and check_circular and allow_nan and (cls is None) and (indent is None) and (separators is None) and (encoding == 'utf-8') and (default is None) and use_decimal and namedtuple_as_object and tuple_as_array and (not iterable_as_array) and (not bigint_as_string) and (not sort_keys) and (not item_sort_key) and (not for_json) and (not ignore_nan) and (int_as_string_bitcount is None) and (not kw)): 
      iterable = _default_encoder.iterencode(obj) 
   else: 
      if (cls is None): 
         cls = JSONEncoder 
      iterable = cls(skipkeys=skipkeys, ensure_ascii=ensure_ascii, check_circular=check_circular, allow_nan=allow_nan, indent=indent, separators=separators, encoding=encoding, default=default, use_decimal=use_decimal, namedtuple_as_object=namedtuple_as_object, tuple_as_array=tuple_as_array, iterable_as_array=iterable_as_array, bigint_as_string=bigint_as_string, sort_keys=sort_keys, item_sort_key=item_sort_key, for_json=for_json, ignore_nan=ignore_nan, int_as_string_bitcount=int_as_string_bitcount, **kw).iterencode(obj) 
   for chunk in iterable: 
      fp.write(chunk)"," 'Dump an object to a stream. 
 :param obj: 
 :param fp: 
 :param skipkeys: 
 :param ensure_ascii: 
 :param check_circular: 
 :param allow_nan: 
 :param cls: 
 :param indent: 
 :param separators: 
 :param encoding: 
 :param default: 
 :param use_decimal: 
 :param namedtuple_as_object: 
 :param tuple_as_array: 
 :param bigint_as_string: 
 :param sort_keys: 
 :param item_sort_key: 
 :param for_json: 
 :param ignore_nan: 
 :param int_as_string_bitcount: 
 :param kw: 
 :return: 
 :rtype: 
 :raises: 
 :version: 0.1'","'Serialize ``obj`` as a JSON formatted stream to ``fp`` (a 
 ``.write()``-supporting file-like object). 
 If *skipkeys* is true then ``dict`` keys that are not basic types 
 (``str``, ``unicode``, ``int``, ``long``, ``float``, ``bool``, ``None``) 
 will be skipped instead of raising a ``TypeError``. 
 If *ensure_ascii* is false, then the some chunks written to ``fp`` 
 may be ``unicode`` instances, subject to normal Python ``str`` to 
 ``unicode`` coercion rules. Unless ``fp.write()`` explicitly 
 understands ``unicode`` (as in ``codecs.getwriter()``) this is likely 
 to cause an error. 
 If *check_circular* is false, then the circular reference check 
 for container types will be skipped and a circular reference will 
 result in an ``OverflowError`` (or worse). 
 If *allow_nan* is false, then it will be a ``ValueError`` to 
 serialize out of range ``float`` values (``nan``, ``inf``, ``-inf``) 
 in strict compliance of the original JSON specification, instead of using 
 the JavaScript equivalents (``NaN``, ``Infinity``, ``-Infinity``). See 
 *ignore_nan* for ECMA-262 compliant behavior. 
 If *indent* is a string, then JSON array elements and object members 
 will be pretty-printed with a newline followed by that string repeated 
 for each level of nesting. ``None`` (the default) selects the most compact 
 representation without any newlines. For backwards compatibility with 
 versions of simplejson earlier than 2.1.0, an integer is also accepted 
 and is converted to a string with that many spaces. 
 If specified, *separators* should be an 
 ``(item_separator, key_separator)`` tuple.  The default is ``(\', \', \': \')`` 
 if *indent* is ``None`` and ``(\',\', \': \')`` otherwise.  To get the most 
 compact JSON representation, you should specify ``(\',\', \':\')`` to eliminate 
 whitespace. 
 *encoding* is the character encoding for str instances, default is UTF-8. 
 *default(obj)* is a function that should return a serializable version 
 of obj or raise ``TypeError``. The default simply raises ``TypeError``. 
 If *use_decimal* is true (default: ``True``) then decimal.Decimal 
 will be natively serialized to JSON with full precision. 
 If *namedtuple_as_object* is true (default: ``True``), 
 :class:`tuple` subclasses with ``_asdict()`` methods will be encoded 
 as JSON objects. 
 If *tuple_as_array* is true (default: ``True``), 
 :class:`tuple` (and subclasses) will be encoded as JSON arrays. 
 If *iterable_as_array* is true (default: ``False``), 
 any object not in the above table that implements ``__iter__()`` 
 will be encoded as a JSON array. 
 If *bigint_as_string* is true (default: ``False``), ints 2**53 and higher 
 or lower than -2**53 will be encoded as strings. This is to avoid the 
 rounding that happens in Javascript otherwise. Note that this is still a 
 lossy operation that will not round-trip correctly and should be used 
 sparingly. 
 If *int_as_string_bitcount* is a positive number (n), then int of size 
 greater than or equal to 2**n or lower than or equal to -2**n will be 
 encoded as strings. 
 If specified, *item_sort_key* is a callable used to sort the items in 
 each dictionary. This is useful if you want to sort items other than 
 in alphabetical order by key. This option takes precedence over 
 *sort_keys*. 
 If *sort_keys* is true (default: ``False``), the output of dictionaries 
 will be sorted by item. 
 If *for_json* is true (default: ``False``), objects with a ``for_json()`` 
 method will use the return value of that method for encoding as JSON 
 instead of the object. 
 If *ignore_nan* is true (default: ``False``), then out of range 
 :class:`float` values (``nan``, ``inf``, ``-inf``) will be serialized as 
 ``null`` in compliance with the ECMA-262 specification. If true, this will 
 override *allow_nan*. 
 To use a custom ``JSONEncoder`` subclass (e.g. one that overrides the 
 ``.default()`` method to serialize additional types), specify it with 
 the ``cls`` kwarg. NOTE: You should use *default* or *for_json* instead 
 of subclassing whenever possible.'"
"def uslugify_encoded(text, sep): 
    if (text is None): 
      return u'' 
   tag_id = RE_TAGS.sub(u'', unicodedata.normalize(u'NFKD', text)).lower() 
   tag_id = RE_WORD.sub(u'', tag_id).replace(u'   ', sep) 
   return quote(tag_id.encode(u'utf-8'))"," 'Uses the \'unicode-slugify\' algorithm to encode a unicode string into 
 a string that is safe for use in URLs. 
 The \'unicode-slugify\' algorithm is described at: 
 http://unicode.org/reports/tr15/ 
 This implementation uses the \'utf-8\' encoding to convert the 
 unicode string into a unicode string. 
 The \'unicode-slugify\' algorithm uses the following rules: 
 * Separators: 
 The separator is a single space character. 
 * Case: 
 Lowercase letters are used. 
 * Dashes: 
 Dashes are used to separate words. 
 * Spaces: 
 Spaces are used to separate words. 
 * Punctuation: 
 Punctuation is removed. 
 * Symbols: 
 Symbols are removed. 
 * Accents: 
 Accents are removed. 
 * Extra characters: 
 Extra characters are removed. 
 * Unassigned characters: 
 Unassigned characters are removed. 
 * Empty strings: 
 Empty strings",'Custom slugify (percent encoded).'
"def find_email(string, unique=True): 
    string = u(string).replace(u'\u2024', '.') 
   matches = [] 
   for m in RE_EMAIL.finditer(string): 
      s = m.group(0) 
      if ((not unique) or (s not in matches)): 
         matches.append(s) 
   return matches"," 'Returns all the email addresses found in a string. 
 The string can contain multiple email addresses separated by \'@\', 
 \'.\' or \'-\'. 
 If unique is True, only one of each email address will be returned. 
 If unique is False, all the email addresses will be returned. 
 If the string is not a valid email address, an empty list will be returned. 
 If the string is empty, an empty list will be returned. 
 The regex used to find email addresses is the same as the one used by the 
 Python email module. 
 :param string: A string containing one or more email addresses. 
 :param unique: If True, only one of each email address will be returned. 
 :return: A list of email addresses found in the string.'",'Returns a list of e-mail addresses parsed from the string.'
"def record_usage(key_prefix, time_slice): 
    key = _make_ratelimit_cache_key(key_prefix, time_slice) 
   try: 
      g.ratelimitcache.add(key, 0, time=time_slice.remaining) 
      try: 
         return g.ratelimitcache.incr(key) 
      except pylibmc.NotFound: 
         now = int(time.time()) 
         if (now < time_slice.end): 
            g.ratelimitcache.add(key, 1, time=((time_slice.end - now) + 1)) 
            g.stats.simple_event('ratelimit.eviction') 
         return 1 
   except pylibmc.Error as e: 
      raise RatelimitError(e)"," 'Record the usage of the ratelimit cache key. 
 :param key_prefix: The key prefix to use. 
 :param time_slice: The time slice to record usage for. 
 :return: The number of hits recorded.'","'Record usage of a ratelimit for the specified time slice. 
 The total usage (including this one) of the ratelimit is returned or 
 RatelimitError is raised if something went wrong during the process.'"
"def setup_platform(hass, config, add_devices, discovery_info=None): 
    try: 
      bbox_data = BboxData() 
      bbox_data.update() 
   except requests.exceptions.HTTPError as error: 
      _LOGGER.error(error) 
      return False 
   name = config.get(CONF_NAME) 
   sensors = [] 
   for variable in config[CONF_MONITORED_VARIABLES]: 
      sensors.append(BboxSensor(bbox_data, variable, name)) 
   add_devices(sensors)", 'Setup the platform.','Set up the Bbox sensor.'
"@require_POST 
 @login_required 
 def watch_forum(request, forum_slug): 
    forum = get_object_or_404(Forum, slug=forum_slug) 
   if (not forum.allows_viewing_by(request.user)): 
      raise Http404 
   if (request.POST.get('watch') == 'yes'): 
      NewThreadEvent.notify(request.user, forum) 
      statsd.incr('forums.watches.forum') 
   else: 
      NewThreadEvent.stop_notifying(request.user, forum) 
   return HttpResponseRedirect(reverse('forums.threads', args=[forum_slug]))","'.watch_forum() - Watch a forum. 
 :param request: The current request. 
 :param forum_slug: The slug of the forum to watch. 
 :return: A redirect to the forum\'s threads page.'",'Watch/unwatch a forum (based on \'watch\' POST param).'
"def overrides_disabled(): 
    return bool(_OVERRIDES_DISABLED.disabled)", 'Returns whether overrides are disabled.',"'Checks to see whether overrides are disabled in the current context. 
 Returns a boolean value.  See `disable_overrides`.'"
"def H_from_ransac(fp, tp, model, maxiter=1000, match_theshold=10): 
    import ransac 
   data = vstack((fp, tp)) 
   (H, ransac_data) = ransac.ransac(data.T, model, 4, maxiter, match_theshold, 10, return_all=True) 
   return (H, ransac_data['inliers'])"," 'Returns the homography H and the inliers of the RANSAC algorithm. 
 Parameters 
 fp : (N, 3) array 
 The points in the first image. 
 tp : (M, 3) array 
 The points in the second image. 
 model : (3, 3) array 
 The model to be fit. 
 maxiter : int 
 The maximum number of iterations allowed. 
 match_theshold : float 
 The threshold for matching. 
 Returns 
 H : (3, 3) array 
 The homography. 
 inliers : (M,) array 
 The inliers. 
 Examples 
 >>> from skimage import data 
 >>> from skimage.morphology import disk 
 >>> from skimage.transform import ransac 
 >>> img1 = data.camera() 
 >>> img2 = data.camera() 
 >>> img1.shape = (100, 100) 
 >>> img2.shape = (100, 100) 
 >>> img1 = disk(img1, 10","'Robust estimation of homography H from point 
 correspondences using RANSAC (ransac.py from 
 http://www.scipy.org/Cookbook/RANSAC). 
 input: fp,tp (3*n arrays) points in hom. coordinates.'"
"def shorten_string(string, max_width): 
    string_len = len(string) 
   if (string_len <= max_width): 
      return string 
   visible = ((max_width - 16) - int(log10(string_len))) 
   if (not isinstance(string, unistr)): 
      visstring = unistr(string[:visible], errors='ignore') 
   else: 
      visstring = string[:visible] 
   return u''.join((visstring, u'...(and   ', unistr((string_len - visible)), u'   more)'))", 'Returns a string with the first visible characters and an ellipsis.',"'make limited length string in form: 
 ""the string is very lo...(and 15 more)""'"
"def test_mouse_key_events(): 
    me = MouseEvent('mouse_press') 
   for fun in (me.pos, me.button, me.buttons, me.modifiers, me.delta, me.press_event, me.last_event, me.is_dragging): 
      fun 
   me.drag_events() 
   me._forget_last_event() 
   me.trail() 
   ke = KeyEvent('key_release') 
   ke.key 
   ke.text 
   ke.modifiers", 'Test MouseEvent and KeyEvent','Test mouse and key events'
"def init_native(): 
    init_subsystem(Native.Factory) 
   return Native.Factory.global_instance().create()", 'Return a Native object.','Initialize and return the `Native` subsystem.'
"def run_supervised_learning(predictor_fp, response_fp, response_name, ntree=1000, errortype='oob', output_dir='.', verbose=False, HALT_EXEC=False): 
    rsl = RSupervisedLearner(HALT_EXEC=HALT_EXEC) 
   rsl.Parameters['-m'].on(response_fp) 
   rsl.Parameters['-c'].on(response_name) 
   rsl.Parameters['-n'].on(str(ntree)) 
   rsl.Parameters['-o'].on(output_dir) 
   rsl.Parameters['-e'].on(errortype) 
   if verbose: 
      rsl.Parameters['-v'].on() 
   app_result = rsl(predictor_fp) 
   remove(join(output_dir, (splitext(split(predictor_fp)[1])[0] + '.txt'))) 
   return app_result"," 'Run supervised learning on a given predictor and response. 
 This function runs the supervised learning algorithm on a given predictor and response 
 file. 
 Parameters 
 predictor_fp : str 
 The name of the predictor file to be used. 
 response_fp : str 
 The name of the response file to be used. 
 response_name : str 
 The name of the response to be used. 
 ntree : int 
 The number of trees to be used in the tree ensemble. 
 errortype : str 
 The type of error to be used in the tree ensemble. 
 output_dir : str 
 The directory to store the results. 
 verbose : bool 
 If True, the verbose output is printed. 
 HALT_EXEC : bool 
 If True, the execution is halted after the first error. 
 Returns 
 app_result : int 
 The result of the application. 
 Examples 
 >>> run_supervised_learning(\'predictor.txt\', \'response.txt\', \'response.txt\', 1000, \'o","'Run supervised learning (random forests here) 
 predictor_fp: path to otu table 
 response_fp: path to metadata table 
 response_name: Column header for gradient variable in metadata table 
 ntree: Number of trees in forest 
 errortype: method for estimating generalization error 
 output_dir: output directory 
 verbose: print verbose output 
 output_dir: directory where output should be written (default \'.\') 
 HALT_EXEC: halt just before running the formatdb command and'"
"def test_no_truncate_using_compare(): 
    w = wcs.WCS(naxis=3) 
   w.wcs.crval = [240.9303333333, 50, 212345678000.0] 
   w.wcs.cdelt = [0.001, 0.001, 100000000.0] 
   w.wcs.ctype = [u'RA---TAN', u'DEC--TAN', u'FREQ'] 
   w.wcs.set() 
   w2 = wcs.WCS(w.to_header()) 
   w.wcs.compare(w2.wcs)", 'Test that compare() does not truncate the input wcs',"'Regression test for https://github.com/astropy/astropy/issues/4612 
 This one uses WCS.wcs.compare and some slightly different values'"
"def _synthesize(browser, update_tryorder=1): 
    cmd = browser.split()[0] 
   if (not _iscommand(cmd)): 
      return [None, None] 
   name = os.path.basename(cmd) 
   try: 
      command = _browsers[name.lower()] 
   except KeyError: 
      return [None, None] 
   controller = command[1] 
   if (controller and (name.lower() == controller.basename)): 
      import copy 
      controller = copy.copy(controller) 
      controller.name = browser 
      controller.basename = os.path.basename(browser) 
      register(browser, None, controller, update_tryorder) 
      return [None, controller] 
   return [None, None]"," 'Synthesize a command for a browser. 
 This function will attempt to synthesize a command for the given browser 
 by looking at the list of available browsers and the given command.  If 
 no command can be found, it will return None. 
 :param browser: The name of the browser to synthesize a command for. 
 :type browser: str 
 :param update_tryorder: If this is True, then the tryorder of the 
 synthesized command will be updated to match the tryorder of the 
 browser. 
 :type update_tryorder: bool 
 :return: A tuple of (command, controller) if a command could be found, 
 otherwise None.'","'Attempt to synthesize a controller base on existing controllers. 
 This is useful to create a controller when a user specifies a path to 
 an entry in the BROWSER environment variable -- we can copy a general 
 controller to operate using a specific installation of the desired 
 browser in this way. 
 If we can\'t create a controller in this way, or if there is no 
 executable for the requested browser, return [None, None].'"
"def timeout(reactor, deferred, timeout_sec, reason=None): 
    def _timeout(): 
      deferred.cancel() 
   delayed_timeout = reactor.callLater(timeout_sec, _timeout) 
   if (reason is not None): 
      def maybe_replace_reason(passthrough): 
         if delayed_timeout.active(): 
            return passthrough 
         return Failure(reason) 
      deferred.addErrback(maybe_replace_reason) 
   def abort_timeout(passthrough): 
      if delayed_timeout.active(): 
         delayed_timeout.cancel() 
      return passthrough 
   deferred.addBoth(abort_timeout) 
   return deferred"," 'Schedule a timeout. 
 :param reactor: A reactor instance. 
 :param deferred: The deferred to be timed out. 
 :param timeout_sec: The number of seconds to wait for the deferred to be 
 resolved. 
 :param reason: A reason to be passed to the deferred\'s errback if the timeout 
 occurs. 
 :returns: The deferred to be timed out. 
 :rtype: Deferred'","'Adds a timeout to an existing deferred.  If the timeout expires before the 
 deferred expires, then the deferred is cancelled. 
 :param IReactorTime reactor: The reactor implementation to schedule the 
 timeout. 
 :param Deferred deferred: The deferred to cancel at a later point in time. 
 :param float timeout_sec: The number of seconds to wait before the deferred 
 should time out. 
 :param Exception reason: An exception used to create a Failure with which 
 to fire the Deferred if the timeout is encountered.  If not given, 
 ``deferred`` retains its original failure behavior. 
 :return: The updated deferred.'"
"def _row_from_json(row, schema): 
    row_data = [] 
   for (field, cell) in zip(schema, row['f']): 
      converter = _CELLDATA_FROM_JSON[field.field_type] 
      if (field.mode == 'REPEATED'): 
         row_data.append([converter(item['v'], field) for item in cell['v']]) 
      else: 
         row_data.append(converter(cell['v'], field)) 
   return tuple(row_data)", 'Convert a row of json data into a row of python data.',"'Convert JSON row data to row with appropriate types. 
 :type row: dict 
 :param row: A JSON response row to be converted. 
 :type schema: tuple 
 :param schema: A tuple of 
 :class:`~google.cloud.bigquery.schema.SchemaField`. 
 :rtype: tuple 
 :returns: A tuple of data converted to native types.'"
"def tree_from_cix(cix): 
    if isinstance(cix, unicode): 
      cix = cix.encode('UTF-8', 'xmlcharrefreplace') 
   tree = ET.XML(cix) 
   version = tree.get('version') 
   if (version == CIX_VERSION): 
      return tree 
   elif (version == '0.1'): 
      return tree_2_0_from_tree_0_1(tree) 
   else: 
      raise CodeIntelError(('unknown   CIX   version:   %r' % version))", 'Returns a tree from a CIX document.',"'Return a (ci)tree for the given CIX content. 
 Raises pyexpat.ExpatError if the CIX content could not be parsed.'"
"@register.filter 
 @stringfilter 
 def issue_status_icon(status): 
    if (status == BaseComment.OPEN): 
      return u'rb-icon-issue-open' 
   elif (status == BaseComment.RESOLVED): 
      return u'rb-icon-issue-resolved' 
   elif (status == BaseComment.DROPPED): 
      return u'rb-icon-issue-dropped' 
   else: 
      raise ValueError((u'Unknown   comment   issue   status   ""%s""' % status))"," 'Returns an icon for a comment\'s status. 
 :param status: A string representing the comment\'s status.'","'Return an icon name for the issue status. 
 Args: 
 status (unicode): 
 The stored issue status for the comment. 
 Returns: 
 unicode: The icon name for the issue status.'"
"def test_integer_sequence_generator(): 
    floatX = theano.config.floatX 
   rng = numpy.random.RandomState(1234) 
   readout_dim = 5 
   feedback_dim = 3 
   dim = 20 
   batch_size = 30 
   n_steps = 10 
   transition = GatedRecurrent(dim=dim, activation=Tanh(), weights_init=Orthogonal()) 
   generator = SequenceGenerator(Readout(readout_dim=readout_dim, source_names=['states'], emitter=SoftmaxEmitter(theano_seed=1234), feedback_brick=LookupFeedback(readout_dim, feedback_dim)), transition, weights_init=IsotropicGaussian(0.1), biases_init=Constant(0), seed=1234) 
   generator.initialize() 
   y = tensor.lmatrix('y') 
   mask = tensor.matrix('mask') 
   costs = generator.cost_matrix(y, mask) 
   assert (costs.ndim == 2) 
   costs_fun = theano.function([y, mask], [costs]) 
   y_test = rng.randint(readout_dim, size=(n_steps, batch_size)) 
   m_test = numpy.ones((n_steps, batch_size), dtype=floatX) 
   costs_val = costs_fun(y_test, m_test)[0] 
   assert (costs_val.shape == (n_steps, batch_size)) 
   assert_allclose(costs_val.sum(), 482.827, rtol=1e-05) 
   cost = generator.cost(y, mask) 
   assert (cost.ndim == 0) 
   cost_val = theano.function([y, mask], [cost])(y_test, m_test) 
   assert_allclose(cost_val, 16.0942, rtol=1e-05) 
   cg = ComputationGraph([cost]) 
   var_filter = VariableFilter(roles=[AUXILIARY]) 
   aux_var_name = '_'.join([generator.name, generator.cost.name, 'per_sequence_element']) 
   cost_per_el = [el for el in var_filter(cg.variables) if (el.name == aux_var_name)][0] 
   assert (cost_per_el.ndim == 0) 
   cost_per_el_val = theano.function([y, mask], [cost_per_el])(y_test, m_test) 
   assert_allclose(cost_per_el_val, 1.60942, rtol=1e-05) 
   (states, outputs, costs) = generator.generate(iterate=True, batch_size=batch_size, n_steps=n_steps) 
   cg = ComputationGraph(((states + outputs) + costs)) 
   (states_val, outputs_val, costs_val) = theano.function([], [states, outputs, costs], updates=cg.updates)() 
   assert (states_val.shape == (n_steps, batch_size, dim)) 
   assert (outputs_val.shape == (n_steps, batch_size)) 
   assert (outputs_val.dtype == 'int64') 
   assert (costs_val.shape == (n_steps, batch_size)) 
   assert_allclose(states_val.sum(), (-17.854), rtol=1e-05) 
   assert_allclose(costs_val.sum(), 482.868, rtol=1e-05) 
   assert (outputs_val.sum() == 629) 
   cost1 = costs_fun([[1], [2]], [[1], [1]])[0] 
   cost2 = costs_fun([[3, 1], [4, 2], [2, 0]], [[1, 1], [1, 1], [1, 0]])[0] 
   assert_allclose(cost1.sum(), cost2[:, 1].sum(), rtol=1e-05)", 'Test that integer sequence generator can be used to generate sequences.',"'Test a sequence generator with integer outputs. 
 Such sequence generators can be used to e.g. model language.'"
"def remove_trailing_string(content, trailing): 
    if (content.endswith(trailing) and (content != trailing)): 
      return content[:(- len(trailing))] 
   return content"," 'Removes trailing string from content. 
 :param content: String to be checked for trailing string. 
 :param trailing: String to be removed. 
 :returns: String without trailing string. 
 :rtype: str'","'Strip trailing component `trailing` from `content` if it exists. 
 Used when generating names from view classes.'"
"def exit_if_empty(): 
    state = twill.get_browser() 
   form = state.get_form('1') 
   if (not form): 
      print 'No   messages;   exiting.' 
      raise SystemExit", 'Exit if there are no messages in the queue.',"'>> exit_if_empty 
 Exit the script currently running, if there are no deferred messages 
 on the current page.'"
"def instances_by_name(name_filter): 
    return [o for o in gc.get_objects() if (name_filter == typename(o))]", 'Return a list of all instances of the specified type.',"'Return the list of objects that exactly match the given 
 name_filter.'"
"def RenderParetoCdf(xmin, alpha, low, high, n=50): 
    if (low < xmin): 
      low = xmin 
   xs = np.linspace(low, high, n) 
   ps = (1 - ((xs / xmin) ** (- alpha))) 
   return (xs, ps)"," 'Return the Pareto CDF for a given set of parameters. 
 Parameters 
 xmin : float 
 The minimum value of the random variable. 
 alpha : float 
 The shape parameter. 
 low : float 
 The lower bound of the random variable. 
 high : float 
 The upper bound of the random variable. 
 n : int 
 The number of points in the Pareto CDF. 
 Returns 
 xs : ndarray 
 The values of the random variable. 
 ps : ndarray 
 The corresponding Pareto CDF values. 
 Examples 
 >>> from sympy.stats import Pareto 
 >>> from sympy.stats import RenderParetoCdf 
 >>> x = Pareto.rvs(0.5, 1) 
 >>> RenderParetoCdf(0, 0.5, 0, 1, 50) 
 (array([0.00000000e+00, 0.00000000e+00, 0.00000000","'Generates sequences of xs and ps for a Pareto CDF. 
 xmin: parameter 
 alpha: parameter 
 low: float 
 high: float 
 n: number of points to render 
 returns: numpy arrays (xs, ps)'"
"def _check_apt(): 
    if (not HAS_APT): 
      raise CommandExecutionError(""Error:   'python-apt'   package   not   installed"")", 'Checks if the apt package is installed.','Abort if python-apt is not installed'
"def __update_loaders(z): 
    non_local['loaders'] = [] 
   for filename in z.namelist(): 
      if (not isinstance(filename, str_cls)): 
         filename = filename.decode('utf-8') 
      non_local['loaders'].append(filename)"," 'Update the loaders list with the files found in the zipped archive. 
 This is needed for the loader to find the files in the archive.'","'Updates the cached list of loaders from a zipfile. The loader_lock MUST 
 be held when calling this function. 
 :param z: 
 The zipfile.ZipFile object to list the files in'"
"def word_ids_to_words(data, id_to_word): 
    return [id_to_word[i] for i in data]", 'Convert a list of word ids to a list of words.',"'Given a context (ids) in list format and the vocabulary, 
 Returns a list of words to represent the context. 
 Parameters 
 data : a list of integer 
 the context in list format 
 id_to_word : a dictionary 
 mapping id to unique word. 
 Returns 
 A list of string or byte to represent the context. 
 Examples 
 >>> see words_to_word_ids'"
"def obrientransform(*args): 
    TINY = np.sqrt(np.finfo(float).eps) 
   arrays = [] 
   for arg in args: 
      a = np.asarray(arg) 
      n = len(a) 
      mu = np.mean(a) 
      sq = ((a - mu) ** 2) 
      sumsq = sq.sum() 
      t = (((((n - 1.5) * n) * sq) - (0.5 * sumsq)) / ((n - 1) * (n - 2))) 
      var = (sumsq / (n - 1)) 
      if (abs((var - np.mean(t))) > TINY): 
         raise ValueError('Lack   of   convergence   in   obrientransform.') 
      arrays.append(t) 
   return np.array(arrays)"," 'Compute the obrientransform of the input arrays. 
 This is a fast method for computing the obrientransform of the input 
 arrays. It is based on the formula given in [1]_. 
 Parameters 
 *args : array_like 
 Arrays of the same shape to compute the obrientransform of. 
 Returns 
 out : ndarray 
 The obrientransform of the input arrays. 
 Notes 
 The obrientransform is a symmetric matrix that is used for computing the 
 inverse of the covariance matrix. It is defined as 
 .. math:: 
 \Omega = \begin{bmatrix} 
 t_1 & t_2 & t_3 & t_4 & t_5 & t_6 \\ 
 t_2 & t_3 & t_4 & t_5 & t_6 & t_7 \\ 
 t_3 & t_4 & t_5 & t_6 & t_7 & t_8 \\ 
 t_4 & t_5 & t_6 & t_7 & t_8 & t_9 \\ ","'Computes the O\'Brien transform on input data (any number of arrays). 
 Used to test for homogeneity of variance prior to running one-way stats. 
 Each array in ``*args`` is one level of a factor. 
 If `f_oneway` is run on the transformed data and found significant, 
 the variances are unequal.  From Maxwell and Delaney [1]_, p.112. 
 Parameters 
 args : tuple of array_like 
 Any number of arrays. 
 Returns 
 obrientransform : ndarray 
 Transformed data for use in an ANOVA.  The first dimension 
 of the result corresponds to the sequence of transformed 
 arrays.  If the arrays given are all 1-D of the same length, 
 the return value is a 2-D array; otherwise it is a 1-D array 
 of type object, with each element being an ndarray. 
 References 
 .. [1] S. E. Maxwell and H. D. Delaney, ""Designing Experiments and 
 Analyzing Data: A Model Comparison Perspective"", Wadsworth, 1990. 
 Examples 
 We\'ll test the following data sets for differences in their variance. 
 >>> x = [10, 11, 13, 9, 7, 12, 12, 9, 10] 
 >>> y = [13, 21, 5, 10, 8, 14, 10, 12, 7, 15] 
 Apply the O\'Brien transform to the data. 
 >>> from scipy.stats import obrientransform 
 >>> tx, ty = obrientransform(x, y) 
 Use `scipy.stats.f_oneway` to apply a one-way ANOVA test to the 
 transformed data. 
 >>> from scipy.stats import f_oneway 
 >>> F, p = f_oneway(tx, ty) 
 >>> p 
 0.1314139477040335 
 If we require that ``p < 0.05`` for significance, we cannot conclude 
 that the variances are different.'"
"def test_install_package_with_target(script): 
    target_dir = (script.scratch_path / 'target') 
   result = script.pip_install_local('-t', target_dir, 'simple==1.0') 
   assert (((Path('scratch') / 'target') / 'simple') in result.files_created), str(result) 
   result = script.pip_install_local('-t', target_dir, 'simple==1.0', expect_stderr=True) 
   assert (not (((Path('scratch') / 'target') / 'simple') in result.files_updated)) 
   result = script.pip_install_local('--upgrade', '-t', target_dir, 'simple==2.0') 
   assert (((Path('scratch') / 'target') / 'simple') in result.files_updated), str(result) 
   egg_folder = (((Path('scratch') / 'target') / 'simple-2.0-py%s.egg-info') % pyversion) 
   assert (egg_folder in result.files_created), str(result) 
   result = script.pip_install_local('-t', target_dir, 'singlemodule==0.0.0') 
   singlemodule_py = ((Path('scratch') / 'target') / 'singlemodule.py') 
   assert (singlemodule_py in result.files_created), str(result) 
   result = script.pip_install_local('-t', target_dir, 'singlemodule==0.0.1', '--upgrade') 
   assert (singlemodule_py in result.files_updated), str(result)", 'Test installing a package with a target directory','Test installing a package using pip install --target'
"def occur_check(var, x): 
    if (var == x): 
      return True 
   elif isinstance(x, Compound): 
      return occur_check(var, x.args) 
   elif is_args(x): 
      if any((occur_check(var, xi) for xi in x)): 
         return True 
   return False"," 'Check if the variable occurs in the expression. 
 >>> occur_check(1, 2) 
 True 
 >>> occur_check(1, 1) 
 False 
 >>> occur_check(1, [1, 2]) 
 True 
 >>> occur_check(1, [1, 2, 3]) 
 False 
 >>> occur_check(1, [1, 2, 3, 4]) 
 True 
 >>> occur_check(1, [1, 2, 3, 4, 5]) 
 False 
 >>> occur_check(1, [1, 2, 3, 4, 5, 6]) 
 True'",'var occurs in subtree owned by x?'
"def predecessor(G, source, target=None, cutoff=None, return_seen=None): 
    if (source not in G): 
      raise nx.NodeNotFound('Source   {}   not   in   G'.format(source)) 
   level = 0 
   nextlevel = [source] 
   seen = {source: level} 
   pred = {source: []} 
   while nextlevel: 
      level = (level + 1) 
      thislevel = nextlevel 
      nextlevel = [] 
      for v in thislevel: 
         for w in G[v]: 
            if (w not in seen): 
               pred[w] = [v] 
               seen[w] = level 
               nextlevel.append(w) 
            elif (seen[w] == level): 
               pred[w].append(v) 
      if (cutoff and (cutoff <= level)): 
         break 
   if (target is not None): 
      if return_seen: 
         if (not (target in pred)): 
            return ([], (-1)) 
         return (pred[target], seen[target]) 
      else: 
         if (not (target in pred)): 
            return [] 
         return pred[target] 
   elif return_seen: 
      return (pred, seen) 
   else: 
      return pred"," 'Find the predecessor of a node in a directed graph. 
 Parameters 
 G : NetworkX graph 
 A directed graph. 
 source : node 
 The source node. 
 target : node, optional 
 The target node. 
 cutoff : int, optional 
 The maximum level of the tree. 
 return_seen : bool, optional 
 If True, return a tuple of (pred, seen), where pred is the predecessor 
 and seen is the set of nodes seen so far. 
 Returns 
 pred : list 
 The predecessor of the source node. 
 seen : set 
 The set of nodes seen so far. 
 Examples 
 >>> G = nx.DiGraph() 
 >>> G.add_edge(0, 1) 
 >>> G.add_edge(0, 2) 
 >>> G.add_edge(1, 2) 
 >>> G.add_edge(2, 3) 
 >>> G.add_edge(2, 4) 
 >>> G.add_edge(3, 4) 
 >>> predecessor(G","'Returns dictionary of predecessors for the path from source to all nodes in G. 
 Parameters 
 G : NetworkX graph 
 source : node label 
 Starting node for path 
 target : node label, optional 
 Ending node for path. If provided only predecessors between 
 source and target are returned 
 cutoff : integer, optional 
 Depth to stop the search. Only paths of length <= cutoff are returned. 
 Returns 
 pred : dictionary 
 Dictionary, keyed by node, of predecessors in the shortest path. 
 Examples 
 >>> G = nx.path_graph(4) 
 >>> list(G) 
 [0, 1, 2, 3] 
 >>> nx.predecessor(G, 0) 
 {0: [], 1: [0], 2: [1], 3: [2]}'"
"def _fingerprint(public_key): 
    try: 
      if six.PY2: 
         raw_key = public_key.decode('base64') 
      else: 
         raw_key = base64.b64decode(public_key, validate=True) 
   except binascii.Error: 
      return None 
   ret = hashlib.md5(raw_key).hexdigest() 
   chunks = [ret[i:(i + 2)] for i in range(0, len(ret), 2)] 
   return ':'.join(chunks)"," 'Returns a fingerprint of the public key. 
 The fingerprint is a hexadecimal string of 40 characters. 
 This is a hash of the public key, using the MD5 hash function. 
 The hash is truncated to 40 characters to ensure that the fingerprint 
 is unique. 
 :param public_key: The public key to fingerprint. 
 :type public_key: str 
 :return: The fingerprint of the public key. 
 :rtype: str'","'Return a public key fingerprint based on its base64-encoded representation 
 The fingerprint string is formatted according to RFC 4716 (ch.4), that is, 
 in the form ""xx:xx:...:xx"" 
 If the key is invalid (incorrect base64 string), return None'"
"def get_cli_body_ssh(command, response, module): 
    if ('^' == response[0]): 
      body = [] 
   elif ('running' in command): 
      body = response 
   else: 
      if (command in response[0]): 
         response = [response[0].split(command)[1]] 
      try: 
         body = [json.loads(response[0])] 
      except ValueError: 
         module.fail_json(msg='Command   does   not   support   JSON   output', command=command) 
   return body", 'Gets the body of the command in response from the ssh session.',"'Get response for when transport=cli.  This is kind of a hack and mainly 
 needed because these modules were originally written for NX-API.  And 
 not every command supports ""| json"" when using cli/ssh.'"
"def hash_(attrs=None, where=None): 
    return _osquery_cmd(table='hash', attrs=attrs, where=where)"," 'Return the hash of the specified attributes for the specified devices. 
 :param attrs: A list of attributes to return. 
 :param where: A list of WHERE clauses to filter the results. 
 :returns: A list of dictionaries containing the hashes. 
 :rtype: list of dict'","'Return hash information from osquery 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' osquery.hash'"
"def load(): 
    data = _get_data() 
   return du.process_recarray(data, endog_idx=0, dtype=float)"," 'Loads the data. 
 Returns 
 X : ndarray 
 The data. 
 Notes 
 This function should be called before any other function in this module. 
 Returns 
 X : ndarray 
 The data. 
 Examples 
 >>> from sklearn.datasets import make_regression 
 >>> X, y = make_regression(n_samples=1000, n_features=2, noise=0.1, random_state=0) 
 >>> load() 
 array([[0.00000000e+00, 1.00000000e+00], 
 [1.00000000e+00, 0.00000000e+00], 
 [2.00000000e+00, 1.00000000e+00], 
 ..., 
 [999.00000000e+00, 0.00000000e","'Load the strikes data and return a Dataset class instance. 
 Returns 
 Dataset instance: 
 See DATASET_PROPOSAL.txt for more information.'"
"def url_unquote_plus(s, charset='utf-8', errors='replace'): 
    if isinstance(s, unicode): 
      s = s.encode(charset) 
   return _decode_unicode(_unquote_plus(s), charset, errors)"," 'Decode a URL-encoded string using the \'plus\' encoding. 
 This is the same as urllib.unquote_plus() but returns a Unicode string. 
 This is used to decode a URL-encoded string to a Unicode string. 
 This is the same as urllib.unquote_plus() but returns a Unicode string. 
 This is used to decode a URL-encoded string to a Unicode string. 
 >>> url_unquote_plus(\'Hello, world!\') 
 \'Hello, world!\' 
 >>> url_unquote_plus(\'Hello, world!%20\') 
 \'Hello, world!%20\' 
 >>> url_unquote_plus(\'Hello, world!%20\', errors=\'replace\') 
 \'Hello, world!%20\' 
 >>> url_unquote_plus(\'Hello, world!%20\', errors=\'replace\', charset=\'utf-8\') 
 \'Hello, world!%20\' 
 >>> url_unquote_plus(\'Hello, world!%20\', errors=\'","'URL decode a single string with the given decoding and decode 
 a ""+"" to whitespace. 
 Per default encoding errors are ignored.  If you want a different behavior 
 you can set `errors` to ``\'replace\'`` or ``\'strict\'``.  In strict mode a 
 `HTTPUnicodeError` is raised. 
 :param s: the string to unquote. 
 :param charset: the charset to be used. 
 :param errors: the error handling for the charset decoding.'"
"def createMemoryWorker(): 
    def perform(): 
      if (not worker._pending): 
         return False 
      if (worker._pending[0] is NoMoreWork): 
         return False 
      worker._pending.pop(0)() 
      return True 
   worker = MemoryWorker() 
   return (worker, perform)"," 'Create a new worker that will use a queue to store the pending 
 tasks. 
 :return: a tuple (worker, perform) where worker is a worker object 
 and perform is a function that will perform the pending 
 task. 
 :rtype: tuple'","'Create an L{IWorker} that does nothing but defer work, to be performed 
 later. 
 @return: a worker that will enqueue work to perform later, and a callable 
 that will perform one element of that work. 
 @rtype: 2-L{tuple} of (L{IWorker}, L{callable})'"
"def is_master_node(client): 
    my_node_id = list(client.nodes.info('_local')['nodes'])[0] 
   master_node_id = client.cluster.state(metric='master_node')['master_node'] 
   return (my_node_id == master_node_id)", 'Returns true if the node is the master node',"'Return `True` if the connected client node is the elected master node in 
 the Elasticsearch cluster, otherwise return `False`. 
 :arg client: An :class:`elasticsearch.Elasticsearch` client object 
 :rtype: bool'"
"def get_config_vars(*args): 
    global _config_vars 
   if (_config_vars is None): 
      func = globals().get(('_init_' + os.name)) 
      if func: 
         func() 
      else: 
         _config_vars = {} 
      _config_vars['prefix'] = PREFIX 
      _config_vars['exec_prefix'] = EXEC_PREFIX 
      if (sys.platform == 'darwin'): 
         import _osx_support 
         _osx_support.customize_config_vars(_config_vars) 
   if args: 
      vals = [] 
      for name in args: 
         vals.append(_config_vars.get(name)) 
      return vals 
   else: 
      return _config_vars"," 'Returns a list of configuration variables for use by the 
 Python interpreter. 
 :param args: 
 The configuration variables to return. 
 :return: 
 A list of configuration variables.'","'With no arguments, return a dictionary of all configuration 
 variables relevant for the current platform.  Generally this includes 
 everything needed to build extensions and install both pure modules and 
 extensions.  On Unix, this means every variable defined in Python\'s 
 installed Makefile; on Windows and Mac OS it\'s a much smaller set. 
 With arguments, return a list of values that result from looking up 
 each argument in the configuration variable dictionary.'"
"@frappe.whitelist() 
 def remove(doctype, name, assign_to): 
    try: 
      todo = frappe.db.get_value(u'ToDo', {u'reference_type': doctype, u'reference_name': name, u'owner': assign_to, u'status': u'Open'}) 
      if todo: 
         todo = frappe.get_doc(u'ToDo', todo) 
         todo.status = u'Closed' 
         todo.save(ignore_permissions=True) 
         notify_assignment(todo.assigned_by, todo.owner, todo.reference_type, todo.reference_name) 
   except frappe.DoesNotExistError: 
      pass 
   if frappe.get_meta(doctype).get_field(u'assigned_to'): 
      frappe.db.set_value(doctype, name, u'assigned_to', None) 
   return get({u'doctype': doctype, u'name': name})"," 'Remove a task assigned to user. 
 :param doctype: Document type of the task 
 :param name: Name of the task 
 :param assign_to: User who has been assigned the task 
 :returns: dict containing the task\'s details'",'remove from todo'
"def create_apppool(name): 
    pscmd = [] 
   current_apppools = list_apppools() 
   apppool_path = 'IIS:\\AppPools\\{0}'.format(name) 
   if (name in current_apppools): 
      _LOG.debug(""Application   pool   '%s'   already   present."", name) 
      return True 
   pscmd.append(""New-Item   -Path   '{0}'"".format(apppool_path)) 
   cmd_ret = _srvmgr(str().join(pscmd)) 
   if (cmd_ret['retcode'] == 0): 
      _LOG.debug('Application   pool   created   successfully:   %s', name) 
      return True 
   _LOG.error('Unable   to   create   application   pool:   %s', name) 
   return False"," 'Create an application pool. 
 :param name: The name of the application pool. 
 :returns: True if the application pool was created, False otherwise. 
 :rtype: bool'","'Create an IIS application pool. 
 .. note: 
 This function only validates against the application pool name, and will return 
 True even if the application pool already exists with a different configuration. 
 It will not modify the configuration of an existing application pool. 
 :param str name: The name of the IIS application pool. 
 :return: A boolean representing whether all changes succeeded. 
 :rtype: bool 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' win_iis.create_apppool name=\'MyTestPool\''"
"def do_baremetal_node_list(cs, _args): 
    _emit_deprecation_warning('baremetal-node-list') 
   nodes = cs.baremetal.list() 
   _print_baremetal_nodes_list(nodes)", 'Lists all baremetal nodes.','DEPRECATED: Print list of available baremetal nodes.'
"def fminbound(func, x1, x2, args=(), xtol=1e-05, maxfun=500, full_output=0, disp=1): 
    options = {'xatol': xtol, 'maxiter': maxfun, 'disp': disp} 
   res = _minimize_scalar_bounded(func, (x1, x2), args, **options) 
   if full_output: 
      return (res['x'], res['fun'], res['status'], res['nfev']) 
   else: 
      return res['x']"," 'Find the minimum of a function bounded between two points. 
 The function is assumed to be continuous on the interval. 
 Parameters 
 func : callable 
 The objective function. 
 x1, x2 : array_like 
 The bounds of the interval. 
 args : tuple, optional 
 Additional arguments to the function. 
 xtol : float, optional 
 The tolerance for the function and gradient. 
 maxfun : int, optional 
 The maximum number of function evaluations. 
 full_output : bool, optional 
 If True, return the function value, the function and gradient 
 evaluations, and the convergence status. 
 disp : bool, optional 
 If True, print convergence messages. 
 Returns 
 x : ndarray 
 The minimizer. 
 fun : float 
 The value of the objective function at the minimizer. 
 status : int 
 The convergence status. 0 means convergence, 1 means convergence 
 to a local minimum, and -1 means convergence to a local maximum. 
 nfev : int 
 The number of function evaluations. 
 Notes 
 This","'Bounded minimization for scalar functions. 
 Parameters 
 func : callable f(x,*args) 
 Objective function to be minimized (must accept and return scalars). 
 x1, x2 : float or array scalar 
 The optimization bounds. 
 args : tuple, optional 
 Extra arguments passed to function. 
 xtol : float, optional 
 The convergence tolerance. 
 maxfun : int, optional 
 Maximum number of function evaluations allowed. 
 full_output : bool, optional 
 If True, return optional outputs. 
 disp : int, optional 
 If non-zero, print messages. 
 0 : no message printing. 
 1 : non-convergence notification messages only. 
 2 : print a message on convergence too. 
 3 : print iteration results. 
 Returns 
 xopt : ndarray 
 Parameters (over given interval) which minimize the 
 objective function. 
 fval : number 
 The function value at the minimum point. 
 ierr : int 
 An error flag (0 if converged, 1 if maximum number of 
 function calls reached). 
 numfunc : int 
 The number of function calls made. 
 See also 
 minimize_scalar: Interface to minimization algorithms for scalar 
 univariate functions. See the \'Bounded\' `method` in particular. 
 Notes 
 Finds a local minimizer of the scalar function `func` in the 
 interval x1 < xopt < x2 using Brent\'s method.  (See `brent` 
 for auto-bracketing).'"
"def primary_key_value(instance, as_string=False): 
    result = getattr(instance, primary_key_for(instance)) 
   if (not as_string): 
      return result 
   try: 
      return str(result) 
   except UnicodeEncodeError: 
      return url_quote_plus(result.encode('utf-8'))"," 'Returns the primary key value of the given instance. 
 If the primary key is not a string, then it will be encoded as a URL 
 encoded string. 
 :param instance: An instance of the model. 
 :param as_string: Whether to return the primary key value as a string. 
 :return: The primary key value of the given instance.'","'Returns the value of the primary key field of the specified `instance` 
 of a SQLAlchemy model. 
 This essentially a convenience function for:: 
 getattr(instance, primary_key_for(instance)) 
 If `as_string` is ``True``, try to coerce the return value to a string.'"
"def between(expr, lower_bound, upper_bound): 
    expr = _literal_as_binds(expr) 
   return expr.between(lower_bound, upper_bound)", 'Returns true if expr is between lower_bound and upper_bound.',"'Produce a ``BETWEEN`` predicate clause. 
 E.g.:: 
 from sqlalchemy import between 
 stmt = select([users_table]).where(between(users_table.c.id, 5, 7)) 
 Would produce SQL resembling:: 
 SELECT id, name FROM user WHERE id BETWEEN :id_1 AND :id_2 
 The :func:`.between` function is a standalone version of the 
 :meth:`.ColumnElement.between` method available on all 
 SQL expressions, as in:: 
 stmt = select([users_table]).where(users_table.c.id.between(5, 7)) 
 All arguments passed to :func:`.between`, including the left side 
 column expression, are coerced from Python scalar values if a 
 the value is not a :class:`.ColumnElement` subclass.   For example, 
 three fixed values can be compared as in:: 
 print(between(5, 3, 7)) 
 Which would produce:: 
 :param_1 BETWEEN :param_2 AND :param_3 
 :param expr: a column expression, typically a :class:`.ColumnElement` 
 instance or alternatively a Python scalar expression to be coerced 
 into a column expression, serving as the left side of the ``BETWEEN`` 
 expression. 
 :param lower_bound: a column or Python scalar expression serving as the lower 
 bound of the right side of the ``BETWEEN`` expression. 
 :param upper_bound: a column or Python scalar expression serving as the 
 upper bound of the right side of the ``BETWEEN`` expression. 
 .. seealso:: 
 :meth:`.ColumnElement.between`'"
"def _windows_commondata_path(): 
    import ctypes 
   from ctypes import wintypes, windll 
   CSIDL_COMMON_APPDATA = 35 
   _SHGetFolderPath = windll.shell32.SHGetFolderPathW 
   _SHGetFolderPath.argtypes = [wintypes.HWND, ctypes.c_int, wintypes.HANDLE, wintypes.DWORD, wintypes.LPCWSTR] 
   path_buf = wintypes.create_unicode_buffer(wintypes.MAX_PATH) 
   _SHGetFolderPath(0, CSIDL_COMMON_APPDATA, 0, 0, path_buf) 
   return path_buf.value"," 'Returns the path to the \'Common Application Data\' folder. 
 This is the same as the \'Documents\' folder on Windows Vista and later. 
 This function is platform-specific, and may not be available on other 
 platforms. 
 This function is not available on Windows XP.'","'Return the common appdata path, using ctypes 
 From http://stackoverflow.com/questions/626796/    how-do-i-find-the-windows-common-application-data-folder-using-python'"
"def get_all_launch_configurations(region=None, key=None, keyid=None, profile=None): 
    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) 
   try: 
      return conn.get_all_launch_configurations() 
   except boto.exception.BotoServerError as e: 
      log.error(e) 
      return []"," 'Get all launch configurations in the region. 
 :param region: Region name. 
 :param key: Access key. 
 :param keyid: Access key id. 
 :param profile: Credentials profile. 
 :return: List of launch configurations. 
 :rtype: list'","'Fetch and return all Launch Configuration with details. 
 CLI example:: 
 salt myminion boto_asg.get_all_launch_configurations'"
"def test_conflicting_path(tmpdir, mocked_aws_cf_simple): 
    with tmpdir.as_cwd(): 
      tmpdir.join('config.yaml').write(mocked_aws_cf_simple) 
      assert (main(['create']) == 0) 
      assert (main(['create']) == 1)", 'Test that conflicting paths are handled properly',"'Ensure default cluster info path is never overwritten 
 by launching successive clusters'"
"def test_StepDict_raise_StepLoadingError_if_load_first_argument_is_not_a_regex(): 
    steps = StepDict() 
   test_load = (lambda : steps.load('an   invalid   regex;)', (lambda : ''))) 
   assert_raises(StepLoadingError, test_load)"," 'Test that StepDict raises a StepLoadingError if load() is called with 
 a non-regex argument.'","'lettuce.STEP_REGISTRY.load(step, func) should raise an error if step is not a regex'"
"def _conf(family='ipv4'): 
    if (__grains__['os_family'] == 'RedHat'): 
      if (family == 'ipv6'): 
         return '/etc/sysconfig/ip6tables' 
      else: 
         return '/etc/sysconfig/iptables' 
   elif (__grains__['os_family'] == 'Arch'): 
      if (family == 'ipv6'): 
         return '/etc/iptables/ip6tables.rules' 
      else: 
         return '/etc/iptables/iptables.rules' 
   elif (__grains__['os_family'] == 'Debian'): 
      if (family == 'ipv6'): 
         return '/etc/iptables/rules.v6' 
      else: 
         return '/etc/iptables/rules.v4' 
   elif (__grains__['os'] == 'Gentoo'): 
      if (family == 'ipv6'): 
         return '/var/lib/ip6tables/rules-save' 
      else: 
         return '/var/lib/iptables/rules-save' 
   elif (__grains__['os_family'] == 'SUSE'): 
      return '/etc/sysconfig/scripts/SuSEfirewall2-custom' 
   else: 
      raise SaltException((('Saving   iptables   to   file   is   not' + '   supported   on   {0}.'.format(__grains__['os'])) + '   Please   file   an   issue   with   SaltStack'))", 'Returns the path to the iptables file for the given family.','Some distros have a specific location for config files'
"def prelu(layer, **kwargs): 
    nonlinearity = getattr(layer, 'nonlinearity', None) 
   if (nonlinearity is not None): 
      layer.nonlinearity = nonlinearities.identity 
   return ParametricRectifierLayer(layer, **kwargs)"," 'Add parametric rectifier to the given layer. 
 Parametric rectifier is defined as: 
 .. math:: 
 f(x) = \max(0, \alpha x + \beta) 
 Parameters are alpha and beta. 
 Parameters 
 layer : Layer 
 The layer to be modified. 
 Returns 
 The modified layer. 
 Examples 
 >>> layer = nn.Dense(10, activation=nn.relu) 
 >>> prelu(layer) 
 ParametricRectifierLayer(Dense(10, activation=identity))'","'Convenience function to apply parametric rectify to a given layer\'s output. 
 Will set the layer\'s nonlinearity to identity if there is one and will 
 apply the parametric rectifier instead. 
 Parameters 
 layer: a :class:`Layer` instance 
 The `Layer` instance to apply the parametric rectifier layer to; 
 note that it will be irreversibly modified as specified above 
 **kwargs 
 Any additional keyword arguments are passed to the 
 :class:`ParametericRectifierLayer` 
 Examples 
 Note that this function modifies an existing layer, like this: 
 >>> from lasagne.layers import InputLayer, DenseLayer, prelu 
 >>> layer = InputLayer((32, 100)) 
 >>> layer = DenseLayer(layer, num_units=200) 
 >>> layer = prelu(layer) 
 In particular, :func:`prelu` can *not* be passed as a nonlinearity.'"
"def setup(templates, *args, **kwargs): 
    test_once = kwargs.get('test_once', False) 
   for arg in args: 
      templates.update(arg) 
   templates['inclusion.html'] = '{{   result   }}' 
   loaders = [('django.template.loaders.cached.Loader', [('django.template.loaders.locmem.Loader', templates)])] 
   def decorator(func): 
      @override_settings(TEMPLATES=None) 
      @functools.wraps(func) 
      def inner(self): 
         libraries = getattr(self, 'libraries', {}) 
         self.engine = Engine(libraries=libraries, loaders=loaders) 
         func(self) 
         if test_once: 
            return 
         func(self) 
         self.engine = Engine(libraries=libraries, loaders=loaders, string_if_invalid='INVALID') 
         func(self) 
         func(self) 
         self.engine = Engine(debug=True, libraries=libraries, loaders=loaders) 
         func(self) 
         func(self) 
      return inner 
   return decorator"," 'Decorator for setting up a test. 
 This decorator sets up the test environment by creating a new 
 Engine, and setting the TEMPLATES setting to None. 
 The decorator also adds a function to the class that will be called 
 after the test is run. 
 This function will be called with the same context as the test, and 
 will be used to restore the TEMPLATES setting to the original value. 
 This is used to make sure that the test is run with the same TEMPLATES 
 setting as the rest of the code. 
 This decorator also adds a function to the class that will be called 
 after the test is run. 
 This function will be called with the same context as the test, and 
 will be used to restore the TEMPLATES setting to the original value. 
 This is used to make sure that the test is run with the same TEMPLATES 
 setting as the rest of the code.'","'Runs test method multiple times in the following order: 
 debug       cached      string_if_invalid 
 False       False 
 False       True 
 False       False       INVALID 
 False       True        INVALID 
 True        False 
 True        True'"
"def get_base_dirs(): 
    if options['basedirlist']: 
      return options['basedirlist'] 
   if os.environ.get('MPLBASEDIRLIST'): 
      return os.environ.get('MPLBASEDIRLIST').split(os.pathsep) 
   win_bases = ['win32_static'] 
   if os.getenv('CONDA_DEFAULT_ENV'): 
      win_bases.append(os.path.join(os.getenv('CONDA_DEFAULT_ENV'), 'Library')) 
   basedir_map = {'win32': win_bases, 'darwin': ['/usr/local/', '/usr', '/usr/X11', '/opt/X11', '/opt/local'], 'sunos5': [(os.getenv('MPLIB_BASE') or '/usr/local')], 'gnu0': ['/usr'], 'aix5': ['/usr/local']} 
   return basedir_map.get(sys.platform, ['/usr/local', '/usr'])"," 'Get the list of base directories to search for MPL. 
 This is a list of directories that are searched for the MPL library. 
 This is a list of directories that are searched for the MPL library. 
 If ``basedirlist`` is set in the configuration file, that will be used. 
 Otherwise, this function will attempt to determine the list of directories 
 to search for the MPL library. 
 If ``basedirlist`` is set, it is assumed that each directory is a directory 
 on the system. 
 If ``basedirlist`` is not set, this function will attempt to determine the 
 list of directories to search for the MPL library. 
 If ``basedirlist`` is not set, this function will attempt to determine the 
 list of directories to search for the MPL library. 
 If ``basedirlist`` is not set, this function will attempt to determine the 
 list of directories to search for the MPL library. 
 If ``basedirlist`` is not set, this function will attempt to determine the 
 list of directories to search for the MPL library. 
 If ``basedirlist`` is not set",'Returns a list of standard base directories on this platform.'
"def ci(a, which=95, axis=None): 
    p = ((50 - (which / 2)), (50 + (which / 2))) 
   return percentiles(a, p, axis)"," 'Return the 1% and 99% quantiles of the data. 
 Parameters 
 a : array_like 
 Input data. 
 which : float, optional 
 The percentile to return. If None, the 95% quantile is returned. 
 axis : {0, 1}, optional 
 Axis along which to return the quantile. 
 Returns 
 a : 1D array 
 The quantile. 
 Notes 
 The default `which` is 95% (50 - 2.5, 50 + 2.5). 
 Examples 
 >>> a = np.array([1, 2, 3, 4, 5]) 
 >>> ci(a) 
 array([2.0, 4.0]) 
 >>> ci(a, which=50) 
 array([2.0, 4.0]) 
 >>> ci(a, which=10) 
 array([1.0, 3.0]) 
 >>> ci(a, which=0.1) 
 array([1.0, 3.0",'Return a percentile range from an array of values.'
"def zpk2sos(z, p, k, pairing='nearest'): 
    valid_pairings = ['nearest', 'keep_odd'] 
   if (pairing not in valid_pairings): 
      raise ValueError(('pairing   must   be   one   of   %s,   not   %s' % (valid_pairings, pairing))) 
   if (len(z) == len(p) == 0): 
      return array([[k, 0.0, 0.0, 1.0, 0.0, 0.0]]) 
   p = np.concatenate((p, np.zeros(max((len(z) - len(p)), 0)))) 
   z = np.concatenate((z, np.zeros(max((len(p) - len(z)), 0)))) 
   n_sections = ((max(len(p), len(z)) + 1) // 2) 
   sos = zeros((n_sections, 6)) 
   if (((len(p) % 2) == 1) and (pairing == 'nearest')): 
      p = np.concatenate((p, [0.0])) 
      z = np.concatenate((z, [0.0])) 
   assert (len(p) == len(z)) 
   z = np.concatenate(_cplxreal(z)) 
   p = np.concatenate(_cplxreal(p)) 
   p_sos = np.zeros((n_sections, 2), np.complex128) 
   z_sos = np.zeros_like(p_sos) 
   for si in range(n_sections): 
      p1_idx = np.argmin(np.abs((1 - np.abs(p)))) 
      p1 = p[p1_idx] 
      p = np.delete(p, p1_idx) 
      if (np.isreal(p1) and (np.isreal(p).sum() == 0)): 
         z1_idx = _nearest_real_complex_idx(z, p1, 'real') 
         z1 = z[z1_idx] 
         z = np.delete(z, z1_idx) 
         p2 = z2 = 0 
      else: 
         if ((not np.isreal(p1)) and (np.isreal(z).sum() == 1)): 
            z1_idx = _nearest_real_complex_idx(z, p1, 'complex') 
            assert (not np.isreal(z[z1_idx])) 
         else: 
            z1_idx = np.argmin(np.abs((p1 - z))) 
         z1 = z[z1_idx] 
         z = np.delete(z, z1_idx) 
         if (not np.isreal(p1)): 
            if (not np.isreal(z1)): 
               p2 = p1.conj() 
               z2 = z1.conj() 
            else: 
               p2 = p1.conj() 
               z2_idx = _nearest_real_complex_idx(z, p1, 'real') 
               z2 = z[z2_idx] 
               assert np.isreal(z2) 
               z = np.delete(z, z2_idx) 
         else: 
            if (not np.isreal(z1)): 
               z2 = z1.conj() 
               p2_idx = _nearest_real_complex_idx(p, z1, 'real') 
               p2 = p[p2_idx] 
               assert np.isreal(p2) 
            else: 
               idx = np.where(np.isreal(p))[0] 
               assert (len(idx) > 0) 
               p2_idx = idx[np.argmin(np.abs((np.abs(p[idx]) - 1)))] 
               p2 = p[p2_idx] 
               assert np.isreal(p2) 
               z2_idx = _nearest_real_complex_idx(z, p2, 'real') 
               z2 = z[z2_idx] 
               assert np.isreal(z2) 
               z = np.delete(z, z2_idx) 
            p = np.delete(p, p2_idx) 
      p_sos[si] = [p1, p2] 
      z_sos[si] = [z1, z2] 
   assert (len(p) == len(z) == 0) 
   del p, z 
   p_sos = np.reshape(p_sos[::(-1)], (n_sections, 2)) 
   z_sos = np.reshape(z_sos[::(-1)], (n_sections, 2)) 
   gains = np.ones(n_sections) 
   gains[0] = k 
   for si in range(n_sections): 
      x = zpk2tf(z_sos[si], p_sos[si], gains[si]) 
      sos[si] = np.concatenate(x) 
   return sos"," 'Convert a ZPK model to a SOS model. 
 Parameters 
 z : array of complex numbers 
 The zeros of the model. 
 p : array of complex numbers 
 The poles of the model. 
 k : scalar 
 The gain of the model. 
 pairing : string 
 The pairing method for the poles and zeros. 
 Returns 
 sos : array of complex numbers 
 The SOS model. 
 Notes 
 The pairing method determines the location of the poles and zeros in 
 the SOS model. 
 The SOS model is returned in the following form: 
 sos[0] = k 
 sos[1:n_sections] = [z_sos, p_sos] 
 The SOS model is returned in the following form: 
 sos[0] = k 
 sos[1:n_sections] = [z_sos, p_sos] 
 References 
 .. [1] ""SOS-based Modeling of Linear Time-Delay Systems"", 
 K. M. Johnson, S. B. Cooper, and R","'Return second-order sections from zeros, poles, and gain of a system 
 Parameters 
 z : array_like 
 Zeros of the transfer function. 
 p : array_like 
 Poles of the transfer function. 
 k : float 
 System gain. 
 pairing : {\'nearest\', \'keep_odd\'}, optional 
 The method to use to combine pairs of poles and zeros into sections. 
 See Notes below. 
 Returns 
 sos : ndarray 
 Array of second-order filter coefficients, with shape 
 ``(n_sections, 6)``. See `sosfilt` for the SOS filter format 
 specification. 
 See Also 
 sosfilt 
 Notes 
 The algorithm used to convert ZPK to SOS format is designed to 
 minimize errors due to numerical precision issues. The pairing 
 algorithm attempts to minimize the peak gain of each biquadratic 
 section. This is done by pairing poles with the nearest zeros, starting 
 with the poles closest to the unit circle. 
 *Algorithms* 
 The current algorithms are designed specifically for use with digital 
 filters. (The output coefficents are not correct for analog filters.) 
 The steps in the ``pairing=\'nearest\'`` and ``pairing=\'keep_odd\'`` 
 algorithms are mostly shared. The ``nearest`` algorithm attempts to 
 minimize the peak gain, while ``\'keep_odd\'`` minimizes peak gain under 
 the constraint that odd-order systems should retain one section 
 as first order. The algorithm steps and are as follows: 
 As a pre-processing step, add poles or zeros to the origin as 
 necessary to obtain the same number of poles and zeros for pairing. 
 If ``pairing == \'nearest\'`` and there are an odd number of poles, 
 add an additional pole and a zero at the origin. 
 The following steps are then iterated over until no more poles or 
 zeros remain: 
 1. Take the (next remaining) pole (complex or real) closest to the 
 unit circle to begin a new filter section. 
 2. If the pole is real and there are no other remaining real poles [#]_, 
 add the closest real zero to the section and leave it as a first 
 order section. Note that after this step we are guaranteed to be 
 left with an even number of real poles, complex poles, real zeros, 
 and complex zeros for subsequent pairing iterations. 
 3. Else: 
 1. If the pole is complex and the zero is the only remaining real 
 zero*, then pair the pole with the *next* closest zero 
 (guaranteed to be complex). This is necessary to ensure that 
 there will be a real zero remaining to eventually create a 
 first-order section (thus keeping the odd order). 
 2. Else pair the pole with the closest remaining zero (complex or 
 real). 
 3. Proceed to complete the second-order section by adding another 
 pole and zero to the current pole and zero in the section: 
 1. If the current pole and zero are both complex, add their 
 conjugates. 
 2. Else if the pole is complex and the zero is real, add the 
 conjugate pole and the next closest real zero. 
 3. Else if the pole is real and the zero is complex, add the 
 conjugate zero and the real pole closest to those zeros. 
 4. Else (we must have a real pole and real zero) add the next 
 real pole closest to the unit circle, and then add the real 
 zero closest to that pole. 
 .. [#] This conditional can only be met for specific odd-order inputs 
 with the ``pairing == \'keep_odd\'`` method. 
 .. versionadded:: 0.16.0 
 Examples 
 Design a 6th order low-pass elliptic digital filter for a system with a 
 sampling rate of 8000 Hz that has a pass-band corner frequency of 
 1000 Hz.  The ripple in the pass-band should not exceed 0.087 dB, and 
 the attenuation in the stop-band should be at least 90 dB. 
 In the following call to `signal.ellip`, we could use ``output=\'sos\'``, 
 but for this example, we\'ll use ``output=\'zpk\'``, and then convert to SOS 
 format with `zpk2sos`: 
 >>> from scipy import signal 
 >>> z, p, k = signal.ellip(6, 0.087, 90, 1000/(0.5*8000), output=\'zpk\') 
 Now convert to SOS format. 
 >>> sos = signal.zpk2sos(z, p, k) 
 The coefficients of the numerators of the sections: 
 >>> sos[:, :3] 
 array([[ 0.0014154 ,  0.00248707,  0.0014154 ], 
 [ 1.        ,  0.72965193,  1.        ], 
 [ 1.        ,  0.17594966,  1.        ]]) 
 The symmetry in the coefficients occurs because all the zeros are on the 
 unit circle. 
 The coefficients of the denominators of the sections: 
 >>> sos[:, 3:] 
 array([[ 1.        , -1.32543251,  0.46989499], 
 [ 1.        , -1.26117915,  0.6262586 ], 
 [ 1.        , -1.25707217,  0.86199667]]) 
 The next example shows the effect of the `pairing` option.  We have a 
 system with three poles and three zeros, so the SOS array will have 
 shape (2, 6).  The means there is, in effect, an extra pole and an extra 
 zero at the origin in the SOS representation. 
 >>> z1 = np.array([-1, -0.5-0.5j, -0.5+0.5j]) 
 >>> p1 = np.array([0.75, 0.8+0.1j, 0.8-0.1j]) 
 With ``pairing=\'nearest\'`` (the default), we obtain 
 >>> signal.zpk2sos(z1, p1, 1) 
 array([[ 1.  ,  1.  ,  0.5 ,  1.  , -0.75,  0.  ], 
 [ 1.  ,  1.  ,  0.  ,  1.  , -1.6 ,  0.65]]) 
 The first section has the zeros {-0.5-0.05j, -0.5+0.5j} and the poles 
 {0, 0.75}, and the second section has the zeros {-1, 0} and poles 
 {0.8+0.1j, 0.8-0.1j}.  Note that the extra pole and zero at the origin 
 have been assigned to different sections. 
 With ``pairing=\'keep_odd\'``, we obtain: 
 >>> signal.zpk2sos(z1, p1, 1, pairing=\'keep_odd\') 
 array([[ 1.  ,  1.  ,  0.  ,  1.  , -0.75,  0.  ], 
 [ 1.  ,  1.  ,  0.5 ,  1.  , -1.6 ,  0.65]]) 
 The extra pole and zero at the origin are in the same section. 
 The first section is, in effect, a first-order section.'"
"def test_to(): 
    with pytest.raises(falcon.http_status.HTTPStatus) as redirect: 
      hug.redirect.to('/') 
   assert ('302' in redirect.value.status)", 'Test redirecting to another resource.','Test that the base redirect to function works as expected'
"def get_load(jid): 
    jid_dir = salt.utils.jid.jid_dir(jid, _job_dir(), __opts__['hash_type']) 
   load_fn = os.path.join(jid_dir, LOAD_P) 
   if ((not os.path.exists(jid_dir)) or (not os.path.exists(load_fn))): 
      return {} 
   serial = salt.payload.Serial(__opts__) 
   ret = serial.load(salt.utils.fopen(os.path.join(jid_dir, LOAD_P), 'rb')) 
   minions_cache = [os.path.join(jid_dir, MINIONS_P)] 
   minions_cache.extend(glob.glob(os.path.join(jid_dir, SYNDIC_MINIONS_P.format('*')))) 
   all_minions = set() 
   for minions_path in minions_cache: 
      log.debug('Reading   minion   list   from   %s', minions_path) 
      try: 
         all_minions.update(serial.load(salt.utils.fopen(minions_path, 'rb'))) 
      except IOError as exc: 
         salt.utils.files.process_read_exception(exc, minions_path) 
   if all_minions: 
      ret['Minions'] = sorted(all_minions) 
   return ret"," 'Get the load data for a given job. 
 Returns a dictionary with the following keys: 
 * Minions: The list of minions currently in the job. 
 * Minion_count: The number of minions currently in the job. 
 * Minion_count_total: The total number of minions that have ever been 
 in the job. 
 * Minion_count_failed: The number of minions that have failed to join the 
 job. 
 * Minion_count_succeeded: The number of minions that have succeeded in 
 joining the job. 
 * Minion_count_skipped: The number of minions that have skipped the job. 
 * Minion_count_removed: The number of minions that have been removed from 
 the job. 
 * Minion_count_unreachable: The number of minions that have been 
 unreachable. 
 * Minion_count_rejoin: The number of minions that have re-joined the job. 
 * Minion_count_failed_rejoin: The number of minions that have failed",'Return the load data that marks a specified jid'
"def parse_features(feature_files, language=None): 
    scenario_collector = FeatureScenarioLocationCollector() 
   features = [] 
   for location in feature_files: 
      if (not isinstance(location, FileLocation)): 
         assert isinstance(location, string_types) 
         location = FileLocation(os.path.normpath(location)) 
      if (location.filename == scenario_collector.filename): 
         scenario_collector.add_location(location) 
         continue 
      elif scenario_collector.feature: 
         current_feature = scenario_collector.build_feature() 
         features.append(current_feature) 
         scenario_collector.clear() 
      assert isinstance(location, FileLocation) 
      filename = os.path.abspath(location.filename) 
      feature = parser.parse_file(filename, language=language) 
      if feature: 
         scenario_collector.feature = feature 
         scenario_collector.add_location(location) 
   if scenario_collector.feature: 
      current_feature = scenario_collector.build_feature() 
      features.append(current_feature) 
   return features"," 'Parses a list of feature files and returns a list of features. 
 If the language is not specified, the language is inferred from the 
 feature files. 
 Args: 
 feature_files: a list of feature files 
 language: the language of the feature files (defaults to the language 
 of the feature files)'","'Parse feature files and return list of Feature model objects. 
 Handles: 
 * feature file names, ala ""alice.feature"" 
 * feature file locations, ala: ""alice.feature:10"" 
 :param feature_files: List of feature file names to parse. 
 :param language:      Default language to use. 
 :return: List of feature objects.'"
"def get_credential(tenant_id, credential_id): 
    session = db.get_session() 
   try: 
      cred = session.query(l2network_models.Credential).filter_by(tenant_id=tenant_id).filter_by(credential_id=credential_id).one() 
      return cred 
   except exc.NoResultFound: 
      raise c_exc.CredentialNotFound(credential_id=credential_id, tenant_id=tenant_id)", 'Get a credential by tenant_id and credential_id.','Lists the creds for given a cred_id and tenant_id'
"@CELERY_APP.task 
 def send_ccx_course_published(course_key): 
    course_key = CourseLocator.from_string(course_key) 
   for ccx in CustomCourseForEdX.objects.filter(course_id=course_key): 
      try: 
         ccx_key = CCXLocator.from_course_locator(course_key, unicode(ccx.id)) 
      except InvalidKeyError: 
         log.info('Attempt   to   publish   course   with   deprecated   id.   Course:   %s.   CCX:   %s', course_key, ccx.id) 
         continue 
      responses = SignalHandler.course_published.send(sender=ccx, course_key=ccx_key) 
      for (rec, response) in responses: 
         log.info('Signal   fired   when   course   is   published.   Receiver:   %s.   Response:   %s', rec, response)", 'Send CCX course published signal to all CCXs for the course.',"'Find all CCX derived from this course, and send course published event for them.'"
"def inverse_hankel_transform(F, k, r, nu, **hints): 
    return InverseHankelTransform(F, k, r, nu).doit(**hints)",'.doit(**hints)',"'Compute the inverse Hankel transform of `F` defined as 
 .. math:: f(r) = \int_{0}^\infty F_\nu(k) J_\nu(k r) k \mathrm{d} k. 
 If the transform cannot be computed in closed form, this 
 function returns an unevaluated :class:`InverseHankelTransform` object. 
 For a description of possible hints, refer to the docstring of 
 :func:`sympy.integrals.transforms.IntegralTransform.doit`. 
 Note that for this transform, by default ``noconds=True``. 
 >>> from sympy import hankel_transform, inverse_hankel_transform, gamma 
 >>> from sympy import gamma, exp, sinh, cosh 
 >>> from sympy.abc import r, k, m, nu, a 
 >>> ht = hankel_transform(1/r**m, r, k, nu) 
 >>> ht 
 2*2**(-m)*k**(m - 2)*gamma(-m/2 + nu/2 + 1)/gamma(m/2 + nu/2) 
 >>> inverse_hankel_transform(ht, k, r, nu) 
 r**(-m) 
 >>> ht = hankel_transform(exp(-a*r), r, k, 0) 
 >>> ht 
 a/(k**3*(a**2/k**2 + 1)**(3/2)) 
 >>> inverse_hankel_transform(ht, k, r, 0) 
 exp(-a*r) 
 See Also 
 fourier_transform, inverse_fourier_transform 
 sine_transform, inverse_sine_transform 
 cosine_transform, inverse_cosine_transform 
 hankel_transform 
 mellin_transform, laplace_transform'"
"def iterable(obj): 
    try: 
      iter(obj) 
   except TypeError: 
      return False 
   return True"," 'Returns True if obj is iterable, False otherwise.'",'return true if *obj* is iterable'
"def normalize(a, axis=None): 
    a_sum = a.sum(axis) 
   if (axis and (a.ndim > 1)): 
      a_sum[(a_sum == 0)] = 1 
      shape = list(a.shape) 
      shape[axis] = 1 
      a_sum.shape = shape 
   a /= a_sum"," 'Normalize the input array. 
 Parameters 
 a : ndarray 
 Input array. 
 axis : int, optional 
 Axis along which to normalize. 
 Returns 
 a : ndarray 
 Normalized input array. 
 Notes 
 If ``axis`` is not specified, it is assumed to be the last axis. 
 Examples 
 >>> x = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) 
 >>> x 
 array([[1, 2, 3], 
 [4, 5, 6], 
 [7, 8, 9]]) 
 >>> normalize(x) 
 array([[0.1, 0.2, 0.3], 
 [0.4, 0.5, 0.6], 
 [0.7, 0.8, 0.9]])'","'Normalizes the input array so that it sums to 1. 
 Parameters 
 a : array 
 Non-normalized input data. 
 axis : int 
 Dimension along which normalization is performed. 
 Notes 
 Modifies the input **inplace**.'"
"def krackhardt_kite_graph(create_using=None): 
    description = ['adjacencylist', 'Krackhardt   Kite   Social   Network', 10, [[2, 3, 4, 6], [1, 4, 5, 7], [1, 4, 6], [1, 2, 3, 5, 6, 7], [2, 4, 7], [1, 3, 4, 7, 8], [2, 4, 5, 6, 8], [6, 7, 9], [8, 10], [9]]] 
   G = make_small_undirected_graph(description, create_using) 
   return G"," 'Generate a graph from the Krackhardt Kite social network. 
 The Krackhardt Kite social network was introduced by Krackhardt [1]_ in 
 order to study the structure of small social networks. 
 The network is composed of 10 nodes and 12 edges. 
 Examples 
 >>> G = krackhardt_kite_graph() 
 References 
 .. [1] Krackhardt, D. (1980). ""The structure of small-world networks"". 
 Psychological Review 87 (4): 432â447. 
 .. [2] http://en.wikipedia.org/wiki/Krackhardt_Kite_network'","'Return the Krackhardt Kite Social Network. 
 A 10 actor social network introduced by David Krackhardt 
 to illustrate: degree, betweenness, centrality, closeness, etc. 
 The traditional labeling is: 
 Andre=1, Beverley=2, Carol=3, Diane=4, 
 Ed=5, Fernando=6, Garth=7, Heather=8, Ike=9, Jane=10.'"
"def save_load(jid, load, minions=None): 
    serv = _get_serv(ret=None) 
   if ('influxdb08' in serv.__module__): 
      req = [{'name': 'jids', 'columns': ['jid', 'load'], 'points': [[jid, json.dumps(load)]]}] 
   else: 
      req = [{'measurement': 'jids', 'tags': {'jid': jid}, 'fields': {'load': json.dumps(load)}}] 
   try: 
      serv.write_points(req) 
   except Exception as ex: 
      log.critical('Failed   to   store   load   with   InfluxDB   returner:   {0}'.format(ex))", 'Save a load measurement to InfluxDB.','Save the load to the specified jid'
"def is_effective_user(user_id_or_name): 
    euid = os.geteuid() 
   if (str(user_id_or_name) == str(euid)): 
      return True 
   effective_user_name = pwd.getpwuid(euid).pw_name 
   return (user_id_or_name == effective_user_name)"," 'Check if the effective user is the same as the user_id_or_name 
 :param user_id_or_name: The user_id or name to check 
 :return: True if the effective user is the same as the user_id_or_name, 
 False otherwise'",'Returns True if user_id_or_name is effective user (id/name).'
"def _match_abbrev(s, wordmap): 
    if (s in wordmap): 
      return s 
   else: 
      possibilities = [word for word in wordmap.keys() if word.startswith(s)] 
      if (len(possibilities) == 1): 
         return possibilities[0] 
      elif (not possibilities): 
         raise BadOptionError(s) 
      else: 
         possibilities.sort() 
         raise AmbiguousOptionError(s, possibilities)"," 'Match an abbreviation to a word in the wordmap 
 :param s: The abbreviation to match 
 :param wordmap: The wordmap to use 
 :raise BadOptionError: if the abbreviation is not in the wordmap 
 :raise AmbiguousOptionError: if the abbreviation is ambiguous'","'_match_abbrev(s : string, wordmap : {string : Option}) -> string 
 Return the string key in \'wordmap\' for which \'s\' is an unambiguous 
 abbreviation.  If \'s\' is found to be ambiguous or doesn\'t match any of 
 \'words\', raise BadOptionError.'"
"def get_key_func(key_func): 
    if (key_func is not None): 
      if callable(key_func): 
         return key_func 
      else: 
         (key_func_module_path, key_func_name) = key_func.rsplit(u'.', 1) 
         key_func_module = import_module(key_func_module_path) 
         return getattr(key_func_module, key_func_name) 
   return default_key_func"," 'Get a key function for the given key_func. 
 :param key_func: A key function or a string containing the module and 
 function name to be used as a key function. 
 :returns: A key function or None. 
 :rtype: callable or None'","'Function to decide which key function to use. 
 Defaults to ``default_key_func``.'"
"def weight_boundary(graph, src, dst, n): 
    default = {'weight': 0.0, 'count': 0} 
   count_src = graph[src].get(n, default)['count'] 
   count_dst = graph[dst].get(n, default)['count'] 
   weight_src = graph[src].get(n, default)['weight'] 
   weight_dst = graph[dst].get(n, default)['weight'] 
   count = (count_src + count_dst) 
   return {'count': count, 'weight': (((count_src * weight_src) + (count_dst * weight_dst)) / count)}"," 'Returns the weight of a boundary node, based on the weights of 
 its two adjacent nodes.'","'Handle merging of nodes of a region boundary region adjacency graph. 
 This function computes the `""weight""` and the count `""count""` 
 attributes of the edge between `n` and the node formed after 
 merging `src` and `dst`. 
 Parameters 
 graph : RAG 
 The graph under consideration. 
 src, dst : int 
 The vertices in `graph` to be merged. 
 n : int 
 A neighbor of `src` or `dst` or both. 
 Returns 
 data : dict 
 A dictionary with the ""weight"" and ""count"" attributes to be 
 assigned for the merged node.'"
"def xstr(*args): 
    if _use_unicode: 
      return unicode(*args) 
   else: 
      return str(*args)"," 'A wrapper for str that returns unicode if the current locale is 
 using unicode, otherwise returns str.'",'call str or unicode depending on current mode'
"def mark_as_titlepage(container, name, move_to_start=True): 
    ver = container.opf_version_parsed 
   if move_to_start: 
      for (item, q, linear) in container.spine_iter: 
         if (name == q): 
            break 
      if (not linear): 
         item.set(u'linear', u'yes') 
      if (item.getparent().index(item) > 0): 
         container.insert_into_xml(item.getparent(), item, 0) 
   if (ver.major < 3): 
      for ref in container.opf_xpath(u'//opf:guide/opf:reference[@type=""cover""]'): 
         ref.getparent().remove(ref) 
      for guide in get_guides(container): 
         container.insert_into_xml(guide, guide.makeelement(OPF(u'reference'), type=u'cover', href=container.name_to_href(name, container.opf_name))) 
   else: 
      container.apply_unique_properties(name, u'calibre:title-page') 
   container.dirty(container.opf_name)"," 'Mark the title page as the cover page. 
 :param container: the container to mark 
 :param name: the name of the title page 
 :param move_to_start: if True, move the title page to the start of the spine 
 :return: None'","'Mark the specified HTML file as the titlepage of the EPUB. 
 :param move_to_start: If True the HTML file is moved to the start of the spine'"
"def spew(trace_names=None, show_values=False): 
    sys.settrace(Spew(trace_names, show_values))"," 'Spew the trace names and values to the screen. 
 This is a debugging tool that allows you to see what is being traced 
 and what values are being passed to the trace functions. 
 trace_names is a list of strings that specify the trace names to spew. 
 show_values is a boolean that specifies whether to show the values 
 passed to the trace functions. 
 If you are using the debugger, you can use the :py:meth:`pdb.set_trace` 
 function to trace the values being passed to the trace functions. 
 Example:: 
 import sympy as sym 
 from sympy.abc import x, y 
 sym.spew() 
 sym.trace(x, x) 
 sym.trace(y, y) 
 sym.trace(x, x) 
 sym.trace(y, y) 
 sym.trace(x, x) 
 sym.trace(y, y) 
 sym.trace(x, x) 
 sym.trace(y, y) 
 sym.trace(x, x) 
 sym.trace(y, y) 
","'Install a trace hook which writes incredibly detailed logs 
 about what code is being executed to stdout.'"
"def batch_det(a): 
    return BatchDet()(a)"," 'Returns a detector for a batch of images. 
 :param a: A batch of images. 
 :type a: torch.Tensor 
 :return: A detector for the batch of images. 
 :rtype: torch.nn.Module'","'Computes the determinant of a batch of square matrices. 
 Args: 
 a (Variable): Input array to compute the determinant for. 
 The first dimension should iterate over each matrix and be 
 of the batchsize. 
 Returns: 
 ~chainer.Variable: vector of determinants for every matrix 
 in the batch.'"
"def add_csrf(request, **kwargs): 
    d = dict(user=request.user, **kwargs) 
   d.update(csrf(request)) 
   return RequestContext(request, d)", 'Adds the csrf token to the context','Add CSRF to dictionary and wrap in a RequestContext (needed for context processor!).'
"def _get_content_range(start, end, total): 
    start = (start or 0) 
   end = ((end or total) - 1) 
   return ('bytes   %s-%s/%s' % (start, end, total))"," 'Return the content range for the given start and end points. 
 :param start: start offset, inclusive. 
 :param end: end offset, exclusive. 
 :param total: total size of the file. 
 :return: content range string. 
 :rtype: str'","'Returns a suitable Content-Range header: 
 >>> print(_get_content_range(None, 1, 4)) 
 bytes 0-0/4 
 >>> print(_get_content_range(1, 3, 4)) 
 bytes 1-2/4 
 >>> print(_get_content_range(None, None, 4)) 
 bytes 0-3/4'"
"def set_main_css(css_file): 
    assert css_file.endswith('.css') 
   new_css = css_file 
   app_globals.main_css = str(new_css)"," 'Set the main css file to be used for the application. 
 :param css_file: The path to the css file.'",'Sets the main_css.  The css_file must be of the form file.css'
"def isPointOfTableInLoop(loop, pointTable): 
    for point in loop: 
      if (point in pointTable): 
         return True 
   return False"," 'Check if a point is inside the loop. 
 Parameters 
 loop : list of Points 
 A list of Points defining the loop. 
 pointTable : list of Points 
 A list of Points that will be checked against the loop. 
 Returns 
 True if the point is inside the loop, False otherwise.'",'Determine if a point in the point table is in the loop.'
"def _compose_linear_fitting_data(mu, u): 
    for k in range((u['nterms'] - 1)): 
      k1 = (k + 1) 
      mu1n = np.power(mu[0], k1) 
      u['y'][k] = (u['w'][k] * (u['fn'][k1] - (mu1n * u['fn'][0]))) 
      for p in range((u['nfit'] - 1)): 
         u['M'][k][p] = (u['w'][k] * (np.power(mu[(p + 1)], k1) - mu1n))"," 'Composes the linear fitting data. 
 Parameters 
 mu : ndarray 
 The mean of the data. 
 u : dict 
 The linear fitting parameters. 
 Returns 
 u : dict 
 The composed linear fitting parameters.'",'Get the linear fitting data.'
"@slow_test 
 def test_io_evoked(): 
    tempdir = _TempDir() 
   ave = read_evokeds(fname, 0) 
   write_evokeds(op.join(tempdir, 'evoked-ave.fif'), ave) 
   ave2 = read_evokeds(op.join(tempdir, 'evoked-ave.fif'))[0] 
   assert_true(np.allclose(ave.data, ave2.data, atol=1e-16, rtol=0.001)) 
   assert_array_almost_equal(ave.times, ave2.times) 
   assert_equal(ave.nave, ave2.nave) 
   assert_equal(ave._aspect_kind, ave2._aspect_kind) 
   assert_equal(ave.kind, ave2.kind) 
   assert_equal(ave.last, ave2.last) 
   assert_equal(ave.first, ave2.first) 
   assert_true(repr(ave)) 
   ave2 = read_evokeds(fname_gz, 0) 
   assert_true(np.allclose(ave.data, ave2.data, atol=1e-16, rtol=1e-08)) 
   condition = 'Left   Auditory' 
   assert_raises(ValueError, read_evokeds, fname, condition, kind='stderr') 
   assert_raises(ValueError, read_evokeds, fname, condition, kind='standard_error') 
   ave3 = read_evokeds(fname, condition) 
   assert_array_almost_equal(ave.data, ave3.data, 19) 
   aves1 = read_evokeds(fname)[1::2] 
   aves2 = read_evokeds(fname, [1, 3]) 
   aves3 = read_evokeds(fname, ['Right   Auditory', 'Right   visual']) 
   write_evokeds(op.join(tempdir, 'evoked-ave.fif'), aves1) 
   aves4 = read_evokeds(op.join(tempdir, 'evoked-ave.fif')) 
   for aves in [aves2, aves3, aves4]: 
      for [av1, av2] in zip(aves1, aves): 
         assert_array_almost_equal(av1.data, av2.data) 
         assert_array_almost_equal(av1.times, av2.times) 
         assert_equal(av1.nave, av2.nave) 
         assert_equal(av1.kind, av2.kind) 
         assert_equal(av1._aspect_kind, av2._aspect_kind) 
         assert_equal(av1.last, av2.last) 
         assert_equal(av1.first, av2.first) 
         assert_equal(av1.comment, av2.comment) 
   fname2 = op.join(tempdir, 'test-bad-name.fif') 
   with warnings.catch_warnings(record=True) as w: 
      warnings.simplefilter('always') 
      write_evokeds(fname2, ave) 
      read_evokeds(fname2) 
   assert_naming(w, 'test_evoked.py', 2) 
   assert_raises(TypeError, Evoked, fname) 
   fname_ms = op.join(tempdir, 'test-ave.fif') 
   assert_true((ave.info['maxshield'] is False)) 
   ave.info['maxshield'] = True 
   ave.save(fname_ms) 
   assert_raises(ValueError, read_evokeds, fname_ms) 
   with warnings.catch_warnings(record=True) as w: 
      aves = read_evokeds(fname_ms, allow_maxshield=True) 
   assert_true(all((('Elekta' in str(ww.message)) for ww in w))) 
   assert_true(all(((ave.info['maxshield'] is True) for ave in aves))) 
   with warnings.catch_warnings(record=True) as w: 
      aves = read_evokeds(fname_ms, allow_maxshield='yes') 
   assert_equal(len(w), 0) 
   assert_true(all(((ave.info['maxshield'] is True) for ave in aves)))", 'Test the io for evoked data','Test IO for evoked data (fif + gz) with integer and str args.'
"def brightness_temperature(beam_area, disp): 
    beam = beam_area.to(si.sr).value 
   nu = disp.to(si.GHz, spectral()) 
   def convert_Jy_to_K(x_jybm): 
      factor = ((((2 * _si.k_B) * si.K) * (nu ** 2)) / (_si.c ** 2)).to(astrophys.Jy).value 
      return ((x_jybm / beam) / factor) 
   def convert_K_to_Jy(x_K): 
      factor = (astrophys.Jy / (((2 * _si.k_B) * (nu ** 2)) / (_si.c ** 2))).to(si.K).value 
      return ((x_K * beam) / factor) 
   return [(astrophys.Jy, si.K, convert_Jy_to_K, convert_K_to_Jy)]", 'Convert brightness temperature to Jy/K and vice versa',"'Defines the conversion between Jy/beam and ""brightness temperature"", 
 :math:`T_B`, in Kelvins.  The brightness temperature is a unit very 
 commonly used in radio astronomy.  See, e.g., ""Tools of Radio Astronomy"" 
 (Wilson 2009) eqn 8.16 and eqn 8.19 (these pages are available on `google 
 books 
 <http://books.google.com/books?id=9KHw6R8rQEMC&pg=PA179&source=gbs_toc_r&cad=4#v=onepage&q&f=false>`__). 
 :math:`T_B \equiv S_\nu / \left(2 k \nu^2 / c^2 \right)` 
 However, the beam area is essential for this computation: the brightness 
 temperature is inversely proportional to the beam area 
 Parameters 
 beam_area : Beam Area equivalent 
 Beam area in angular units, i.e. steradian equivalent 
 disp : `~astropy.units.Quantity` with spectral units 
 The observed `spectral` equivalent `~astropy.units.Unit` (e.g., 
 frequency or wavelength) 
 Examples 
 Arecibo C-band beam:: 
 >>> import numpy as np 
 >>> from astropy import units as u 
 >>> beam_sigma = 50*u.arcsec 
 >>> beam_area = 2*np.pi*(beam_sigma)**2 
 >>> freq = 5*u.GHz 
 >>> equiv = u.brightness_temperature(beam_area, freq) 
 >>> u.Jy.to(u.K, equivalencies=equiv)  # doctest: +FLOAT_CMP 
 3.526294429423223 
 >>> (1*u.Jy).to(u.K, equivalencies=equiv)  # doctest: +FLOAT_CMP 
 <Quantity 3.526294429423223 K> 
 VLA synthetic beam:: 
 >>> bmaj = 15*u.arcsec 
 >>> bmin = 15*u.arcsec 
 >>> fwhm_to_sigma = 1./(8*np.log(2))**0.5 
 >>> beam_area = 2.*np.pi*(bmaj*bmin*fwhm_to_sigma**2) 
 >>> freq = 5*u.GHz 
 >>> equiv = u.brightness_temperature(beam_area, freq) 
 >>> u.Jy.to(u.K, equivalencies=equiv)  # doctest: +FLOAT_CMP 
 217.2658703625732'"
"def datatype(dbtype, description): 
    dt = connection.introspection.get_field_type(dbtype, description) 
   if (type(dt) is tuple): 
      return dt[0] 
   else: 
      return dt"," 'Returns the Python type that should be used to represent a given 
 database type. 
 If the database type is a string, the Python type should be a string. 
 If the database type is a number, the Python type should be a number. 
 If the database type is a boolean, the Python type should be a boolean. 
 If the database type is a datetime, the Python type should be a datetime. 
 If the database type is a datetime64, the Python type should be a datetime64.'",'Helper to convert a data type into a string.'
"def get_exploration_components_from_dir(dir_path): 
    yaml_content = None 
   assets_list = [] 
   dir_path_array = dir_path.split('/') 
   while (dir_path_array[(-1)] == ''): 
      dir_path_array = dir_path_array[:(-1)] 
   dir_path_length = len(dir_path_array) 
   for (root, dirs, files) in os.walk(dir_path): 
      for directory in dirs: 
         if ((root == dir_path) and (directory != 'assets')): 
            raise Exception(('The   only   directory   in   %s   should   be   assets/' % dir_path)) 
      for filename in files: 
         filepath = os.path.join(root, filename) 
         if (root == dir_path): 
            if filepath.endswith('.DS_Store'): 
               continue 
            if (yaml_content is not None): 
               raise Exception(('More   than   one   non-asset   file   specified   for   %s' % dir_path)) 
            elif (not filepath.endswith('.yaml')): 
               raise Exception(('Found   invalid   non-asset   file   %s.   There   should   only   be   a   single   non-asset   file,   and   it   should   have   a   .yaml   suffix.' % filepath)) 
            else: 
               yaml_content = get_file_contents(filepath) 
         else: 
            filepath_array = filepath.split('/') 
            filename = '/'.join(filepath_array[(dir_path_length + 1):]) 
            assets_list.append((filename, get_file_contents(filepath, raw_bytes=True))) 
   if (yaml_content is None): 
      raise Exception(('No   yaml   file   specifed   for   %s' % dir_path)) 
   return (yaml_content, assets_list)"," 'Returns a tuple (yaml_content, assets_list) where yaml_content is a 
 string containing the yaml representation of the exploration and 
 assets_list is a list of tuples (filename, bytes) where filename is 
 the filename of the asset and bytes is the contents of the asset.'","'Gets the (yaml, assets) from the contents of an exploration data dir. 
 Args: 
 dir_path: a full path to the exploration root directory. 
 Returns: 
 a 2-tuple, the first element of which is a yaml string, and the second 
 element of which is a list of (filepath, content) 2-tuples. The filepath 
 does not include the assets/ prefix. 
 Raises: 
 Exception: if the following condition doesn\'t hold: ""There is exactly one 
 file not in assets/, and this file has a .yaml suffix"".'"
"def _fit_dipoles(fun, min_dist_to_inner_skull, data, times, guess_rrs, guess_data, fwd_data, whitener, proj_op, ori, n_jobs): 
    from scipy.optimize import fmin_cobyla 
   (parallel, p_fun, _) = parallel_func(fun, n_jobs) 
   res = parallel((p_fun(min_dist_to_inner_skull, B, t, guess_rrs, guess_data, fwd_data, whitener, proj_op, fmin_cobyla, ori) for (B, t) in zip(data.T, times))) 
   pos = np.array([r[0] for r in res]) 
   amp = np.array([r[1] for r in res]) 
   ori = np.array([r[2] for r in res]) 
   gof = (np.array([r[3] for r in res]) * 100) 
   residual = np.array([r[4] for r in res]).T 
   return (pos, amp, ori, gof, residual)"," 'Fit the dipoles to the data. 
 Parameters 
 fun : function 
 Function to be minimized. 
 min_dist_to_inner_skull : float 
 Minimum distance to the inner skull. 
 data : array 
 Data. 
 times : array 
 Times. 
 guess_rrs : array 
 Guessed rrs. 
 guess_data : array 
 Guessed data. 
 fwd_data : array 
 Forward model. 
 whitener : array 
 Whitener. 
 proj_op : array 
 Projection operator. 
 ori : array 
 Orientation of the dipoles. 
 n_jobs : int 
 Number of jobs. 
 Returns 
 pos : array 
 Positions of the dipoles. 
 amp : array 
 Amplitudes of the dipoles. 
 ori : array 
 Orientation of the dipoles. 
 gof : array 
 Goodness of fit. 
 residual : array 
 Residual. 
 Examples 
 >>> from nipype.","'Fit a single dipole to the given whitened, projected data.'"
"def copy_snapshot(kwargs=None, call=None): 
    if (call != 'function'): 
      log.error('The   copy_snapshot   function   must   be   called   with   -f   or   --function.') 
      return False 
   if ('source_region' not in kwargs): 
      log.error('A   source_region   must   be   specified   to   copy   a   snapshot.') 
      return False 
   if ('source_snapshot_id' not in kwargs): 
      log.error('A   source_snapshot_id   must   be   specified   to   copy   a   snapshot.') 
      return False 
   if ('description' not in kwargs): 
      kwargs['description'] = '' 
   params = {'Action': 'CopySnapshot'} 
   if ('source_region' in kwargs): 
      params['SourceRegion'] = kwargs['source_region'] 
   if ('source_snapshot_id' in kwargs): 
      params['SourceSnapshotId'] = kwargs['source_snapshot_id'] 
   if ('description' in kwargs): 
      params['Description'] = kwargs['description'] 
   log.debug(params) 
   data = aws.query(params, return_url=True, location=get_location(), provider=get_provider(), opts=__opts__, sigver='4') 
   return data"," 'Copy a snapshot to a different region. 
 This will copy the snapshot to the specified region. 
 The ``source_region`` parameter is required and must be the 
 region where the snapshot was created. 
 The ``source_snapshot_id`` parameter is required and must be 
 the ID of the snapshot to copy. 
 The ``description`` parameter is optional and will be used 
 as the description of the copied snapshot.'",'Copy a snapshot'
"def scenario_tests_need_service_tags(physical_line, filename, previous_logical): 
    if (('tempest/scenario/' in filename) and ('/test_' in filename)): 
      if TEST_DEFINITION.match(physical_line): 
         if (not SCENARIO_DECORATOR.match(previous_logical)): 
            return (physical_line.find('def'), 'T104:   Scenario   tests   require   a   service   decorator')"," 'Check if the scenario test file has a service decorator. 
 If it does not have one, it will raise an exception.'","'Check that scenario tests have service tags 
 T104: Scenario tests require a services decorator'"
"def isclass(object): 
    return isinstance(object, (type, types.ClassType))"," 'Checks if object is a class. 
 :param object: 
 :return: 
 :rtype: 
 :raises: 
 :example: 
 >>> isclass(int) 
 True 
 >>> isclass(str) 
 True 
 >>> isclass(dict) 
 True 
 >>> isclass(tuple) 
 True 
 >>> isclass(list) 
 True 
 >>> isclass(object) 
 True 
 >>> isclass(1) 
 False'","'Return true if the object is a class. 
 Class objects provide these attributes: 
 __doc__         documentation string 
 __module__      name of module in which this class was defined'"
"@receiver(post_save, sender=UserLog) 
 def cull_records(sender, **kwargs): 
    if (settings.USER_LOG_MAX_RECORDS_PER_USER and kwargs['created']): 
      current_models = UserLog.objects.filter(user=kwargs['instance'].user, activity_type=kwargs['instance'].activity_type) 
      if (current_models.count() > settings.USER_LOG_MAX_RECORDS_PER_USER): 
         to_discard = current_models.order_by('start_datetime')[0:(current_models.count() - settings.USER_LOG_MAX_RECORDS_PER_USER)] 
         UserLog.objects.filter(pk__in=to_discard).delete()", 'Cull the UserLogs that are over the max number of records per user.','Listen in to see when videos become available.'
"def JarContents(jar_path): 
    with zipfile.ZipFile(jar_path) as jar: 
      for name in jar.namelist(): 
         (yield (name, jar.read(name)))"," 'Returns a generator of (name, content) tuples for the contents of the 
 given jar file. 
 :param jar_path: path to the jar file 
 :returns: generator of (name, content) tuples'","'Generates (name, contents) pairs for the given jar. 
 Each generated tuple consists of the relative name within the jar of an entry, 
 for example \'java/lang/Object.class\', and a str that is the corresponding 
 contents. 
 Args: 
 jar_path: a str that is the path to the jar. 
 Yields: 
 A (name, contents) pair.'"
"def color(columns=None, palette=None, bin=False, **kwargs): 
    if (palette is not None): 
      kwargs['palette'] = palette 
   kwargs['columns'] = columns 
   kwargs['bin'] = bin 
   return ColorAttr(**kwargs)"," 'Create a color attribute. 
 Parameters 
 columns : list 
 A list of column names to use for coloring. 
 palette : dict 
 A dictionary of color names to RGB values. 
 bin : bool 
 If True, use a 256 color palette. 
 Returns 
 A :class:`~pandas.core.groupby.GroupBy` object.'","'Produces a ColorAttr specification for coloring groups of data based on columns. 
 Args: 
 columns (str or list(str), optional): a column or list of columns for coloring 
 palette (list(str), optional): a list of colors to use for assigning to unique 
 values in `columns`. 
 **kwargs: any keyword, arg supported by :class:`AttrSpec` 
 Returns: 
 a `ColorAttr` object'"
"def import_from_cwd(module, imp=None, package=None): 
    if (imp is None): 
      imp = importlib.import_module 
   with cwd_in_path(): 
      return imp(module, package=package)"," 'Import a module from the current working directory. 
 :param module: the name of the module to import. 
 :param imp: a function to import the module, or None. 
 :param package: a package to import the module into, or None. 
 :returns: the imported module.'","'Import module, temporarily including modules in the current directory. 
 Modules located in the current directory has 
 precedence over modules located in `sys.path`.'"
"@staff_member_required 
 def dashboard(request, template_name=u'admin/dashboard.html'): 
    profile = Profile.objects.get_or_create(user=request.user)[0] 
   if (profile.extra_data is None): 
      profile.extra_data = {} 
      profile.save(update_fields=(u'extra_data',)) 
   profile_data = profile.extra_data 
   selected_primary_widgets = [] 
   unselected_primary_widgets = [] 
   primary_widget_selections = profile_data.get(u'primary_widget_selections') 
   if primary_widget_selections: 
      for p in primary_widgets: 
         if (primary_widget_selections[p.widget_id] == u'1'): 
            selected_primary_widgets.append(p) 
         else: 
            unselected_primary_widgets.append(p) 
   else: 
      selected_primary_widgets = primary_widgets 
      unselected_primary_widgets = None 
   selected_secondary_widgets = [] 
   unselected_secondary_widgets = [] 
   secondary_widget_selections = profile_data.get(u'secondary_widget_selections') 
   if secondary_widget_selections: 
      for s in secondary_widgets: 
         if (secondary_widget_selections[s.widget_id] == u'1'): 
            selected_secondary_widgets.append(s) 
         else: 
            unselected_secondary_widgets.append(s) 
   else: 
      selected_secondary_widgets = secondary_widgets 
      unselected_secondary_widgets = None 
   primary_widget_positions = profile_data.get(u'primary_widget_positions') 
   if primary_widget_positions: 
      sorted_primary_widgets = sorted(selected_primary_widgets, key=(lambda y: (primary_widget_positions[y.widget_id] or len(primary_widget_positions)))) 
   else: 
      sorted_primary_widgets = selected_primary_widgets 
   secondary_widget_positions = profile_data.get(u'secondary_widget_positions') 
   if secondary_widget_positions: 
      sorted_secondary_widgets = sorted(selected_secondary_widgets, key=(lambda y: (secondary_widget_positions[y.widget_id] or len(secondary_widget_positions)))) 
   else: 
      sorted_secondary_widgets = selected_secondary_widgets 
   return render_to_response(template_name, RequestContext(request, {u'primary_widgets': primary_widgets, u'root_path': (settings.SITE_ROOT + u'admin/db/'), u'secondary_widgets': secondary_widgets, u'selected_primary_widgets': sorted_primary_widgets, u'selected_secondary_widgets': sorted_secondary_widgets, u'support_data': serialize_support_data(request, True), u'title': _(u'Admin   Dashboard'), u'unselected_primary_widgets': unselected_primary_widgets, u'unselected_secondary_widgets': unselected_secondary_widgets}))"," 'Render the dashboard page. 
 :param request: The current request. 
 :param template_name: The name of the template to render.'","'Display the administration dashboard. 
 This is the entry point to the admin site, containing news updates and 
 useful administration tasks.'"
"def dump_all(documents, stream=None, Dumper=Dumper, default_style=None, default_flow_style=None, canonical=None, indent=None, width=None, allow_unicode=None, line_break=None, encoding='utf-8', explicit_start=None, explicit_end=None, version=None, tags=None): 
    getvalue = None 
   if (stream is None): 
      if (encoding is None): 
         from StringIO import StringIO 
      else: 
         from cStringIO import StringIO 
      stream = StringIO() 
      getvalue = stream.getvalue 
   dumper = Dumper(stream, default_style=default_style, default_flow_style=default_flow_style, canonical=canonical, indent=indent, width=width, allow_unicode=allow_unicode, line_break=line_break, encoding=encoding, version=version, tags=tags, explicit_start=explicit_start, explicit_end=explicit_end) 
   dumper.open() 
   for data in documents: 
      dumper.represent(data) 
   dumper.close() 
   if getvalue: 
      return getvalue()"," 'Dump all documents in the given list to a stream. 
 :param documents: A list of documents to dump. 
 :param stream: A stream to dump to. If None, a StringIO object is created. 
 :param Dumper: A subclass of :class:`~.Dumper` to use for dumping. 
 :param default_style: A :class:`~.Style` object to use for dumping. 
 :param default_flow_style: A :class:`~.FlowStyle` object to use for dumping. 
 :param canonical: A boolean indicating whether to use the canonical form of 
 :class:`~.Doc` objects. 
 :param indent: A number of spaces to indent the output. 
 :param width: A number of spaces to pad the output. 
 :param allow_unicode: A boolean indicating whether to allow Unicode characters 
 in the output. 
 :param line_break: A string indicating how to break lines. 
 :param encoding: The encoding to use for the output. 
 :param explicit_start: A string indicating how to start the output. 
 :param explicit_end","'Serialize a sequence of Python objects into a YAML stream. 
 If stream is None, return the produced string instead.'"
"def _compare_by_version(path1, path2): 
    if (path1.source == path2.source): 
      if (path1.source_version_num > path2.source_version_num): 
         return path1 
      else: 
         return path2 
   return None"," 'Compares two paths by version number. 
 :param path1: first path 
 :param path2: second path 
 :return: the path with the higher version number 
 :rtype: Path'","'Returns the current/latest learned path. 
 Checks if given paths are from same source/peer and then compares their 
 version number to determine which path is received later. If paths are from 
 different source/peer return None.'"
"def graphviz_layout(G, prog='neato', root=None, args=''): 
    return pygraphviz_layout(G, prog=prog, root=root, args=args)"," 'Layout a graph using Graphviz. 
 Parameters 
 G : graph 
 The graph to layout. 
 prog : str 
 The Graphviz layout program to use. 
 root : str 
 The name of the root node. 
 args : str 
 Additional arguments to pass to Graphviz. 
 Returns 
 The Graphviz layout string. 
 Examples 
 >>> from networkx.drawing.nx_agraph import graphviz_layout 
 >>> G = nx.Graph() 
 >>> G.add_edge(0, 1) 
 >>> G.add_edge(1, 2) 
 >>> G.add_edge(2, 3) 
 >>> G.add_edge(3, 4) 
 >>> G.add_edge(4, 5) 
 >>> G.add_edge(5, 6) 
 >>> G.add_edge(6, 7) 
 >>> G.add_edge(7, 8) 
 >>> G.add_edge(8, 9) 
 >>> G.add_edge(9, ","'Create node positions for G using Graphviz. 
 Parameters 
 G : NetworkX graph 
 A graph created with NetworkX 
 prog : string 
 Name of Graphviz layout program 
 root : string, optional 
 Root node for twopi layout 
 args : string, optional 
 Extra arguments to Graphviz layout program 
 Returns : dictionary 
 Dictionary of x,y, positions keyed by node. 
 Examples 
 >>> G = nx.petersen_graph() 
 >>> pos = nx.nx_agraph.graphviz_layout(G) 
 >>> pos = nx.nx_agraph.graphviz_layout(G, prog=\'dot\') 
 Notes 
 This is a wrapper for pygraphviz_layout.'"
"def get_level_tags(): 
    level_tags = constants.DEFAULT_TAGS.copy() 
   level_tags.update(getattr(settings, 'MESSAGE_TAGS', {})) 
   return level_tags", 'Returns a dictionary of all the tags used for levels.','Returns the message level tags.'
"def list_rows(dataset_name, table_name, project=None): 
    bigquery_client = bigquery.Client(project=project) 
   dataset = bigquery_client.dataset(dataset_name) 
   table = dataset.table(table_name) 
   if (not table.exists()): 
      print 'Table   {}:{}   does   not   exist.'.format(dataset_name, table_name) 
      return 
   table.reload() 
   rows = list(table.fetch_data(max_results=25)) 
   format_string = ('{!s:<16}   ' * len(table.schema)) 
   field_names = [field.name for field in table.schema] 
   print format_string.format(*field_names) 
   for row in rows: 
      print format_string.format(*row)", 'Lists the rows of a table.',"'Prints rows in the given table. 
 Will print 25 rows at most for brevity as tables can contain large amounts 
 of rows. 
 If no project is specified, then the currently active project is used.'"
"@environmentfilter 
 def do_first(environment, seq): 
    try: 
      return next(iter(seq)) 
   except StopIteration: 
      return environment.undefined('No   first   item,   sequence   was   empty.')"," 'Return the first item of a sequence. 
 :param seq: A sequence of values to search through. 
 :return: The first item of the sequence, or ``undefined`` if the sequence 
 is empty. 
 :rtype: :py:class:`~sympy.core.symbols.Symbol` or :py:class:`~sympy.core.symbols.Undefined`'",'Return the first item of a sequence.'
"def main(): 
    if (len(sys.argv) > 1): 
      writeOutput('   '.join(sys.argv[1:])) 
   else: 
      settings.startMainLoopFromConstructor(getNewRepository())", 'Start the main loop.','Display the feed dialog.'
"@asyncio.coroutine 
 @hook.command 
 def flirt(text, conn, nick, message): 
    target = text.strip() 
   if (not is_valid(target)): 
      return ""I   can't   attack   that."" 
   if is_self(conn, target): 
      target = nick 
   message('{},   {}'.format(target, random.choice(flirts)))"," 'Flirt with a target. 
 Usage: 
 !flirt <target> 
 Flirts: 
 * ""I'm   the   best   at   what   I   do"" 
 * ""I'm   better   than   you"" 
 * ""I'm   not   like   other   people"" 
 * ""I'm   the   best"" 
 * ""I'm   the   best   in   the   world"" 
 * ""I'm   the   best   in   the   room"" 
 * ""I'm   the   best   in   the   world"" 
 * ""I'm   the   best   in   the   room"" 
 * ""I'm   the   best   in   the   world"" 
 * ""I'm   the   best   in   the   room"" 
 * ""I'm   the   best   in   the   world"" 
 * ""I'm   the   best   in   the   room"" 
 * ""I'm   the   best   in   the   world"" 
 * ""I'",'<user> - flirts with <user>'
"def urlsplit(url, scheme='', allow_fragments=True): 
    allow_fragments = bool(allow_fragments) 
   key = (url, scheme, allow_fragments, type(url), type(scheme)) 
   cached = _parse_cache.get(key, None) 
   if cached: 
      return cached 
   if (len(_parse_cache) >= MAX_CACHE_SIZE): 
      clear_cache() 
   netloc = query = fragment = '' 
   i = url.find(':') 
   if (i > 0): 
      if (url[:i] == 'http'): 
         scheme = url[:i].lower() 
         url = url[(i + 1):] 
         if (url[:2] == '//'): 
            (netloc, url) = _splitnetloc(url, 2) 
            if ((('[' in netloc) and (']' not in netloc)) or ((']' in netloc) and ('[' not in netloc))): 
               raise ValueError('Invalid   IPv6   URL') 
         if (allow_fragments and ('#' in url)): 
            (url, fragment) = url.split('#', 1) 
         if ('?' in url): 
            (url, query) = url.split('?', 1) 
         v = SplitResult(scheme, netloc, url, query, fragment) 
         _parse_cache[key] = v 
         return v 
      for c in url[:i]: 
         if (c not in scheme_chars): 
            break 
      else: 
         try: 
            _testportnum = int(url[(i + 1):]) 
         except ValueError: 
            (scheme, url) = (url[:i].lower(), url[(i + 1):]) 
   if (url[:2] == '//'): 
      (netloc, url) = _splitnetloc(url, 2) 
      if ((('[' in netloc) and (']' not in netloc)) or ((']' in netloc) and ('[' not in netloc))): 
         raise ValueError('Invalid   IPv6   URL') 
   if (allow_fragments and (scheme in uses_fragment) and ('#' in url)): 
      (url, fragment) = url.split('#', 1) 
   if ((scheme in uses_query) and ('?' in url)): 
      (url, query) = url.split('?', 1) 
   v = SplitResult(scheme, netloc, url, query, fragment) 
   _parse_cache[key] = v 
   return v"," 'Parse a URL into its components. 
 :param url: A URL to parse 
 :param scheme: The scheme of the URL 
 :param allow_fragments: Whether to allow fragment identifiers 
 :return: A SplitResult object containing the parsed components 
 :rtype: SplitResult'","'Parse a URL into 5 components: 
 <scheme>://<netloc>/<path>?<query>#<fragment> 
 Return a 5-tuple: (scheme, netloc, path, query, fragment). 
 Note that we don\'t break the components up in smaller bits 
 (e.g. netloc is a single string) and we don\'t expand % escapes.'"
"def bygroups(*args): 
    def callback(lexer, match, ctx=None): 
      for (i, action) in enumerate(args): 
         if (action is None): 
            continue 
         elif (type(action) is _TokenType): 
            data = match.group((i + 1)) 
            if data: 
               (yield (match.start((i + 1)), action, data)) 
         else: 
            if ctx: 
               ctx.pos = match.start((i + 1)) 
            for item in action(lexer, _PseudoMatch(match.start((i + 1)), match.group((i + 1))), ctx): 
               if item: 
                  (yield item) 
      if ctx: 
         ctx.pos = match.end() 
   return callback"," 'A decorator that takes a list of callbacks and applies them to the 
 :meth:`~.Token.bygroups` method of the lexer. 
 Each callback should take a :class:`~.Lexer` instance, a :class:`~.Match` 
 object and an optional :class:`~.Context` instance. 
 :param args: a list of callbacks 
 :type args: list of callable'",'Callback that yields multiple actions for each group in the match.'
"def describe_policy(policyName, region=None, key=None, keyid=None, profile=None): 
    try: 
      conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) 
      policy = conn.get_policy(policyName=policyName) 
      if policy: 
         keys = ('policyName', 'policyArn', 'policyDocument', 'defaultVersionId') 
         return {'policy': dict([(k, policy.get(k)) for k in keys])} 
      else: 
         return {'policy': None} 
   except ClientError as e: 
      err = salt.utils.boto3.get_error(e) 
      if (e.response.get('Error', {}).get('Code') == 'ResourceNotFoundException'): 
         return {'policy': None} 
      return {'error': salt.utils.boto3.get_error(e)}"," 'Returns a dictionary describing the policy. 
 .. versionadded:: 2015.7.0'","'Given a policy name describe its properties. 
 Returns a dictionary of interesting properties. 
 CLI Example: 
 .. code-block:: bash 
 salt myminion boto_iot.describe_policy mypolicy'"
"def _gen_gce_as_policy(as_params): 
    asp_data = {} 
   asp_data['maxNumReplicas'] = as_params['max_instances'] 
   if ('min_instances' in as_params): 
      asp_data['minNumReplicas'] = as_params['min_instances'] 
   if ('cool_down_period' in as_params): 
      asp_data['coolDownPeriodSec'] = as_params['cool_down_period'] 
   if (('cpu_utilization' in as_params) and ('target' in as_params['cpu_utilization'])): 
      asp_data['cpuUtilization'] = {'utilizationTarget': as_params['cpu_utilization']['target']} 
   if (('load_balancing_utilization' in as_params) and ('target' in as_params['load_balancing_utilization'])): 
      asp_data['loadBalancingUtilization'] = {'utilizationTarget': as_params['load_balancing_utilization']['target']} 
   return asp_data"," 'Generate the policy as a dict. 
 :param as_params: the as_params dict. 
 :returns: the policy as a dict.'","'Take Autoscaler params and generate GCE-compatible policy. 
 :param as_params: Dictionary in Ansible-playbook format 
 containing policy arguments. 
 :type as_params: ``dict`` 
 :return: GCE-compatible policy dictionary 
 :rtype: ``dict``'"
"def parse_atom(tokens, options): 
    token = tokens.current() 
   result = [] 
   if (token in '(['): 
      tokens.move() 
      (matching, pattern) = {'(': [')', Required], '[': [']', Optional]}[token] 
      result = pattern(*parse_expr(tokens, options)) 
      if (tokens.move() != matching): 
         raise tokens.error((""unmatched   '%s'"" % token)) 
      return [result] 
   elif (token == 'options'): 
      tokens.move() 
      return [OptionsShortcut()] 
   elif (token.startswith('--') and (token != '--')): 
      return parse_long(tokens, options) 
   elif (token.startswith('-') and (token not in ('-', '--'))): 
      return parse_shorts(tokens, options) 
   elif ((token.startswith('<') and token.endswith('>')) or token.isupper()): 
      return [Argument(tokens.move())] 
   else: 
      return [Command(tokens.move())]"," 'Parse an atom, and return a list of :class:`Argument` or :class:`Command` 
 objects. 
 :param tokens: The tokenizer. 
 :param options: The current set of options. 
 :return: A list of :class:`Argument` or :class:`Command` objects. 
 :rtype: list'","'atom ::= \'(\' expr \')\' | \'[\' expr \']\' | \'options\' 
 | long | shorts | argument | command ;'"
"def __test_html(): 
    with open('test.rpt', 'r') as input_file: 
      data_text = input_file.read() 
   data = yaml.safe_load(data_text) 
   string_file = StringIO.StringIO() 
   _generate_html(data, string_file) 
   string_file.seek(0) 
   result = string_file.read() 
   with open('test.html', 'w') as output: 
      output.write(result)", 'Test the HTML generation',"'HTML generation test only used when called from the command line: 
 python ./highstate.py 
 Typical options for generating the report file: 
 highstate: 
 report_format: yaml 
 report_delivery: file 
 file_output: \'/srv/salt/_returners/test.rpt\''"
"def str2bin(value, classic_mode=True): 
    text = '' 
   for character in value: 
      if (text != ''): 
         text += '   ' 
      byte = ord(character) 
      text += byte2bin(byte, classic_mode) 
   return text"," 'Converts a string to binary, with spaces between bytes. 
 If classic_mode is True, the bytes are converted to 7-bit ASCII. 
 Otherwise, the bytes are converted to 8-bit binary. 
 >>> str2bin(""12345678"") 
 \'00000012\', \'00000134\', \'00000256\', \'00000378\''","'Convert binary string to binary numbers. 
 If classic_mode  is true (default value), reverse bits. 
 >>> str2bin(""\x03\xFF"") 
 \'00000011 11111111\' 
 >>> str2bin(""\x03\xFF"", False) 
 \'11000000 11111111\''"
"def from_files(job, form): 
    if form.textfile_use_local_files.data: 
      job.labels_file = form.textfile_local_labels_file.data.strip() 
   else: 
      flask.request.files[form.textfile_labels_file.name].save(os.path.join(job.dir(), utils.constants.LABELS_FILE)) 
      job.labels_file = utils.constants.LABELS_FILE 
   shuffle = bool(form.textfile_shuffle.data) 
   backend = form.backend.data 
   encoding = form.encoding.data 
   compression = form.compression.data 
   if form.textfile_use_local_files.data: 
      train_file = form.textfile_local_train_images.data.strip() 
   else: 
      flask.request.files[form.textfile_train_images.name].save(os.path.join(job.dir(), utils.constants.TRAIN_FILE)) 
      train_file = utils.constants.TRAIN_FILE 
   image_folder = form.textfile_train_folder.data.strip() 
   if (not image_folder): 
      image_folder = None 
   job.tasks.append(tasks.CreateDbTask(job_dir=job.dir(), input_file=train_file, db_name=utils.constants.TRAIN_DB, backend=backend, image_dims=job.image_dims, image_folder=image_folder, resize_mode=job.resize_mode, encoding=encoding, compression=compression, mean_file=utils.constants.MEAN_FILE_CAFFE, labels_file=job.labels_file, shuffle=shuffle)) 
   if form.textfile_use_val.data: 
      if form.textfile_use_local_files.data: 
         val_file = form.textfile_local_val_images.data.strip() 
      else: 
         flask.request.files[form.textfile_val_images.name].save(os.path.join(job.dir(), utils.constants.VAL_FILE)) 
         val_file = utils.constants.VAL_FILE 
      image_folder = form.textfile_val_folder.data.strip() 
      if (not image_folder): 
         image_folder = None 
      job.tasks.append(tasks.CreateDbTask(job_dir=job.dir(), input_file=val_file, db_name=utils.constants.VAL_DB, backend=backend, image_dims=job.image_dims, image_folder=image_folder, resize_mode=job.resize_mode, encoding=encoding, compression=compression, labels_file=job.labels_file, shuffle=shuffle)) 
   if form.textfile_use_test.data: 
      if form.textfile_use_local_files.data: 
         test_file = form.textfile_local_test_images.data.strip() 
      else: 
         flask.request.files[form.textfile_test_images.name].save(os.path.join(job.dir(), utils.constants.TEST_FILE)) 
         test_file = utils.constants.TEST_FILE 
      image_folder = form.textfile_test_folder.data.strip() 
      if (not image_folder): 
         image_folder = None 
      job.tasks.append(tasks.CreateDbTask(job_dir=job.dir(), input_file=test_file, db_name=utils.constants.TEST_DB, backend=backend, image_dims=job.image_dims, image_folder=image_folder, resize_mode=job.resize_mode, encoding=encoding, compression=compression, labels_file=job.labels_file, shuffle=shuffle))", 'Create the db tasks from the form values.','Add tasks for creating a dataset by reading textfiles'
"def debug(msg, *args, **kwargs): 
    if (len(root.handlers) == 0): 
      basicConfig() 
   root.debug(*((msg,) + args), **kwargs)", 'Print a debug message to the console.','Log a message with severity \'DEBUG\' on the root logger.'
"def greater_than_zero(): 
    return st.floats(min_value=0.0, allow_infinity=False).filter((lambda x: (x > 0.0)))", 'Asks the user to provide a positive number.','A strategy that yields floats greater than zero.'
"def _int64_feature_list(values): 
    return tf.train.FeatureList(feature=[_int64_feature(v) for v in values])"," 'Create a TF feature list from a list of int64 values. 
 This function is a wrapper around tf.train.FeatureList, which is used to 
 serialize the values into a TensorFlow feature list. 
 Args: 
 values: A list of int64 values. 
 Returns: 
 A TensorFlow feature list.'",'Wrapper for inserting an int64 FeatureList into a SequenceExample proto.'
"def python_implementation(): 
    return _sys_version()[0]"," 'Returns the Python version string. 
 This function is a wrapper around the built-in ``sys.version``. 
 :return: The Python version string. 
 :rtype: str'","'Returns a string identifying the Python implementation. 
 Currently, the following implementations are identified: 
 \'CPython\' (C implementation of Python), 
 \'IronPython\' (.NET implementation of Python), 
 \'Jython\' (Java implementation of Python), 
 \'PyPy\' (Python implementation of Python).'"
"def construct_sort_part(model_cls, part): 
    assert part, 'part   must   be   a   field   name   and   +   or   -' 
   field = part[:(-1)] 
   assert field, 'field   is   missing' 
   direction = part[(-1)] 
   assert (direction in ('+', '-')), 'part   must   end   with   +   or   -' 
   is_ascending = (direction == '+') 
   case_insensitive = beets.config['sort_case_insensitive'].get(bool) 
   if (field in model_cls._sorts): 
      sort = model_cls._sorts[field](model_cls, is_ascending, case_insensitive) 
   elif (field in model_cls._fields): 
      sort = query.FixedFieldSort(field, is_ascending, case_insensitive) 
   else: 
      sort = query.SlowFieldSort(field, is_ascending, case_insensitive) 
   return sort"," 'Construct a sort part for a given model and field name. 
 :param model_cls: The model class. 
 :param part: A field name and + or -. 
 :return: A Sort object.'","'Create a `Sort` from a single string criterion. 
 `model_cls` is the `Model` being queried. `part` is a single string 
 ending in ``+`` or ``-`` indicating the sort.'"
"def add_resource(zone, resource_type, **kwargs): 
    return _resource('add', zone, resource_type, None, **kwargs)"," 'Add a resource to a zone. 
 :param zone: the zone to add the resource to 
 :param resource_type: the resource type to add 
 :param kwargs: the kwargs to pass to the add method 
 :returns: the added resource 
 :rtype: `cloudify.resources.base.Resource`'","'Add a resource 
 zone : string 
 name of zone 
 resource_type : string 
 type of resource 
 **kwargs : string|int|... 
 resource properties 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' zonecfg.add_resource tallgeese rctl name=zone.max-locked-memory value=\'(priv=privileged,limit=33554432,action=deny)\''"
"def bits_str(s, endian='big', zero='0', one='1'): 
    return ''.join(bits(s, endian, zero, one))"," 'Convert a binary string to a string of bits. 
 This is the inverse of the str() method. 
 Parameters 
 s : str 
 The binary string to convert. 
 endian : str, optional 
 The endianness of the string. 
 zero : str, optional 
 The string for zero. 
 one : str, optional 
 The string for one. 
 Returns 
 str 
 The string of bits. 
 Examples 
 >>> bits_str(0b10101010) 
 \'10101010\' 
 >>> bits_str(0b10101010, endian=\'big\') 
 \'10101010\' 
 >>> bits_str(0b10101010, endian=\'big\', zero=\'0\') 
 \'01010101\' 
 >>> bits_str(0b10101010, endian=\'big\', zero=\'0\', one=\'1\') 
 \","'bits_str(s, endian = \'big\', zero = \'0\', one = \'1\') -> str 
 A wrapper around :func:`bits`, which converts the output into a string. 
 Examples: 
 >>> bits_str(511) 
 \'0000000111111111\' 
 >>> bits_str(""bits_str"", endian = ""little"") 
 \'0100011010010110001011101100111011111010110011100010111001001110\''"
"def CreateGRRTempFileVFS(directory=None, filename=None, lifetime=0, mode='w+b', suffix=''): 
    fd = CreateGRRTempFile(directory=directory, filename=filename, lifetime=lifetime, mode=mode, suffix=suffix) 
   pathspec = rdf_paths.PathSpec(path=fd.name, pathtype=rdf_paths.PathSpec.PathType.TMPFILE) 
   return (fd, pathspec)"," 'Creates a temporary file in the VFS. 
 Parameters 
 directory : str, optional 
 The directory to create the file in. 
 filename : str, optional 
 The name of the file to create. 
 lifetime : int, optional 
 The number of seconds the file should be kept alive. 
 mode : str, optional 
 The mode to open the file with. 
 suffix : str, optional 
 The suffix to append to the file name. 
 Returns 
 (fd, pathspec) 
 A tuple of the file descriptor and the path specification for the file. 
 Raises 
 ValueError 
 If the file cannot be created. 
 Examples 
 >>> fd, pathspec = CreateGRRTempFileVFS() 
 >>> fd, pathspec = CreateGRRTempFileVFS(directory=""tmp"") 
 >>> fd, pathspec = CreateGRRTempFileVFS(directory=""tmp"", filename=""foo"") 
 >>> fd, pathspec = CreateGRRTempFileVFS(directory=""tmp"", filename=""foo"", lifetime=10) 
 >>> fd, pathspec =","'Creates a GRR VFS temp file. 
 This function is analogous to CreateGRRTempFile but returns an open VFS handle 
 to the newly created file. Arguments are the same as for CreateGRRTempFile: 
 Args: 
 directory: string representing absolute directory where file should be 
 written. If None, use \'tmp\' under the directory we\'re running 
 from. 
 filename: The name of the file to use. Note that setting both filename and 
 directory name is not allowed. 
 lifetime: time in seconds before we should delete this tempfile. 
 mode: The mode to open the file. 
 suffix: optional suffix to use for the temp file 
 Returns: 
 An open file handle to the new file and the corresponding pathspec.'"
"def pylab_setup(): 
    if backend.startswith('module://'): 
      backend_name = backend[9:] 
   else: 
      backend_name = ('backend_' + backend) 
      backend_name = backend_name.lower() 
      backend_name = ('matplotlib.backends.%s' % backend_name.lower()) 
   backend_mod = __import__(backend_name, globals(), locals(), [backend_name]) 
   new_figure_manager = backend_mod.new_figure_manager 
   def do_nothing_show(*args, **kwargs): 
      frame = inspect.currentframe() 
      fname = frame.f_back.f_code.co_filename 
      if (fname in ('<stdin>', '<ipython   console>')): 
         warnings.warn((""\nYour   currently   selected   backend,   '%s'   does   not   support   show().\nPlease   select   a   GUI   backend   in   your   matplotlibrc   file   ('%s')\nor   with   matplotlib.use()"" % (backend, matplotlib.matplotlib_fname()))) 
   def do_nothing(*args, **kwargs): 
      pass 
   backend_version = getattr(backend_mod, 'backend_version', 'unknown') 
   show = getattr(backend_mod, 'show', do_nothing_show) 
   draw_if_interactive = getattr(backend_mod, 'draw_if_interactive', do_nothing) 
   if (backend.lower() in ['wx', 'wxagg']): 
      Toolbar = backend_mod.Toolbar 
      __all__.append('Toolbar') 
   matplotlib.verbose.report(('backend   %s   version   %s' % (backend, backend_version))) 
   return (new_figure_manager, draw_if_interactive, show)"," 'Returns a new figure manager and draw_if_interactive and show 
 for the given backend. 
 This is used to set up matplotlib\'s figure manager and 
 show()/draw_if_interactive() methods for a given backend. 
 The backend must be a string, and is assumed to be the 
 name of a module that exports a new_figure_manager() method. 
 If the backend is not a string, it will be assumed to be a 
 string that can be used as a backend name in matplotlib.matplotlibrc. 
 If the backend is not a string, an error will be raised. 
 The backend can be a string that is the name of a module, 
 or a string that is the name of a backend. 
 If the backend is a string that is the name of a backend, 
 it will be assumed to be a string that can be used as a 
 backend name in matplotlib.matplotlibrc. 
 If the backend is a string that is the name of a module, 
 it will be assumed to be a string that can be used as a 
 backend name in matplotlib.matplotli","'return new_figure_manager, draw_if_interactive and show for pylab'"
"@ensure_csrf_cookie 
 @cache_if_anonymous() 
 def course_about(request, course_id): 
    course_key = SlashSeparatedCourseKey.from_deprecated_string(course_id) 
   if hasattr(course_key, 'ccx'): 
      return redirect(reverse('dashboard')) 
   with modulestore().bulk_operations(course_key): 
      permission = get_permission_for_course_about() 
      course = get_course_with_access(request.user, permission, course_key) 
      course_details = CourseDetails.populate(course) 
      modes = CourseMode.modes_for_course_dict(course_key) 
      if configuration_helpers.get_value('ENABLE_MKTG_SITE', settings.FEATURES.get('ENABLE_MKTG_SITE', False)): 
         return redirect(reverse('info', args=[course.id.to_deprecated_string()])) 
      registered = registered_for_course(course, request.user) 
      staff_access = bool(has_access(request.user, 'staff', course)) 
      studio_url = get_studio_url(course, 'settings/details') 
      if has_access(request.user, 'load', course): 
         course_target = reverse('info', args=[course.id.to_deprecated_string()]) 
      else: 
         course_target = reverse('about_course', args=[course.id.to_deprecated_string()]) 
      show_courseware_link = bool(((has_access(request.user, 'load', course) and has_access(request.user, 'view_courseware_with_prerequisites', course)) or settings.FEATURES.get('ENABLE_LMS_MIGRATION'))) 
      in_cart = False 
      reg_then_add_to_cart_link = '' 
      _is_shopping_cart_enabled = is_shopping_cart_enabled() 
      if _is_shopping_cart_enabled: 
         if request.user.is_authenticated(): 
            cart = shoppingcart.models.Order.get_cart_for_user(request.user) 
            in_cart = (shoppingcart.models.PaidCourseRegistration.contained_in_order(cart, course_key) or shoppingcart.models.CourseRegCodeItem.contained_in_order(cart, course_key)) 
         reg_then_add_to_cart_link = '{reg_url}?course_id={course_id}&enrollment_action=add_to_cart'.format(reg_url=reverse('register_user'), course_id=urllib.quote(str(course_id))) 
      ecomm_service = EcommerceService() 
      ecommerce_checkout = ecomm_service.is_enabled(request.user) 
      ecommerce_checkout_link = '' 
      ecommerce_bulk_checkout_link = '' 
      professional_mode = None 
      is_professional_mode = ((CourseMode.PROFESSIONAL in modes) or (CourseMode.NO_ID_PROFESSIONAL_MODE in modes)) 
      if (ecommerce_checkout and is_professional_mode): 
         professional_mode = (modes.get(CourseMode.PROFESSIONAL, '') or modes.get(CourseMode.NO_ID_PROFESSIONAL_MODE, '')) 
         if professional_mode.sku: 
            ecommerce_checkout_link = ecomm_service.checkout_page_url(professional_mode.sku) 
         if professional_mode.bulk_sku: 
            ecommerce_bulk_checkout_link = ecomm_service.checkout_page_url(professional_mode.bulk_sku) 
      registration_price = CourseMode.min_course_price_for_currency(course_key, settings.PAID_COURSE_REGISTRATION_CURRENCY[0]) 
      course_price = get_cosmetic_display_price(course, registration_price) 
      can_add_course_to_cart = (_is_shopping_cart_enabled and registration_price and (not ecommerce_checkout_link)) 
      can_enroll = bool(has_access(request.user, 'enroll', course)) 
      invitation_only = course.invitation_only 
      is_course_full = CourseEnrollment.objects.is_course_full(course) 
      active_reg_button = (not (registered or is_course_full or (not can_enroll))) 
      is_shib_course = uses_shib(course) 
      pre_requisite_courses = get_prerequisite_courses_display(course) 
      overview = CourseOverview.get_from_id(course.id) 
      context = {'course': course, 'course_details': course_details, 'staff_access': staff_access, 'studio_url': studio_url, 'registered': registered, 'course_target': course_target, 'is_cosmetic_price_enabled': settings.FEATURES.get('ENABLE_COSMETIC_DISPLAY_PRICE'), 'course_price': course_price, 'in_cart': in_cart, 'ecommerce_checkout': ecommerce_checkout, 'ecommerce_checkout_link': ecommerce_checkout_link, 'ecommerce_bulk_checkout_link': ecommerce_bulk_checkout_link, 'professional_mode': professional_mode, 'reg_then_add_to_cart_link': reg_then_add_to_cart_link, 'show_courseware_link': show_courseware_link, 'is_course_full': is_course_full, 'can_enroll': can_enroll, 'invitation_only': invitation_only, 'active_reg_button': active_reg_button, 'is_shib_course': is_shib_course, 'disable_courseware_header': True, 'can_add_course_to_cart': can_add_course_to_cart, 'cart_link': reverse('shoppingcart.views.show_cart'), 'pre_requisite_courses': pre_requisite_courses, 'course_image_urls': overview.image_urls} 
      inject_coursetalk_keys_into_context(context, course_key) 
      return render_to_response('courseware/course_about.html', context)"," 'Renders the course page. 
 This function is called when a user visits a course page. 
 Args: 
 request (HttpRequest): The HTTP request. 
 course_id (str): The course id. 
 Examples: 
 >>> from django.contrib.auth.decorators import login_required 
 >>> @login_required 
 >>> def course_about(request, course_id): 
 >>> return course_about(request, course_id) 
 >>> course_about(request, course_id) 
 '","'Display the course\'s about page. 
 Assumes the course_id is in a valid format.'"
"def is_private(ip_addr): 
    return ipaddress.ip_address(ip_addr).is_private"," 'Checks if the given IP address is a private IP address. 
 Returns True if it is a private address, False otherwise.'","'Check if the given IP address is a private address 
 .. versionadded:: 2014.7.0 
 .. versionchanged:: 2015.8.0 
 IPv6 support 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' network.is_private 10.0.0.3'"
"def hessenberg(a, calc_q=False, overwrite_a=False, check_finite=True): 
    a1 = _asarray_validated(a, check_finite=check_finite) 
   if ((len(a1.shape) != 2) or (a1.shape[0] != a1.shape[1])): 
      raise ValueError('expected   square   matrix') 
   overwrite_a = (overwrite_a or _datacopied(a1, a)) 
   if (a1.shape[0] <= 2): 
      if calc_q: 
         return (a1, numpy.eye(a1.shape[0])) 
      return a1 
   (gehrd, gebal, gehrd_lwork) = get_lapack_funcs(('gehrd', 'gebal', 'gehrd_lwork'), (a1,)) 
   (ba, lo, hi, pivscale, info) = gebal(a1, permute=0, overwrite_a=overwrite_a) 
   if (info < 0): 
      raise ValueError(('illegal   value   in   %d-th   argument   of   internal   gebal   (hessenberg)' % (- info))) 
   n = len(a1) 
   lwork = _compute_lwork(gehrd_lwork, ba.shape[0], lo=lo, hi=hi) 
   (hq, tau, info) = gehrd(ba, lo=lo, hi=hi, lwork=lwork, overwrite_a=1) 
   if (info < 0): 
      raise ValueError(('illegal   value   in   %d-th   argument   of   internal   gehrd   (hessenberg)' % (- info))) 
   h = numpy.triu(hq, (-1)) 
   if (not calc_q): 
      return h 
   (orghr, orghr_lwork) = get_lapack_funcs(('orghr', 'orghr_lwork'), (a1,)) 
   lwork = _compute_lwork(orghr_lwork, n, lo=lo, hi=hi) 
   (q, info) = orghr(a=hq, tau=tau, lo=lo, hi=hi, lwork=lwork, overwrite_a=1) 
   if (info < 0): 
      raise ValueError(('illegal   value   in   %d-th   argument   of   internal   orghr   (hessenberg)' % (- info))) 
   return (h, q)"," 'Compute the Hessenberg matrix from an input matrix. 
 This function computes the Hessenberg matrix from an input matrix. 
 Parameters 
 a : array_like 
 Input matrix. 
 calc_q : bool, optional 
 If True, the Q factor is also computed. 
 overwrite_a : bool, optional 
 If True, the input matrix is overwritten. 
 check_finite : bool, optional 
 If True, the input matrix is checked for finite values. 
 Returns 
 h : 2-D array 
 The Hessenberg matrix. 
 q : 2-D array 
 The Q factor. 
 Notes 
 This function is based on the algorithm described in the paper 
 ""Hessenberg, Schur, and Quadratic Forms"" by H. J. Strang, 
 SIAM Journal on Numerical Analysis, Vol. 2, No. 2, pp. 169-177, 
 June 1965. 
 Examples 
 >>> from scipy import linalg 
 >>> a = numpy.array([[1, 2, 3],","'Compute Hessenberg form of a matrix. 
 The Hessenberg decomposition is:: 
 A = Q H Q^H 
 where `Q` is unitary/orthogonal and `H` has only zero elements below 
 the first sub-diagonal. 
 Parameters 
 a : (M, M) array_like 
 Matrix to bring into Hessenberg form. 
 calc_q : bool, optional 
 Whether to compute the transformation matrix.  Default is False. 
 overwrite_a : bool, optional 
 Whether to overwrite `a`; may improve performance. 
 Default is False. 
 check_finite : bool, optional 
 Whether to check that the input matrix contains only finite numbers. 
 Disabling may give a performance gain, but may result in problems 
 (crashes, non-termination) if the inputs do contain infinities or NaNs. 
 Returns 
 H : (M, M) ndarray 
 Hessenberg form of `a`. 
 Q : (M, M) ndarray 
 Unitary/orthogonal similarity transformation matrix ``A = Q H Q^H``. 
 Only returned if ``calc_q=True``.'"
"def require_cuda_ndarray(obj): 
    if (not is_cuda_ndarray(obj)): 
      raise ValueError('require   an   cuda   ndarray   object')"," 'Check if obj is an cuda ndarray object. 
 Parameters 
 obj : any 
 The object to check. 
 Returns 
 obj : any 
 The object to check.'",'Raises ValueError is is_cuda_ndarray(obj) evaluates False'
"def getRotationMatrix(arrayDictionary, derivation, path, point, pointIndex): 
    if ((len(path) < 2) or (not derivation.track)): 
      return matrix.Matrix() 
   point = point.dropAxis() 
   begin = path[(((pointIndex + len(path)) - 1) % len(path))].dropAxis() 
   end = path[((pointIndex + 1) % len(path))].dropAxis() 
   pointMinusBegin = (point - begin) 
   pointMinusBeginLength = abs(pointMinusBegin) 
   endMinusPoint = (end - point) 
   endMinusPointLength = abs(endMinusPoint) 
   if (not derivation.closed): 
      if ((pointIndex == 0) and (endMinusPointLength > 0.0)): 
         return getRotationMatrixByPolar(arrayDictionary, endMinusPoint, endMinusPointLength) 
      elif ((pointIndex == (len(path) - 1)) and (pointMinusBeginLength > 0.0)): 
         return getRotationMatrixByPolar(arrayDictionary, pointMinusBegin, pointMinusBeginLength) 
   if (pointMinusBeginLength <= 0.0): 
      print 'Warning,   point   equals   previous   point   in   getRotationMatrix   in   array   for:' 
      print path 
      print pointIndex 
      print derivation.elementNode 
      return matrix.Matrix() 
   pointMinusBegin /= pointMinusBeginLength 
   if (endMinusPointLength <= 0.0): 
      print 'Warning,   point   equals   next   point   in   getRotationMatrix   in   array   for:' 
      print path 
      print pointIndex 
      print derivation.elementNode 
      return matrix.Matrix() 
   endMinusPoint /= endMinusPointLength 
   averagePolar = (pointMinusBegin + endMinusPoint) 
   averagePolarLength = abs(averagePolar) 
   if (averagePolarLength <= 0.0): 
      print 'Warning,   averagePolarLength   is   zero   in   getRotationMatrix   in   array   for:' 
      print path 
      print pointIndex 
      print derivation.elementNode 
      return matrix.Matrix() 
   return getRotationMatrixByPolar(arrayDictionary, averagePolar, averagePolarLength)"," 'Returns the rotation matrix for a given path in an array. 
 The rotation matrix is computed by the method of polar coordinates. 
 The path is assumed to be closed. 
 Parameters 
 arrayDictionary : Dictionary 
 Dictionary of arrays with the coordinates of the points of the path. 
 derivation : Derivation 
 Derivation of the path. 
 path : List 
 List of points of the path. 
 point : Point 
 Point of the path. 
 pointIndex : int 
 Index of the point in the path. 
 Returns 
 rotationMatrix : Matrix 
 Rotation matrix.'",'Get rotationMatrix.'
"def slicable(dim, pad=0): 
    dim0 = (np.prod(dim[:(-1)]) + pad) 
   return (dim0, dim[(-1)])"," 'Returns the dimensions of the slice, including the padded dimension 
 if it is not 0.'","'colapse outer dimensions into one and preserve inner dimension 
 this allows for easy cpu convolution in numpy 
 Arguments: 
 dim (tuple): dimensions list in a tuple 
 pad (int):  how many pixel paddings'"
"def _indent(text, prefix, predicate=None): 
    if (predicate is None): 
      predicate = (lambda line: line.strip()) 
   def prefixed_lines(): 
      for line in text.splitlines(True): 
         (yield ((prefix + line) if predicate(line) else line)) 
   return ''.join(prefixed_lines())"," 'Returns a string with all lines prefixed with a given string. 
 This is used for indentation in the documentation. 
 :param prefix: 
 The string to prefix all lines with. 
 :param predicate: 
 A predicate function that takes a single line and returns True if 
 the line should be prefixed. 
 :returns: 
 A string with all lines prefixed with the given string.'","'Adds \'prefix\' to the beginning of selected lines in \'text\'. 
 If \'predicate\' is provided, \'prefix\' will only be added to the lines 
 where \'predicate(line)\' is True. If \'predicate\' is not provided, 
 it will default to adding \'prefix\' to all non-empty lines that do not 
 consist solely of whitespace characters.'"
"def first_value(obj): 
    return six.next(six.itervalues(obj))"," 'Return the first value of a dict, set or other iterable object.'","'Return the first value 
 Parameters 
 obj: dict-like object'"
"def is_ssh_uri(url): 
    return (urllib_parse(url)[0] in ssh_uri_schemes)", 'Returns True if the given URL is an SSH URI.','Returns whether or not a URL represents an SSH connection.'
"def _Cobject(cls, ctype): 
    o = object.__new__(cls) 
   o._as_parameter_ = ctype 
   return o", 'Return a C object.','(INTERNAL) New instance from ctypes.'
"def plate_scale(platescale): 
    if platescale.unit.is_equivalent((si.arcsec / si.m)): 
      platescale_val = platescale.to((si.radian / si.m)).value 
   elif platescale.unit.is_equivalent((si.m / si.arcsec)): 
      platescale_val = (1 / platescale).to((si.radian / si.m)).value 
   else: 
      raise UnitsError(u'The   pixel   scale   must   be   in   angle/distance   or   distance/angle') 
   return [(si.m, si.radian, (lambda d: (d * platescale_val)), (lambda rad: (rad / platescale_val)))]"," 'Converts the pixel scale to the angle/distance units. 
 Parameters 
 platescale : astropy.units.Quantity 
 The pixel scale. 
 Returns 
 astropy.units.Quantity 
 The pixel scale in angle/distance units.'","'Convert between lengths (to be interpreted as lengths in the focal plane) 
 and angular units with a specified ``platescale``. 
 Parameters 
 platescale : `~astropy.units.Quantity` 
 The pixel scale either in units of distance/pixel or distance/angle.'"
"@sensitive_post_parameters() 
 @never_cache 
 def password_reset_confirm(request, uidb36=None, token=None, template_name='registration/password_reset_confirm.html', token_generator=default_token_generator, set_password_form=SetPasswordForm, post_reset_redirect=None, current_app=None, extra_context=None): 
    assert ((uidb36 is not None) and (token is not None)) 
   if (post_reset_redirect is None): 
      post_reset_redirect = reverse('django.contrib.auth.views.password_reset_complete') 
   try: 
      uid_int = base36_to_int(uidb36) 
      user = User.objects.get(id=uid_int) 
   except (ValueError, User.DoesNotExist): 
      user = None 
   if ((user is not None) and token_generator.check_token(user, token)): 
      validlink = True 
      if (request.method == 'POST'): 
         form = set_password_form(user, request.POST) 
         if form.is_valid(): 
            form.save() 
            return HttpResponseRedirect(post_reset_redirect) 
      else: 
         form = set_password_form(None) 
   else: 
      validlink = False 
      form = None 
   context = {'form': form, 'validlink': validlink} 
   if (extra_context is not None): 
      context.update(extra_context) 
   return TemplateResponse(request, template_name, context, current_app=current_app)"," 'Confirmation view for password reset. 
 The user is redirected to this view after the password reset token is 
 sent. 
 If the token is invalid or the user is not authenticated, the user is 
 redirected to the password reset page. 
 If the user is authenticated and the token is valid, the user is 
 redirected to the password reset complete view. 
 If the user is authenticated and the token is valid, the user is 
 redirected to the password reset complete view. 
 If the user is authenticated and the token is valid, the user is 
 redirected to the password reset complete view. 
 The user is redirected to the password reset complete view. 
 If the user is authenticated and the token is valid, the user is 
 redirected to the password reset complete view. 
 If the user is authenticated and the token is valid, the user is 
 redirected to the password reset complete view. 
 If the user is authenticated and the token is valid, the user is 
 redirected to the password reset complete view. 
 The user is redirected to the password reset complete view. 
 If","'View that checks the hash in a password reset link and presents a 
 form for entering a new password.'"
"def randitems_from_structure(fmt, t): 
    (memlen, itemsize, _, _, _, _) = t 
   return gen_items((memlen // itemsize), ('#' + fmt), 'numpy')"," 'Return a list of random items from a given structure. 
 Parameters 
 fmt : str 
 A string describing the format of the structure. 
 t : tuple 
 A tuple describing the structure. 
 Returns 
 items : list 
 A list of random items from the structure.'","'Return a list of random items for structure \'t\' with format 
 \'fmtchar\'.'"
"def track_to_mpd_format(track, position=None, stream_title=None): 
    if isinstance(track, TlTrack): 
      (tlid, track) = track 
   else: 
      (tlid, track) = (None, track) 
   if (not track.uri): 
      logger.warning(u'Ignoring   track   without   uri') 
      return [] 
   result = [(u'file', track.uri), (u'Time', ((track.length and (track.length // 1000)) or 0)), (u'Artist', concat_multi_values(track.artists, u'name')), (u'Album', ((track.album and track.album.name) or u''))] 
   if (stream_title is not None): 
      result.append((u'Title', stream_title)) 
      if track.name: 
         result.append((u'Name', track.name)) 
   else: 
      result.append((u'Title', (track.name or u''))) 
   if track.date: 
      result.append((u'Date', track.date)) 
   if ((track.album is not None) and (track.album.num_tracks is not None)): 
      result.append((u'Track', (u'%d/%d' % ((track.track_no or 0), track.album.num_tracks)))) 
   else: 
      result.append((u'Track', (track.track_no or 0))) 
   if ((position is not None) and (tlid is not None)): 
      result.append((u'Pos', position)) 
      result.append((u'Id', tlid)) 
   if ((track.album is not None) and (track.album.musicbrainz_id is not None)): 
      result.append((u'MUSICBRAINZ_ALBUMID', track.album.musicbrainz_id)) 
   if ((track.album is not None) and track.album.artists): 
      result.append((u'AlbumArtist', concat_multi_values(track.album.artists, u'name'))) 
      musicbrainz_ids = concat_multi_values(track.album.artists, u'musicbrainz_id') 
      if musicbrainz_ids: 
         result.append((u'MUSICBRAINZ_ALBUMARTISTID', musicbrainz_ids)) 
   if track.artists: 
      musicbrainz_ids = concat_multi_values(track.artists, u'musicbrainz_id') 
      if musicbrainz_ids: 
         result.append((u'MUSICBRAINZ_ARTISTID', musicbrainz_ids)) 
   if track.composers: 
      result.append((u'Composer', concat_multi_values(track.composers, u'name'))) 
   if track.performers: 
      result.append((u'Performer', concat_multi_values(track.performers, u'name'))) 
   if track.genre: 
      result.append((u'Genre', track.genre)) 
   if track.disc_no: 
      result.append((u'Disc', track.disc_no)) 
   if track.last_modified: 
      datestring = datetime.datetime.utcfromtimestamp((track.last_modified // 1000)).isoformat() 
      result.append((u'Last-Modified', (datestring + u'Z'))) 
   if (track.musicbrainz_id is not None): 
      result.append((u'MUSICBRAINZ_TRACKID', track.musicbrainz_id)) 
   if (track.album and track.album.uri): 
      result.append((u'X-AlbumUri', track.album.uri)) 
   if (track.album and track.album.images): 
      images = u';'.join((i for i in track.album.images if (i is not u''))) 
      result.append((u'X-AlbumImage', images)) 
   result = [element for element in result if _has_value(*element)] 
   return result"," 'Convert a track to the format expected by MPD. 
 :param track: A track or a TlTrack. 
 :param position: The current position in the track. 
 :param stream_title: The title of the stream, if different from the track title. 
 :return: A list of tuples in the format expected by MPD. 
 :rtype: list(tuple)'","'Format track for output to MPD client. 
 :param track: the track 
 :type track: :class:`mopidy.models.Track` or :class:`mopidy.models.TlTrack` 
 :param position: track\'s position in playlist 
 :type position: integer 
 :param stream_title: the current streams title 
 :type position: string 
 :rtype: list of two-tuples'"
"def getReadRepository(repository): 
    text = archive.getFileText(archive.getProfilesPath(getProfileBaseName(repository)), False) 
   if (text == ''): 
      if (repository.baseNameSynonym != None): 
         text = archive.getFileText(archive.getProfilesPath(getProfileBaseNameSynonym(repository)), False) 
   if (text == ''): 
      print ('The   default   %s   will   be   written   in   the   .skeinforge   folder   in   the   home   directory.' % repository.title.lower()) 
      text = archive.getFileText(getProfilesDirectoryInAboveDirectory(getProfileBaseName(repository)), False) 
      if (text != ''): 
         readSettingsFromText(repository, text) 
      writeSettings(repository) 
      temporaryApplyOverrides(repository) 
      return repository 
   readSettingsFromText(repository, text) 
   temporaryApplyOverrides(repository) 
   return repository"," 'Get the read repository, and apply any overrides. 
 :param repository: The repository to get the read settings for. 
 :return: The read repository.'",'Read and return settings from a file.'
"def findTypeParent(element, tag): 
    p = element 
   while True: 
      p = p.getparent() 
      if (p.tag == tag): 
         return p 
   return None"," 'Find the parent element of a given element. 
 @param element: the element to find the parent for 
 @param tag: the tag to find the parent for'","'Finds fist parent of element of the given type 
 @param object element: etree element 
 @param string the tag parent to search for 
 @return object element: the found parent or None when not found'"
"def log(repo='.', paths=None, outstream=sys.stdout, max_entries=None, reverse=False, name_status=False): 
    with open_repo_closing(repo) as r: 
      walker = r.get_walker(max_entries=max_entries, paths=paths, reverse=reverse) 
      for entry in walker: 
         decode = (lambda x: commit_decode(entry.commit, x)) 
         print_commit(entry.commit, decode, outstream) 
         if name_status: 
            outstream.writelines([(l + '\n') for l in print_name_status(entry.changes())])"," 'Log the changes in the repository. 
 :param repo: The path to the repository. 
 :param paths: The paths to the files to log. 
 :param outstream: The output stream to write to. 
 :param max_entries: The maximum number of entries to log. 
 :param reverse: Whether to log the changes in reverse order. 
 :param name_status: Whether to print the status of the file names. 
 :type repo: str 
 :type paths: list 
 :type outstream: file 
 :type max_entries: int 
 :type reverse: bool 
 :type name_status: bool 
 :rtype: None'","'Write commit logs. 
 :param repo: Path to repository 
 :param paths: Optional set of specific paths to print entries for 
 :param outstream: Stream to write log output to 
 :param reverse: Reverse order in which entries are printed 
 :param name_status: Print name status 
 :param max_entries: Optional maximum number of entries to display'"
"def get_rule_handle(table='filter', chain=None, rule=None, family='ipv4'): 
    if (not chain): 
      return 'Error:   Chain   needs   to   be   specified' 
   if (not rule): 
      return 'Error:   Rule   needs   to   be   specified' 
   if (not check_table(table, family=family)): 
      return 'Error:   table   {0}   in   family   {1}   does   not   exist'.format(table, family) 
   if (not check_chain(table, chain, family=family)): 
      return 'Error:   chain   {0}   in   table   {1}   in   family   {2}   does   not   exist'.format(chain, table, family) 
   if (not check(table, chain, rule, family=family)): 
      return 'Error:   rule   {0}   chain   {1}   in   table   {2}   in   family   {3}   does   not   exist'.format(rule, chain, table, family) 
   nft_family = _NFTABLES_FAMILIES[family] 
   cmd = '{0}   --numeric   --numeric   --numeric   --handle   list   chain   {1}   {2}   {3}'.format(_nftables_cmd(), nft_family, table, chain) 
   out = __salt__['cmd.run'](cmd, python_shell=False) 
   rules = re.split('\n+', out) 
   pat = re.compile('{0}   #   handle   (?P<handle>\\d+)'.format(rule)) 
   for r in rules: 
      match = pat.search(r) 
      if match: 
         return match.group('handle') 
   return 'Error:   could   not   find   rule   {0}'.format(rule)"," 'Returns the handle of the rule matching the specified rule, chain and table 
 :param table: Name of the table 
 :param chain: Name of the chain 
 :param rule: Name of the rule 
 :param family: IP family (ipv4, ipv6) 
 :return: The handle of the rule or an error message 
 :rtype: str'","'Get the handle for a particular rule 
 This function accepts a rule in a standard nftables command format, 
 starting with the chain. Trying to force users to adapt to a new 
 method of creating rules would be irritating at best, and we 
 already have a parser that can handle it. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' nftables.get_rule_handle filter input \ 
 rule=\'input tcp dport 22 log accept\' 
 IPv6: 
 salt \'*\' nftables.get_rule_handle filter input \ 
 rule=\'input tcp dport 22 log accept\' \ 
 family=ipv6'"
"def to_names(domain_obj_list): 
    objs = [] 
   for obj in domain_obj_list: 
      objs.append((obj.name if obj else None)) 
   return objs"," 'Convert a list of Domain objects into a list of strings, with each 
 string representing the name of the domain. 
 :param domain_obj_list: A list of Domain objects. 
 :type domain_obj_list: list(django.db.models.Domain)'","'Takes a list of domain objects and returns a corresponding list 
 of their names.'"
"def load_json_dict(filename, *args): 
    data = {} 
   if os.path.exists(filename): 
      lock.acquire() 
      with open(filename, 'r') as f: 
         try: 
            data = _json.load(f) 
            if (not isinstance(data, dict)): 
               data = {} 
         except: 
            data = {} 
      lock.release() 
      if args: 
         return {key: data[key] for key in args if (key in data)} 
   return data"," 'Loads a JSON file and returns it as a dictionary. 
 If *args* is specified, only the keys specified will be returned. 
 :param filename: Path to the JSON file 
 :param args: Keys to return. 
 :return: Dictionary'",'Checks if file exists. Returns {} if something fails.'
"def _alarms_present(name, min_size_equals_max_size, alarms, alarms_from_pillar, region, key, keyid, profile): 
    tmp = copy.deepcopy(__salt__['config.option'](alarms_from_pillar, {})) 
   if alarms: 
      tmp = dictupdate.update(tmp, alarms) 
   merged_return_value = {'name': name, 'result': True, 'comment': '', 'changes': {}} 
   for (_, info) in six.iteritems(tmp): 
      info['name'] = ((name + '   ') + info['name']) 
      info['attributes']['description'] = ((name + '   ') + info['attributes']['description']) 
      if ('dimensions' not in info['attributes']): 
         info['attributes']['dimensions'] = {'AutoScalingGroupName': [name]} 
      scaling_policy_actions_only = True 
      for action_type in ['alarm_actions', 'insufficient_data_actions', 'ok_actions']: 
         if (action_type in info['attributes']): 
            new_actions = [] 
            for action in info['attributes'][action_type]: 
               if ('scaling_policy' not in action): 
                  scaling_policy_actions_only = False 
               if (':self:' in action): 
                  action = action.replace(':self:', ':{0}:'.format(name)) 
               new_actions.append(action) 
            info['attributes'][action_type] = new_actions 
      if (scaling_policy_actions_only and min_size_equals_max_size): 
         continue 
      kwargs = {'name': info['name'], 'attributes': info['attributes'], 'region': region, 'key': key, 'keyid': keyid, 'profile': profile} 
      results = __states__['boto_cloudwatch_alarm.present'](**kwargs) 
      if (not results['result']): 
         merged_return_value['result'] = False 
      if (results.get('changes', {}) != {}): 
         merged_return_value['changes'][info['name']] = results['changes'] 
      if ('comment' in results): 
         merged_return_value['comment'] += results['comment'] 
   return merged_return_value"," 'Check if the alarm exists, create it if not. 
 This function checks if the alarm exists in the current region. 
 If it does not exist, it creates it. 
 The function checks if the alarm is of the correct size. 
 If the alarm is of the correct size, the function returns True. 
 If the alarm is not of the correct size, the function returns False. 
 The function checks if the alarm is of the correct size. 
 If the alarm is of the correct size, the function returns True. 
 If the alarm is not of the correct size, the function returns False. 
 :param name: The name of the alarm. 
 :param min_size_equals_max_size: The min size equals the max size. 
 :param alarms: The alarm to check. 
 :param alarms_from_pillar: The alarm from pillar. 
 :param region: The region. 
 :param key: The access key. 
 :param keyid: The access key id. 
 :param profile: The profile. 
 :return: The merged return value. 
 :rtype: dict",'helper method for present.  ensure that cloudwatch_alarms are set'
"@app.route('/stream/<int:n>') 
 def stream_n_messages(n): 
    response = get_dict('url', 'args', 'headers', 'origin') 
   n = min(n, 100) 
   def generate_stream(): 
      for i in range(n): 
         response['id'] = i 
         (yield (json.dumps(response) + '\n')) 
   return Response(generate_stream(), headers={'Content-Type': 'application/json'})"," 'Returns the first `n` messages. 
 :param int n: The number of messages to return. 
 :return: A Response object with the requested messages.'",'Stream n JSON messages'
"def partial_velocity(vel_vecs, gen_speeds, frame): 
    if (not iterable(vel_vecs)): 
      raise TypeError('Velocity   vectors   must   be   contained   in   an   iterable.') 
   if (not iterable(gen_speeds)): 
      raise TypeError('Generalized   speeds   must   be   contained   in   an   iterable') 
   vec_partials = [] 
   for vec in vel_vecs: 
      partials = [] 
      for speed in gen_speeds: 
         partials.append(vec.diff(speed, frame, var_in_dcm=False)) 
      vec_partials.append(partials) 
   return vec_partials"," 'Returns a list of partial velocities. 
 Each partial velocity is a list of the partial derivatives of the 
 velocity vector with respect to the generalized speeds. 
 Parameters 
 vel_vecs : list 
 A list of velocity vectors. 
 gen_speeds : list 
 A list of generalized speeds. 
 Returns 
 vec_partials : list 
 A list of lists of partial velocities. 
 Notes 
 This is a list of lists of partial derivatives of the velocity vector 
 with respect to the generalized speeds. 
 Examples 
 >>> from sympy.physics.mechanics import PartialVelocity 
 >>> vel = [1, 2, 3] 
 >>> gen_speeds = [1, 2, 3] 
 >>> vec_partials = PartialVelocity(vel, gen_speeds) 
 >>> vec_partials 
 [[1, 2, 3], [1, 2, 3], [1, 2, 3]] 
 References 
 .. [1] http://en.wikipedia.org/wiki/Partial_der","'Returns a list of partial velocities with respect to the provided 
 generalized speeds in the given reference frame for each of the supplied 
 velocity vectors. 
 The output is a list of lists. The outer list has a number of elements 
 equal to the number of supplied velocity vectors. The inner lists are, for 
 each velocity vector, the partial derivatives of that velocity vector with 
 respect to the generalized speeds supplied. 
 Parameters 
 vel_vecs : iterable 
 An iterable of velocity vectors (angular or linear). 
 gen_speeds : iterable 
 An iterable of generalized speeds. 
 frame : ReferenceFrame 
 The reference frame that the partial derivatives are going to be taken 
 in. 
 Examples 
 >>> from sympy.physics.vector import Point, ReferenceFrame 
 >>> from sympy.physics.vector import dynamicsymbols 
 >>> from sympy.physics.vector import partial_velocity 
 >>> u = dynamicsymbols(\'u\') 
 >>> N = ReferenceFrame(\'N\') 
 >>> P = Point(\'P\') 
 >>> P.set_vel(N, u * N.x) 
 >>> vel_vecs = [P.vel(N)] 
 >>> gen_speeds = [u] 
 >>> partial_velocity(vel_vecs, gen_speeds, N) 
 [[N.x]]'"
"def is_ascii(string): 
    return all(((ord(c) < 128) for c in string))"," 'Return True if the string is ASCII, False otherwise.'",'Return whether a string is in ascii.'
"def docstring_errors(filename, global_dict=None): 
    if (global_dict is None): 
      global_dict = {} 
   if ('__file__' not in global_dict): 
      global_dict['__file__'] = filename 
   if ('__doc__' not in global_dict): 
      global_dict['__doc__'] = None 
   try: 
      with open(filename) as f: 
         code = compile(f.read(), filename, 'exec') 
         exec code in global_dict 
   except SystemExit: 
      pass 
   except SkipTest: 
      raise AssertionError(((""Couldn't   verify   format   of   "" + filename) + 'due   to   SkipTest')) 
   all_errors = [] 
   for (key, val) in six.iteritems(global_dict): 
      if (not key.startswith('_')): 
         module_name = '' 
         if hasattr(inspect.getmodule(val), '__name__'): 
            module_name = inspect.getmodule(val).__name__ 
         if ((inspect.isfunction(val) or inspect.isclass(val)) and ((inspect.getmodule(val) is None) or (module_name == '__builtin__'))): 
            if inspect.isfunction(val): 
               all_errors.extend(handle_function(val, key)) 
            elif inspect.isclass(val): 
               all_errors.extend(handle_class(val, key)) 
      elif (key == '__doc__'): 
         all_errors.extend(handle_module(val, key)) 
   if all_errors: 
      all_errors.insert(0, (('%s:' % filename),)) 
   return all_errors"," 'Checks for docstring errors in a module. 
 Returns a list of errors. 
 :param filename: the filename of the module 
 :param global_dict: a dictionary of globals 
 :return: a list of errors'","'Run a Python file, parse the docstrings of all the classes 
 and functions it declares, and return them. 
 Parameters 
 filename : str 
 Filename of the module to run. 
 global_dict : dict, optional 
 Globals dictionary to pass along to `execfile()`. 
 Returns 
 all_errors : list 
 Each entry of the list is a tuple, of length 2 or 3, with 
 format either 
 (func_or_class_name, docstring_error_description) 
 or 
 (class_name, method_name, docstring_error_description)'"
"def run_doctest(target_dir=None, *args, **kwargs): 
    import doctest 
   default_kwargs = {'optionflags': doctest.ELLIPSIS} 
   kwargs.update(default_kwargs) 
   cur_dir = os.path.abspath(os.curdir) 
   print('Running   doctests...') 
   try: 
      os.chdir(find_test_dir(target_dir)) 
      doctest.testmod(*args, **kwargs) 
   finally: 
      os.chdir(cur_dir) 
   print('Done')"," 'Runs doctests on the current directory and all subdirectories. 
 :param target_dir: The directory to run doctests in. 
 :param args: Arguments to pass to doctest. 
 :param kwargs: Keyword arguments to pass to doctest. 
 :return: None'",'Runs doctest for the importing module.'
"@verbose 
 def _get_ico_tris(grade, verbose=None, return_surf=False): 
    ico = _get_ico_surface(grade) 
   if (not return_surf): 
      return ico['tris'] 
   else: 
      return ico"," 'Get the tris of the ico surface. 
 Parameters 
 grade : int 
 The grade of the ico surface. 
 verbose : bool, optional 
 If True, print extra information. 
 return_surf : bool, optional 
 If True, return the surface as well. 
 Returns 
 tris : ndarray 
 The tris of the ico surface. 
 Examples 
 >>> from skimage.morphology import disk 
 >>> from skimage.filters.rank import _get_ico_tris 
 >>> disk(3).astype(np.int32) 
 array([[0, 0, 0], 
 [0, 0, 0], 
 [0, 0, 0]], dtype=int32) 
 >>> _get_ico_tris(10) 
 array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ",'Get triangles for ico surface.'
"def get_last_modified(files): 
    files = list(files) 
   if files: 
      return max((datetime.datetime.fromtimestamp(os.path.getmtime(f)) for f in files)) 
   return datetime.datetime(1970, 1, 1)", 'Returns the last modified timestamp of the files in the given list.',"'Returns the modification time of the most recently modified 
 file provided 
 :param list(str) files: names of files to check 
 :return: most recent modification time amongst the fileset 
 :rtype: datetime.datetime'"
"def discoverInfo(disp, jid, node=None): 
    '   According   to   JEP-0030:\n                        query   MAY   have   node   attribute\n                        identity:   MUST   HAVE   category   and   name   attributes   and   MAY   HAVE   type   attribute.\n                        feature:   MUST   HAVE   var   attribute' 
   (identities, features) = ([], []) 
   for i in _discover(disp, NS_DISCO_INFO, jid, node): 
      if (i.getName() == 'identity'): 
         identities.append(i.attrs) 
      elif (i.getName() == 'feature'): 
         features.append(i.getAttr('var')) 
      elif (i.getName() == 'agent'): 
         if i.getTag('name'): 
            i.setAttr('name', i.getTagData('name')) 
         if i.getTag('description'): 
            i.setAttr('name', i.getTagData('description')) 
         identities.append(i.attrs) 
         if i.getTag('groupchat'): 
            features.append(NS_GROUPCHAT) 
         if i.getTag('register'): 
            features.append(NS_REGISTER) 
         if i.getTag('search'): 
            features.append(NS_SEARCH) 
   return (identities, features)", 'Discovery of info about the node.','Query remote object about info that it publishes. Returns identities and features lists.'
"def is_on(hass, entity_id=None): 
    entity_id = (entity_id or ENTITY_ID) 
   return hass.states.is_state(entity_id, STATE_ABOVE_HORIZON)", 'Returns if the entity is on.','Test if the sun is currently up based on the statemachine.'
"def p_postfix_expression_2(t): 
    pass"," 'Postfix expression 
 postfix_expression ::= 
 primary 
 | primary \'+\' primary 
 | primary \'-\' primary 
 | primary \'*\' primary 
 | primary \'/\' primary 
 | primary \'%\' primary 
 | primary \'<<\' primary 
 | primary \'>>\' primary 
 | primary \'!\' primary 
 | primary \'&\' primary 
 | primary \'|\' primary 
 | primary \'^\' primary 
 | primary \'~\' primary 
 | primary \'<<\' primary 
 | primary \'>>\' primary 
 | primary \'++\' 
 | primary \'--\' 
 | primary \'++\' primary 
 | primary \'--\' primary 
 | \'0\' 
 | \'1\' 
 | \'2\' 
 | \'3\' 
 | \'4\' 
 | \'5\' 
 | \'6\' 
 | \'7\' 
 | \'8\' 
 | \'9\' 
 | \'a\' 
 | \",'postfix_expression : postfix_expression LBRACKET expression RBRACKET'
"def available_oficial_plugins(): 
    return _availables_plugins(resources.PLUGINS_WEB)", 'Returns a list of available plugins','Returns a dict with OFICIAL availables plugins in NINJA-IDE web page'
"def can_introspect(field): 
    if (hasattr(field, '_south_introspects') and field._south_introspects): 
      return True 
   full_name = ('%s.%s' % (field.__class__.__module__, field.__class__.__name__)) 
   for regex in allowed_fields: 
      if re.match(regex, full_name): 
         return True 
   return False"," 'Returns True if field can be introspected, False otherwise.'","'Returns True if we are allowed to introspect this field, False otherwise. 
 (\'allowed\' means \'in core\'. Custom fields can declare they are introspectable 
 by the default South rules by adding the attribute _south_introspects = True.)'"
"def user_passes_test(test_func, login_url=LOGIN_URL): 
    def _dec(view_func): 
      def _checklogin(request, *args, **kwargs): 
         if test_func(request.user): 
            return view_func(request, *args, **kwargs) 
         return HttpResponseRedirect(('%s?%s=%s' % (login_url, REDIRECT_FIELD_NAME, quote(request.get_full_path())))) 
      _checklogin.__doc__ = view_func.__doc__ 
      _checklogin.__dict__ = view_func.__dict__ 
      return _checklogin 
   return _dec"," 'Decorator to check if a user is authenticated. 
 If the user is not authenticated, it will redirect the user to the login 
 page. 
 :param login_url: The URL to redirect to if the user is not authenticated. 
 :type login_url: str 
 :param test_func: A callable that takes a user and returns True if the 
 user is authenticated. 
 :type test_func: callable 
 :return: The decorated view function. 
 :rtype: function'","'Decorator for views that checks that the user passes the given test, 
 redirecting to the log-in page if necessary. The test should be a callable 
 that takes the user object and returns True if the user passes.'"
"@handle_response_format 
 @treeio_login_required 
 @_process_mass_form 
 def task_view(request, task_id, response_format='html'): 
    task = get_object_or_404(Task, pk=task_id) 
   if (not request.user.profile.has_permission(task)): 
      return user_denied(request, message=""You   don't   have   access   to   this   Task"") 
   if request.user.profile.has_permission(task, mode='x'): 
      if request.POST: 
         if ('add-work' in request.POST): 
            return HttpResponseRedirect(reverse('projects_task_time_slot_add', args=[task.id])) 
         elif ('start-work' in request.POST): 
            return HttpResponseRedirect(reverse('projects_task_view', args=[task.id])) 
         record = UpdateRecord() 
         record.record_type = 'manual' 
         form = TaskRecordForm(request.user.profile, request.POST, instance=record) 
         if form.is_valid(): 
            record = form.save() 
            record.set_user_from_request(request) 
            record.save() 
            record.about.add(task) 
            task.set_last_updated() 
            return HttpResponseRedirect(reverse('projects_task_view', args=[task.id])) 
      else: 
         form = TaskRecordForm(request.user.profile) 
   else: 
      form = None 
   subtasks = Object.filter_by_request(request, Task.objects.filter(parent=task)) 
   time_slots = Object.filter_by_request(request, TaskTimeSlot.objects.filter(task=task)) 
   context = _get_default_context(request) 
   context.update({'task': task, 'subtasks': subtasks, 'record_form': form, 'time_slots': time_slots}) 
   if (('massform' in context) and ('project' in context['massform'].fields)): 
      del context['massform'].fields['project'] 
   return render_to_response('projects/task_view', context, context_instance=RequestContext(request), response_format=response_format)"," 'View a task. 
 The user must have permission to view this task. 
 The user must be logged in. 
 The user must be a member of the project. 
 The user must have permission to view the project. 
 The user must have permission to view this task\'s subtasks. 
 The user must have permission to view this task\'s time slots. 
 The user must have permission to edit this task. 
 The user must have permission to edit this task\'s subtasks. 
 The user must have permission to edit this task\'s time slots. 
 The user must have permission to edit this task\'s subtasks\' time slots. 
 The user must have permission to edit this task\'s subtasks\' time slots\' time slots. 
 The user must have permission to edit this task\'s subtasks\' time slots\' time slots\' time slots. 
 The user must have permission to edit this task\'s subtasks\' time slots\' time slots\' time slots\' time slots. 
 The user must have permission to edit this task\'s subtasks\' time slots\' time slots\'",'Single task view page'
"def decodeString(string): 
    decodedString = string 
   octalNumbers = re.findall('\\\\([0-7]{1-3})', decodedString, re.DOTALL) 
   for octal in octalNumbers: 
      try: 
         decodedString = decodedString.replace(('\\\\' + octal), chr(int(octal, 8))) 
      except: 
         return ((-1), 'Error   decoding   string') 
   return (0, decodedString)"," 'Decodes a string from the octal number system. 
 :param string: The string to be decoded 
 :return: A tuple of the form (success, decodedString) 
 :rtype: tuple'","'Decode the given PDF string 
 @param string: A PDFString to decode 
 @return A tuple (status,statusContent), where statusContent is the decoded PDF string in case status = 0 or an error in case status = -1'"
"def checkRecursive(paths, reporter): 
    warnings = 0 
   for sourcePath in iterSourceCode(paths): 
      warnings += checkPath(sourcePath, reporter) 
   return warnings", 'Checks for recursive includes in the source code',"'Recursively check all source files in C{paths}. 
 @param paths: A list of paths to Python source files and directories 
 containing Python source files. 
 @param reporter: A L{Reporter} where all of the warnings and errors 
 will be reported to. 
 @return: The number of warnings found.'"
"def setup_platform(hass, config, add_devices, discovery_info=None): 
    add_devices([VlcDevice(config.get(CONF_NAME), config.get(CONF_ARGUMENTS))])", 'Setup a VLC device.','Setup the vlc platform.'
"def test_hsl_to_rgb_part_10(): 
    assert (hsl_to_rgb(180, 20, 50) == (102, 153, 153)) 
   assert (hsl_to_rgb(180, 60, 50) == (51, 204, 204)) 
   assert (hsl_to_rgb(180, 100, 50) == (0, 255, 255))", 'Test hsl to rgb color conversion','Test hsl to rgb color function'
"def paths_from_event(self, event): 
    md = event.mimeData() 
   if (md.hasFormat(u'text/uri-list') and (not md.hasFormat(u'application/calibre+from_library'))): 
      urls = [unicode(u.toLocalFile()) for u in md.urls()] 
      return [u for u in urls if (os.path.splitext(u)[1] and os.path.exists(u))]", 'Return paths to files from event.',"'Accept a drop event and return a list of paths that can be read from 
 and represent files with extensions.'"
"def f(t): 
    s1 = np.cos(((2 * np.pi) * t)) 
   e1 = np.exp((- t)) 
   return (s1 * e1)", 't is time in years','a damped exponential'
"def func_np(a, b): 
    return np.exp(((2.1 * a) + (3.2 * b)))"," 'A function with two inputs, a and b. 
 Parameters 
 a : float 
 b : float 
 Returns 
 np.exp(((2.1 * a) + (3.2 * b)))'",'Control function using Numpy.'
"def _parse_file(document_file, validate, entry_class, entry_keyword='r', start_position=None, end_position=None, section_end_keywords=(), extra_args=()): 
    if start_position: 
      document_file.seek(start_position) 
   else: 
      start_position = document_file.tell() 
   if section_end_keywords: 
      first_keyword = None 
      line_match = KEYWORD_LINE.match(stem.util.str_tools._to_unicode(document_file.readline())) 
      if line_match: 
         first_keyword = line_match.groups()[0] 
      document_file.seek(start_position) 
      if (first_keyword in section_end_keywords): 
         return 
   while ((end_position is None) or (document_file.tell() < end_position)): 
      (desc_lines, ending_keyword) = _read_until_keywords(((entry_keyword,) + section_end_keywords), document_file, ignore_first=True, end_position=end_position, include_ending_keyword=True) 
      desc_content = bytes.join('', desc_lines) 
      if desc_content: 
         (yield entry_class(desc_content, validate, *extra_args)) 
         if (ending_keyword in section_end_keywords): 
            break 
      else: 
         break"," 'Parses a file into entries. 
 :param document_file: file to parse 
 :param validate: function to validate the entry 
 :param entry_class: class to use for entries 
 :param entry_keyword: string to use for entry keywords 
 :param start_position: position to start parsing from 
 :param end_position: position to stop parsing at 
 :param section_end_keywords: list of section end keywords 
 :param extra_args: list of arguments to pass to the entry class 
 :return: list of entries'","'Reads a range of the document_file containing some number of entry_class 
 instances. We deliminate the entry_class entries by the keyword on their 
 first line (entry_keyword). When finished the document is left at the 
 end_position. 
 Either an end_position or section_end_keywords must be provided. 
 :param file document_file: file with network status document content 
 :param bool validate: checks the validity of the document\'s contents if 
 **True**, skips these checks otherwise 
 :param class entry_class: class to construct instance for 
 :param str entry_keyword: first keyword for the entry instances 
 :param int start_position: start of the section, default is the current position 
 :param int end_position: end of the section 
 :param tuple section_end_keywords: keyword(s) that deliminate the end of the 
 section if no end_position was provided 
 :param tuple extra_args: extra arguments for the entry_class (after the 
 content and validate flag) 
 :returns: iterator over entry_class instances 
 :raises: 
 * **ValueError** if the contents is malformed and validate is **True** 
 * **IOError** if the file can\'t be read'"
"@memoized 
 def flavor_list(request): 
    return novaclient(request).flavors.list()", 'Returns a list of flavors.','Get the list of available instance sizes (flavors).'
"def get_policy_string(base, policy_or_index): 
    if isinstance(policy_or_index, BaseStoragePolicy): 
      policy = policy_or_index 
   else: 
      policy = POLICIES.get_by_index(policy_or_index) 
      if (policy is None): 
         raise PolicyError('Unknown   policy', index=policy_or_index) 
   return _get_policy_string(base, int(policy))", 'Returns the policy string for the given policy or index.',"'Helper function to construct a string from a base and the policy. 
 Used to encode the policy index into either a file name or a 
 directory name by various modules. 
 :param base: the base string 
 :param policy_or_index: StoragePolicy instance, or an index 
 (string or int), if None the legacy 
 storage Policy-0 is assumed. 
 :returns: base name with policy index added 
 :raises: PolicyError if no policy exists with the given policy_index'"
"def upgrade(refresh=True): 
    ret = {'changes': {}, 'result': True, 'comment': ''} 
   old = list_pkgs() 
   if salt.utils.is_true(refresh): 
      refresh_db() 
   result = _call_brew('brew   upgrade', failhard=False) 
   __context__.pop('pkg.list_pkgs', None) 
   new = list_pkgs() 
   ret = salt.utils.compare_dicts(old, new) 
   if (result['retcode'] != 0): 
      raise CommandExecutionError('Problem   encountered   upgrading   packages', info={'changes': ret, 'result': result}) 
   return ret"," 'Upgrade all packages. 
 If refresh is True, then the database will be refreshed. 
 Returns: 
 changes: dict 
 A dictionary of changes from the old list of packages to the new list 
 of packages. 
 result: dict 
 A dictionary of results. 
 result[\'retcode\']: int 
 The return code from the brew upgrade command. 
 result[\'stdout\']: str 
 The output from the brew upgrade command. 
 result[\'stderr\']: str 
 The error output from the brew upgrade command. 
 result[\'comment\']: str 
 A comment about the results. 
 refresh: bool 
 If True, then the database will be refreshed.'","'Upgrade outdated, unpinned brews. 
 refresh 
 Fetch the newest version of Homebrew and all formulae from GitHub before installing. 
 Returns a dictionary containing the changes: 
 .. code-block:: python 
 {\'<package>\':  {\'old\': \'<old-version>\', 
 \'new\': \'<new-version>\'}} 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' pkg.upgrade'"
"def resolve(): 
    filename = '/'.join(request.args) 
   path = apath(filename, r=request) 
   a = safe_read(path).split('\n') 
   try: 
      b = safe_read((path + '.1')).split('\n') 
   except IOError: 
      session.flash = 'Other   file,   no   longer   there' 
      redirect(URL('edit', args=request.args)) 
   d = difflib.ndiff(a, b) 
   def leading(line): 
      '      ' 
      z = '' 
      for (k, c) in enumerate(line): 
         if (c == '   '): 
            z += '&nbsp;' 
         elif (c == '    DCTB '): 
            z += '&nbsp;' 
         elif ((k == 0) and (c == '?')): 
            pass 
         else: 
            break 
      return XML(z) 
   def getclass(item): 
      '   Determine   item   class   ' 
      operators = {'   ': 'normal', '+': 'plus', '-': 'minus'} 
      return operators[item[0]] 
   if request.vars: 
      c = '\n'.join([item[2:].rstrip() for (i, item) in enumerate(d) if ((item[0] == '   ') or (('line%i' % i) in request.vars))]) 
      safe_write(path, c) 
      session.flash = 'files   merged' 
      redirect(URL('edit', args=request.args)) 
   else: 
      gen_data = (lambda index, item: (((not (item[:1] in ['+', '-'])) and '') or INPUT(_type='checkbox', _name=('line%i' % index), value=(item[0] == '+')))) 
      diff = TABLE(*[TR(TD(gen_data(i, item)), TD(item[0]), TD(leading(item[2:]), TT(item[2:].rstrip())), _class=getclass(item)) for (i, item) in enumerate(d) if (item[0] != '?')]) 
   return dict(diff=diff, filename=filename)"," 'Merges two files, returns a dictionary of the diff'",''
"def getmode(mode): 
    global _modes 
   if (not _modes): 
      from . import Image 
      modes = {} 
      for (m, (basemode, basetype, bands)) in Image._MODEINFO.items(): 
         modes[m] = ModeDescriptor(m, bands, basemode, basetype) 
      modes['RGBa'] = ModeDescriptor('RGBa', ('R', 'G', 'B', 'a'), 'RGB', 'L') 
      modes['LA'] = ModeDescriptor('LA', ('L', 'A'), 'L', 'L') 
      modes['La'] = ModeDescriptor('La', ('L', 'a'), 'L', 'L') 
      modes['PA'] = ModeDescriptor('PA', ('P', 'A'), 'RGB', 'L') 
      modes['I;16'] = ModeDescriptor('I;16', 'I', 'L', 'L') 
      modes['I;16L'] = ModeDescriptor('I;16L', 'I', 'L', 'L') 
      modes['I;16B'] = ModeDescriptor('I;16B', 'I', 'L', 'L') 
      _modes = modes 
   return _modes[mode]"," 'Returns a ModeDescriptor for the given mode. 
 :param mode: The mode to return a descriptor for 
 :type mode: str 
 :returns: A ModeDescriptor object for the given mode'",'Gets a mode descriptor for the given mode.'
"def _save_and_restart(plugin, title=None): 
    try: 
      plugin.save(title) 
      plugin.restart() 
      return True 
   except le_errors.Error as error: 
      logger.error('Plugin   failed   to   save   and   restart   server:') 
      logger.exception(error) 
      return False", 'Save and restart the plugin.',"'Saves and restart the plugin, returning True if no errors occurred'"
"def det_perm(M): 
    args = [] 
   s = True 
   n = M.rows 
   try: 
      list = M._mat 
   except AttributeError: 
      list = flatten(M.tolist()) 
   for perm in generate_bell(n): 
      fac = [] 
      idx = 0 
      for j in perm: 
         fac.append(list[(idx + j)]) 
         idx += n 
      term = Mul(*fac) 
      args.append((term if s else (- term))) 
      s = (not s) 
   return Add(*args)"," 'Return determinant of a permutation matrix. 
 Examples 
 >>> from sympy.matrices.dsolve import det_perm 
 >>> from sympy import Permutation 
 >>> Permutation(1, 2, 3, 4).perm_to_list() 
 [1, 2, 3, 4] 
 >>> det_perm(Permutation(1, 2, 3, 4).perm_to_matrix()) 
 1 
 >>> Permutation(1, 2, 3, 4).perm_to_list() 
 [1, 2, 3, 4] 
 >>> det_perm(Permutation(1, 2, 3, 4).perm_to_matrix()) 
 -1'","'Return the det(``M``) by using permutations to select factors. 
 For size larger than 8 the number of permutations becomes prohibitively 
 large, or if there are no symbols in the matrix, it is better to use the 
 standard determinant routines, e.g. `M.det()`. 
 See Also 
 det_minor 
 det_quick'"
"def config_validator(user): 
    from hadoop import job_tracker 
   from hadoop.fs import webhdfs 
   res = [] 
   submit_to = [] 
   has_default = False 
   for name in HDFS_CLUSTERS.keys(): 
      cluster = HDFS_CLUSTERS[name] 
      res.extend(webhdfs.test_fs_configuration(cluster)) 
      if (name == 'default'): 
         has_default = True 
   if (not has_default): 
      res.append(('hadoop.hdfs_clusters', ""You   should   have   an   HDFS   called   'default'."")) 
   mr_down = [] 
   for name in MR_CLUSTERS.keys(): 
      cluster = MR_CLUSTERS[name] 
      if cluster.SUBMIT_TO.get(): 
         mr_down.extend(job_tracker.test_jt_configuration(cluster)) 
         submit_to.append(('mapred_clusters.' + name)) 
   if (mr_down and (len(mr_down) == len(MR_CLUSTERS.keys()))): 
      res.extend(mr_down) 
   if YARN_CLUSTERS.keys(): 
      res.extend(test_yarn_configurations(user)) 
   for name in YARN_CLUSTERS.keys(): 
      cluster = YARN_CLUSTERS[name] 
      if cluster.SUBMIT_TO.get(): 
         submit_to.append(('yarn_clusters.' + name)) 
   if (not submit_to): 
      res.append(('hadoop', ""Please   designate   one   of   the   MapReduce   or   Yarn   clusters   with   `submit_to=true'   in   order   to   run   jobs."")) 
   return res"," 'Validate the configuration of the cluster. 
 :param user: the user running the cluster. 
 :returns: a list of errors.'","'config_validator() -> [ (config_variable, error_message) ] 
 Called by core check_config() view.'"
"def get_group(name, match_obj): 
    try: 
      obj = match_obj.group(name) 
   except: 
      return '' 
   else: 
      if (obj is not None): 
         return obj 
      else: 
         return ''"," 'Returns the group with the given name, or an empty string if not found.'",'return a blank string if the match group is None'
"def salted_hmac(key_salt, value, secret=None): 
    if (secret is None): 
      secret = settings.SECRET_KEY 
   key_salt = force_bytes(key_salt) 
   secret = force_bytes(secret) 
   key = hashlib.sha1((key_salt + secret)).digest() 
   return hmac.new(key, msg=force_bytes(value), digestmod=hashlib.sha1)"," 'Generates a HMAC-SHA1 signature of the given value using the given key 
 salt and secret. 
 The key salt is a random string used to compute the key for the HMAC. 
 The secret is a string used to compute the key for the HMAC. 
 The value is the value to sign. 
 The secret and key salt are stored in the session and used to compute the 
 signature. 
 This method is used to sign the ``csrf_token`` in the request. 
 :param key_salt: The key salt used to compute the key for the HMAC. 
 :type key_salt: str 
 :param value: The value to sign. 
 :type value: str 
 :param secret: The secret used to compute the key for the HMAC. 
 :type secret: str 
 :returns: A string containing the HMAC signature. 
 :rtype: str'","'Returns the HMAC-SHA1 of \'value\', using a key generated from key_salt and a 
 secret (which defaults to settings.SECRET_KEY). 
 A different key_salt should be passed in for every application of HMAC.'"
"def etc_hosts(attrs=None, where=None): 
    return _osquery_cmd(table='etc_hosts', attrs=attrs, where=where)"," 'Returns a list of all hostnames found in /etc/hosts. 
 This is a read-only table, and is only populated if the OS has a 
 /etc/hosts file. 
 :param attrs: A dict of attributes to return. 
 :param where: A dict of WHERE clauses to apply. 
 :returns: A list of dictionaries containing the results.'","'Return etc_hosts information from osquery 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' osquery.etc_hosts'"
"def skip_if_config(*args): 
    def decorator(f): 
      group = args[0] 
      name = args[1] 
      @functools.wraps(f) 
      def wrapper(self, *func_args, **func_kwargs): 
         if hasattr(CONF, group): 
            conf_group = getattr(CONF, group) 
            if hasattr(conf_group, name): 
               value = getattr(conf_group, name) 
               if value: 
                  if (len(args) == 3): 
                     msg = args[2] 
                  else: 
                     msg = ('Config   option   %s.%s   is   false' % (group, name)) 
                  raise testtools.TestCase.skipException(msg) 
         return f(self, *func_args, **func_kwargs) 
      return wrapper 
   return decorator"," 'Skip test if the config option is false. 
 :param group: config group name 
 :param name: config option name 
 :param args: (optional) additional args to be passed to the decorator 
 :return: decorator function'","'Raise a skipException if a config exists and is True 
 :param str group: The first arg, the option group to check 
 :param str name: The second arg, the option name to check 
 :param str msg: Optional third arg, the skip msg to use if a skip is raised 
 :raises testtools.TestCase.skipException: If the specified config option 
 exists and evaluates to True'"
"def lasso_path(X, y, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, **params): 
    return enet_path(X, y, l1_ratio=1.0, eps=eps, n_alphas=n_alphas, alphas=alphas, precompute=precompute, Xy=Xy, copy_X=copy_X, coef_init=coef_init, verbose=verbose, positive=positive, return_n_iter=return_n_iter, **params)"," 'Lasso path (lasso-path) 
 Lasso path is a path in the l1-ratio parameter space. 
 Parameters 
 X : array-like 
 Input data. 
 y : array-like 
 Target values. 
 eps : float, optional 
 Epsilon for the stopping criterion. 
 n_alphas : int, optional 
 Number of alphas to evaluate. 
 alphas : array-like, optional 
 Alphas to evaluate. 
 precompute : \'auto\', \'fast\', \'fast_coef\', \'fast_X\', optional 
 The precomputation strategy. 
 Xy : array-like, optional 
 If provided, Xy is used instead of X and y. 
 copy_X : bool, optional 
 If True, X is returned. 
 coef_init : array-like, optional 
 If provided, coef_init is used instead of the default value. 
 verbose : bool, optional 
 If True, the number of iterations is printed. 
 return_n_iter : bool, optional 
 If True,","'Compute Lasso path with coordinate descent 
 The Lasso optimization function varies for mono and multi-outputs. 
 For mono-output tasks it is:: 
 (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1 
 For multi-output tasks it is:: 
 (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21 
 Where:: 
 ||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2} 
 i.e. the sum of norm of each row. 
 Read more in the :ref:`User Guide <lasso>`. 
 Parameters 
 X : {array-like, sparse matrix}, shape (n_samples, n_features) 
 Training data. Pass directly as Fortran-contiguous data to avoid 
 unnecessary memory duplication. If ``y`` is mono-output then ``X`` 
 can be sparse. 
 y : ndarray, shape (n_samples,), or (n_samples, n_outputs) 
 Target values 
 eps : float, optional 
 Length of the path. ``eps=1e-3`` means that 
 ``alpha_min / alpha_max = 1e-3`` 
 n_alphas : int, optional 
 Number of alphas along the regularization path 
 alphas : ndarray, optional 
 List of alphas where to compute the models. 
 If ``None`` alphas are set automatically 
 precompute : True | False | \'auto\' | array-like 
 Whether to use a precomputed Gram matrix to speed up 
 calculations. If set to ``\'auto\'`` let us decide. The Gram 
 matrix can also be passed as argument. 
 Xy : array-like, optional 
 Xy = np.dot(X.T, y) that can be precomputed. It is useful 
 only when the Gram matrix is precomputed. 
 copy_X : boolean, optional, default True 
 If ``True``, X will be copied; else, it may be overwritten. 
 coef_init : array, shape (n_features, ) | None 
 The initial values of the coefficients. 
 verbose : bool or integer 
 Amount of verbosity. 
 params : kwargs 
 keyword arguments passed to the coordinate descent solver. 
 positive : bool, default False 
 If set to True, forces coefficients to be positive. 
 return_n_iter : bool 
 whether to return the number of iterations or not. 
 Returns 
 alphas : array, shape (n_alphas,) 
 The alphas along the path where models are computed. 
 coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas) 
 Coefficients along the path. 
 dual_gaps : array, shape (n_alphas,) 
 The dual gaps at the end of the optimization for each alpha. 
 n_iters : array-like, shape (n_alphas,) 
 The number of iterations taken by the coordinate descent optimizer to 
 reach the specified tolerance for each alpha. 
 Notes 
 See examples/linear_model/plot_lasso_coordinate_descent_path.py 
 for an example. 
 To avoid unnecessary memory duplication the X argument of the fit method 
 should be directly passed as a Fortran-contiguous numpy array. 
 Note that in certain cases, the Lars solver may be significantly 
 faster to implement this functionality. In particular, linear 
 interpolation can be used to retrieve model coefficients between the 
 values output by lars_path 
 Examples 
 Comparing lasso_path and lars_path with interpolation: 
 >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T 
 >>> y = np.array([1, 2, 3.1]) 
 >>> # Use lasso_path to compute a coefficient path 
 >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5]) 
 >>> print(coef_path) 
 [[ 0.          0.          0.46874778] 
 [ 0.2159048   0.4425765   0.23689075]] 
 >>> # Now use lars_path and 1D linear interpolation to compute the 
 >>> # same path 
 >>> from sklearn.linear_model import lars_path 
 >>> alphas, active, coef_path_lars = lars_path(X, y, method=\'lasso\') 
 >>> from scipy import interpolate 
 >>> coef_path_continuous = interpolate.interp1d(alphas[::-1], 
 ...                                             coef_path_lars[:, ::-1]) 
 >>> print(coef_path_continuous([5., 1., .5])) 
 [[ 0.          0.          0.46915237] 
 [ 0.2159048   0.4425765   0.23668876]] 
 See also 
 lars_path 
 Lasso 
 LassoLars 
 LassoCV 
 LassoLarsCV 
 sklearn.decomposition.sparse_encode'"
"def available_modules(): 
    extra_service_versions = set([]) 
   _tempest_modules = set(tempest_modules()) 
   plugin_services = ClientsRegistry().get_service_clients() 
   for plugin_name in plugin_services: 
      plug_service_versions = set([x['service_version'] for x in plugin_services[plugin_name]]) 
      if plug_service_versions: 
         if (not plug_service_versions.isdisjoint(extra_service_versions)): 
            detailed_error = ('Plugin   %s   is   trying   to   register   a   service   %s   already   claimed   by   another   one' % (plugin_name, (extra_service_versions & plug_service_versions))) 
            raise exceptions.PluginRegistrationException(name=plugin_name, detailed_error=detailed_error) 
         if (not plug_service_versions.isdisjoint(_tempest_internal_modules())): 
            detailed_error = ('Plugin   %s   is   trying   to   register   a   service   %s   already   claimed   by   a   Tempest   one' % (plugin_name, (_tempest_internal_modules() & plug_service_versions))) 
            raise exceptions.PluginRegistrationException(name=plugin_name, detailed_error=detailed_error) 
      extra_service_versions |= plug_service_versions 
   return (_tempest_modules | extra_service_versions)", 'Returns a list of all the available plugins.',"'Set of service client modules available in Tempest and plugins 
 Set of stable service clients from Tempest and service clients exposed 
 by plugins. This set of available modules can be used for automatic 
 configuration. 
 :raise PluginRegistrationException: if a plugin exposes a service_version 
 already defined by Tempest or another plugin. 
 Examples: 
 >>> from tempest import config 
 >>> params = {} 
 >>> for service_version in available_modules(): 
 >>>     service = service_version.split(\'.\')[0] 
 >>>     params[service] = config.service_client_config(service) 
 >>> service_clients = ServiceClients(creds, identity_uri, 
 >>>                                  client_parameters=params)'"
"def print_exc(limit=None, file=None): 
    if (file is None): 
      file = sys.stderr 
   try: 
      (etype, value, tb) = sys.exc_info() 
      print_exception(etype, value, tb, limit, file) 
   finally: 
      etype = value = tb = None"," 'Print a stack trace to the file, if the limit is exceeded, print to sys.stderr.'","'Shorthand for \'print_exception(sys.exc_type, sys.exc_value, sys.exc_traceback, limit, file)\'. 
 (In fact, it uses sys.exc_info() to retrieve the same information 
 in a thread-safe way.)'"
"def lombscargle_fastchi2(t, y, dy, f0, df, Nf, normalization='standard', fit_mean=True, center_data=True, nterms=1, use_fft=True, trig_sum_kwds=None): 
    if ((nterms == 0) and (not fit_mean)): 
      raise ValueError('Cannot   have   nterms   =   0   without   fitting   bias') 
   if (dy is None): 
      dy = 1 
   (t, y, dy) = np.broadcast_arrays(t, y, dy) 
   if (t.ndim != 1): 
      raise ValueError('t,   y,   dy   should   be   one   dimensional') 
   if (f0 < 0): 
      raise ValueError('Frequencies   must   be   positive') 
   if (df <= 0): 
      raise ValueError('Frequency   steps   must   be   positive') 
   if (Nf <= 0): 
      raise ValueError('Number   of   frequencies   must   be   positive') 
   w = (dy ** (-2.0)) 
   ws = np.sum(w) 
   if (center_data or fit_mean): 
      y = (y - (np.dot(w, y) / ws)) 
   yw = (y / dy) 
   chi2_ref = np.dot(yw, yw) 
   kwargs = dict.copy((trig_sum_kwds or {})) 
   kwargs.update(f0=f0, df=df, use_fft=use_fft, N=Nf) 
   yws = np.sum((y * w)) 
   SCw = [(np.zeros(Nf), (ws * np.ones(Nf)))] 
   SCw.extend([trig_sum(t, w, freq_factor=i, **kwargs) for i in range(1, ((2 * nterms) + 1))]) 
   (Sw, Cw) = zip(*SCw) 
   SCyw = [(np.zeros(Nf), (yws * np.ones(Nf)))] 
   SCyw.extend([trig_sum(t, (w * y), freq_factor=i, **kwargs) for i in range(1, (nterms + 1))]) 
   (Syw, Cyw) = zip(*SCyw) 
   order = ([('C', 0)] if fit_mean else []) 
   order.extend(sum([[('S', i), ('C', i)] for i in range(1, (nterms + 1))], [])) 
   funcs = dict(S=(lambda m, i: Syw[m][i]), C=(lambda m, i: Cyw[m][i]), SS=(lambda m, n, i: (0.5 * (Cw[abs((m - n))][i] - Cw[(m + n)][i]))), CC=(lambda m, n, i: (0.5 * (Cw[abs((m - n))][i] + Cw[(m + n)][i]))), SC=(lambda m, n, i: (0.5 * ((np.sign((m - n)) * Sw[abs((m - n))][i]) + Sw[(m + n)][i]))), CS=(lambda m, n, i: (0.5 * ((np.sign((n - m)) * Sw[abs((n - m))][i]) + Sw[(n + m)][i])))) 
   def compute_power(i): 
      XTX = np.array([[funcs[(A[0] + B[0])](A[1], B[1], i) for A in order] for B in order]) 
      XTy = np.array([funcs[A[0]](A[1], i) for A in order]) 
      return np.dot(XTy.T, np.linalg.solve(XTX, XTy)) 
   p = np.array([compute_power(i) for i in range(Nf)]) 
   if (normalization == 'psd'): 
      p *= 0.5 
   elif (normalization == 'standard'): 
      p /= chi2_ref 
   elif (normalization == 'log'): 
      p = (- np.log((1 - (p / chi2_ref)))) 
   elif (normalization == 'model'): 
      p /= (chi2_ref - p) 
   else: 
      raise ValueError(""normalization='{0}'   not   recognized"".format(normalization)) 
   return p"," 'Compute Lomb-Scargle power spectrum of a time series. 
 Parameters 
 t : 1D array 
 Time array. 
 y : 1D array 
 Time series. 
 dy : 1D array 
 Time step. 
 f0 : float 
 Frequency of the reference signal. 
 df : float 
 Frequency step. 
 Nf : int 
 Number of frequencies. 
 normalization : \'psd\', \'standard\', \'log\', \'model\' 
 The normalization of the power spectrum. 
 fit_mean : bool 
 Whether to fit a mean to the time series. 
 center_data : bool 
 Whether to center the time series. 
 nterms : int 
 The number of terms in the Lomb-Scargle decomposition. 
 use_fft : bool 
 Whether to use the FFT for the computation. 
 trig_sum_kwds : dict 
 Additional keyword arguments to pass to the trigonometric sum. 
 Returns 
 p : 1D array 
 Power spectrum. 
 Examples 
 >>>","'Lomb-Scargle Periodogram 
 This implements a fast chi-squared periodogram using the algorithm 
 outlined in [4]_. The result is identical to the standard Lomb-Scargle 
 periodogram. The advantage of this algorithm is the 
 ability to compute multiterm periodograms relatively quickly. 
 Parameters 
 t, y, dy : array_like  (NOT astropy.Quantities) 
 times, values, and errors of the data points. These should be 
 broadcastable to the same shape. 
 f0, df, Nf : (float, float, int) 
 parameters describing the frequency grid, f = f0 + df * arange(Nf). 
 normalization : string (optional, default=\'standard\') 
 Normalization to use for the periodogram. 
 Options are \'standard\', \'model\', \'log\', or \'psd\'. 
 fit_mean : bool (optional, default=True) 
 if True, include a constant offset as part of the model at each 
 frequency. This can lead to more accurate results, especially in the 
 case of incomplete phase coverage. 
 center_data : bool (optional, default=True) 
 if True, pre-center the data by subtracting the weighted mean 
 of the input data. This is especially important if ``fit_mean = False`` 
 nterms : int (optional, default=1) 
 Number of Fourier terms in the fit 
 Returns 
 power : array_like 
 Lomb-Scargle power associated with each frequency. 
 Units of the result depend on the normalization. 
 References 
 .. [1] M. Zechmeister and M. Kurster, A&A 496, 577-584 (2009) 
 .. [2] W. Press et al, Numerical Recipies in C (2002) 
 .. [3] Scargle, J.D. ApJ 263:835-853 (1982) 
 .. [4] Palmer, J. ApJ 695:496-502 (2009)'"
"def test_BoundaryNorm(): 
    boundaries = [0, 1.1, 2.2] 
   vals = [(-1), 0, 1, 2, 2.2, 4] 
   expected = [(-1), 0, 0, 1, 2, 2] 
   ncolors = (len(boundaries) - 1) 
   bn = mcolors.BoundaryNorm(boundaries, ncolors) 
   assert_array_equal(bn(vals), expected) 
   expected = [(-1), 0, 0, 2, 3, 3] 
   ncolors = len(boundaries) 
   bn = mcolors.BoundaryNorm(boundaries, ncolors) 
   assert_array_equal(bn(vals), expected) 
   boundaries = [0, 1, 2, 3] 
   vals = [(-1), 0.1, 1.1, 2.2, 4] 
   ncolors = 5 
   expected = [(-1), 0, 2, 4, 5] 
   bn = mcolors.BoundaryNorm(boundaries, ncolors) 
   assert_array_equal(bn(vals), expected) 
   boundaries = [0, 1, 2] 
   vals = [(-1), 0.1, 1.1, 2.2] 
   bn = mcolors.BoundaryNorm(boundaries, 2) 
   expected = [(-1), 0, 1, 2] 
   for (v, ex) in zip(vals, expected): 
      ret = bn(v) 
      assert isinstance(ret, six.integer_types) 
      assert_array_equal(ret, ex) 
      assert_array_equal(bn([v]), ex) 
   bn = mcolors.BoundaryNorm(boundaries, 3) 
   expected = [(-1), 0, 2, 3] 
   for (v, ex) in zip(vals, expected): 
      ret = bn(v) 
      assert isinstance(ret, six.integer_types) 
      assert_array_equal(ret, ex) 
      assert_array_equal(bn([v]), ex) 
   bn = mcolors.BoundaryNorm(boundaries, 3, clip=True) 
   expected = [0, 0, 2, 2] 
   for (v, ex) in zip(vals, expected): 
      ret = bn(v) 
      assert isinstance(ret, six.integer_types) 
      assert_array_equal(ret, ex) 
      assert_array_equal(bn([v]), ex) 
   boundaries = [0, 1.1, 2.2] 
   vals = np.ma.masked_invalid([(-1.0), np.NaN, 0, 1.4, 9]) 
   ncolors = (len(boundaries) - 1) 
   bn = mcolors.BoundaryNorm(boundaries, ncolors) 
   expected = np.ma.masked_array([(-1), (-99), 0, 1, 2], mask=[0, 1, 0, 0, 0]) 
   assert_array_equal(bn(vals), expected) 
   bn = mcolors.BoundaryNorm(boundaries, len(boundaries)) 
   expected = np.ma.masked_array([(-1), (-99), 0, 2, 3], mask=[0, 1, 0, 0, 0]) 
   assert_array_equal(bn(vals), expected) 
   vals = np.ma.masked_invalid([np.Inf, np.NaN]) 
   assert np.all(bn(vals).mask) 
   vals = np.ma.masked_invalid([np.Inf]) 
   assert np.all(bn(vals).mask)", 'Test the BoundaryNorm class',"'Github issue #1258: interpolation was failing with numpy 
 1.7 pre-release.'"
"@require_context 
 def instance_create(context, values): 
    values = values.copy() 
   values['metadata'] = _metadata_refs(values.get('metadata'), models.InstanceMetadata) 
   values['system_metadata'] = _metadata_refs(values.get('system_metadata'), models.InstanceSystemMetadata) 
   instance_ref = models.Instance() 
   if (not values.get('uuid')): 
      values['uuid'] = str(uuid.uuid4()) 
   instance_ref['info_cache'] = models.InstanceInfoCache() 
   info_cache = values.pop('info_cache', None) 
   if (info_cache is not None): 
      instance_ref['info_cache'].update(info_cache) 
   security_groups = values.pop('security_groups', []) 
   instance_ref.update(values) 
   def _get_sec_group_models(session, security_groups): 
      models = [] 
      (_existed, default_group) = security_group_ensure_default(context, session=session) 
      if ('default' in security_groups): 
         models.append(default_group) 
         security_groups = [x for x in security_groups if (x != 'default')] 
      if security_groups: 
         models.extend(_security_group_get_by_names(context, session, context.project_id, security_groups)) 
      return models 
   session = get_session() 
   with session.begin(): 
      if ('hostname' in values): 
         _validate_unique_server_name(context, session, values['hostname']) 
      instance_ref.security_groups = _get_sec_group_models(session, security_groups) 
      instance_ref.save(session=session) 
   ec2_instance_create(context, instance_ref['uuid']) 
   return instance_ref"," 'Create an instance. 
 :param values: The instance values to create. 
 :returns: A dictionary of the created instance.'","'Create a new Instance record in the database. 
 context - request context object 
 values - dict containing column values.'"
"def validate_ok_for_update(update): 
    validate_is_mapping('update', update) 
   if (not update): 
      raise ValueError('update   only   works   with   $   operators') 
   first = next(iter(update)) 
   if (not first.startswith('$')): 
      raise ValueError('update   only   works   with   $   operators')"," 'Validates that the update is a valid mapping. 
 :param update: A mapping to be validated. 
 :raises: ValueError if the update is not a valid mapping.'",'Validate an update document.'
"def find_prepositions(chunked): 
    for ch in chunked: 
      ch.append(u'O') 
   for (i, chunk) in enumerate(chunked): 
      if (chunk[2].endswith(u'PP') and (chunk[(-1)] == u'O')): 
         if ((i < (len(chunked) - 1)) and (chunked[(i + 1)][2].endswith((u'NP', u'PP')) or (chunked[(i + 1)][1] in (u'VBG', u'VBN')))): 
            chunk[(-1)] = u'B-PNP' 
            pp = True 
            for ch in chunked[(i + 1):]: 
               if (not (ch[2].endswith((u'NP', u'PP')) or (ch[1] in (u'VBG', u'VBN')))): 
                  break 
               if (ch[2].endswith(u'PP') and pp): 
                  ch[(-1)] = u'I-PNP' 
               if (not ch[2].endswith(u'PP')): 
                  ch[(-1)] = u'I-PNP' 
                  pp = False 
   return chunked"," 'Removes prepositions from the chunked list. 
 Parameters 
 chunked : list of (word, lemma, pos) tuples 
 Returns 
 chunked : list of (word, lemma, pos) tuples 
 :rtype: list'","'The input is a list of [token, tag, chunk]-items. 
 The output is a list of [token, tag, chunk, preposition]-items. 
 PP-chunks followed by NP-chunks make up a PNP-chunk.'"
"def eval(expression, _dict={}, **kw): 
    args = ops.copy() 
   args.update(_dict) 
   args.update(kw) 
   for (k, v) in list(args.items()): 
      if hasattr(v, 'im'): 
         args[k] = _Operand(v) 
   out = builtins.eval(expression, args) 
   try: 
      return out.im 
   except AttributeError: 
      return out"," 'Evaluate an expression and return the result. 
 This function will evaluate an expression and return the result. 
 The expression can be a string or a Python expression. 
 Examples 
 >>> from sympy.physics.engines import Engines 
 >>> from sympy.physics.engines.engines import eval 
 >>> import sympy 
 >>> import numpy 
 >>> import matplotlib 
 >>> import matplotlib.pyplot 
 >>> import matplotlib.cm 
 >>> import matplotlib.colors 
 >>> import matplotlib.ticker 
 >>> import matplotlib.collections 
 >>> import matplotlib.patches 
 >>> import matplotlib.transforms 
 >>> import matplotlib.transforms.Affine2D 
 >>> import matplotlib.transforms.Transform 
 >>> import matplotlib.transforms.Transformer 
 >>> import matplotlib.transforms.TransformList 
 >>> import matplotlib.transforms.TransformState 
 >>> import matplotlib.collections.PatchCollection 
 >>> import matplotlib.collections.PatchCollection as PatchCollection 
 >>> import matplotlib.collections.LineCollection 
 >>> import matplotlib.collections.LineCollection as LineCollection 
 >>> import matplotlib.collections.PathCollection ","'Evaluates an image expression. 
 :param expression: A string containing a Python-style expression. 
 :param options: Values to add to the evaluation context.  You 
 can either use a dictionary, or one or more keyword 
 arguments. 
 :return: The evaluated expression. This is usually an image object, but can 
 also be an integer, a floating point value, or a pixel tuple, 
 depending on the expression.'"
"def evaluate(op, op_str, a, b, raise_on_error=False, use_numexpr=True, **eval_kwargs): 
    use_numexpr = (use_numexpr and _bool_arith_check(op_str, a, b)) 
   if use_numexpr: 
      return _evaluate(op, op_str, a, b, raise_on_error=raise_on_error, **eval_kwargs) 
   return _evaluate_standard(op, op_str, a, b, raise_on_error=raise_on_error)"," 'Evaluate the operation \'op\' on the operands \'a\' and \'b\'. 
 Parameters 
 op : str 
 The operation to evaluate. 
 op_str : str 
 The string representation of the operation. 
 a : str 
 The first operand. 
 b : str 
 The second operand. 
 raise_on_error : bool 
 If True, raise an exception if the operation cannot be evaluated. 
 use_numexpr : bool 
 If True, use the ``numexpr`` library to evaluate the operation. 
 **eval_kwargs : dict 
 Keyword arguments to pass to ``eval``. 
 Returns 
 result : str 
 The result of the operation.'","'evaluate and return the expression of the op on a and b 
 Parameters 
 op :    the actual operand 
 op_str: the string version of the op 
 a :     left operand 
 b :     right operand 
 raise_on_error : pass the error to the higher level if indicated 
 (default is False), otherwise evaluate the op with and 
 return the results 
 use_numexpr : whether to try to use numexpr (default True)'"
"def gzip_file(source_path, archive_path): 
    with gzip_open(archive_path, 'wb') as archive: 
      if os.path.isfile(source_path): 
         with open(source_path, 'rb') as source: 
            copyfileobj(source, archive)", 'Gzip the given file and write it to the archive path.',"'Create a gzip compressed archive of ``source_path`` at ``archive_path``. 
 An empty archive file will be created if the source file does not exist. 
 This gives the diagnostic archive a consistent set of files which can 
 easily be tested.'"
"def _get_mask(X, value_to_mask): 
    if ((value_to_mask == 'NaN') or np.isnan(value_to_mask)): 
      return np.isnan(X) 
   else: 
      return (X == value_to_mask)"," 'Returns a boolean mask for the values in X that match the value_to_mask. 
 Parameters 
 X : ndarray 
 Input array. 
 value_to_mask : string, float or bool 
 Value to mask. If string, it must be the name of a column in X. 
 If float, it must be in the range [0, 1]. 
 Returns 
 mask : ndarray 
 Boolean mask. 
 Examples 
 >>> X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) 
 >>> mask = _get_mask(X, 5) 
 >>> mask 
 array([[False,  True,  True], 
 [False,  True,  True], 
 [False,  True,  True]], dtype=bool)'",'Compute the boolean mask X == missing_values.'
"def benchmark_relu(): 
    x = T.ftensor4('inputs') 
   ops = [relu_(x).sum(), relu(x).sum(), relu__(x).sum(), T.grad(relu_(x).sum(), x), T.grad(relu(x).sum(), x), T.grad(relu__(x).sum(), x)] 
   names = ['fprop_old', 'fprop_alter', 'fprop_alter2', 'grad_old', 'grad_alter', 'grad_alter2'] 
   value = numpy.random.uniform(size=(512, 32, 32, 100)).astype(floatX) 
   times = [] 
   for (op, name) in zip(ops, names): 
      f = theano.function(inputs=[x], outputs=op, name=name) 
      n_loops = 10 
      t0 = time.time() 
      for i in range(n_loops): 
         f(value) 
      t1 = time.time() 
      benchmark = (t1 - t0) 
      times.append(benchmark) 
      print(name) 
      theano.printing.debugprint(f, print_type=True) 
   print(names) 
   print(times)", 'Benchmark relu function',"'Benchmark the speed of different relu implementations. 
 Both fprop and grad are tested.'"
"def load_parser_result_store(package_dirpath, open_for_write=False): 
    open_flag = ((open_for_write and 'c') or 'r') 
   sto_filepath = path.join(package_dirpath, PARSER_RESULT_STORE) 
   return shelve_open(sto_filepath, flag=open_flag)"," 'Loads the parser result store. 
 :param package_dirpath: The directory path to the package. 
 :param open_for_write: If True, the store will be opened for writing. 
 :return: The parser result store.'","'Load parser result store from specified scenario package. 
 Args: 
 package_dirpath: str; Path to scenario package directory. 
 open_for_write: bool; Open store for writing. 
 Returns: 
 shelve.DbfilenameShelf; Looks and acts like a dict'"
"def colname(colx): 
    alphabet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' 
   if (colx <= 25): 
      return alphabet[colx] 
   else: 
      (xdiv26, xmod26) = divmod(colx, 26) 
      return (alphabet[(xdiv26 - 1)] + alphabet[xmod26])"," 'Return the column name of the given column number. 
 The column number can be 0 to 25, or 26*x to 26*x+25. 
 The column name is the first letter of the column number. 
 If the column number is 26*x, the column name is the first 2 letters of 
 the column number. 
 Examples 
 >>> colname(0) 
 \'A\' 
 >>> colname(26) 
 \'Z\' 
 >>> colname(26*2) 
 \'AB\' 
 >>> colname(26*2+1) 
 \'AC\' 
 >>> colname(26*2+2) 
 \'AD\' 
 >>> colname(26*2+3) 
 \'AE\' 
 >>> colname(26*2+4) 
 \'AF\' 
 >>> colname(26*2+5) 
 \'AG\' 
 >>> colname(26*2+6) 
 \'AH\' 
 >>> colname(2","'Utility function: ``7`` => ``\'H\'``, ``27`` => ``\'AB\'``'"
"def validate_bool_maybe_none(b): 
    if isinstance(b, six.string_types): 
      b = b.lower() 
   if ((b is None) or (b == u'none')): 
      return None 
   if (b in (u't', u'y', u'yes', u'on', u'true', u'1', 1, True)): 
      return True 
   elif (b in (u'f', u'n', u'no', u'off', u'false', u'0', 0, False)): 
      return False 
   else: 
      raise ValueError((u'Could   not   convert   ""%s""   to   boolean' % b))"," 'Validate a value for a boolean field. 
 :param b: The value to validate. 
 :type b: str, bool, None 
 :returns: The validated value. 
 :rtype: bool'",'Convert b to a boolean or raise'
"def seek_wrapped_response(response): 
    if (not hasattr(response, 'seek')): 
      wrapper_class = get_seek_wrapper_class(response) 
      response = wrapper_class(response) 
   assert hasattr(response, 'get_data') 
   return response"," 'Seek a response object to the first byte of the body. 
 :param response: A response object. 
 :return: The wrapped response object.'","'Return a copy of response that supports seekable response interface. 
 Accepts responses from both mechanize and urllib2 handlers. 
 Copes with both oridinary response instances and HTTPError instances (which 
 can\'t be simply wrapped due to the requirement of preserving the exception 
 base class).'"
"def load_module(name, file, filename, details): 
    (suffix, mode, type_) = details 
   if (mode and ((not mode.startswith(('r', 'U'))) or ('+' in mode))): 
      raise ValueError('invalid   file   open   mode   {!r}'.format(mode)) 
   elif ((file is None) and (type_ in {PY_SOURCE, PY_COMPILED})): 
      msg = 'file   object   required   for   import   (type   code   {})'.format(type_) 
      raise ValueError(msg) 
   elif (type_ == PY_SOURCE): 
      return load_source(name, filename, file) 
   elif (type_ == PY_COMPILED): 
      return load_compiled(name, filename, file) 
   elif ((type_ == C_EXTENSION) and (load_dynamic is not None)): 
      if (file is None): 
         with open(filename, 'rb') as opened_file: 
            return load_dynamic(name, filename, opened_file) 
      else: 
         return load_dynamic(name, filename, file) 
   elif (type_ == PKG_DIRECTORY): 
      return load_package(name, filename) 
   elif (type_ == C_BUILTIN): 
      return init_builtin(name) 
   elif (type_ == PY_FROZEN): 
      return init_frozen(name) 
   else: 
      msg = ""Don't   know   how   to   import   {}   (type   code   {})"".format(name, type_) 
      raise ImportError(msg, name=name)"," 'Load a module from a file. 
 This is the main function for loading modules.  It takes the module name, 
 the file to load, and the filename.  It then calls the appropriate 
 function to load the module. 
 This function is not meant to be called directly, but instead to be 
 called by the import machinery. 
 Parameters 
 name : str 
 The name of the module to load. 
 file : file-like object, optional 
 A file-like object to read the module from.  If None, a file object is 
 opened from the filename. 
 filename : str, optional 
 The name of the file to load.  If None, the module is assumed to be in 
 the current directory. 
 details : tuple of str, optional 
 The details about the file to load.  This is a tuple of the type of the 
 file, the mode to open it in, and the suffix to use.  If None, the 
 file is assumed to be a regular file.  If it is a regular file, the 
 mode is assumed to be \'r\'.  If it is a","'**DEPRECATED** 
 Load a module, given information returned by find_module(). 
 The module name must include the full package name, if any.'"
"def main(): 
    def _iterate(test_suite_or_case): 
      'Iterate   through   all   the   test   cases   in   `test_suite_or_case`.' 
      try: 
         suite = iter(test_suite_or_case) 
      except TypeError: 
         (yield test_suite_or_case) 
      else: 
         for test in suite: 
            for subtest in _iterate(test): 
               (yield subtest) 
   feconf.PLATFORM = 'gae' 
   for directory in DIRS_TO_ADD_TO_SYS_PATH: 
      if (not os.path.exists(os.path.dirname(directory))): 
         raise Exception(('Directory   %s   does   not   exist.' % directory)) 
      sys.path.insert(0, directory) 
   import dev_appserver 
   dev_appserver.fix_sys_path() 
   parsed_args = _PARSER.parse_args() 
   suites = create_test_suites(parsed_args.test_target) 
   results = [unittest.TextTestRunner(verbosity=2).run(suite) for suite in suites] 
   tests_run = 0 
   for result in results: 
      tests_run += result.testsRun 
      if (result.errors or result.failures): 
         raise Exception(('Test   suite   failed:   %s   tests   run,   %s   errors,   %s   failures.' % (result.testsRun, len(result.errors), len(result.failures)))) 
   if (tests_run == 0): 
      raise Exception('No   tests   were   run.')", 'Main entry point for the test runner.','Runs the tests.'
"@ensure_csrf_cookie 
 @cache_control(no_cache=True, no_store=True, must_revalidate=True) 
 @coach_dashboard 
 def ccx_invite(request, course, ccx=None): 
    if (not ccx): 
      raise Http404 
   action = request.POST.get('enrollment-button') 
   identifiers_raw = request.POST.get('student-ids') 
   identifiers = _split_input_list(identifiers_raw) 
   email_students = ('email-students' in request.POST) 
   course_key = CCXLocator.from_course_locator(course.id, unicode(ccx.id)) 
   email_params = get_email_params(course, auto_enroll=True, course_key=course_key, display_name=ccx.display_name) 
   ccx_students_enrolling_center(action, identifiers, email_students, course_key, email_params, ccx.coach) 
   url = reverse('ccx_coach_dashboard', kwargs={'course_id': course_key}) 
   return redirect(url)"," 'Invite students to a CCX course. 
 This action is used by coaches to invite students to a CCX course. 
 The request must be POSTed, and must include a list of student identifiers 
 separated by commas. 
 This action will enroll the students in the course, and send an email to the 
 students with a link to the course. 
 The request must include a \'enrollment-button\' parameter, which can be either 
 \'enroll\' or \'invite\'. 
 The request can also include a \'email-students\' parameter, which will send an 
 email to the students. 
 The request must include a \'student-ids\' parameter, which must be a list of 
 student identifiers, separated by commas. 
 The request will return a redirect to the coach\'s dashboard. 
 The course must be a CCX course. 
 The course must be visible to coaches. 
 The course must be visible to students. 
 The course must be visible to the coach. 
 The course must be visible to the coach\'s organization. 
 The course must",'Invite users to new ccx'
"def socktype_to_enum(num): 
    if (enum is None): 
      return num 
   else: 
      try: 
         return socket.AddressType(num) 
      except (ValueError, AttributeError): 
         return num", 'Convert a socket type number to an enum.',"'Convert a numeric socket type value to an IntEnum member. 
 If it\'s not a known member, return the numeric value itself.'"
"def is_asn1_token(token): 
    return (token[:3] == PKI_ASN1_PREFIX)"," 'Returns True if the token is an ASN.1 token. 
 :param token: 
 :type token: str 
 :return: 
 :rtype: bool'","'Determine if a token appears to be PKI-based. 
 thx to ayoung for sorting this out. 
 base64 decoded hex representation of MII is 3082:: 
 In [3]: binascii.hexlify(base64.b64decode(\'MII=\')) 
 Out[3]: \'3082\' 
 re: http://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf 
 pg4:  For tags from 0 to 30 the first octet is the identfier 
 pg10: Hex 30 means sequence, followed by the length of that sequence. 
 pg5:  Second octet is the length octet 
 first bit indicates short or long form, next 7 bits encode the 
 number of subsequent octets that make up the content length octets 
 as an unsigned binary int 
 82 = 10000010 (first bit indicates long form) 
 0000010 = 2 octets of content length 
 so read the next 2 octets to get the length of the content. 
 In the case of a very large content length there could be a requirement to 
 have more than 2 octets to designate the content length, therefore 
 requiring us to check for MIM, MIQ, etc. 
 In [4]: base64.b64encode(binascii.a2b_hex(\'3083\')) 
 Out[4]: \'MIM=\' 
 In [5]: base64.b64encode(binascii.a2b_hex(\'3084\')) 
 Out[5]: \'MIQ=\' 
 Checking for MI would become invalid at 16 octets of content length 
 10010000 = 90 
 In [6]: base64.b64encode(binascii.a2b_hex(\'3090\')) 
 Out[6]: \'MJA=\' 
 Checking for just M is insufficient 
 But we will only check for MII: 
 Max length of the content using 2 octets is 3FFF or 16383. 
 It\'s not practical to support a token of this length or greater in http 
 therefore, we will check for MII only and ignore the case of larger tokens'"
"def readsav(file_name, idict=None, python_dict=False, uncompressed_file_name=None, verbose=False): 
    records = [] 
   if (python_dict or idict): 
      variables = {} 
   else: 
      variables = AttrDict() 
   f = open(file_name, 'rb') 
   signature = _read_bytes(f, 2) 
   if (signature != 'SR'): 
      raise Exception(('Invalid   SIGNATURE:   %s' % signature)) 
   recfmt = _read_bytes(f, 2) 
   if (recfmt == '\x00\x04'): 
      pass 
   elif (recfmt == '\x00\x06'): 
      if verbose: 
         print('IDL   Save   file   is   compressed') 
      if uncompressed_file_name: 
         fout = open(uncompressed_file_name, 'w+b') 
      else: 
         fout = tempfile.NamedTemporaryFile(suffix='.sav') 
      if verbose: 
         print(('   ->   expanding   to   %s' % fout.name)) 
      fout.write('SR\x00\x04') 
      while True: 
         rectype = _read_long(f) 
         fout.write(struct.pack('>l', int(rectype))) 
         nextrec = _read_uint32(f) 
         nextrec += (_read_uint32(f) * (2 ** 32)) 
         unknown = f.read(4) 
         if (RECTYPE_DICT[rectype] == 'END_MARKER'): 
            fout.write(struct.pack('>I', (int(nextrec) % (2 ** 32)))) 
            fout.write(struct.pack('>I', int(((nextrec - (nextrec % (2 ** 32))) / (2 ** 32))))) 
            fout.write(unknown) 
            break 
         pos = f.tell() 
         rec_string = zlib.decompress(f.read((nextrec - pos))) 
         nextrec = ((fout.tell() + len(rec_string)) + 12) 
         fout.write(struct.pack('>I', int((nextrec % (2 ** 32))))) 
         fout.write(struct.pack('>I', int(((nextrec - (nextrec % (2 ** 32))) / (2 ** 32))))) 
         fout.write(unknown) 
         fout.write(rec_string) 
      f.close() 
      f = fout 
      f.seek(4) 
   else: 
      raise Exception(('Invalid   RECFMT:   %s' % recfmt)) 
   while True: 
      r = _read_record(f) 
      records.append(r) 
      if ('end' in r): 
         if r['end']: 
            break 
   f.close() 
   heap = {} 
   for r in records: 
      if (r['rectype'] == 'HEAP_DATA'): 
         heap[r['heap_index']] = r['data'] 
   for r in records: 
      if (r['rectype'] == 'VARIABLE'): 
         (replace, new) = _replace_heap(r['data'], heap) 
         if replace: 
            r['data'] = new 
         variables[r['varname'].lower()] = r['data'] 
   if verbose: 
      for record in records: 
         if (record['rectype'] == 'TIMESTAMP'): 
            print(('-' * 50)) 
            print(('Date:   %s' % record['date'])) 
            print(('User:   %s' % record['user'])) 
            print(('Host:   %s' % record['host'])) 
            break 
      for record in records: 
         if (record['rectype'] == 'VERSION'): 
            print(('-' * 50)) 
            print(('Format:   %s' % record['format'])) 
            print(('Architecture:   %s' % record['arch'])) 
            print(('Operating   System:   %s' % record['os'])) 
            print(('IDL   Version:   %s' % record['release'])) 
            break 
      for record in records: 
         if (record['rectype'] == 'IDENTIFICATON'): 
            print(('-' * 50)) 
            print(('Author:   %s' % record['author'])) 
            print(('Title:   %s' % record['title'])) 
            print(('ID   Code:   %s' % record['idcode'])) 
            break 
      for record in records: 
         if (record['rectype'] == 'DESCRIPTION'): 
            print(('-' * 50)) 
            print(('Description:   %s' % record['description'])) 
            break 
      print(('-' * 50)) 
      print(('Successfully   read   %i   records   of   which:' % len(records))) 
      rectypes = [r['rectype'] for r in records] 
      for rt in set(rectypes): 
         if (rt != 'END_MARKER'): 
            print(('   -   %i   are   of   type   %s' % (rectypes.count(rt), rt))) 
      print(('-' * 50)) 
      if ('VARIABLE' in rectypes): 
         print('Available   variables:') 
         for var in variables: 
            print(('   -   %s   [%s]' % (var, type(variables[var])))) 
         print(('-' * 50)) 
   if idict: 
      for var in variables: 
         idict[var] = variables[var] 
      return idict 
   else: 
      return variables"," 'Reads an IDL save file and returns the variables as a dictionary. 
 Parameters 
 file_name : str 
 Name of the file to read. 
 idict : dict, optional 
 If provided, the variables are stored in this dictionary. 
 python_dict : bool, optional 
 If True, the variables are stored as Python dictionaries. 
 uncompressed_file_name : str, optional 
 If provided, the file is uncompressed to this file name. 
 verbose : bool, optional 
 If True, the file is read and the records printed to stdout. 
 Returns 
 idict : dict 
 Dictionary with the variables. 
 Examples 
 >>> from idl_utils import readsav 
 >>> readsav(\'example.sav\') 
 >>> readsav(\'example.sav\', idict={}) 
 >>> readsav(\'example.sav\', python_dict=True) 
 >>> readsav(\'example.sav\', python_dict=True, idict={}) 
 >>> readsav(\'example.sav\', python_dict=True, idict","'Read an IDL .sav file. 
 Parameters 
 file_name : str 
 Name of the IDL save file. 
 idict : dict, optional 
 Dictionary in which to insert .sav file variables. 
 python_dict : bool, optional 
 By default, the object return is not a Python dictionary, but a 
 case-insensitive dictionary with item, attribute, and call access 
 to variables. To get a standard Python dictionary, set this option 
 to True. 
 uncompressed_file_name : str, optional 
 This option only has an effect for .sav files written with the 
 /compress option. If a file name is specified, compressed .sav 
 files are uncompressed to this file. Otherwise, readsav will use 
 the `tempfile` module to determine a temporary filename 
 automatically, and will remove the temporary file upon successfully 
 reading it in. 
 verbose : bool, optional 
 Whether to print out information about the save file, including 
 the records read, and available variables. 
 Returns 
 idl_dict : AttrDict or dict 
 If `python_dict` is set to False (default), this function returns a 
 case-insensitive dictionary with item, attribute, and call access 
 to variables. If `python_dict` is set to True, this function 
 returns a Python dictionary with all variable names in lowercase. 
 If `idict` was specified, then variables are written to the 
 dictionary specified, and the updated dictionary is returned.'"
"def surround_quotes(string): 
    if (in_bash() and string): 
      return '""{0}""'.format(string) 
   return string"," 'Returns the string surrounded by single quotes. 
 If the string is empty, it returns an empty string.'","'Bash has problems dealing with certain paths so we\'re surrounding all 
 path outputs with quotes.'"
"def add_log_redaction_filter_to_logger(engine, logger): 
    if engine.policies: 
      redaction_filter = RedactionFilter(engine) 
      for handler in logger.handlers: 
         handler.addFilter(redaction_filter)", 'Add redaction filter to logger.',"'`add_redaction_filter` injects the redaction filter into all of the `logger` 
 handlers. This must be called after all of the handlers have been added to 
 `logger`, otherwise those handlers may expose unredacted strings.'"
"def instance_key(model, instance_or_pk): 
    return ('%s.%s:%d' % (model._meta.app_label, model._meta.model_name, getattr(instance_or_pk, 'pk', instance_or_pk)))", 'Return the model instance key for the given model and instance or pk.',"'Returns the cache key for this (model, instance) pair.'"
"def stream_encode_multipart(values, use_tempfile=True, threshold=(1024 * 500), boundary=None, charset='utf-8'): 
    if (boundary is None): 
      boundary = ('---------------WerkzeugFormPart_%s%s' % (time(), random())) 
   _closure = [BytesIO(), 0, False] 
   if use_tempfile: 
      def write_binary(string): 
         (stream, total_length, on_disk) = _closure 
         if on_disk: 
            stream.write(string) 
         else: 
            length = len(string) 
            if ((length + _closure[1]) <= threshold): 
               stream.write(string) 
            else: 
               new_stream = TemporaryFile('wb+') 
               new_stream.write(stream.getvalue()) 
               new_stream.write(string) 
               _closure[0] = new_stream 
               _closure[2] = True 
            _closure[1] = (total_length + length) 
   else: 
      write_binary = _closure[0].write 
   def write(string): 
      write_binary(string.encode(charset)) 
   if (not isinstance(values, MultiDict)): 
      values = MultiDict(values) 
   for (key, values) in iterlists(values): 
      for value in values: 
         write(('--%s\r\nContent-Disposition:   form-data;   name=""%s""' % (boundary, key))) 
         reader = getattr(value, 'read', None) 
         if (reader is not None): 
            filename = getattr(value, 'filename', getattr(value, 'name', None)) 
            content_type = getattr(value, 'content_type', None) 
            if (content_type is None): 
               content_type = ((filename and mimetypes.guess_type(filename)[0]) or 'application/octet-stream') 
            if (filename is not None): 
               write((';   filename=""%s""\r\n' % filename)) 
            else: 
               write('\r\n') 
            write(('Content-Type:   %s\r\n\r\n' % content_type)) 
            while 1: 
               chunk = reader(16384) 
               if (not chunk): 
                  break 
               write_binary(chunk) 
         else: 
            if (not isinstance(value, string_types)): 
               value = str(value) 
            value = to_bytes(value, charset) 
            write('\r\n\r\n') 
            write_binary(value) 
         write('\r\n') 
   write(('--%s--\r\n' % boundary)) 
   length = int(_closure[0].tell()) 
   _closure[0].seek(0) 
   return (_closure[0], length, boundary)"," 'Encodes a set of values into a multipart/form-data encoded string. 
 :param values: The values to encode. 
 :param use_tempfile: If True, the data is written to a temporary file 
 instead of being written directly to the output stream. 
 :param threshold: The maximum number of bytes to write to the temporary 
 file before creating a new one. 
 :param boundary: The boundary to use. 
 :param charset: The encoding to use. 
 :return: The encoded string, the total length of the encoded string and 
 the boundary string.'","'Encode a dict of values (either strings or file descriptors or 
 :class:`FileStorage` objects.) into a multipart encoded string stored 
 in a file descriptor.'"
"def setup_cuda_fft_resample(n_jobs, W, new_len): 
    cuda_dict = dict(use_cuda=False, fft_plan=None, ifft_plan=None, x_fft=None, x=None, y_fft=None, y=None) 
   (n_fft_x, n_fft_y) = (len(W), new_len) 
   cuda_fft_len_x = int((((n_fft_x - (n_fft_x % 2)) // 2) + 1)) 
   cuda_fft_len_y = int((((n_fft_y - (n_fft_y % 2)) // 2) + 1)) 
   if (n_jobs == 'cuda'): 
      n_jobs = 1 
      init_cuda() 
      if _cuda_capable: 
         from pycuda import gpuarray 
         cudafft = _get_cudafft() 
         try: 
            W = gpuarray.to_gpu((W[:cuda_fft_len_x].astype('complex_') / n_fft_y)) 
            cuda_dict.update(use_cuda=True, fft_plan=cudafft.Plan(n_fft_x, np.float64, np.complex128), ifft_plan=cudafft.Plan(n_fft_y, np.complex128, np.float64), x_fft=gpuarray.zeros(max(cuda_fft_len_x, cuda_fft_len_y), np.complex128), x=gpuarray.empty(max(int(n_fft_x), int(n_fft_y)), np.float64)) 
            logger.info('Using   CUDA   for   FFT   resampling') 
         except Exception: 
            logger.info('CUDA   not   used,   could   not   instantiate   memory   (arrays   may   be   too   large),   falling   back   to   n_jobs=1') 
      else: 
         logger.info('CUDA   not   used,   CUDA   could   not   be   initialized,   falling   back   to   n_jobs=1') 
   return (n_jobs, cuda_dict, W)"," 'Returns the number of jobs to use for the FFT resampling, and the 
 dictionary to use for the FFT resampling. 
 Parameters 
 n_jobs : int, optional 
 The number of jobs to use for the FFT resampling. 
 If n_jobs is None, then the number of jobs to use will be determined 
 based on the number of cores in the machine. 
 W : ndarray 
 The input image to be resampled. 
 new_len : int 
 The new length of the image to be resampled. 
 Returns 
 n_jobs : int 
 The number of jobs to use for the FFT resampling. 
 cuda_dict : dict 
 The dictionary to use for the FFT resampling. 
 W : ndarray 
 The input image to be resampled. 
 Examples 
 >>> import numpy as np 
 >>> from nipype.interfaces.freesurfer import Resample 
 >>> from nipype.interfaces.freesurfer import ResampleFFT 
 >>> import os 
","'Set up CUDA FFT resampling. 
 Parameters 
 n_jobs : int | str 
 If n_jobs == \'cuda\', the function will attempt to set up for CUDA 
 FFT resampling. 
 W : array 
 The filtering function to be used during resampling. 
 If n_jobs=\'cuda\', this function will be shortened (since CUDA 
 assumes FFTs of real signals are half the length of the signal) 
 and turned into a gpuarray. 
 new_len : int 
 The size of the array following resampling. 
 Returns 
 n_jobs : int 
 Sets n_jobs = 1 if n_jobs == \'cuda\' was passed in, otherwise 
 original n_jobs is passed. 
 cuda_dict : dict 
 Dictionary with the following CUDA-related variables: 
 use_cuda : bool 
 Whether CUDA should be used. 
 fft_plan : instance of FFTPlan 
 FFT plan to use in calculating the FFT. 
 ifft_plan : instance of FFTPlan 
 FFT plan to use in calculating the IFFT. 
 x_fft : instance of gpuarray 
 Empty allocated GPU space for storing the result of the 
 frequency-domain multiplication. 
 x : instance of gpuarray 
 Empty allocated GPU space for the data to resample. 
 W : array | instance of gpuarray 
 This will either be a gpuarray (if CUDA enabled) or np.ndarray. 
 If CUDA is enabled, W will be modified appropriately for use 
 with filter.fft_multiply(). 
 Notes 
 This function is designed to be used with fft_resample().'"
"def distorted_inputs(dataset, batch_size=None, num_preprocess_threads=None): 
    if (not batch_size): 
      batch_size = FLAGS.batch_size 
   with tf.device('/cpu:0'): 
      (images, labels) = batch_inputs(dataset, batch_size, train=True, num_preprocess_threads=num_preprocess_threads, num_readers=FLAGS.num_readers) 
   return (images, labels)", 'Constructs the input tensors for training.',"'Generate batches of distorted versions of ImageNet images. 
 Use this function as the inputs for training a network. 
 Distorting images provides a useful technique for augmenting the data 
 set during training in order to make the network invariant to aspects 
 of the image that do not effect the label. 
 Args: 
 dataset: instance of Dataset class specifying the dataset. 
 batch_size: integer, number of examples in batch 
 num_preprocess_threads: integer, total number of preprocessing threads but 
 None defaults to FLAGS.num_preprocess_threads. 
 Returns: 
 images: Images. 4D tensor of size [batch_size, FLAGS.image_size, 
 FLAGS.image_size, 3]. 
 labels: 1-D integer Tensor of [batch_size].'"
"def use_gl(target='gl2'): 
    target = (target or 'gl2') 
   target = target.replace('+', 'plus') 
   (target, _, options) = target.partition('   ') 
   debug = (config['gl_debug'] or ('debug' in options)) 
   try: 
      mod = __import__(target, globals(), level=1) 
   except ImportError as err: 
      msg = ('Could   not   import   gl   target   ""%s"":\n%s' % (target, str(err))) 
      raise RuntimeError(msg) 
   global current_backend 
   current_backend = mod 
   _clear_namespace() 
   if ('plus' in target): 
      _copy_gl_functions(mod._pyopengl2, globals()) 
      _copy_gl_functions(mod, globals(), True) 
   elif debug: 
      _copy_gl_functions(_debug_proxy, globals()) 
   else: 
      _copy_gl_functions(mod, globals())"," 'Select the OpenGL backend to use. 
 Use ``use_gl(target)`` to select the backend to use. 
 The target can be one of: 
 * ``gl`` - use the OpenGL backend 
 * ``gl2`` - use the OpenGL 2 backend 
 * ``gl3`` - use the OpenGL 3 backend 
 * ``gl3plus`` - use the OpenGL 3.2 backend 
 * ``gl4`` - use the OpenGL 4 backend 
 * ``gl4plus`` - use the OpenGL 4.2 backend 
 * ``gl4plusplus`` - use the OpenGL 4.3 backend 
 * ``gl5`` - use the OpenGL 5 backend 
 * ``gl5plus`` - use the OpenGL 5.1 backend 
 * ``gl5plusplus`` - use the OpenGL 5.2 backend 
 * ``gl6`` - use the OpenGL 6 backend 
 * ``gl6plus`` - use the OpenGL 6.1 backend 
 * ``gl6plusplus`` - use the OpenGL 6.2 backend 
 * ``gl7`` - use the OpenGL 7 backend 
 * ``","'Let Vispy use the target OpenGL ES 2.0 implementation 
 Also see ``vispy.use()``. 
 Parameters 
 target : str 
 The target GL backend to use. 
 Available backends: 
 * gl2 - Use ES 2.0 subset of desktop (i.e. normal) OpenGL 
 * gl+ - Use the desktop ES 2.0 subset plus all non-deprecated GL 
 functions on your system (requires PyOpenGL) 
 * es2 - Use the ES2 library (Angle/DirectX on Windows) 
 * pyopengl2 - Use ES 2.0 subset of pyopengl (for fallback and testing) 
 * dummy - Prevent usage of gloo.gl (for when rendering occurs elsewhere) 
 You can use vispy\'s config option ""gl_debug"" to check for errors 
 on each API call. Or, one can specify it as the target, e.g. ""gl2 
 debug"". (Debug does not apply to \'gl+\', since PyOpenGL has its own 
 debug mechanism)'"
"def gen_arg_base_type(fn): 
    mod = fn.module 
   fnty = fn.type.pointee 
   consts = [lc.MetaDataString.get(mod, str(a)) for a in fnty.args] 
   name = lc.MetaDataString.get(mod, 'kernel_arg_base_type') 
   return lc.MetaData.get(mod, ([name] + consts))"," 'Returns the type of the kernel argument base type. 
 The kernel argument base type is the type of the arguments that are passed 
 to the kernel function. 
 This function returns a tuple of strings. The first string is the name of 
 the kernel argument base type. The second string is the name of the kernel 
 argument base type. 
 :param fn: The function to get the type of the kernel argument base type from 
 :return: A tuple of strings'",'Generate kernel_arg_base_type metadata'
"def _make_request(token, method_name, method='get', params=None, files=None, base_url=API_URL): 
    request_url = base_url.format(token, method_name) 
   logger.debug('Request:   method={0}   url={1}   params={2}   files={3}'.format(method, request_url, params, files)) 
   read_timeout = READ_TIMEOUT 
   connect_timeout = CONNECT_TIMEOUT 
   if params: 
      if ('timeout' in params): 
         read_timeout = (params['timeout'] + 10) 
      if ('connect-timeout' in params): 
         connect_timeout = (params['connect-timeout'] + 10) 
   result = req_session.request(method, request_url, params=params, files=files, timeout=(connect_timeout, read_timeout)) 
   logger.debug(""The   server   returned:   '{0}'"".format(result.text.encode('utf8'))) 
   return _check_result(method_name, result)['result']"," 'Make a request to the server and return the result. 
 Args: 
 token: 
 The token used to authenticate with the server. 
 method_name: 
 The name of the method to call. 
 method: 
 The HTTP method to use. 
 params: 
 The parameters to pass to the server. 
 files: 
 A dictionary of file uploads. 
 base_url: 
 The base URL to prepend to the request. 
 Returns: 
 A dictionary with the result of the request. 
 Raises: 
 Exception: 
 If the request fails.'","'Makes a request to the Telegram API. 
 :param token: The bot\'s API token. (Created with @BotFather) 
 :param method_name: Name of the API method to be called. (E.g. \'getUpdates\') 
 :param method: HTTP method to be used. Defaults to \'get\'. 
 :param params: Optional parameters. Should be a dictionary with key-value pairs. 
 :param files: Optional files. 
 :return: The result parsed to a JSON dictionary.'"
"def loads(source): 
    return UnspacedList(RawNginxParser(source).as_list())", 'Parse a string containing Nginx raw config as a list of dicts.',"'Parses from a string. 
 :param str souce: The string to parse 
 :returns: The parsed tree 
 :rtype: list'"
"def grey_dilation(input, size=None, footprint=None, structure=None, output=None, mode='reflect', cval=0.0, origin=0): 
    if ((size is None) and (footprint is None) and (structure is None)): 
      raise ValueError('size,   footprint   or   structure   must   be   specified') 
   if (structure is not None): 
      structure = numpy.asarray(structure) 
      structure = structure[tuple(([slice(None, None, (-1))] * structure.ndim))] 
   if (footprint is not None): 
      footprint = numpy.asarray(footprint) 
      footprint = footprint[tuple(([slice(None, None, (-1))] * footprint.ndim))] 
   input = numpy.asarray(input) 
   origin = _ni_support._normalize_sequence(origin, input.ndim) 
   for ii in range(len(origin)): 
      origin[ii] = (- origin[ii]) 
      if (footprint is not None): 
         sz = footprint.shape[ii] 
      elif (structure is not None): 
         sz = structure.shape[ii] 
      elif numpy.isscalar(size): 
         sz = size 
      else: 
         sz = size[ii] 
      if (not (sz & 1)): 
         origin[ii] -= 1 
   return filters._min_or_max_filter(input, size, footprint, structure, output, mode, cval, origin, 0)"," 'Apply a grey-dilation to an image. 
 Parameters 
 input : ndarray 
 Input image. 
 size : tuple 
 Size of the dilation. 
 footprint : tuple 
 Footprint of the dilation. 
 structure : tuple 
 Structure of the dilation. 
 output : tuple 
 Output image. 
 mode : string 
 Mode of the dilation (see `mode` in :ref:`filter_modes`). 
 cval : scalar 
 Value to fill uncovered pixels with. 
 origin : int 
 Origin of the dilation. 
 Returns 
 output : ndarray 
 Output image. 
 Examples 
 >>> from nipype.interfaces.freesurfer import FSGreyDilation 
 >>> fsgd = FSGreyDilation() 
 >>> fsgd.inputs.input = np.arange(12).reshape(3, 4) 
 >>> fsgd.inputs.size = (3, 3) 
 >>> fsgd.inputs.footprint = (3, 3)","'Calculate a greyscale dilation, using either a structuring element, 
 or a footprint corresponding to a flat structuring element. 
 Grayscale dilation is a mathematical morphology operation. For the 
 simple case of a full and flat structuring element, it can be viewed 
 as a maximum filter over a sliding window. 
 Parameters 
 input : array_like 
 Array over which the grayscale dilation is to be computed. 
 size : tuple of ints 
 Shape of a flat and full structuring element used for the grayscale 
 dilation. Optional if `footprint` or `structure` is provided. 
 footprint : array of ints, optional 
 Positions of non-infinite elements of a flat structuring element 
 used for the grayscale dilation. Non-zero values give the set of 
 neighbors of the center over which the maximum is chosen. 
 structure : array of ints, optional 
 Structuring element used for the grayscale dilation. `structure` 
 may be a non-flat structuring element. 
 output : array, optional 
 An array used for storing the ouput of the dilation may be provided. 
 mode : {\'reflect\',\'constant\',\'nearest\',\'mirror\', \'wrap\'}, optional 
 The `mode` parameter determines how the array borders are 
 handled, where `cval` is the value when mode is equal to 
 \'constant\'. Default is \'reflect\' 
 cval : scalar, optional 
 Value to fill past edges of input if `mode` is \'constant\'. Default 
 is 0.0. 
 origin : scalar, optional 
 The `origin` parameter controls the placement of the filter. 
 Default 0 
 Returns 
 grey_dilation : ndarray 
 Grayscale dilation of `input`. 
 See also 
 binary_dilation, grey_erosion, grey_closing, grey_opening 
 generate_binary_structure, ndimage.maximum_filter 
 Notes 
 The grayscale dilation of an image input by a structuring element s defined 
 over a domain E is given by: 
 (input+s)(x) = max {input(y) + s(x-y), for y in E} 
 In particular, for structuring elements defined as 
 s(y) = 0 for y in E, the grayscale dilation computes the maximum of the 
 input image inside a sliding window defined by E. 
 Grayscale dilation [1]_ is a *mathematical morphology* operation [2]_. 
 References 
 .. [1] http://en.wikipedia.org/wiki/Dilation_%28morphology%29 
 .. [2] http://en.wikipedia.org/wiki/Mathematical_morphology 
 Examples 
 >>> from scipy import ndimage 
 >>> a = np.zeros((7,7), dtype=int) 
 >>> a[2:5, 2:5] = 1 
 >>> a[4,4] = 2; a[2,3] = 3 
 >>> a 
 array([[0, 0, 0, 0, 0, 0, 0], 
 [0, 0, 0, 0, 0, 0, 0], 
 [0, 0, 1, 3, 1, 0, 0], 
 [0, 0, 1, 1, 1, 0, 0], 
 [0, 0, 1, 1, 2, 0, 0], 
 [0, 0, 0, 0, 0, 0, 0], 
 [0, 0, 0, 0, 0, 0, 0]]) 
 >>> ndimage.grey_dilation(a, size=(3,3)) 
 array([[0, 0, 0, 0, 0, 0, 0], 
 [0, 1, 3, 3, 3, 1, 0], 
 [0, 1, 3, 3, 3, 1, 0], 
 [0, 1, 3, 3, 3, 2, 0], 
 [0, 1, 1, 2, 2, 2, 0], 
 [0, 1, 1, 2, 2, 2, 0], 
 [0, 0, 0, 0, 0, 0, 0]]) 
 >>> ndimage.grey_dilation(a, footprint=np.ones((3,3))) 
 array([[0, 0, 0, 0, 0, 0, 0], 
 [0, 1, 3, 3, 3, 1, 0], 
 [0, 1, 3, 3, 3, 1, 0], 
 [0, 1, 3, 3, 3, 2, 0], 
 [0, 1, 1, 2, 2, 2, 0], 
 [0, 1, 1, 2, 2, 2, 0], 
 [0, 0, 0, 0, 0, 0, 0]]) 
 >>> s = ndimage.generate_binary_structure(2,1) 
 >>> s 
 array([[False,  True, False], 
 [ True,  True,  True], 
 [False,  True, False]], dtype=bool) 
 >>> ndimage.grey_dilation(a, footprint=s) 
 array([[0, 0, 0, 0, 0, 0, 0], 
 [0, 0, 1, 3, 1, 0, 0], 
 [0, 1, 3, 3, 3, 1, 0], 
 [0, 1, 1, 3, 2, 1, 0], 
 [0, 1, 1, 2, 2, 2, 0], 
 [0, 0, 1, 1, 2, 0, 0], 
 [0, 0, 0, 0, 0, 0, 0]]) 
 >>> ndimage.grey_dilation(a, size=(3,3), structure=np.ones((3,3))) 
 array([[1, 1, 1, 1, 1, 1, 1], 
 [1, 2, 4, 4, 4, 2, 1], 
 [1, 2, 4, 4, 4, 2, 1], 
 [1, 2, 4, 4, 4, 3, 1], 
 [1, 2, 2, 3, 3, 3, 1], 
 [1, 2, 2, 3, 3, 3, 1], 
 [1, 1, 1, 1, 1, 1, 1]])'"
"def _buildFakeFKTable(cls, fakeTableName): 
    countCols = 0 
   attrs = {} 
   for col in cls._imdbpySchema.cols: 
      countCols += 1 
      if (col.name == 'id'): 
         continue 
      if (not col.foreignKey): 
         attrs[col.name] = MAP_COLS[col.kind](**col.params) 
         continue 
      thisColName = col.name 
      if thisColName.endswith('ID'): 
         thisColName = thisColName[:(-2)] 
      fks = col.foreignKey.split('.', 1) 
      foreignTableName = fks[0] 
      if (len(fks) == 2): 
         foreignColName = fks[1] 
      else: 
         foreignColName = 'id' 
      fk = ForeignKey(foreignTableName, name=thisColName, default=None) 
      attrs[thisColName] = fk 
   newcls = type(fakeTableName, (SQLObject,), attrs) 
   return newcls"," 'Create a fake table with all the fields of the real table, but with 
 fake foreign keys. 
 :param cls: The real table. 
 :param fakeTableName: The name of the fake table. 
 :return: The fake table.'","'Return a ""fake"" table, with foreign keys where needed.'"
"def get_locked_port_and_binding(context, port_id): 
    try: 
      port = context.session.query(models_v2.Port).enable_eagerloads(False).filter_by(id=port_id).with_lockmode('update').one() 
      binding = context.session.query(models.PortBinding).enable_eagerloads(False).filter_by(port_id=port_id).with_lockmode('update').one() 
      return (port, binding) 
   except exc.NoResultFound: 
      return (None, None)"," 'Returns a tuple of (port, binding) if the port is locked.'",'Get port and port binding records for update within transaction.'
"def missing_whitespace_around_operator(logical_line, tokens): 
    parens = 0 
   need_space = False 
   prev_type = tokenize.OP 
   prev_text = prev_end = None 
   for (token_type, text, start, end, line) in tokens: 
      if (token_type in (tokenize.NL, tokenize.NEWLINE, tokenize.ERRORTOKEN)): 
         continue 
      if (text in ('(', 'lambda')): 
         parens += 1 
      elif (text == ')'): 
         parens -= 1 
      if need_space: 
         if (start != prev_end): 
            if ((need_space is not True) and (not need_space[1])): 
               (yield (need_space[0], 'E225   missing   whitespace   around   operator')) 
            need_space = False 
         elif ((text == '>') and (prev_text in ('<', '-'))): 
            pass 
         else: 
            if ((need_space is True) or need_space[1]): 
               (yield (prev_end, 'E225   missing   whitespace   around   operator')) 
            else: 
               (code, optype) = ('E226', 'arithmetic') 
               if (prev_text == '%'): 
                  (code, optype) = ('E228', 'modulo') 
               elif (prev_text not in ARITHMETIC_OP): 
                  (code, optype) = ('E227', 'bitwise   or   shift') 
               (yield (need_space[0], ('%s   missing   whitespace   around   %s   operator' % (code, optype)))) 
            need_space = False 
      elif ((token_type == tokenize.OP) and (prev_end is not None)): 
         if ((text == '=') and parens): 
            pass 
         elif (text in WS_NEEDED_OPERATORS): 
            need_space = True 
         elif (text in UNARY_OPERATORS): 
            if (prev_type == tokenize.OP): 
               binary_usage = (prev_text in '}])') 
            elif (prev_type == tokenize.NAME): 
               binary_usage = (prev_text not in KEYWORDS) 
            else: 
               binary_usage = (prev_type not in SKIP_TOKENS) 
            if binary_usage: 
               need_space = None 
         elif (text in WS_OPTIONAL_OPERATORS): 
            need_space = None 
         if (need_space is None): 
            need_space = (prev_end, (start != prev_end)) 
         elif (need_space and (start == prev_end)): 
            (yield (prev_end, 'E225   missing   whitespace   around   operator')) 
            need_space = False 
      prev_type = token_type 
      prev_text = text 
      prev_end = end"," 'Check for missing whitespace around operators. 
 This rule checks for missing whitespace around operators, 
 such as \'<\' and \'>\' in relational expressions, 
 \'*\' and \'/\' in arithmetic expressions, 
 \'%\' in modulo expressions, 
 \'&\' and \'|\' in bitwise expressions, 
 and \'^\' and \'<<\' in shift expressions. 
 It also checks for missing whitespace around unary operators, 
 such as \'!\' and \'~\' in arithmetic expressions. 
 It also checks for missing whitespace around assignment operators, 
 such as \'=, \'+=, \'-=, \'*=, \'/=, \'%=, \'&=, \'|=, \'^=, \'<<=, \'>>=, 
 \'>>>=\', and \'>>>=. 
 It also checks for missing whitespace around comparison operators, 
 such as \'<, \'<=, \'==, \'!=, \'>, \'>=, \'!=\', and \'is\'. ","'- Always surround these binary operators with a single space on 
 either side: assignment (=), augmented assignment (+=, -= etc.), 
 comparisons (==, <, >, !=, <>, <=, >=, in, not in, is, is not), 
 Booleans (and, or, not). 
 - Use spaces around arithmetic operators. 
 Okay: i = i + 1 
 Okay: submitted += 1 
 Okay: x = x * 2 - 1 
 Okay: hypot2 = x * x + y * y 
 Okay: c = (a + b) * (a - b) 
 Okay: foo(bar, key=\'word\', *args, **kwargs) 
 Okay: alpha[:-i] 
 E225: i=i+1 
 E225: submitted +=1 
 E225: x = x /2 - 1 
 E225: z = x **y 
 E226: c = (a+b) * (a-b) 
 E226: hypot2 = x*x + y*y 
 E227: c = a|b 
 E228: msg = fmt%(errno, errmsg)'"
"def _fake_check_ldev_status(*args, **kwargs): 
    return None", 'Fake check_ldev_status function for tests.','Assume LDEV status has changed as desired.'
"def test_diagonal_gaussian_sample_from_epsilon(): 
    mlp = MLP(layers=[Linear(layer_name='h', dim=5, irange=0.01, max_col_norm=0.01)]) 
   conditional = DiagonalGaussian(mlp=mlp, name='conditional') 
   vae = DummyVAE() 
   conditional.set_vae(vae) 
   input_space = VectorSpace(dim=5) 
   conditional.initialize_parameters(input_space=input_space, ndim=5) 
   conditional.sample_from_epsilon((2, 10, 5))", 'Test that the conditional Gaussian samples from the epsilon','DiagonalGaussian.sample_from_epsilon doesn\'t crash'
"def edns_from_text(text): 
    return _from_text(text, _edns_by_text)"," 'Parse an EDNS0 text record and return a dict of the values. 
 :param text: The text record to parse. 
 :type text: str 
 :return: A dict of the values.'","'Convert a space-separated list of EDNS flag text values into a EDNS 
 flags value. 
 @rtype: int'"
"def _date_to_datetime(value): 
    if (not isinstance(value, datetime.date)): 
      raise TypeError(('Cannot   convert   to   datetime   expected   date   value;   received   %s' % value)) 
   return datetime.datetime(value.year, value.month, value.day)"," 'Convert a date to a datetime. 
 :param value: The date to convert. 
 :type value: datetime.date 
 :returns: The datetime object. 
 :rtype: datetime.datetime'","'Convert a date to a datetime for datastore storage. 
 Args: 
 value: A datetime.date object. 
 Returns: 
 A datetime object with time set to 0:00.'"
"def find_bad_registrations(): 
    registrations = models.Node.find(Q('is_registration', 'eq', True)) 
   for registration in registrations: 
      meta = (registration.registered_meta or {}) 
      keys = meta.keys() 
      if (len(keys) != 1): 
         print 'Inconsistency:   Number   of   keys   on   project   {}   ({})   !=   1'.format(registration.title, registration._primary_key) 
         continue 
      if (keys[0] not in known_schemas): 
         print 'Inconsistency:   Registration   schema   {}   on   project   {}   ({})   not   in   known   schemas'.format(keys[0], registration.title, registration._primary_key)"," 'Finds any registration schemas that are not in known_schemas. 
 This is to catch any new schemas that are not in known_schemas, 
 and to catch any registration schemas that are not in the 
 known_schemas for the project.'","'Find registrations with unexpected numbers of template keys or 
 outdated templates.'"
"def assert_has_n_elements_with_path(output, path, n): 
    xml = to_xml(output) 
   n = int(n) 
   num_elements = len(xml.findall(path)) 
   if (num_elements != n): 
      errmsg = ('Expected   to   find   %d   elements   with   path   %s,   but   %d   were   found.' % (n, path, num_elements)) 
      raise AssertionError(errmsg)"," 'Assert that the output has a certain number of elements with a certain 
 path. 
 :param output: The output to check. 
 :param path: The path to look for. 
 :param n: The number of elements to expect. 
 :raise AssertionError: If the number of elements with the path is not 
 equal to `n`. 
 :raise ValueError: If `n` is not an integer.'","'Asserts the specified output has exactly n elements matching the 
 path specified.'"
"def test_warning_config_google_home_listen_port(): 
    with patch.object(_LOGGER, 'warning') as mock_warn: 
      Config(None, {'type': 'google_home', 'host_ip': '123.123.123.123', 'listen_port': 8300}) 
      assert mock_warn.called 
      assert (mock_warn.mock_calls[0][1][0] == 'When   targetting   Google   Home,   listening   port   has   to   be   port   80')", 'Test that a warning is raised when the listen_port is not 80.','Test we warn when non-default port is used for Google Home.'
"def auc(actual, posterior): 
    r = tied_rank(posterior) 
   num_positive = len([0 for x in actual if (x == 1)]) 
   num_negative = (len(actual) - num_positive) 
   sum_positive = sum([r[i] for i in range(len(r)) if (actual[i] == 1)]) 
   auc = ((sum_positive - ((num_positive * (num_positive + 1)) / 2.0)) / (num_negative * num_positive)) 
   return auc"," 'Calculates the area under the ROC curve. 
 Parameters 
 actual : array 
 The actual values of the ROC curve. 
 posterior : array 
 The posterior probabilities of the ROC curve. 
 Returns 
 auc : float 
 The area under the ROC curve.'","'Computes the area under the receiver-operater characteristic (AUC) 
 This function computes the AUC error metric for binary classification. 
 Parameters 
 actual : list of binary numbers, numpy array 
 The ground truth value 
 posterior : same type as actual 
 Defines a ranking on the binary numbers, from most likely to 
 be positive to least likely to be positive. 
 Returns 
 score : double 
 The mean squared error between actual and posterior'"
"def addYGroove(derivation, negatives, x): 
    if (derivation.topBevel <= 0.0): 
      return 
   bottom = (derivation.height - derivation.topBevel) 
   top = derivation.height 
   groove = [complex(x, bottom), complex((x - derivation.topBevel), top), complex((x + derivation.topBevel), top)] 
   triangle_mesh.addSymmetricYPath(negatives, groove, (1.0001 * derivation.topRight.imag))"," 'Adds a Y groove to the derivation. 
 Parameters 
 derivation : Derivation 
 The derivation to add the groove to. 
 negatives : list 
 A list of negative numbers to use as the vertices of the groove. 
 x : float 
 The x-coordinate of the groove. 
 Returns 
 A list of vertices.'",'Add y groove'
"def _get_channel_stub(): 
    return apiproxy_stub_map.apiproxy.GetStub('channel')", 'Returns the Channel API stub',"'Gets the ChannelServiceStub instance from the API proxy stub map. 
 Returns: 
 The ChannelServiceStub instance as registered in the API stub map.'"
"@given('the   model   elements   with   name   and   tags') 
 def step_given_named_model_elements_with_tags(context): 
    assert context.table, 'REQUIRE:   context.table' 
   context.table.require_columns(['name', 'tags']) 
   model_element_names = set() 
   model_elements = [] 
   for row in context.table.rows: 
      name = row['name'].strip() 
      tags = convert_model_element_tags(row['tags']) 
      assert (name not in model_element_names), ('DUPLICATED:   name=%s' % name) 
      model_elements.append(ModelElement(name, tags=tags)) 
      model_element_names.add(name) 
   context.model_elements = model_elements", 'Given a table containing model elements with name and tags.',"'.. code-block:: gherkin 
 Given the model elements with name and tags: 
 | name | tags   | 
 | S1   | @foo   | 
 Then the tag expression select model elements with: 
 | tag expression | selected?    | 
 |  @foo          | S1, S3       | 
 | -@foo          | S0, S2, S3   |'"
"def check_map(infile, disable_primer_check, barcode_type='golay_12', added_demultiplex_field=None, has_barcodes=True): 
    if (barcode_type == 'variable_length'): 
      var_len_barcodes = True 
   else: 
      var_len_barcodes = False 
   if (barcode_type == '0'): 
      has_barcodes = False 
   (hds, mapping_data, run_description, errors, warnings) = process_id_map(infile, has_barcodes=has_barcodes, disable_primer_check=disable_primer_check, added_demultiplex_field=added_demultiplex_field, variable_len_barcodes=var_len_barcodes) 
   if errors: 
      raise ValueError((('Errors   were   found   with   mapping   file,   ' + 'please   run   validate_mapping_file.py   to   ') + 'identify   problems.')) 
   id_map = {} 
   for curr_data in mapping_data: 
      id_map[curr_data[0]] = {} 
   for header in range(len(hds)): 
      for curr_data in mapping_data: 
         id_map[curr_data[0]][hds[header]] = curr_data[header] 
   barcode_to_sample_id = {} 
   primer_seqs_lens = {} 
   all_primers = {} 
   for (sample_id, sample) in id_map.items(): 
      if added_demultiplex_field: 
         barcode_to_sample_id[((sample['BarcodeSequence'].upper() + ',') + sample[added_demultiplex_field])] = sample_id 
      else: 
         barcode_to_sample_id[sample['BarcodeSequence'].upper()] = sample_id 
      if (not disable_primer_check): 
         raw_primers = sample['LinkerPrimerSequence'].upper().split(',') 
         if (len(raw_primers[0].strip()) == 0): 
            raise ValueError(('No   primers   detected,   please   use   the   ' + '-p   parameter   to   disable   primer   detection.')) 
         expanded_primers = expand_degeneracies(raw_primers) 
         curr_bc_primers = {} 
         for primer in expanded_primers: 
            curr_bc_primers[primer] = len(primer) 
            all_primers[primer] = len(primer) 
         primer_seqs_lens[sample['BarcodeSequence']] = curr_bc_primers 
   return (hds, id_map, barcode_to_sample_id, warnings, errors, primer_seqs_lens, all_primers)"," 'Checks the mapping file and returns the header names, 
 the mapping data, the run description, warnings, errors, 
 the barcode to sample id mapping, and the primer sequence 
 lengths.'","'Check mapping file and extract list of valid barcodes, primers'"
"def upload_template_and_reload(name): 
    template = get_templates()[name] 
   local_path = template[u'local_path'] 
   if (not os.path.exists(local_path)): 
      project_root = os.path.dirname(os.path.abspath(__file__)) 
      local_path = os.path.join(project_root, local_path) 
   remote_path = template[u'remote_path'] 
   reload_command = template.get(u'reload_command') 
   owner = template.get(u'owner') 
   mode = template.get(u'mode') 
   remote_data = u'' 
   if exists(remote_path): 
      with hide(u'stdout'): 
         remote_data = sudo((u'cat   %s' % remote_path), show=False) 
   with open(local_path, u'r') as f: 
      local_data = f.read() 
      local_data = re.sub(u'%(?!\\(\\w+\\)s)', u'%%', local_data) 
      if (u'%(db_pass)s' in local_data): 
         env.db_pass = db_pass() 
      local_data %= env 
   clean = (lambda s: s.replace(u'\n', u'').replace(u'\r', u'').strip()) 
   if (clean(remote_data) == clean(local_data)): 
      return 
   upload_template(local_path, remote_path, env, use_sudo=True, backup=False) 
   if owner: 
      sudo((u'chown   %s   %s' % (owner, remote_path))) 
   if mode: 
      sudo((u'chmod   %s   %s' % (mode, remote_path))) 
   if reload_command: 
      sudo(reload_command)"," 'Upload the template and reload the template. 
 If the template is already uploaded, the template will not be uploaded again.'","'Uploads a template only if it has changed, and if so, reload the 
 related service.'"
"@pytest.mark.parametrize('confcutdir,passed,error', [('.', 2, 0), ('src', 1, 1), (None, 1, 1)]) 
 def test_search_conftest_up_to_inifile(testdir, confcutdir, passed, error): 
    root = testdir.tmpdir 
   src = root.join('src').ensure(dir=1) 
   src.join('pytest.ini').write('[pytest]') 
   src.join('conftest.py').write(_pytest._code.Source('\n                        import   pytest\n                        @pytest.fixture\n                        def   fix1():   pass\n            ')) 
   src.join('test_foo.py').write(_pytest._code.Source('\n                        def   test_1(fix1):\n                                    pass\n                        def   test_2(out_of_reach):\n                                    pass\n            ')) 
   root.join('conftest.py').write(_pytest._code.Source('\n                        import   pytest\n                        @pytest.fixture\n                        def   out_of_reach():   pass\n            ')) 
   args = [str(src)] 
   if confcutdir: 
      args = [('--confcutdir=%s' % root.join(confcutdir))] 
   result = testdir.runpytest(*args) 
   match = '' 
   if passed: 
      match += ('*%d   passed*' % passed) 
   if error: 
      match += ('*%d   error*' % error) 
   result.stdout.fnmatch_lines(match)", 'Test that the conftest.py file is searched up to the inifile',"'Test that conftest files are detected only up to a ini file, unless 
 an explicit --confcutdir option is given.'"
"def is_valid_number_for_region(numobj, region_code): 
    country_code = numobj.country_code 
   if (region_code is None): 
      return False 
   metadata = PhoneMetadata.metadata_for_region_or_calling_code(country_code, region_code.upper()) 
   if ((metadata is None) or ((region_code != REGION_CODE_FOR_NON_GEO_ENTITY) and (country_code != country_code_for_valid_region(region_code)))): 
      return False 
   nsn = national_significant_number(numobj) 
   return (_number_type_helper(nsn, metadata) != PhoneNumberType.UNKNOWN)"," 'Returns True if the number is valid for the given region, False otherwise. 
 :param numobj: a PhoneNumber object 
 :param region_code: a region code 
 :return: True if the number is valid for the given region, False otherwise'","'Tests whether a phone number is valid for a certain region. 
 Note this doesn\'t verify the number is actually in use, which is 
 impossible to tell by just looking at a number itself. If the country 
 calling code is not the same as the country calling code for the region, 
 this immediately exits with false. After this, the specific number pattern 
 rules for the region are examined. This is useful for determining for 
 example whether a particular number is valid for Canada, rather than just 
 a valid NANPA number. 
 Warning: In most cases, you want to use is_valid_number instead. For 
 example, this method will mark numbers from British Crown dependencies 
 such as the Isle of Man as invalid for the region ""GB"" (United Kingdom), 
 since it has its own region code, ""IM"", which may be undesirable. 
 Arguments: 
 numobj -- The phone number object that we want to validate. 
 region_code -- The region that we want to validate the phone number for. 
 Returns a boolean that indicates whether the number is of a valid pattern.'"
"def test_elemwise_collapse2(): 
    shape = (4, 5, 9) 
   a = cuda_ndarray.CudaNdarray(theano._asarray(numpy.random.rand(*shape), dtype='float32')) 
   a = theano._asarray(numpy.random.rand(*shape), dtype='float32') 
   a2 = tcn.shared_constructor(a, 'a') 
   a3 = a2.dimshuffle(0, 'x', 1, 2) 
   b = tcn.CudaNdarrayType((False, False, False, False))() 
   c = (a3 + b) 
   f = pfunc([b], [c], mode=mode_with_gpu) 
   v = theano._asarray(numpy.random.rand(shape[0], 5, *shape[1:]), dtype='float32') 
   v = cuda_ndarray.CudaNdarray(v) 
   out = f(v)[0] 
   assert numpy.allclose(out, (a.reshape(shape[0], 1, *shape[1:]) + v))", 'Test elemwise collapse for 2D tensors','Test when only one inputs have one broadcastable dimension'
"def is_lyrics(text, artist=None): 
    if (not text): 
      return False 
   badTriggersOcc = [] 
   nbLines = text.count('\n') 
   if (nbLines <= 1): 
      log.debug(u""Ignoring   too   short   lyrics   '{0}'"".format(text)) 
      return False 
   elif (nbLines < 5): 
      badTriggersOcc.append('too_short') 
   else: 
      text = remove_credits(text) 
   badTriggers = ['lyrics', 'copyright', 'property', 'links'] 
   if artist: 
      badTriggersOcc += [artist] 
   for item in badTriggers: 
      badTriggersOcc += ([item] * len(re.findall(('\\W%s\\W' % item), text, re.I))) 
   if badTriggersOcc: 
      log.debug(u'Bad   triggers   detected:   {0}'.format(badTriggersOcc)) 
   return (len(badTriggersOcc) < 2)"," 'Check if the text is a song lyrics. 
 :param text: text to check 
 :type text: str 
 :param artist: artist name to check 
 :type artist: str 
 :return: True if it is a song lyrics, False otherwise'",'Determine whether the text seems to be valid lyrics.'
"def _as_meg_type_evoked(evoked, ch_type='grad', mode='fast'): 
    evoked = evoked.copy() 
   if (ch_type not in ['mag', 'grad']): 
      raise ValueError(('to_type   must   be   ""mag""   or   ""grad"",   not   ""%s""' % ch_type)) 
   pick_from = pick_types(evoked.info, meg=True, eeg=False, ref_meg=False) 
   pick_to = pick_types(evoked.info, meg=ch_type, eeg=False, ref_meg=False) 
   if (len(pick_to) == 0): 
      raise ValueError('No   channels   matching   the   destination   channel   type   found   in   info.   Please   pass   an   evoked   containingboth   the   original   and   destination   channels.   Only   the   locations   of   the   destination   channels   will   be   used   for   interpolation.') 
   info_from = pick_info(evoked.info, pick_from) 
   info_to = pick_info(evoked.info, pick_to) 
   mapping = _map_meg_channels(info_from, info_to, mode=mode) 
   data = np.dot(mapping, evoked.data[pick_from]) 
   evoked.pick_types(meg=ch_type, eeg=False, ref_meg=False) 
   evoked.data = data 
   for ch in evoked.info['chs']: 
      ch['ch_name'] += '_virtual' 
   evoked.info._update_redundant() 
   evoked.info._check_consistency() 
   return evoked"," 'Convert an evoked to the specified channel type. 
 This function takes an evoked with a specified channel type (e.g. ""mag"" or ""grad"") 
 and converts it to the specified channel type. 
 Parameters 
 evoked : Evoked 
 The evoked to be converted. 
 ch_type : str 
 The channel type to convert to (e.g. ""mag"" or ""grad""). 
 mode : str 
 The interpolation mode (e.g. ""nearest"", ""linear"", ""spline""). 
 Returns 
 evoked : Evoked 
 The evoked converted to the specified channel type. 
 Notes 
 This function is used to convert an evoked with the specified channel type to the 
 specified channel type. 
 Examples 
 >>> from nipype.interfaces.io.nib import get_data 
 >>> from nipype.interfaces.io.fsl import read_fsl 
 >>> data = get_data(""fsl/data/sample.nii.gz"") 
 >>> data = read_fsl(data) 
 >>> evoked = _as_meg","'Compute virtual evoked using interpolated fields in mag/grad channels. 
 Parameters 
 evoked : instance of mne.Evoked 
 The evoked object. 
 ch_type : str 
 The destination channel type. It can be \'mag\' or \'grad\'. 
 mode : str 
 Either `\'accurate\'` or `\'fast\'`, determines the quality of the 
 Legendre polynomial expansion used. `\'fast\'` should be sufficient 
 for most applications. 
 Returns 
 evoked : instance of mne.Evoked 
 The transformed evoked object containing only virtual channels.'"
"def getFunctionsWithStringByFileNames(fileNames, searchString): 
    functions = [] 
   for fileName in fileNames: 
      functions += getFunctionsWithStringByFileName(fileName, searchString) 
   functions.sort() 
   return functions"," 'Get all functions with searchString in the files with fileNames. 
 :param fileNames: list of file names 
 :param searchString: search string 
 :return: list of functions'",'Get the functions with the search string in the files.'
"def libvlc_video_get_title_description(p_mi): 
    f = (_Cfunctions.get('libvlc_video_get_title_description', None) or _Cfunction('libvlc_video_get_title_description', ((1,),), None, ctypes.POINTER(TrackDescription), MediaPlayer)) 
   return f(p_mi)"," 'Get the title description for a media. 
 This function is only available for media with a title description. 
 @param p_mi: the media player. 
 @return: the title description or None if there is none.'","'Get the description of available titles. 
 @param p_mi: the media player. 
 @return: list containing description of available titles.'"
"@validator 
 def domain(value): 
    return pattern.match(value)"," 'Validate the domain. 
 :param value: the domain 
 :type value: str 
 :returns: a tuple with the matched pattern and the arguments 
 :rtype: tuple'","'Return whether or not given value is a valid domain. 
 If the value is valid domain name this function returns ``True``, otherwise 
 :class:`~validators.utils.ValidationFailure`. 
 Examples:: 
 >>> domain(\'example.com\') 
 True 
 >>> domain(\'example.com/\') 
 ValidationFailure(func=domain, ...) 
 Supports IDN domains as well:: 
 >>> domain(\'xn----gtbspbbmkef.xn--p1ai\') 
 True 
 .. versionadded:: 0.9 
 .. versionchanged:: 0.10 
 Added support for internationalized domain name (IDN) validation. 
 :param value: domain string to validate'"
"@login_required 
 @ensure_csrf_cookie 
 @require_http_methods(('GET', 'POST', 'PUT')) 
 @expect_json 
 def advanced_settings_handler(request, course_key_string): 
    course_key = CourseKey.from_string(course_key_string) 
   with modulestore().bulk_operations(course_key): 
      course_module = get_course_and_check_access(course_key, request.user) 
      if (('text/html' in request.META.get('HTTP_ACCEPT', '')) and (request.method == 'GET')): 
         return render_to_response('settings_advanced.html', {'context_course': course_module, 'advanced_dict': CourseMetadata.fetch(course_module), 'advanced_settings_url': reverse_course_url('advanced_settings_handler', course_key)}) 
      elif ('application/json' in request.META.get('HTTP_ACCEPT', '')): 
         if (request.method == 'GET'): 
            return JsonResponse(CourseMetadata.fetch(course_module)) 
         else: 
            try: 
               (is_valid, errors, updated_data) = CourseMetadata.validate_and_update_from_json(course_module, request.json, user=request.user) 
               if is_valid: 
                  try: 
                     _refresh_course_tabs(request, course_module) 
                  except InvalidTabsException as err: 
                     log.exception(err.message) 
                     response_message = [{'message': _('An   error   occurred   while   trying   to   save   your   tabs'), 'model': {'display_name': _('Tabs   Exception')}}] 
                     return JsonResponseBadRequest(response_message) 
                  modulestore().update_item(course_module, request.user.id) 
                  return JsonResponse(updated_data) 
               else: 
                  return JsonResponseBadRequest(errors) 
            except (TypeError, ValueError, InvalidTabsException) as err: 
               return HttpResponseBadRequest(django.utils.html.escape(err.message), content_type='text/plain')","'.advanced_settings_handler(request, course_key_string): 
 Handles advanced settings for a course. 
 This endpoint allows the user to update the advanced settings for a course. 
 Arguments: 
 request (Request): The request to handle. 
 course_key_string (str): The string representation of the course key. 
 Returns: 
 JsonResponse: A JSON response. 
 Raises: 
 Http404: If the course cannot be found. 
 Http400: If the request is invalid.'","'Course settings configuration 
 GET 
 html: get the page 
 json: get the model 
 PUT, POST 
 json: update the Course\'s settings. The payload is a json rep of the 
 metadata dicts.'"
"def NeedsMacOSXProxyFakes(): 
    return ((sys.platform == 'darwin') and ((2, 6, 0) <= sys.version_info < (2, 6, 4)))"," 'Returns True if the current system is Mac OS X, and the 
 Python version is 2.6.0 or earlier.'",'Returns True if the MacOS X urllib fakes should be installed.'
"def generateUUID(): 
    return str(uuid.uuid4())"," 'Returns a unique identifier for the current request. 
 This is a string of 36 characters, and is guaranteed to be unique 
 for the lifetime of the request. 
 :return: a string of 36 characters'",'Utility function; generates UUIDs'
"def event(name, priority=128): 
    def decorator(func): 
      add_event_handler(name, func, priority) 
      return func 
   return decorator"," 'Decorator that adds an event handler to the event loop. 
 :param name: The name of the event to listen for. 
 :type name: str 
 :param priority: The priority of the event handler. 
 :type priority: int 
 :return: The decorated function. 
 :rtype: function'",'Register event to function with a decorator'
"def show_frontends(socket='/var/run/haproxy.sock'): 
    ha_conn = _get_conn(socket) 
   ha_cmd = haproxy.cmds.showFrontends() 
   return ha_conn.sendCmd(ha_cmd)"," 'Show the frontend configuration. 
 :param socket: The socket to connect to. 
 :type socket: str 
 :return: The configuration as a dictionary. 
 :rtype: dict'","'Show HaProxy frontends 
 socket 
 haproxy stats socket 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' haproxy.show_frontends'"
"def copy(src, dst, createpath=0, copydates=1, forcetype=None): 
    src = File.pathname(src) 
   dst = File.pathname(dst) 
   if createpath: 
      mkdirs(os.path.split(dst)[0]) 
   ifp = open(src, 'rb') 
   ofp = open(dst, 'wb') 
   d = ifp.read(BUFSIZ) 
   while d: 
      ofp.write(d) 
      d = ifp.read(BUFSIZ) 
   ifp.close() 
   ofp.close() 
   ifp = openrf(src, '*rb') 
   ofp = openrf(dst, '*wb') 
   d = ifp.read(BUFSIZ) 
   while d: 
      ofp.write(d) 
      d = ifp.read(BUFSIZ) 
   ifp.close() 
   ofp.close() 
   srcfss = File.FSSpec(src) 
   dstfss = File.FSSpec(dst) 
   sf = srcfss.FSpGetFInfo() 
   df = dstfss.FSpGetFInfo() 
   (df.Creator, df.Type) = (sf.Creator, sf.Type) 
   if (forcetype is not None): 
      df.Type = forcetype 
   df.Flags = (sf.Flags & COPY_FLAGS) 
   dstfss.FSpSetFInfo(df) 
   if copydates: 
      srcfsr = File.FSRef(src) 
      dstfsr = File.FSRef(dst) 
      (catinfo, _, _, _) = srcfsr.FSGetCatalogInfo(Files.kFSCatInfoAllDates) 
      dstfsr.FSSetCatalogInfo(Files.kFSCatInfoAllDates, catinfo)"," 'Copy a file from src to dst. 
 If createpath is true, the directory path of dst will be created. 
 If copydates is true, the creation date of the file will be copied. 
 If forcetype is specified, the type of the file will be forced. 
 If src and dst are directories, this method will copy the contents of src 
 to dst. 
 Parameters 
 src : str 
 Path of the source file. 
 dst : str 
 Path of the destination file. 
 createpath : bool 
 If true, the directory path of dst will be created. 
 copydates : bool 
 If true, the creation date of the file will be copied. 
 forcetype : str 
 If specified, the type of the file will be forced. 
 Returns 
 None'","'Copy a file, including finder info, resource fork, etc'"
"def assert_snr(actual, desired, tol): 
    from nose.tools import assert_true 
   snr = (linalg.norm(desired, ord='fro') / linalg.norm((desired - actual), ord='fro')) 
   assert_true((snr >= tol), msg=('%f   <   %f' % (snr, tol)))"," 'Assert that the SNR of the desired signal is at least ``tol`` dB above 
 the SNR of the actual signal.'",'Assert actual and desired arrays are within some SNR tolerance'
"def get_widgets(request): 
    widgets = {} 
   widgets.update(WIDGETS) 
   try: 
      agent = request.user.profile.serviceagent_set.all()[0] 
   except Exception: 
      agent = None 
   if (not agent): 
      del widgets['widget_index_assigned'] 
   return widgets"," 'Get the widgets to show. 
 This function returns a dict of widgets to show. 
 If the user is not logged in, then no widgets are shown. 
 If the user is logged in but does not have an assigned agent, then 
 no widgets are shown. 
 If the user is logged in and has an assigned agent, then the widgets 
 shown are determined by the agent. 
 :param request: The current request object. 
 :return: A dict of widgets to show.'",'Returns a set of all available widgets'
"def prepare_wmt_data(data_dir, en_vocabulary_size, fr_vocabulary_size, tokenizer=None): 
    train_path = get_wmt_enfr_train_set(data_dir) 
   dev_path = get_wmt_enfr_dev_set(data_dir) 
   from_train_path = (train_path + '.en') 
   to_train_path = (train_path + '.fr') 
   from_dev_path = (dev_path + '.en') 
   to_dev_path = (dev_path + '.fr') 
   return prepare_data(data_dir, from_train_path, to_train_path, from_dev_path, to_dev_path, en_vocabulary_size, fr_vocabulary_size, tokenizer)"," 'Prepares data for training and testing. 
 :param data_dir: path to data directory 
 :param en_vocabulary_size: number of words in English vocabulary 
 :param fr_vocabulary_size: number of words in French vocabulary 
 :param tokenizer: tokenizer for word segmentation'","'Get WMT data into data_dir, create vocabularies and tokenize data. 
 Args: 
 data_dir: directory in which the data sets will be stored. 
 en_vocabulary_size: size of the English vocabulary to create and use. 
 fr_vocabulary_size: size of the French vocabulary to create and use. 
 tokenizer: a function to use to tokenize each data sentence; 
 if None, basic_tokenizer will be used. 
 Returns: 
 A tuple of 6 elements: 
 (1) path to the token-ids for English training data-set, 
 (2) path to the token-ids for French training data-set, 
 (3) path to the token-ids for English development data-set, 
 (4) path to the token-ids for French development data-set, 
 (5) path to the English vocabulary file, 
 (6) path to the French vocabulary file.'"
"def vacuum(verbose=False): 
    ret = {} 
   imgadm = _check_imgadm() 
   cmd = '{0}   vacuum   -f'.format(imgadm) 
   res = __salt__['cmd.run_all'](cmd) 
   retcode = res['retcode'] 
   if (retcode != 0): 
      ret['Error'] = _exit_status(retcode) 
      return ret 
   result = {} 
   for image in res['stdout'].splitlines(): 
      image = [var for var in image.split('   ') if var] 
      result[image[2]] = {'name': image[3][1:image[3].index('@')], 'version': image[3][(image[3].index('@') + 1):(-1)]} 
   if verbose: 
      return result 
   else: 
      return list(result.keys())"," 'Returns a list of all images currently on the system 
 Returns a list of all images currently on the system 
 Example: 
 vacuum() 
 {name, version} 
 {image1, 1.2.3} 
 {image2, 1.2.3} 
 {image3, 1.2.3}'","'Remove unused images 
 verbose : boolean (False) 
 toggle verbose output 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' imgadm.vacuum [verbose=True]'"
"def diff_tree(repo, old_tree, new_tree, outstream=sys.stdout): 
    with open_repo_closing(repo) as r: 
      write_tree_diff(outstream, r.object_store, old_tree, new_tree)"," 'Write a tree diff to the given stream. 
 :param repo: The repository to read from. 
 :param old_tree: The old tree. 
 :param new_tree: The new tree. 
 :param outstream: The stream to write to.'","'Compares the content and mode of blobs found via two tree objects. 
 :param repo: Path to repository 
 :param old_tree: Id of old tree 
 :param new_tree: Id of new tree 
 :param outstream: Stream to write to'"
"def test_markovchain_pmatrices(): 
    Q = np.zeros((6, 6)) 
   (Q[(0, 1)], Q[(1, 0)]) = (1, 1) 
   Q[(2, [0, 3])] = (1 / 2) 
   (Q[(3, 4)], Q[(4, 5)], Q[(5, 3)]) = (1, 1, 1) 
   Q_stationary_dists = np.array([[(1 / 2), (1 / 2), 0, 0, 0, 0], [0, 0, 0, (1 / 3), (1 / 3), (1 / 3)]]) 
   testset = [{'P': np.array([[0.4, 0.6], [0.2, 0.8]]), 'stationary_dists': np.array([[0.25, 0.75]]), 'comm_classes': [np.arange(2)], 'rec_classes': [np.arange(2)], 'is_irreducible': True, 'period': 1, 'is_aperiodic': True, 'cyclic_classes': [np.arange(2)]}, {'P': sparse.csr_matrix([[0.4, 0.6], [0.2, 0.8]]), 'stationary_dists': np.array([[0.25, 0.75]]), 'comm_classes': [np.arange(2)], 'rec_classes': [np.arange(2)], 'is_irreducible': True, 'period': 1, 'is_aperiodic': True, 'cyclic_classes': [np.arange(2)]}, {'P': np.array([[0, 1], [1, 0]]), 'stationary_dists': np.array([[0.5, 0.5]]), 'comm_classes': [np.arange(2)], 'rec_classes': [np.arange(2)], 'is_irreducible': True, 'period': 2, 'is_aperiodic': False, 'cyclic_classes': [np.array([0]), np.array([1])]}, {'P': np.eye(2), 'stationary_dists': np.array([[1, 0], [0, 1]]), 'comm_classes': [np.array([0]), np.array([1])], 'rec_classes': [np.array([0]), np.array([1])], 'is_irreducible': False, 'period': 1, 'is_aperiodic': True}, {'P': np.array([[1, 0], [1, 0]]), 'stationary_dists': np.array([[1, 0]]), 'comm_classes': [np.array([0]), np.array([1])], 'rec_classes': [np.array([0])], 'is_irreducible': False, 'period': 1, 'is_aperiodic': True}, {'P': Q, 'stationary_dists': Q_stationary_dists, 'comm_classes': [np.array([0, 1]), np.array([2]), np.array([3, 4, 5])], 'rec_classes': [np.array([0, 1]), np.array([3, 4, 5])], 'is_irreducible': False, 'period': 6, 'is_aperiodic': False}, {'P': sparse.csr_matrix(Q), 'stationary_dists': Q_stationary_dists, 'comm_classes': [np.array([0, 1]), np.array([2]), np.array([3, 4, 5])], 'rec_classes': [np.array([0, 1]), np.array([3, 4, 5])], 'is_irreducible': False, 'period': 6, 'is_aperiodic': False}] 
   for test_dict in testset: 
      mc = MarkovChain(test_dict['P']) 
      computed = mc.stationary_distributions 
      assert_allclose(computed, test_dict['stationary_dists']) 
      assert (mc.num_communication_classes == len(test_dict['comm_classes'])) 
      assert (mc.is_irreducible == test_dict['is_irreducible']) 
      assert (mc.num_recurrent_classes == len(test_dict['rec_classes'])) 
      list_of_array_equal(sorted(mc.communication_classes, key=(lambda x: x[0])), sorted(test_dict['comm_classes'], key=(lambda x: x[0]))) 
      list_of_array_equal(sorted(mc.recurrent_classes, key=(lambda x: x[0])), sorted(test_dict['rec_classes'], key=(lambda x: x[0]))) 
      assert (mc.period == test_dict['period']) 
      assert (mc.is_aperiodic == test_dict['is_aperiodic']) 
      try: 
         list_of_array_equal(sorted(mc.cyclic_classes, key=(lambda x: x[0])), sorted(test_dict['cyclic_classes'], key=(lambda x: x[0]))) 
      except NotImplementedError: 
         assert (mc.is_irreducible is False) 
      computed = mc_compute_stationary(test_dict['P']) 
      assert_allclose(computed, test_dict['stationary_dists'])", 'Test the stationary distribution computation',"'Test the methods of MarkovChain, as well as mc_compute_stationary, 
 with P matrix and known solutions'"
"def words(string, filter=(lambda w: w.strip(""'"").isalnum()), punctuation=PUNCTUATION, **kwargs): 
    string = decode_utf8(string) 
   string = re.sub(""([a-z|A-Z])'(m|s|ve|re|ll|d)"", u'\\1   <QUOTE/>\\2', string) 
   string = re.sub(""(c|d|gl|j|l|m|n|s|t|un)'([a-z|A-Z])"", u'\\1<QUOTE/>   \\2', string) 
   words = (w.strip(punctuation).replace(u'<QUOTE/>', ""'"", 1) for w in string.split()) 
   words = (w for w in words if ((filter is None) or (filter(w) is not False))) 
   words = [w for w in words if w] 
   return words"," 'Return a list of words from a string. 
 :param string: The string to split. 
 :param filter: A callable that takes a word and returns True if it should be included 
 in the list of words. 
 :param punctuation: A set of punctuation characters. 
 :param kwargs: Additional arguments to pass to the filter function. 
 :return: A list of words. 
 :rtype: list'","'Returns a list of words (alphanumeric character sequences) from the given string. 
 Common punctuation marks are stripped from words.'"
"@login_required 
 def invitation_error(request, error_message='You   do   not   have   any   invitations   at   this   time.', template_name='invitations/invitation_error.html'): 
    return render(request, template_name, {'error_message': error_message})", 'Display an error message if the user has no invitations.',"'Returns an error template. 
 Template: ``invitations/invitation_error.html`` 
 Context: 
 error_message 
 String containing the error message.'"
"def getTypeFromProgID(prog_id): 
    return Type.GetTypeFromProgID(prog_id)"," 'Get the type from a progID. 
 :param prog_id: The progID. 
 :returns: The type. 
 :rtype: :class:`Type`'",'Returns the Type object for prog_id.'
"def resolve_hostname(hostname): 
    res = socket.getaddrinfo(hostname, None)[0] 
   (family, socktype, proto, canonname, sockaddr) = res 
   return sockaddr[0]"," 'Resolve a hostname to an IP address. 
 This function is a thin wrapper around socket.getaddrinfo(). 
 Parameters 
 hostname : str 
 The hostname to resolve. 
 Returns 
 ip_address : str 
 The resolved IP address. 
 Raises 
 ValueError 
 If the hostname cannot be resolved.'",'Resolves host name to IP address.'
"def get_d3_section_grade_distrib(course_id, section): 
    course = modulestore().get_course(course_id, depth=4) 
   problem_set = [] 
   problem_info = {} 
   c_subsection = 0 
   for subsection in course.get_children()[section].get_children(): 
      c_subsection += 1 
      c_unit = 0 
      for unit in subsection.get_children(): 
         c_unit += 1 
         c_problem = 0 
         for child in unit.get_children(): 
            if (child.location.category == 'problem'): 
               c_problem += 1 
               problem_set.append(child.location) 
               problem_info[child.location] = {'id': child.location.to_deprecated_string(), 'x_value': 'P{0}.{1}.{2}'.format(c_subsection, c_unit, c_problem), 'display_name': own_metadata(child).get('display_name', '')} 
   grade_distrib = get_problem_set_grade_distrib(course_id, problem_set) 
   d3_data = [] 
   for problem in problem_set: 
      stack_data = [] 
      if (problem in grade_distrib): 
         max_grade = float(grade_distrib[problem]['max_grade']) 
         for (grade, count_grade) in grade_distrib[problem]['grade_distrib']: 
            percent = 0.0 
            if (max_grade > 0): 
               percent = round(((grade * 100.0) / max_grade), 1) 
            tooltip = {'type': 'problem', 'problem_info_x': problem_info[problem]['x_value'], 'count_grade': count_grade, 'percent': percent, 'problem_info_n': problem_info[problem]['display_name'], 'grade': grade, 'max_grade': max_grade} 
            stack_data.append({'color': percent, 'value': count_grade, 'tooltip': tooltip}) 
      d3_data.append({'xValue': problem_info[problem]['x_value'], 'stackData': stack_data}) 
   return d3_data"," 'Returns a d3 data array for a section of a course 
 :param course_id: The id of the course 
 :param section: The section to get the data for 
 :return: A d3 data array'","'Returns the grade distribution for the problems in the `section` section in a format for the d3 code. 
 `course_id` a string that is the course\'s ID. 
 `section` an int that is a zero-based index into the course\'s list of sections. 
 Navigates to the section specified to find all the problems associated with that section and then finds the grade 
 distribution for those problems. Finally returns an object formated the way the d3_stacked_bar_graph.js expects its 
 data object to be in. 
 If this is requested multiple times quickly for the same course, it is better to call 
 get_d3_problem_grade_distrib and pick out the sections of interest. 
 Returns an array of dicts with the following keys (taken from d3_stacked_bar_graph.js\'s documentation) 
 \'xValue\' - Corresponding value for the x-axis 
 \'stackData\' - Array of objects with key, value pairs that represent a bar: 
 \'color\' - Defines what ""color"" the bar will map to 
 \'value\' - Maps to the height of the bar, along the y-axis 
 \'tooltip\' - (Optional) Text to display on mouse hover'"
"def _minimize_scalar_golden(func, brack=None, args=(), xtol=_epsilon, maxiter=5000, **unknown_options): 
    _check_unknown_options(unknown_options) 
   tol = xtol 
   if (brack is None): 
      (xa, xb, xc, fa, fb, fc, funcalls) = bracket(func, args=args) 
   elif (len(brack) == 2): 
      (xa, xb, xc, fa, fb, fc, funcalls) = bracket(func, xa=brack[0], xb=brack[1], args=args) 
   elif (len(brack) == 3): 
      (xa, xb, xc) = brack 
      if (xa > xc): 
         (xc, xa) = (xa, xc) 
      if (not ((xa < xb) and (xb < xc))): 
         raise ValueError('Not   a   bracketing   interval.') 
      fa = func(*((xa,) + args)) 
      fb = func(*((xb,) + args)) 
      fc = func(*((xc,) + args)) 
      if (not ((fb < fa) and (fb < fc))): 
         raise ValueError('Not   a   bracketing   interval.') 
      funcalls = 3 
   else: 
      raise ValueError('Bracketing   interval   must   be   length   2   or   3   sequence.') 
   _gR = 0.61803399 
   _gC = (1.0 - _gR) 
   x3 = xc 
   x0 = xa 
   if (numpy.abs((xc - xb)) > numpy.abs((xb - xa))): 
      x1 = xb 
      x2 = (xb + (_gC * (xc - xb))) 
   else: 
      x2 = xb 
      x1 = (xb - (_gC * (xb - xa))) 
   f1 = func(*((x1,) + args)) 
   f2 = func(*((x2,) + args)) 
   funcalls += 2 
   nit = 0 
   for i in xrange(maxiter): 
      if (numpy.abs((x3 - x0)) <= (tol * (numpy.abs(x1) + numpy.abs(x2)))): 
         break 
      if (f2 < f1): 
         x0 = x1 
         x1 = x2 
         x2 = ((_gR * x1) + (_gC * x3)) 
         f1 = f2 
         f2 = func(*((x2,) + args)) 
      else: 
         x3 = x2 
         x2 = x1 
         x1 = ((_gR * x2) + (_gC * x0)) 
         f2 = f1 
         f1 = func(*((x1,) + args)) 
      funcalls += 1 
      nit += 1 
   if (f1 < f2): 
      xmin = x1 
      fval = f1 
   else: 
      xmin = x2 
      fval = f2 
   return OptimizeResult(fun=fval, nfev=funcalls, x=xmin, nit=nit, success=(nit < maxiter))"," 'Minimize a scalar function using the golden section method. 
 Parameters 
 func : function 
 The scalar function to minimize. 
 brack : sequence, optional 
 The bracketing interval. 
 args : sequence, optional 
 The function arguments. 
 xtol : float, optional 
 The tolerance for the bracketing interval. 
 maxiter : int, optional 
 The maximum number of iterations. 
 unknown_options : dict, optional 
 A dictionary of additional options. 
 Returns 
 OptimizeResult 
 The result of the optimization. 
 Examples 
 >>> from sympy import sin, cos, sqrt, Symbol, Function, Abs, tan, E 
 >>> from sympy.optimize.optimize import _minimize_scalar_golden 
 >>> x = Symbol(\'x\') 
 >>> f = Function(\'f\') 
 >>> _minimize_scalar_golden(f(x), brack=(-1, 1), args=(x,), xtol=1e-12, maxiter=1000) 
 Opt","'Options 
 maxiter : int 
 Maximum number of iterations to perform. 
 xtol : float 
 Relative error in solution `xopt` acceptable for convergence.'"
"@task() 
 @timeit 
 def maybe_award_badge(badge_template, year, user): 
    badge = get_or_create_badge(badge_template, year) 
   if badge.is_awarded_to(user): 
      return 
   qs = Reply.objects.filter(user=user, created__gte=date(year, 1, 1), created__lt=date((year + 1), 1, 1)) 
   if (qs.count() >= 50): 
      badge.award_to(user) 
      return True"," 'Award the badge to the user if they have not already been awarded it. 
 The badge template is used to determine which badge is awarded. 
 If the user has not posted 50 replies by the end of the year, 
 the badge will not be awarded.'",'Award the specific badge to the user if they\'ve earned it.'
"def purge(name=None, pkgs=None, **kwargs): 
    return remove(name=name, pkgs=pkgs)"," 'Remove the package from the cache. 
 :param name: Name of the package to remove. 
 :type name: str 
 :param pkgs: List of packages to remove. 
 :type pkgs: list(str) 
 :param kwargs: Additional arguments to pass to :func:`remove`. 
 :type kwargs: dict 
 :return: True if the package was removed, False otherwise. 
 :rtype: bool'","'.. versionchanged:: 2015.8.12,2016.3.3,2016.11.0 
 On minions running systemd>=205, `systemd-run(1)`_ is now used to 
 isolate commands which modify installed packages from the 
 ``salt-minion`` daemon\'s control group. This is done to keep systemd 
 from killing any yum/dnf commands spawned by Salt when the 
 ``salt-minion`` service is restarted. (see ``KillMode`` in the 
 `systemd.kill(5)`_ manpage for more information). If desired, usage of 
 `systemd-run(1)`_ can be suppressed by setting a :mod:`config option 
 <salt.modules.config.get>` called ``systemd.scope``, with a value of 
 ``False`` (no quotes). 
 .. _`systemd-run(1)`: https://www.freedesktop.org/software/systemd/man/systemd-run.html 
 .. _`systemd.kill(5)`: https://www.freedesktop.org/software/systemd/man/systemd.kill.html 
 Package purges are not supported by yum, this function is identical to 
 :mod:`pkg.remove <salt.modules.yumpkg.remove>`. 
 name 
 The name of the package to be purged 
 Multiple Package Options: 
 pkgs 
 A list of packages to delete. Must be passed as a python list. The 
 ``name`` parameter will be ignored if this option is passed. 
 .. versionadded:: 0.16.0 
 Returns a dict containing the changes. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' pkg.purge <package name> 
 salt \'*\' pkg.purge <package1>,<package2>,<package3> 
 salt \'*\' pkg.purge pkgs=\'[""foo"", ""bar""]\''"
"def buildRequestBytes(headers, data, frameFactory=None, streamID=1): 
    frames = buildRequestFrames(headers, data, frameFactory, streamID) 
   return ''.join((f.serialize() for f in frames))"," 'Builds a bytes object containing the HTTP request. 
 :param headers: The HTTP headers. 
 :param data: The HTTP data. 
 :param frameFactory: A factory for creating :class:`twisted.web.http.Frame` 
 objects.  If None, a default factory is used. 
 :param streamID: The stream ID to use.  If None, a default stream ID is used. 
 :return: A bytes object containing the HTTP request. 
 :rtype: bytes'","'Provides the byte sequence for a collection of HTTP/2 frames representing 
 the provided request. 
 @param headers: The HTTP/2 headers to send. 
 @type headers: L{list} of L{tuple} of L{bytes} 
 @param data: The HTTP data to send. Each list entry will be sent in its own 
 frame. 
 @type data: L{list} of L{bytes} 
 @param frameFactory: The L{FrameFactory} that will be used to construct the 
 frames. 
 @type frameFactory: L{FrameFactory} 
 @param streamID: The ID of the stream on which to send the request. 
 @type streamID: L{int}'"
"def request_fingerprint(request, include_headers=None): 
    if include_headers: 
      include_headers = tuple([h.lower() for h in sorted(include_headers)]) 
   cache = _fingerprint_cache.setdefault(request, {}) 
   if (include_headers not in cache): 
      fp = hashlib.sha1() 
      fp.update(request.method) 
      fp.update(canonicalize_url(request.url)) 
      fp.update((request.body or '')) 
      if include_headers: 
         for hdr in include_headers: 
            if (hdr in request.headers): 
               fp.update(hdr) 
               for v in request.headers.getlist(hdr): 
                  fp.update(v) 
      cache[include_headers] = fp.hexdigest() 
   return cache[include_headers]"," 'Return a fingerprint of the request. 
 :param include_headers: list of headers to include in the fingerprint. 
 :type include_headers: tuple'","'Return the request fingerprint. 
 The request fingerprint is a hash that uniquely identifies the resource the 
 request points to. For example, take the following two urls: 
 http://www.example.com/query?id=111&cat=222 
 http://www.example.com/query?cat=222&id=111 
 Even though those are two different URLs both point to the same resource 
 and are equivalent (ie. they should return the same response). 
 Another example are cookies used to store session ids. Suppose the 
 following page is only accesible to authenticated users: 
 http://www.example.com/members/offers.html 
 Lot of sites use a cookie to store the session id, which adds a random 
 component to the HTTP Request and thus should be ignored when calculating 
 the fingerprint. 
 For this reason, request headers are ignored by default when calculating 
 the fingeprint. If you want to include specific headers use the 
 include_headers argument, which is a list of Request headers to include.'"
"@mock_streams('stderr') 
 @with_patched_object(output, 'warnings', True) 
 def test_warn(): 
    warn('Test') 
   eq_('\nWarning:   Test\n\n', sys.stderr.getvalue())", 'Test warnings.warn','warn() should print \'Warning\' plus given text'
"def setraw(fd, when=TCSAFLUSH): 
    mode = tcgetattr(fd) 
   mode[IFLAG] = (mode[IFLAG] & (~ ((((BRKINT | ICRNL) | INPCK) | ISTRIP) | IXON))) 
   mode[OFLAG] = (mode[OFLAG] & (~ OPOST)) 
   mode[CFLAG] = (mode[CFLAG] & (~ (CSIZE | PARENB))) 
   mode[CFLAG] = (mode[CFLAG] | CS8) 
   mode[LFLAG] = (mode[LFLAG] & (~ (((ECHO | ICANON) | IEXTEN) | ISIG))) 
   mode[CC][VMIN] = 1 
   mode[CC][VTIME] = 0 
   tcsetattr(fd, when, mode)"," 'Set raw mode on a file descriptor. 
 :param fd: File descriptor. 
 :param when: When to set the mode. 
 :return: None'",'Put terminal into a raw mode.'
"def validate(cls, model): 
    models.get_apps() 
   opts = model._meta 
   validate_base(cls, model) 
   if hasattr(cls, 'list_display'): 
      check_isseq(cls, 'list_display', cls.list_display) 
      for (idx, field) in enumerate(cls.list_display): 
         if (not callable(field)): 
            if (not hasattr(cls, field)): 
               if (not hasattr(model, field)): 
                  try: 
                     opts.get_field(field) 
                  except models.FieldDoesNotExist: 
                     raise ImproperlyConfigured(('%s.list_display[%d],   %r   is   not   a   callable   or   an   attribute   of   %r   or   found   in   the   model   %r.' % (cls.__name__, idx, field, cls.__name__, model._meta.object_name))) 
               else: 
                  f = fetch_attr(cls, model, opts, ('list_display[%d]' % idx), field) 
                  if isinstance(f, models.ManyToManyField): 
                     raise ImproperlyConfigured((""'%s.list_display[%d]',   '%s'   is   a   ManyToManyField   which   is   not   supported."" % (cls.__name__, idx, field))) 
   if hasattr(cls, 'list_display_links'): 
      check_isseq(cls, 'list_display_links', cls.list_display_links) 
      for (idx, field) in enumerate(cls.list_display_links): 
         if (field not in cls.list_display): 
            raise ImproperlyConfigured((""'%s.list_display_links[%d]'   refers   to   '%s'   which   is   not   defined   in   'list_display'."" % (cls.__name__, idx, field))) 
   if hasattr(cls, 'list_filter'): 
      check_isseq(cls, 'list_filter', cls.list_filter) 
      for (idx, fpath) in enumerate(cls.list_filter): 
         try: 
            get_fields_from_path(model, fpath) 
         except (NotRelationField, FieldDoesNotExist) as e: 
            raise ImproperlyConfigured((""'%s.list_filter[%d]'   refers   to   '%s'   which   does   not   refer   to   a   Field."" % (cls.__name__, idx, fpath))) 
   if (hasattr(cls, 'list_per_page') and (not isinstance(cls.list_per_page, int))): 
      raise ImproperlyConfigured((""'%s.list_per_page'   should   be   a   integer."" % cls.__name__)) 
   if (hasattr(cls, 'list_editable') and cls.list_editable): 
      check_isseq(cls, 'list_editable', cls.list_editable) 
      for (idx, field_name) in enumerate(cls.list_editable): 
         try: 
            field = opts.get_field_by_name(field_name)[0] 
         except models.FieldDoesNotExist: 
            raise ImproperlyConfigured((""'%s.list_editable[%d]'   refers   to   a   field,   '%s',   not   defined   on   %s."" % (cls.__name__, idx, field_name, model.__name__))) 
         if (field_name not in cls.list_display): 
            raise ImproperlyConfigured((""'%s.list_editable[%d]'   refers   to   '%s'   which   is   not   defined   in   'list_display'."" % (cls.__name__, idx, field_name))) 
         if (field_name in cls.list_display_links): 
            raise ImproperlyConfigured((""'%s'   cannot   be   in   both   '%s.list_editable'   and   '%s.list_display_links'"" % (field_name, cls.__name__, cls.__name__))) 
         if ((not cls.list_display_links) and (cls.list_display[0] in cls.list_editable)): 
            raise ImproperlyConfigured((""'%s.list_editable[%d]'   refers   to   the   first   field   in   list_display,   '%s',   which   can't   be   used   unless   list_display_links   is   set."" % (cls.__name__, idx, cls.list_display[0]))) 
         if (not field.editable): 
            raise ImproperlyConfigured((""'%s.list_editable[%d]'   refers   to   a   field,   '%s',   which   isn't   editable   through   the   admin."" % (cls.__name__, idx, field_name))) 
   if hasattr(cls, 'search_fields'): 
      check_isseq(cls, 'search_fields', cls.search_fields) 
   if cls.date_hierarchy: 
      f = get_field(cls, model, opts, 'date_hierarchy', cls.date_hierarchy) 
      if (not isinstance(f, (models.DateField, models.DateTimeField))): 
         raise ImproperlyConfigured((""'%s.date_hierarchy   is   neither   an   instance   of   DateField   nor   DateTimeField."" % cls.__name__)) 
   if cls.ordering: 
      check_isseq(cls, 'ordering', cls.ordering) 
      for (idx, field) in enumerate(cls.ordering): 
         if ((field == '?') and (len(cls.ordering) != 1)): 
            raise ImproperlyConfigured((""'%s.ordering'   has   the   random   ordering   marker   '?',   but   contains   other   fields   as   well.   Please   either   remove   '?'   or   the   other   fields."" % cls.__name__)) 
         if (field == '?'): 
            continue 
         if field.startswith('-'): 
            field = field[1:] 
         if ('__' in field): 
            continue 
         get_field(cls, model, opts, ('ordering[%d]' % idx), field) 
   if hasattr(cls, 'readonly_fields'): 
      check_readonly_fields(cls, model, opts) 
   for attr in ('list_select_related', 'save_as', 'save_on_top'): 
      if (not isinstance(getattr(cls, attr), bool)): 
         raise ImproperlyConfigured((""'%s.%s'   should   be   a   boolean."" % (cls.__name__, attr))) 
   if hasattr(cls, 'inlines'): 
      check_isseq(cls, 'inlines', cls.inlines) 
      for (idx, inline) in enumerate(cls.inlines): 
         if (not issubclass(inline, BaseModelAdmin)): 
            raise ImproperlyConfigured((""'%s.inlines[%d]'   does   not   inherit   from   BaseModelAdmin."" % (cls.__name__, idx))) 
         if (not inline.model): 
            raise ImproperlyConfigured((""'model'   is   a   required   attribute   of   '%s.inlines[%d]'."" % (cls.__name__, idx))) 
         if (not issubclass(inline.model, models.Model)): 
            raise ImproperlyConfigured((""'%s.inlines[%d].model'   does   not   inherit   from   models.Model."" % (cls.__name__, idx))) 
         validate_base(inline, inline.model) 
         validate_inline(inline, cls, model)"," 'Validate a ModelAdmin subclass. 
 Raises ImproperlyConfigured if the class is invalid.'","'Does basic ModelAdmin option validation. Calls custom validation 
 classmethod in the end if it is provided in cls. The signature of the 
 custom validation classmethod should be: def validate(cls, model).'"
"@task 
 def foo(): 
    pass", 'foo','Foos!'
"def posixToNtSlashes(filepath): 
    return filepath.replace('/', '\\')", 'Replaces posix style slashes with nt style slashes',"'Replaces all occurances of Posix slashes (/) in provided 
 filepath with NT ones (\) 
 >>> posixToNtSlashes(\'C:/Windows\') 
 \'C:\\Windows\''"
"def hexdump(data): 
    values = [] 
   ascii = [] 
   offset = 0 
   for (h, a) in sixteen(data): 
      if (h is None): 
         (yield (offset, '   '.join([''.join(values), ''.join(ascii)]))) 
         del values[:] 
         del ascii[:] 
         offset += 16 
      else: 
         values.append(h) 
         ascii.append(a)"," 'Convert hexadecimal data into a string of ASCII characters. 
 Arguments: 
 data: A byte array. 
 Returns: 
 A string of ASCII characters.'",'yield lines with hexdump of data'
"def _make_dssp_dict(handle): 
    dssp = {} 
   start = 0 
   keys = [] 
   for l in handle.readlines(): 
      sl = l.split() 
      if (len(sl) < 2): 
         continue 
      if (sl[1] == 'RESIDUE'): 
         start = 1 
         continue 
      if (not start): 
         continue 
      if (l[9] == '   '): 
         continue 
      dssp_index = int(l[:5]) 
      resseq = int(l[5:10]) 
      icode = l[10] 
      chainid = l[11] 
      aa = l[13] 
      ss = l[16] 
      if (ss == '   '): 
         ss = '-' 
      try: 
         NH_O_1_relidx = int(l[38:45]) 
         NH_O_1_energy = float(l[46:50]) 
         O_NH_1_relidx = int(l[50:56]) 
         O_NH_1_energy = float(l[57:61]) 
         NH_O_2_relidx = int(l[61:67]) 
         NH_O_2_energy = float(l[68:72]) 
         O_NH_2_relidx = int(l[72:78]) 
         O_NH_2_energy = float(l[79:83]) 
         acc = int(l[34:38]) 
         phi = float(l[103:109]) 
         psi = float(l[109:115]) 
      except ValueError as exc: 
         if (l[34] != '   '): 
            shift = l[34:].find('   ') 
            NH_O_1_relidx = int(l[(38 + shift):(45 + shift)]) 
            NH_O_1_energy = float(l[(46 + shift):(50 + shift)]) 
            O_NH_1_relidx = int(l[(50 + shift):(56 + shift)]) 
            O_NH_1_energy = float(l[(57 + shift):(61 + shift)]) 
            NH_O_2_relidx = int(l[(61 + shift):(67 + shift)]) 
            NH_O_2_energy = float(l[(68 + shift):(72 + shift)]) 
            O_NH_2_relidx = int(l[(72 + shift):(78 + shift)]) 
            O_NH_2_energy = float(l[(79 + shift):(83 + shift)]) 
            acc = int(l[(34 + shift):(38 + shift)]) 
            phi = float(l[(103 + shift):(109 + shift)]) 
            psi = float(l[(109 + shift):(115 + shift)]) 
         else: 
            raise ValueError(exc) 
      res_id = ('   ', resseq, icode) 
      dssp[(chainid, res_id)] = (aa, ss, acc, phi, psi, dssp_index, NH_O_1_relidx, NH_O_1_energy, O_NH_1_relidx, O_NH_1_energy, NH_O_2_relidx, NH_O_2_energy, O_NH_2_relidx, O_NH_2_energy) 
      keys.append((chainid, res_id)) 
   return (dssp, keys)"," 'Return a dictionary of dssp records for a PDB file. 
 The dssp records are ordered by chain and residue number. 
 The dictionary keys are tuples of (chainid, residue_id) where 
 chainid is the chain id and residue_id is a tuple of (resseq, icode, 
 acc, phi, psi). 
 The dictionary values are tuples of (resseq, icode, acc, phi, psi, 
 NH_O_1_relidx, NH_O_1_energy, O_NH_1_relidx, O_NH_1_energy, 
 NH_O_2_relidx, NH_O_2_energy, O_NH_2_relidx, O_NH_2_energy). 
 The NH_O_1_relidx, NH_O_2_relidx, and O_NH_1_relidx are the relative 
 indices of the NH, O, and NH atoms in the first and second 
 NH-O bond. 
 The NH","'Internal function used by mask_dssp_dict (PRIVATE). 
 Return a DSSP dictionary that maps (chainid, resid) to an amino acid, 
 secondary structure symbol, solvent accessibility value, and hydrogen bond 
 information (relative dssp indices and hydrogen bond energies) from an open 
 DSSP file object. :: 
 Parameters 
 handle : file 
 the open DSSP output file handle'"
"def null_safe(rule): 
    def null_safe_rl(expr): 
      result = rule(expr) 
      if (result is None): 
         return expr 
      else: 
         return result 
   return null_safe_rl"," 'Returns a rule that will return the original expression if it is not 
 null. 
 This is useful for rules that should not raise exceptions if the 
 expression is null.'",'Return original expr if rule returns None'
"def lucene_search(trans, cntrller, search_term, search_url, **kwd): 
    message = escape(kwd.get('message', '')) 
   status = kwd.get('status', 'done') 
   full_url = ('%s/find?%s' % (search_url, urllib.urlencode({'kwd': search_term}))) 
   response = urllib2.urlopen(full_url) 
   ldda_ids = loads(response.read())['ids'] 
   response.close() 
   lddas = [trans.sa_session.query(trans.app.model.LibraryDatasetDatasetAssociation).get(ldda_id) for ldda_id in ldda_ids] 
   return (status, message, get_sorted_accessible_library_items(trans, cntrller, lddas, 'name'))"," 'Searches for a LibraryDatasetDatasetAssociation. 
 :param trans: the current transaction 
 :param cntrller: the LibraryDatasetDatasetAssociation controller 
 :param search_term: the search term 
 :param search_url: the search url 
 :param kwargs: keyword arguments to be passed to the search 
 :returns: a tuple of (status, message, library_items) 
 :rtype: tuple'",'Return display of results from a full-text lucene search of data libraries.'
"def _generate_graphs(): 
    with gzip.open(ATLAS_FILE, 'rb') as f: 
      line = f.readline() 
      while (line and line.startswith('GRAPH')): 
         graph_index = int(line[6:].rstrip()) 
         line = f.readline() 
         num_nodes = int(line[6:].rstrip()) 
         edgelist = [] 
         line = f.readline() 
         while (line and (not line.startswith('GRAPH'))): 
            edgelist.append(line.rstrip()) 
            line = f.readline() 
         G = nx.Graph() 
         G.name = 'G{}'.format(graph_index) 
         G.add_nodes_from(range(num_nodes)) 
         G.add_edges_from((tuple(map(int, e.split())) for e in edgelist)) 
         (yield G)", 'Reads the ATLAS file and generates the graphs',"'Sequentially read the file containing the edge list data for the 
 graphs in the atlas and generate the graphs one at a time. 
 This function reads the file given in :data:`.ATLAS_FILE`.'"
"@jinja2.contextfunction 
 @library.global_function 
 def number(context, n): 
    if (n is None): 
      return '' 
   return format_decimal(n, locale=_babel_locale(_contextual_locale(context)))"," 'Format a number. 
 The number will be formatted using the current locale settings. 
 :param n: The number to format. 
 :type n: number 
 :return: The formatted number.'","'Return the localized representation of an integer or decimal. 
 For None, print nothing.'"
"def split_qexpr_parts(e): 
    expr_part = [] 
   qexpr_part = [] 
   for arg in e.args: 
      if (not isinstance(arg, QExpr)): 
         expr_part.append(arg) 
      else: 
         qexpr_part.append(arg) 
   return (expr_part, qexpr_part)"," 'Split a QExpr into its parts, one for the expression and one for the 
 QExpr.'",'Split an expression into Expr and noncommutative QExpr parts.'
"def test_column_width(): 
    assert_equals(strings.column_width(u'\u3042\u3044\u3046\u3048\u304a'), 10)", 'Test column width','strings.column_width'
"def timefunc(num_tries=1, verbose=True): 
    def real_decorator(function): 
      @wraps(function) 
      def wrapper(*args, **kwargs): 
         ts = time.time() 
         for i in range(num_tries): 
            result = function(*args, **kwargs) 
         te = time.time() 
         tt = ((te - ts) / num_tries) 
         if verbose: 
            log.info(u'{0}   took   {1}   s   on   AVERAGE   for   {2}   call(s).'.format(function.__name__, tt, num_tries)) 
         return (tt, result) 
      return wrapper 
   return real_decorator"," 'Decorator to wrap a function with a time measurement. 
 This decorator is used to measure the time it takes to run a function. 
 This is useful for debugging purposes. 
 :param num_tries: The number of times to run the function before 
 measuring the time. 
 :param verbose: Whether to print a log message. 
 :return: The function decorator. 
 :rtype: function'","'Decorator to time a function or method. 
 Parameters 
 num_tries : int, optional 
 Number of calls to make. Timer will take the 
 average run time. 
 verbose : bool, optional 
 Extra log information. 
 Returns 
 tt : float 
 Average run time in seconds. 
 result 
 Output(s) from the function. 
 Examples 
 To add timer to time `numpy.log` for 100 times with 
 verbose output:: 
 import numpy as np 
 from astropy.utils.timer import timefunc 
 @timefunc(100) 
 def timed_log(x): 
 return np.log(x) 
 To run the decorated function above: 
 >>> t, y = timed_log(100) 
 INFO: timed_log took 9.29832458496e-06 s on AVERAGE for 100 call(s). [...] 
 >>> t 
 9.298324584960938e-06 
 >>> y 
 4.6051701859880918'"
"def pmap_field(key_type, value_type, optional=False, invariant=_valid, initial=_UNDEFINED, factory=None): 
    input_factory = factory 
   class TheMap(CheckedPMap, ): 
      __key_type__ = key_type 
      __value_type__ = value_type 
   TheMap.__name__ = ((key_type.__name__.capitalize() + value_type.__name__.capitalize()) + 'PMap') 
   if optional: 
      def mapping_factory(argument): 
         if (argument is None): 
            return None 
         else: 
            return TheMap(argument) 
   else: 
      mapping_factory = TheMap 
   if input_factory: 
      factory = (lambda x: mapping_factory(input_factory(x))) 
   else: 
      factory = mapping_factory 
   if (initial is _UNDEFINED): 
      initial = TheMap() 
   else: 
      initial = factory(initial) 
   return field(mandatory=True, initial=initial, type=(optional_type(TheMap) if optional else TheMap), factory=factory, invariant=invariant)"," 'Create a new PMap field. 
 :param key_type: The type of the key. 
 :param value_type: The type of the value. 
 :param optional: If True, the field is optional. 
 :param invariant: The type checker for the field. 
 :param initial: The initial value of the field. 
 :param factory: The factory function for creating new instances of the 
 :py:class:`~.CheckedPMap`. 
 :rtype: :py:class:`~.field.Field` 
 :returns: A new field instance.'","'Create a checked ``PMap`` field. 
 :param key: The required type for the keys of the map. 
 :param value: The required type for the values of the map. 
 :param bool optional: If true, ``None`` can be used as a value for this 
 field. 
 :param invariant: Pass-through to ``field``. 
 :param initial: An initial value for the field.  This will first be coerced 
 using the field\'s factory.  If not given, the initial value is an empty 
 map. 
 :param factory: A factory used to convert input arguments to the stored 
 value whenever it is set. Note that this will be composed with the 
 constructor for the ``CheckedPMap`` class constructed for this field. 
 :return: A ``field`` containing a ``CheckedPMap``.'"
"def is_safe_path_component(path): 
    return (path and ('/' not in path) and (path not in ('.', '..')))"," 'Checks if the given path is safe to use as a component of a path. 
 The path is considered safe if it is not a dot or a double dot.'","'Check if path is a single component of a path. 
 Check that the path is safe to join too.'"
"def upload(): 
    os.system('cd   build/html;   rsync   -avz   .   pandas@pandas.pydata.org:/usr/share/nginx/pandas/pandas-docs/vbench/   -essh')", 'Upload the docs to the pandas server.','push a copy to the site'
"def send_mail(to_addr, mail, mimetype='plain', from_addr=None, mailer=None, username=None, password=None, callback=None, **context): 
    from_addr = (from_addr or settings.FROM_EMAIL) 
   mailer = (mailer or tasks.send_email) 
   subject = mail.subject(**context) 
   message = (mail.text(**context) if (mimetype in ('plain', 'txt')) else mail.html(**context)) 
   ttls = login = (not settings.DEBUG_MODE) 
   logger.debug('Sending   email...') 
   logger.debug(u'To:   {to_addr}\nFrom:   {from_addr}\nSubject:   {subject}\nMessage:   {message}'.format(**locals())) 
   kwargs = dict(from_addr=from_addr, to_addr=to_addr, subject=subject, message=message, mimetype=mimetype, ttls=ttls, login=login, username=username, password=password, categories=mail.categories) 
   if settings.USE_EMAIL: 
      if settings.USE_CELERY: 
         return mailer.apply_async(kwargs=kwargs, link=callback) 
      else: 
         ret = mailer(**kwargs) 
         if callback: 
            callback() 
         return ret"," 'Send an email to the given address. 
 The default mailer is sendmail, but you can use any mailer you want. 
 The default from_addr is settings.FROM_EMAIL, but you can specify any email 
 address you want. 
 The default mimetype is plain, but you can specify html or txt. 
 If you are using celery, you can use the link keyword to specify a callback 
 function to be called after the email is sent. 
 The default subject is the subject of the email, but you can specify a 
 different subject. 
 The default message is the message of the email, but you can specify a 
 different message. 
 You can also specify a custom mailer function, a custom from_addr, a custom 
 subject, a custom message, and a custom mimetype. 
 :param to_addr: the address to send the email to 
 :param mail: the email message to send 
 :param mimetype: the mimetype of the message 
 :param from_addr: the address to send the email from 
 :param mailer: the mailer to use 
 :param","'Send an email from the OSF. 
 Example: :: 
 from website import mails 
 mails.send_email(\'foo@bar.com\', mails.TEST, name=""Foo"") 
 :param str to_addr: The recipient\'s email address 
 :param Mail mail: The mail object 
 :param str mimetype: Either \'plain\' or \'html\' 
 :param function callback: celery task to execute after send_mail completes 
 :param **context: Context vars for the message template 
 .. note: 
 Uses celery if available'"
"def templatize(src): 
    from django.template import Lexer, TOKEN_TEXT, TOKEN_VAR, TOKEN_BLOCK 
   out = StringIO() 
   intrans = False 
   inplural = False 
   singular = [] 
   plural = [] 
   for t in Lexer(src, None).tokenize(): 
      if intrans: 
         if (t.token_type == TOKEN_BLOCK): 
            endbmatch = endblock_re.match(t.contents) 
            pluralmatch = plural_re.match(t.contents) 
            if endbmatch: 
               if inplural: 
                  out.write(('   ngettext(%r,%r,count)   ' % (''.join(singular), ''.join(plural)))) 
                  for part in singular: 
                     out.write(blankout(part, 'S')) 
                  for part in plural: 
                     out.write(blankout(part, 'P')) 
               else: 
                  out.write(('   gettext(%r)   ' % ''.join(singular))) 
                  for part in singular: 
                     out.write(blankout(part, 'S')) 
               intrans = False 
               inplural = False 
               singular = [] 
               plural = [] 
            elif pluralmatch: 
               inplural = True 
            else: 
               raise SyntaxError, ('Translation   blocks   must   not   include   other   block   tags:   %s' % t.contents) 
         elif (t.token_type == TOKEN_VAR): 
            if inplural: 
               plural.append(('%%(%s)s' % t.contents)) 
            else: 
               singular.append(('%%(%s)s' % t.contents)) 
         elif (t.token_type == TOKEN_TEXT): 
            if inplural: 
               plural.append(t.contents) 
            else: 
               singular.append(t.contents) 
      elif (t.token_type == TOKEN_BLOCK): 
         imatch = inline_re.match(t.contents) 
         bmatch = block_re.match(t.contents) 
         cmatches = constant_re.findall(t.contents) 
         if imatch: 
            g = imatch.group(1) 
            if (g[0] == '""'): 
               g = g.strip('""') 
            elif (g[0] == ""'""): 
               g = g.strip(""'"") 
            out.write(('   gettext(%r)   ' % g)) 
         elif bmatch: 
            intrans = True 
            inplural = False 
            singular = [] 
            plural = [] 
         elif cmatches: 
            for cmatch in cmatches: 
               out.write(('   _(%s)   ' % cmatch)) 
         else: 
            out.write(blankout(t.contents, 'B')) 
      elif (t.token_type == TOKEN_VAR): 
         parts = t.contents.split('|') 
         cmatch = constant_re.match(parts[0]) 
         if cmatch: 
            out.write(('   _(%s)   ' % cmatch.group(1))) 
         for p in parts[1:]: 
            if (p.find(':_(') >= 0): 
               out.write(('   %s   ' % p.split(':', 1)[1])) 
            else: 
               out.write(blankout(p, 'F')) 
      else: 
         out.write(blankout(t.contents, 'X')) 
   return out.getvalue()"," 'Templatize a string of gettext calls. 
 This function takes a string of gettext calls and returns a string of 
 calls with the %s, _(), and gettext() calls replaced with the actual 
 translations. 
 The string is templatized using the Python template language. 
 This function is useful for debugging, and for creating a translation 
 file for a particular locale. 
 :param src: A string of gettext calls. 
 :return: A string of translated calls. 
 :rtype: str'","'Turns a Django template into something that is understood by xgettext. It 
 does so by translating the Django translation tags into standard gettext 
 function invocations.'"
"def processSVGElementpath(svgReader, xmlElement): 
    if ('d' not in xmlElement.attributeDictionary): 
      print 'Warning,   in   processSVGElementpath   in   svgReader   can   not   get   a   value   for   d   in:' 
      print xmlElement.attributeDictionary 
      return 
   rotatedLoopLayer = svgReader.getRotatedLoopLayer() 
   PathReader(rotatedLoopLayer.loops, xmlElement, svgReader.yAxisPointingUpward)"," 'Process the elementpath attribute of an svg element. 
 The elementpath attribute is an svg element that contains an svg path 
 element. 
 This function takes an svg element and processes the elementpath attribute 
 to extract the path information. 
 @param svgReader: The svg reader. 
 @param xmlElement: The xml element that contains the elementpath attribute. 
 @type xmlElement: lxml.etree.Element 
 @return: None'",'Process xmlElement by svgReader.'
"def record_usage_multi(prefix_slices): 
    keys = [_make_ratelimit_cache_key(k, t) for (k, t) in prefix_slices] 
   try: 
      now = int(time.time()) 
      for (key, (_, time_slice)) in zip(keys, prefix_slices): 
         g.ratelimitcache.add(key, 0, time=((time_slice.end - now) + 1)) 
      try: 
         recent_usage = g.ratelimitcache.incr_multi(keys) 
      except pylibmc.NotFound: 
         now = int(time.time()) 
         if (now < time_slice.end): 
            recent_usage = [] 
            for (key, (_, time_slice)) in zip(keys, prefix_slices): 
               if g.ratelimitcache.add(key, 1, time=((time_slice.end - now) + 1)): 
                  recent_usage.append(1) 
                  g.stats.simple_event('ratelimit.eviction') 
               else: 
                  recent_usage.append(g.ratelimitcache.get(key)) 
      return recent_usage 
   except pylibmc.Error as e: 
      raise RatelimitError(e)"," 'Record the recent usage of each prefix slice. 
 :param prefix_slices: A list of (prefix, time_slice) pairs. 
 :returns: A list of recent usage values for each prefix slice. 
 :raises: RatelimitError if an error occurs.'","'Record usage of multiple rate limits. 
 If any of the of the rate limits expire during the processing of the 
 function, the usage counts may be inaccurate and it is not defined 
 which, if any, of the keys have been updated in the underlying cache. 
 Arguments: 
 prefix_slices: A list of (prefix, timeslice) 
 Returns: 
 A list of the usage counts in the same order as prefix_slices. 
 Raises: 
 RateLimitError if anything goes wrong. 
 It is not defined which, if any, of the keys have been updated if this 
 happens.'"
"@command(name='hash', usage='compute   hashes') 
 def print_hash(args): 
    import lixian_hash 
   import lixian_cli_parser 
   lixian_hash.main(lixian_cli_parser.expand_command_line(args))", 'print hash',"'lx hash --sha1 file... 
 lx hash --md5 file... 
 lx hash --md4 file... 
 lx hash --dcid file... 
 lx hash --ed2k file... 
 lx hash --info-hash xxx.torrent... 
 lx hash --verify-sha1 file hash 
 lx hash --verify-md5 file hash 
 lx hash --verify-md4 file hash 
 lx hash --verify-dcid file hash 
 lx hash --verify-ed2k file ed2k://... 
 lx hash --verify-bt file xxx.torrent'"
"def print_environ(environ=os.environ): 
    keys = environ.keys() 
   keys.sort() 
   print 
   print '<H3>Shell   Environment:</H3>' 
   print '<DL>' 
   for key in keys: 
      print '<DT>', escape(key), '<DD>', escape(environ[key]) 
   print '</DL>' 
   print", 'Prints the shell environment.','Dump the shell environment as HTML.'
"def for_signed_dtypes_combination(names=('dtype',), full=None): 
    return for_dtypes_combination(_signed_dtypes, names=names, full=full)", 'Return a list of combinations of signed dtypes.',"'Decorator for parameterized test w.r.t. the product set of signed dtypes. 
 Args: 
 names(list of str): Argument names to which dtypes are passed. 
 full(bool): If ``True``, then all combinations of dtypes 
 will be tested. 
 Otherwise, the subset of combinations will be tested 
 (see description in :func:`cupy.testing.for_dtypes_combination`). 
 .. seealso:: :func:`cupy.testing.for_dtypes_combination`'"
"def urldecode(query): 
    if (query and (not (set(query) <= urlencoded))): 
      raise ValueError('Invalid   characters   in   query   string.') 
   invalid_hex = u'%[^0-9A-Fa-f]|%[0-9A-Fa-f][^0-9A-Fa-f]' 
   if len(re.findall(invalid_hex, query)): 
      raise ValueError('Invalid   hex   encoding   in   query   string.') 
   query = (query.decode('utf-8') if isinstance(query, str) else query) 
   params = urlparse.parse_qsl(query, keep_blank_values=True) 
   return decode_params_utf8(params)"," 'Decode a query string into a dictionary of key/value pairs. 
 This function decodes the query string into a dictionary of key/value 
 pairs. 
 The query string is assumed to be in UTF-8 encoding. 
 The keys and values are not checked for validity. 
 :param query: The query string to decode. 
 :type query: str 
 :return: A dictionary of key/value pairs. 
 :rtype: dict'","'Decode a query string in x-www-form-urlencoded format into a sequence 
 of two-element tuples. 
 Unlike urlparse.parse_qsl(..., strict_parsing=True) urldecode will enforce 
 correct formatting of the query string by validation. If validation fails 
 a ValueError will be raised. urllib.parse_qsl will only raise errors if 
 any of name-value pairs omits the equals sign.'"
"def test_emacs_cursor_movements(): 
    (result, cli) = _feed_cli_with_input(u'hello\x01X\n') 
   assert (result.text == u'Xhello') 
   (result, cli) = _feed_cli_with_input(u'hello\x01X\x05Y\n') 
   assert (result.text == u'XhelloY') 
   (result, cli) = _feed_cli_with_input(u'hello\x08X\n') 
   assert (result.text == u'hellX') 
   (result, cli) = _feed_cli_with_input(u'hello\x1b[D\x1b[D\x1b[3~\n') 
   assert (result.text == u'helo') 
   (result, cli) = _feed_cli_with_input(u'hello\x1b[DX\n') 
   assert (result.text == u'hellXo') 
   (result, cli) = _feed_cli_with_input(u'hello\x01\x1b[CX\n') 
   assert (result.text == u'hXello') 
   (result, cli) = _feed_cli_with_input(u'hello\x01\x1b[CX\n') 
   assert (result.text == u'hXello') 
   (result, cli) = _feed_cli_with_input(u'hello\x02X\n') 
   assert (result.text == u'hellXo') 
   (result, cli) = _feed_cli_with_input(u'hello\x01\x06X\n') 
   assert (result.text == u'hXello') 
   with pytest.raises(KeyboardInterrupt): 
      (result, cli) = _feed_cli_with_input(u'hello\x03\n') 
      assert (result.text == u'hello') 
   with pytest.raises(EOFError): 
      (result, cli) = _feed_cli_with_input(u'\x04\n') 
      assert (result.text == u'hello') 
   (result, cli) = _feed_cli_with_input(u'hello\x01\x04\n') 
   assert (result.text == u'ello') 
   (result, cli) = _feed_cli_with_input(u'hello\x04\n') 
   assert (result.text == u'hello') 
   (result, cli) = _feed_cli_with_input(u'hello\x1b[D\x1b[D\x0b\n') 
   assert (result.text == u'hel') 
   (result, cli) = _feed_cli_with_input(u'hello\x1b[D\x1b[D\x1b-\x0b\n') 
   assert (result.text == u'lo') 
   (result, cli) = _feed_cli_with_input(u'hello\x0c\n') 
   assert (result.text == u'hello') 
   (result, cli) = _feed_cli_with_input(u'hello   world\x01X\x1b[1;5CY\n') 
   assert (result.text == u'XhelloY   world') 
   (result, cli) = _feed_cli_with_input(u'hello   world\x1b[1;5DY\n') 
   assert (result.text == u'hello   Yworld') 
   (result, cli) = _feed_cli_with_input(u'hello   world   abc   def\x01\x1b3\x1bfX\n') 
   assert (result.text == u'hello   world   abcX   def') 
   (result, cli) = _feed_cli_with_input(u'hello   world   abc   def\x1b-\x1b3\x1bfX\n') 
   assert (result.text == u'hello   Xworld   abc   def') 
   (result, cli) = _feed_cli_with_input(u'hello   world   abc   def\x1b3\x1bbX\n') 
   assert (result.text == u'hello   Xworld   abc   def') 
   (result, cli) = _feed_cli_with_input(u'hello   world   abc   def\x01\x1b-\x1b3\x1bbX\n') 
   assert (result.text == u'hello   world   abc   Xdef') 
   (result, cli) = _feed_cli_with_input(u'hello   world\x17\n') 
   assert (result.text == u'hello   ') 
   assert (cli.clipboard.get_data().text == u'world') 
   (result, cli) = _feed_cli_with_input(u'test   hello   world\x1b2\x17\n') 
   assert (result.text == u'test   ') 
   (result, cli) = _feed_cli_with_input(u'hello   world\x1b\x7f\n') 
   assert (result.text == u'hello   ') 
   assert (cli.clipboard.get_data().text == u'world') 
   (result, cli) = _feed_cli_with_input(u'hello   world\x1b\x08\n') 
   assert (result.text == u'hello   ') 
   assert (cli.clipboard.get_data().text == u'world') 
   (result, cli) = _feed_cli_with_input(u'hello   world\x7f\n') 
   assert (result.text == u'hello   worl') 
   assert (result.cursor_position == len(u'hello   worl')) 
   (result, cli) = _feed_cli_with_input(u'hello   world\x08\n') 
   assert (result.text == u'hello   worl') 
   assert (result.cursor_position == len(u'hello   worl')) 
   (result, cli) = _feed_cli_with_input(u'hello   world\x01\x1b[3~\n') 
   assert (result.text == u'ello   world') 
   assert (result.cursor_position == 0) 
   (result, cli) = _feed_cli_with_input(u'hello               world\x1b8\x02\x1b\\\n') 
   assert (result.text == u'helloworld') 
   assert (result.cursor_position == len(u'hello'))", 'Test cursor movements.','Test cursor movements with Emacs key bindings.'
"def _RecurseOverObject(obj, factory, parent=None): 
    if _IsSudsIterable(obj): 
      copy_of_obj = tuple(obj) 
      for item in copy_of_obj: 
         if _IsSudsIterable(item): 
            if ('xsi_type' in item): 
               if isinstance(obj, tuple): 
                  parent[obj[0]] = _PackForSuds(obj[1], factory) 
               else: 
                  obj.remove(item) 
                  obj.append(_PackForSuds(item, factory)) 
            _RecurseOverObject(item, factory, obj)"," 'Recursively packs an object for the purposes of serialization. 
 This is used to handle nested structures. 
 :param obj: The object to be packed. 
 :param factory: The factory to use when creating new objects. 
 :param parent: The parent object. 
 :return: None'","'Recurses over a nested structure to look for changes in Suds objects. 
 Args: 
 obj: A parameter for a SOAP request field which is to be inspected and 
 will be packed for Suds if an xsi_type is specified, otherwise will be 
 left unaltered. 
 factory: The suds.client.Factory object which can create instances of the 
 classes generated from the WSDL. 
 parent: The parent object that contains the obj parameter to be inspected.'"
"def s_byte(value, endian='<', format='binary', signed=False, full_range=False, fuzzable=True, name=None): 
    byte = primitives.byte(value, endian, format, signed, full_range, fuzzable, name) 
   blocks.CURRENT.push(byte)"," 'Create a new byte block. 
 :param value: the byte to encode 
 :param endian: the endianness to use (default: \'<\') 
 :param format: the format to use (default: \'binary\') 
 :param signed: whether to encode the value as signed (default: False) 
 :param full_range: whether to encode the value as full range (default: False) 
 :param fuzzable: whether to fuzz the value (default: True) 
 :param name: the name of the block (default: None) 
 :return: the byte block'","'Push a byte onto the current block stack. 
 @see: Aliases: s_char() 
 @type  value:      Integer 
 @param value:      Default integer value 
 @type  endian:     Character 
 @param endian:     (Optional, def=LITTLE_ENDIAN) Endianess of the bit field (LITTLE_ENDIAN: <, BIG_ENDIAN: >) 
 @type  format:     String 
 @param format:     (Optional, def=binary) Output format, ""binary"" or ""ascii"" 
 @type  signed:     Boolean 
 @param signed:     (Optional, def=False) Make size signed vs. unsigned (applicable only with format=""ascii"") 
 @type  full_range: Boolean 
 @param full_range: (Optional, def=False) If enabled the field mutates through *all* possible values. 
 @type  fuzzable:   Boolean 
 @param fuzzable:   (Optional, def=True) Enable/disable fuzzing of this primitive 
 @type  name:       String 
 @param name:       (Optional, def=None) Specifying a name gives you direct access to a primitive'"
"def publish_collection(committer_id, collection_id): 
    _publish_activity(committer_id, collection_id, feconf.ACTIVITY_TYPE_COLLECTION)"," 'Publishes the collection with the given id. 
 Args: 
 committer_id: str. ID of the committer. 
 collection_id: str. ID of the collection. 
 Raises: 
 ValueError: if committer_id is None or collection_id is None.'","'This is called by the publish_collection_and_update_user_profiles 
 function in collection_services.py. It publishes an collection and commits 
 changes. 
 It is the responsibility of the caller to check that the collection is 
 valid prior to publication.'"
"def _ipconfig_getnode(): 
    import os, re 
   dirs = ['', 'c:\\windows\\system32', 'c:\\winnt\\system32'] 
   try: 
      import ctypes 
      buffer = ctypes.create_string_buffer(300) 
      ctypes.windll.kernel32.GetSystemDirectoryA(buffer, 300) 
      dirs.insert(0, buffer.value.decode('mbcs')) 
   except: 
      pass 
   for dir in dirs: 
      try: 
         pipe = os.popen((os.path.join(dir, 'ipconfig') + '   /all')) 
      except IOError: 
         continue 
      for line in pipe: 
         value = line.split(':')[(-1)].strip().lower() 
         if re.match('([0-9a-f][0-9a-f]-){5}[0-9a-f][0-9a-f]', value): 
            return int(value.replace('-', ''), 16)"," 'Get the node ID from the IPCONFIG /ALL command. 
 This is the same as the IPCONFIG /ALL command, but it returns the 
 IP address as an integer. 
 :return: node ID as an integer 
 :rtype: int'",'Get the hardware address on Windows by running ipconfig.exe.'
"def findTwistedProjects(baseDirectory): 
    projects = [] 
   for filePath in baseDirectory.walk(): 
      if (filePath.basename() == 'topfiles'): 
         projectDirectory = filePath.parent() 
         projects.append(Project(projectDirectory)) 
   return projects"," 'Finds all Twisted projects in the given base directory. 
 :param baseDirectory: The base directory to search. 
 :return: A list of projects found.'","'Find all Twisted-style projects beneath a base directory. 
 @param baseDirectory: A L{twisted.python.filepath.FilePath} to look inside. 
 @return: A list of L{Project}.'"
"def aes_encrypt(data, expanded_key): 
    rounds = ((len(expanded_key) // BLOCK_SIZE_BYTES) - 1) 
   data = xor(data, expanded_key[:BLOCK_SIZE_BYTES]) 
   for i in range(1, (rounds + 1)): 
      data = sub_bytes(data) 
      data = shift_rows(data) 
      if (i != rounds): 
         data = mix_columns(data) 
      data = xor(data, expanded_key[(i * BLOCK_SIZE_BYTES):((i + 1) * BLOCK_SIZE_BYTES)]) 
   return data"," 'Encrypts data using the AES algorithm. 
 The expanded_key must be the expanded key as returned by 
 :py:func:`~sympy.crypto.encryption.expand_key`. 
 Examples 
 >>> from sympy.crypto.encryption import aes_encrypt 
 >>> from sympy.crypto.encryption import expand_key 
 >>> key = \'0123456789abcdef\' 
 >>> expanded_key = expand_key(key) 
 >>> aes_encrypt(\'hello world\', expanded_key) 
 \'140000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000","'Encrypt one block with aes 
 @param {int[]} data          16-Byte state 
 @param {int[]} expanded_key  176/208/240-Byte expanded key 
 @returns {int[]}             16-Byte cipher'"
"def group(seq, size): 
    if (not hasattr(seq, 'next')): 
      seq = iter(seq) 
   while True: 
      (yield [seq.next() for i in xrange(size)])"," 'Returns a generator that yields groups of size `size` from `seq`. 
 >>> list(group(range(10), 3)) 
 [(0, 1, 2), (3, 4, 5), (6, 7, 8), (9)] 
 >>> list(group(range(10), 10)) 
 [(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)]'",'Breaks \'seq\' into a generator of lists with length \'size\'.'
"def test_wraps(): 
    def foo(a, b, c=1, d=2, e=3, **kwargs): 
      u'A   test   function.' 
      return (a, b, c, d, e, kwargs) 
   @wraps(foo) 
   def bar(*args, **kwargs): 
      return ((u'test',) + foo(*args, **kwargs)) 
   expected = (u'test', 1, 2, 3, 4, 5, {u'f': 6, u'g': 7}) 
   assert (bar(1, 2, 3, 4, 5, f=6, g=7) == expected) 
   assert (bar.__name__ == u'foo') 
   if (foo.__doc__ is not None): 
      assert (bar.__doc__ == u'A   test   function.') 
   if hasattr(foo, u'__qualname__'): 
      assert (bar.__qualname__ == foo.__qualname__) 
   if six.PY2: 
      argspec = inspect.getargspec(bar) 
      assert (argspec.keywords == u'kwargs') 
   else: 
      argspec = inspect.getfullargspec(bar) 
      assert (argspec.varkw == u'kwargs') 
   assert (argspec.args == [u'a', u'b', u'c', u'd', u'e']) 
   assert (argspec.defaults == (1, 2, 3))", 'Test that wraps works properly.',"'Tests the compatibility replacement for functools.wraps which supports 
 argument preservation across all supported Python versions.'"
"def agent_settings(name, contact, location, services=None): 
    ret = {'name': name, 'changes': {}, 'comment': str(), 'result': None} 
   ret_settings = {'changes': dict(), 'failures': dict()} 
   if (not services): 
      services = ['None'] 
   services = sorted(set(services)) 
   settings = {'contact': contact, 'location': location, 'services': services} 
   current_settings = __salt__['win_snmp.get_agent_settings']() 
   for setting in settings: 
      if (str(settings[setting]) != str(current_settings[setting])): 
         ret_settings['changes'][setting] = {'old': current_settings[setting], 'new': settings[setting]} 
   if (not ret_settings['changes']): 
      ret['comment'] = 'Agent   settings   already   contain   the   provided   values.' 
      ret['result'] = True 
      return ret 
   elif __opts__['test']: 
      ret['comment'] = 'Agent   settings   will   be   changed.' 
      ret['changes'] = ret_settings 
      return ret 
   __salt__['win_snmp.set_agent_settings'](**settings) 
   new_settings = __salt__['win_snmp.get_agent_settings']() 
   for setting in settings: 
      if (settings[setting] != new_settings[setting]): 
         ret_settings['failures'][setting] = {'old': current_settings[setting], 'new': new_settings[setting]} 
         ret_settings['changes'].pop(setting, None) 
   if ret_settings['failures']: 
      ret['comment'] = 'Some   agent   settings   failed   to   change.' 
      ret['changes'] = ret_settings 
      ret['result'] = False 
   else: 
      ret['comment'] = 'Set   agent   settings   to   contain   the   provided   values.' 
      ret['changes'] = ret_settings['changes'] 
      ret['result'] = True 
   return ret"," 'Set agent settings to contain the provided values. 
 This function will attempt to set the agent settings to contain the provided 
 values. 
 If the agent settings already contain the provided values, then the function 
 will return without making any changes. 
 If the agent settings do not contain the provided values, then the function 
 will return without making any changes. 
 If the agent settings contain some of the provided values and some of the 
 provided values fail to change, then the function will return with the 
 provided values and the failed values. 
 The returned dictionary will contain a `changes` key which will contain a 
 dictionary of the settings that were changed and a `failures` key which will 
 contain a dictionary of the settings that failed to change. 
 The returned dictionary will also contain a `comment` key which will contain 
 a comment about the changes that were made. 
 If the function returns with a `failures` key, then the `comment` key will 
 contain a comment about the failed settings. 
 If the function returns with a `changes` key, then the `comment` key will 
 contain a comment about the successful settings. 
 If","'Manage the SNMP sysContact, sysLocation, and sysServices settings. 
 :param str contact: The SNMP contact. 
 :param str location: The SNMP location. 
 :param str services: A list of selected services. 
 Example of usage: 
 .. code-block:: yaml 
 snmp-agent-settings: 
 win_snmp.agent_settings: 
 - contact: Test Contact 
 - location: Test Location 
 - services: 
 - Physical 
 - Internet'"
"def displayFile(filename): 
    print (('File   ' + filename) + '   is   being   displayed.') 
   fileText = archive.getFileText(filename) 
   gcodec.writeFileMessageSuffix(filename, displayText(fileText), 'The   gcode   log   file   is   saved   as   ', '_log')"," 'Display the contents of a file. 
 :param filename: The name of the file to display. 
 :type filename: str 
 :return: None 
 :rtype: None'",'Parse a gcode file and display the commands.'
"def test_extract_Array_float(): 
    for a in np.arange(2.51, 3.49, 0.1): 
      assert np.all((extract_array(np.arange(5), 3, a) == np.array([2, 3, 4])))", 'Test extract_array for Array of float.','integer is at bin center'
"def _heapify_max(x): 
    n = len(x) 
   for i in reversed(range((n // 2))): 
      _siftup_max(x, i)"," 'Heapify the list x from the max heap position to the end. 
 The algorithm is from the book \'Introduction to Algorithms\', 
 by Cormen, Leiserson, Rivest, and Stein. 
 Parameters 
 x : list 
 The list to heapify. 
 Returns 
 None 
 Examples 
 >>> from sympy.utilities.iterables import heapify 
 >>> x = [1, 3, 2, 4] 
 >>> heapify(x) 
 >>> x 
 [4, 3, 2, 1] 
 >>> x = [1, 3, 2, 4, 5] 
 >>> heapify(x) 
 >>> x 
 [5, 4, 3, 2, 1] 
 >>> x = [1, 3, 2, 4, 5, 6] 
 >>> heapify(x) 
 >>> x 
 [6, 5, 4, 3, 2, 1]'","'Transform list into a maxheap, in-place, in O(len(x)) time.'"
"def curve4_bezier(p1, p2, p3, p4): 
    (x1, y1) = p1 
   (x2, y2) = p2 
   (x3, y3) = p3 
   (x4, y4) = p4 
   points = [] 
   _curve4_recursive_bezier(points, x1, y1, x2, y2, x3, y3, x4, y4) 
   (dx, dy) = ((points[0][0] - x1), (points[0][1] - y1)) 
   if (((dx * dx) + (dy * dy)) > 1e-10): 
      points.insert(0, (x1, y1)) 
   (dx, dy) = ((points[(-1)][0] - x4), (points[(-1)][1] - y4)) 
   if (((dx * dx) + (dy * dy)) > 1e-10): 
      points.append((x4, y4)) 
   return np.array(points).reshape(len(points), 2)"," 'Compute a Bezier curve. 
 Parameters 
 p1 : 2-tuple 
 The first point. 
 p2 : 2-tuple 
 The second point. 
 p3 : 2-tuple 
 The third point. 
 p4 : 2-tuple 
 The fourth point. 
 Returns 
 points : ndarray 
 The points of the Bezier curve. 
 Examples 
 >>> from sympy.geometry import curve4_bezier 
 >>> curve4_bezier((1, 2), (3, 4), (5, 6), (7, 8)) 
 array([[ 1,  2], [ 5,  6], [ 7,  8]]) 
 References 
 .. [1] http://en.wikipedia.org/wiki/Bezier_curve'","'Generate the vertices for a third order Bezier curve. 
 The vertices returned by this function can be passed to a LineVisual or 
 ArrowVisual. 
 Parameters 
 p1 : array 
 2D coordinates of the start point 
 p2 : array 
 2D coordinates of the first curve point 
 p3 : array 
 2D coordinates of the second curve point 
 p4 : array 
 2D coordinates of the end point 
 Returns 
 coords : list 
 Vertices for the Bezier curve. 
 See Also 
 curve3_bezier 
 Notes 
 For more information about Bezier curves please refer to the `Wikipedia`_ 
 page. 
 .. _Wikipedia: https://en.wikipedia.org/wiki/B%C3%A9zier_curve'"
"def num_windows_of_length_M_on_buffers_of_length_N(M, N): 
    return ((N - M) + 1)"," 'Returns the number of windows of length M on buffers of length N. 
 >>> num_windows_of_length_M_on_buffers_of_length_N(10, 10) 
 10 
 >>> num_windows_of_length_M_on_buffers_of_length_N(10, 11) 
 11'","'For a window of length M rolling over a buffer of length N, 
 there are (N - M) + 1 legal windows. 
 Example: 
 If my array has N=4 rows, and I want windows of length M=2, there are 
 3 legal windows: data[0:2], data[1:3], and data[2:4].'"
"def instance_type_access_remove(context, flavor_id, project_id): 
    return IMPL.instance_type_access_remove(context, flavor_id, project_id)", 'Remove an access entry for the specified flavor and project.','Remove flavor access for project.'
"def cram(text, maxlen): 
    if (len(text) > maxlen): 
      pre = max(0, ((maxlen - 3) // 2)) 
      post = max(0, ((maxlen - 3) - pre)) 
      return ((text[:pre] + '...') + text[(len(text) - post):]) 
   return text"," 'Return a text with its length reduced to maxlen. 
 If the length is larger than maxlen, the text is truncated to 
 maxlen - 3 characters on the left and right sides. 
 >>> cram(\'This is a test\', 20) 
 \'This is a test\' 
 >>> cram(\'This is a test\', 15) 
 \'This is a test...\' 
 >>> cram(\'This is a test\', 14) 
 \'This is a test\''",'Omit part of a string if needed to make it fit in a maximum length.'
"def quota_usage_update(context, project_id, resource, **kwargs): 
    return IMPL.quota_usage_update(context, project_id, resource, **kwargs)"," 'Update quota usage for a project. 
 :param context: context to use. 
 :param project_id: project ID. 
 :param resource: resource name. 
 :param \'user_id\': user ID. 
 :param \'user_id_list\': list of user IDs. 
 :param \'tenant_id\': tenant ID. 
 :param \'tenant_id_list\': list of tenant IDs. 
 :param \'quota_class_name\': quota class name. 
 :param \'quota_class_list\': list of quota class names. 
 :param \'in_use\': in_use. 
 :param \'in_use_list\': list of in_use. 
 :param \'in_use_percentage\': in_use percentage. 
 :param \'in_use_percentage_list\': list of in_use percentage. 
 :param \'in_use_delta\': in_use delta. 
 :param \'in_use_delta_list\':",'Update a quota usage or raise if it does not exist.'
"def build_provider_location(system, lun_type, lun_id, base_lun_name, version): 
    location_dict = {'system': system, 'type': lun_type, 'id': six.text_type(lun_id), 'base_lun_name': six.text_type(base_lun_name), 'version': version} 
   return dump_provider_location(location_dict)"," 'Builds a provider location dict from the passed in values. 
 :param system: The system name. 
 :param lun_type: The lun type. 
 :param lun_id: The lun id. 
 :param base_lun_name: The base lun name. 
 :param version: The version. 
 :return: A dict representing the provider location.'","'Builds provider_location for volume or snapshot. 
 :param system: VNX serial number 
 :param lun_id: LUN ID in VNX 
 :param lun_type: \'lun\' or \'smp\' 
 :param base_lun_name: primary LUN name, 
 it will be used when creating snap lun 
 :param version: driver version'"
"def cr_notification_dispatcher(r, **attr): 
    if ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): 
      T = current.T 
      msg = current.msg 
      s3db = current.s3db 
      record = r.record 
      ctable = s3db.pr_contact 
      stable = s3db.cr_shelter 
      message = '' 
      text = '' 
      s_id = record.id 
      s_name = record.name 
      s_phone = record.phone 
      s_email = record.email 
      s_status = record.status 
      if (s_phone in ('', None)): 
         s_phone = T('Not   Defined') 
      if (s_email in ('', None)): 
         s_phone = T('Not   Defined') 
      if (s_status in ('', None)): 
         s_status = T('Not   Defined') 
      elif (s_status == 1): 
         s_status = 'Open' 
      elif (s_status == 2): 
         s_status = 'Close' 
      else: 
         s_status = 'Unassigned   Shelter   Status' 
      text += '************************************************' 
      text += ('\n%s   ' % T('Automatic   Message')) 
      text += ('\n%s:   %s   ' % (T('Shelter   ID'), s_id)) 
      text += ('   %s:   %s' % (T('Shelter   name'), s_name)) 
      text += ('\n%s:   %s   ' % (T('Email'), s_email)) 
      text += ('   %s:   %s' % (T('Phone'), s_phone)) 
      text += ('\n%s:   %s   ' % (T('Working   Status'), s_status)) 
      text += '\n************************************************\n' 
      url = URL(c='cr', f='shelter', args=r.id) 
      opts = dict(type='SMS', subject=T('Deployment   Request'), message=(message + text), url=url) 
      output = msg.compose(**opts) 
      if attr.get('rheader'): 
         rheader = attr['rheader'](r) 
         if rheader: 
            output['rheader'] = rheader 
      output['title'] = T('Send   Notification') 
      current.response.view = 'msg/compose.html' 
      return output 
   else: 
      raise HTTP(501, current.messages.BADMETHOD)"," 'Dispatch a notification to the relevant contact(s) for the record. 
 This is called by the cr_shelter controller when the \'send notification\' 
 button is clicked. 
 :param r: the current request 
 :return: a response to send to the client'",'Send a notification.'
"def _image_member_format(member_ref): 
    return {'id': member_ref['id'], 'image_id': member_ref['image_id'], 'member': member_ref['member'], 'can_share': member_ref['can_share'], 'status': member_ref['status'], 'created_at': member_ref['created_at'], 'updated_at': member_ref['updated_at'], 'deleted': member_ref['deleted']}", 'Returns a dict of image member format.','Format a member ref for consumption outside of this module.'
"def emitter(p=0.03): 
    while True: 
      v = np.random.rand(1) 
      if (v > p): 
         (yield 0.0) 
      else: 
         (yield np.random.rand(1))"," 'A simple emitter. 
 The emitter emits a sample from a uniform distribution with probability 
 ``p``. 
 Parameters 
 p : float, optional 
 The probability of emitting a sample. 
 Examples 
 >>> emitter() 
 0.0 
 >>> emitter(0.05) 
 0.0 
 >>> emitter(0.1) 
 0.0 
 >>> emitter(0.2) 
 0.0 
 >>> emitter(0.3) 
 0.0 
 >>> emitter(0.4) 
 0.0 
 >>> emitter(0.5) 
 0.0 
 >>> emitter(0.6) 
 0.0 
 >>> emitter(0.7) 
 0.0 
 >>> emitter(0.8) 
 0.0 
 >>> emitter(0.9) 
 0.0 
 >>> emitter(1.0) 
 0.0 
 >>> emitter(1.1) 
 0.","'return a random value with probability p, else 0'"
"def _policyFileReplaceOrAppend(this_string, policy_data, append_only=False): 
    if (not policy_data): 
      policy_data = '' 
   specialValueRegex = '(\\*\\*Del\\.|\\*\\*DelVals\\.){0,1}' 
   item_key = None 
   item_value_name = None 
   data_to_replace = None 
   if (not append_only): 
      item_key = this_string.split('{0};'.format(chr(0)))[0].lstrip('[') 
      item_value_name = re.sub(specialValueRegex, '', this_string.split('{0};'.format(chr(0)))[1], flags=re.IGNORECASE) 
      log.debug('item   value   name   is   {0}'.format(item_value_name)) 
      data_to_replace = _regexSearchKeyValueCombo(policy_data, item_key, item_value_name) 
   if data_to_replace: 
      log.debug('replacing   {0}   with   {1}'.format([data_to_replace], [this_string])) 
      policy_data = policy_data.replace(data_to_replace, this_string) 
   else: 
      log.debug('appending   {0}'.format([this_string])) 
      policy_data = ''.join([policy_data, this_string]) 
   return policy_data", 'Replace or append a string to the policy file.',"'helper function to take a ADMX policy string for registry.pol file data and 
 update existing string or append the string to the data'"
"def emits_warning_on(db, *warnings): 
    spec = db_spec(db) 
   @decorator 
   def decorate(fn, *args, **kw): 
      if isinstance(db, util.string_types): 
         if (not spec(config._current)): 
            return fn(*args, **kw) 
         else: 
            wrapped = emits_warning(*warnings)(fn) 
            return wrapped(*args, **kw) 
      elif (not _is_excluded(*db)): 
         return fn(*args, **kw) 
      else: 
         wrapped = emits_warning(*warnings)(fn) 
         return wrapped(*args, **kw) 
   return decorate"," 'Decorator to emit warnings when calling a function on a database. 
 This decorator takes a list of warnings to emit. 
 By default, the following warnings are emitted: 
 * :py:func:`~sqlalchemy.engine.dialect.warnings.NOT_SUPPORTED_FOR_SQLITE` 
 * :py:func:`~sqlalchemy.engine.dialect.warnings.NOT_SUPPORTED_FOR_MYSQL` 
 * :py:func:`~sqlalchemy.engine.dialect.warnings.NOT_SUPPORTED_FOR_POSTGRES` 
 * :py:func:`~sqlalchemy.engine.dialect.warnings.NOT_SUPPORTED_FOR_ORACLE` 
 * :py:func:`~sqlalchemy.engine.dialect.warnings.NOT_SUPPORTED_FOR_SQLITE` 
 * :py:func:`~sqlalchemy.engine.dialect.warnings.NOT_SUPPORTED_FOR_MSSQL` 
 * :py:func:`~","'Mark a test as emitting a warning on a specific dialect. 
 With no arguments, squelches all SAWarning failures.  Or pass one or more 
 strings; these will be matched to the root of the warning description by 
 warnings.filterwarnings().'"
"def uldap(): 
    def construct(): 
      try: 
         secret_file = open('/etc/ldap.secret', 'r') 
         bind_dn = 'cn=admin,{}'.format(base_dn()) 
      except IOError: 
         secret_file = open('/etc/machine.secret', 'r') 
         bind_dn = config_registry()['ldap/hostdn'] 
      pwd_line = secret_file.readline() 
      pwd = re.sub('\n', '', pwd_line) 
      import univention.admin.uldap 
      return univention.admin.uldap.access(host=config_registry()['ldap/master'], base=base_dn(), binddn=bind_dn, bindpw=pwd, start_tls=1) 
   return _singleton('uldap', construct)", 'Returns a singleton to the ULdap module.','Return a configured univention uldap object'
"def _getTagsWith(s, cont, toClosure=False, maxRes=None): 
    lres = [] 
   bi = s.find(cont) 
   if (bi != (-1)): 
      btag = s[:bi].rfind('<') 
      if (btag != (-1)): 
         if (not toClosure): 
            etag = s[(bi + 1):].find('>') 
            if (etag != (-1)): 
               endidx = ((bi + 2) + etag) 
               lres.append(s[btag:endidx]) 
               if ((maxRes is not None) and (len(lres) >= maxRes)): 
                  return lres 
               lres += _getTagsWith(s[endidx:], cont, toClosure=toClosure) 
         else: 
            spaceidx = s[btag:].find('   ') 
            if (spaceidx != (-1)): 
               ctag = ('</%s>' % s[(btag + 1):(btag + spaceidx)]) 
               closeidx = s[bi:].find(ctag) 
               if (closeidx != (-1)): 
                  endidx = ((bi + closeidx) + len(ctag)) 
                  lres.append(s[btag:endidx]) 
                  if ((maxRes is not None) and (len(lres) >= maxRes)): 
                     return lres 
                  lres += _getTagsWith(s[endidx:], cont, toClosure=toClosure) 
   return lres"," 'Returns a list of tags found in s, with the same tag name as cont. 
 If toClosure is True, the tags are closed. 
 If maxRes is specified, only the first maxRes tags are returned.'","'Return the html tags in the \'s\' string containing the \'cont\' 
 string; if toClosure is True, everything between the opening 
 tag and the closing tag is returned.'"
"def digest_auth(realm, get_ha1, key, debug=False): 
    request = cherrypy.serving.request 
   auth_header = request.headers.get('authorization') 
   nonce_is_stale = False 
   if (auth_header is not None): 
      with cherrypy.HTTPError.handle(ValueError, 400, 'The   Authorization   header   could   not   be   parsed.'): 
         auth = HttpDigestAuthorization(auth_header, request.method, debug=debug) 
      if debug: 
         TRACE(str(auth)) 
      if auth.validate_nonce(realm, key): 
         ha1 = get_ha1(realm, auth.username) 
         if (ha1 is not None): 
            digest = auth.request_digest(ha1, entity_body=request.body) 
            if (digest == auth.response): 
               if debug: 
                  TRACE('digest   matches   auth.response') 
               nonce_is_stale = auth.is_nonce_stale(max_age_seconds=600) 
               if (not nonce_is_stale): 
                  request.login = auth.username 
                  if debug: 
                     TRACE(('authentication   of   %s   successful' % auth.username)) 
                  return 
   header = www_authenticate(realm, key, stale=nonce_is_stale) 
   if debug: 
      TRACE(header) 
   cherrypy.serving.response.headers['WWW-Authenticate'] = header 
   raise cherrypy.HTTPError(401, 'You   are   not   authorized   to   access   that   resource')"," 'Digest-based authentication. 
 :param realm: The realm to authenticate against 
 :param get_ha1: A function that takes a realm and a username and returns a 
 hash value. 
 :param key: The key to use for digest authentication 
 :param debug: If True, debug messages will be printed to the log'","'A CherryPy tool which hooks at before_handler to perform 
 HTTP Digest Access Authentication, as specified in :rfc:`2617`. 
 If the request has an \'authorization\' header with a \'Digest\' scheme, 
 this tool authenticates the credentials supplied in that header. 
 If the request has no \'authorization\' header, or if it does but the 
 scheme is not ""Digest"", or if authentication fails, the tool sends 
 a 401 response with a \'WWW-Authenticate\' Digest header. 
 realm 
 A string containing the authentication realm. 
 get_ha1 
 A callable which looks up a username in a credentials store 
 and returns the HA1 string, which is defined in the RFC to be 
 MD5(username : realm : password).  The function\'s signature is: 
 ``get_ha1(realm, username)`` 
 where username is obtained from the request\'s \'authorization\' header. 
 If username is not found in the credentials store, get_ha1() returns 
 None. 
 key 
 A secret string known only to the server, used in the synthesis 
 of nonces.'"
"def make_tarball(base_name, base_dir, compress='gzip', verbose=0, dry_run=0): 
    compress_ext = {'gzip': '.gz', 'bzip2': '.bz2', 'compress': '.Z'} 
   compress_flags = {'gzip': ['-f9'], 'compress': ['-f'], 'bzip2': ['-f9']} 
   if ((compress is not None) and (compress not in compress_ext.keys())): 
      raise ValueError, ""bad   value   for   'compress':   must   be   None,   'gzip',   or   'compress'"" 
   archive_name = (base_name + '.tar') 
   mkpath(os.path.dirname(archive_name), dry_run=dry_run) 
   cmd = ['tar', '-cf', archive_name, base_dir] 
   spawn(cmd, dry_run=dry_run) 
   if compress: 
      spawn((([compress] + compress_flags[compress]) + [archive_name]), dry_run=dry_run) 
      return (archive_name + compress_ext[compress]) 
   else: 
      return archive_name"," 'Make a tarball from the given directory. 
 :param base_name: The name of the tarball to make 
 :param base_dir: The directory to tar 
 :param compress: The compression method to use (None, \'gzip\', or \'compress\') 
 :param verbose: The verbosity level of the tar command (0, 1, or 2) 
 :param dry_run: If True, do not actually execute the command'","'Create a (possibly compressed) tar file from all the files under 
 \'base_dir\'.  \'compress\' must be ""gzip"" (the default), ""compress"", 
 ""bzip2"", or None.  Both ""tar"" and the compression utility named by 
 \'compress\' must be on the default program search path, so this is 
 probably Unix-specific.  The output tar file will be named \'base_dir\' + 
 "".tar"", possibly plus the appropriate compression extension ("".gz"", 
 "".bz2"" or "".Z"").  Return the output filename.'"
"def TRIX(ds, count, timeperiod=(- (2 ** 31))): 
    return call_talib_with_ds(ds, count, talib.TRIX, timeperiod)"," 'TRIX(data, count, [timeperiod]) 
 Returns the TRIX indicator. 
 TRIX is a short-term trend indicator that is based on a 3-period exponential 
 moving average of price, with a 10-day simple moving average of price 
 subtracted from the 3-period exponential moving average. 
 Parameters 
 data : array_like 
 The data to calculate the TRIX indicator. 
 count : int 
 The number of values to use to calculate the TRIX indicator. 
 timeperiod : int 
 The time period of the input data. 
 Returns 
 TRIX : float 
 The TRIX indicator. 
 Examples 
 >>> from numpy.testing import assert_array_almost_equal 
 >>> from numpy.testing import assert_array_equal 
 >>> from numpy.testing import assert_almost_equal 
 >>> from numpy.testing import assert_equal 
 >>> import numpy.testing as N 
 >>> import talib 
 >>> N.assert_array_almost_equal(talib.TRIX(close, 10),",'1-day Rate-Of-Change (ROC) of a Triple Smooth EMA'
"def _acquire_download_cache_lock(): 
    lockdir = os.path.join(_get_download_cache_locs()[0], u'lock') 
   for i in range(conf.download_cache_lock_attempts): 
      try: 
         os.mkdir(lockdir) 
         with open(os.path.join(lockdir, u'pid'), u'w') as f: 
            f.write(str(os.getpid())) 
      except OSError: 
         time.sleep(1) 
      else: 
         return 
   msg = u""Unable   to   acquire   lock   for   cache   directory   ({0}   exists).   You   may   need   to   delete   the   lock   if   the   python   interpreter   wasn't   shut   down   properly."" 
   raise RuntimeError(msg.format(lockdir))"," 'Acquire a lock to the download cache directory. 
 This is necessary to prevent multiple processes from writing to the same 
 cache directory. 
 If the lock is not acquired, an exception is raised. 
 :return: None'","'Uses the lock directory method.  This is good because `mkdir` is 
 atomic at the system call level, so it\'s thread-safe.'"
"def get_options(select_browser_query): 
    return Select(select_browser_query.first.results[0]).options"," 'Returns the options from the select element. 
 :param select_browser_query: Select query 
 :return: options'",'Returns all the options for the given select.'
"def wrap_paragraphs(text, ncols=80): 
    paragraph_re = re.compile('\\n(\\s*\\n)+', re.MULTILINE) 
   text = dedent(text).strip() 
   paragraphs = paragraph_re.split(text)[::2] 
   out_ps = [] 
   indent_re = re.compile('\\n\\s+', re.MULTILINE) 
   for p in paragraphs: 
      if (indent_re.search(p) is None): 
         p = textwrap.fill(p, ncols) 
      out_ps.append(p) 
   return out_ps", 'Wraps paragraphs to fit into ncols characters.',"'Wrap multiple paragraphs to fit a specified width. 
 This is equivalent to textwrap.wrap, but with support for multiple 
 paragraphs, as separated by empty lines. 
 Returns 
 list of complete paragraphs, wrapped to fill `ncols` columns.'"
"def remove_file(path): 
    if os.path.isfile(path): 
      os.remove(path)"," 'Remove a file. 
 :param path: Path to file. 
 :return: None 
 :rtype: None'",':type path: str'
"def parse_content(documentation_type, content): 
    try: 
      to_index = PyQuery(content).text() 
   except ValueError: 
      return '' 
   return to_index"," 'Parse content from documentation_type to index. 
 :param documentation_type: The documentation type to parse. 
 :param content: The content to parse. 
 :return: The parsed content. 
 :rtype: str'","'Prepare the text of the html file. 
 Returns the body text of a document'"
"@register.tag('blocktrans') 
 def do_block_translate(parser, token): 
    bits = token.split_contents() 
   options = {} 
   remaining_bits = bits[1:] 
   while remaining_bits: 
      option = remaining_bits.pop(0) 
      if (option in options): 
         raise TemplateSyntaxError(('The   %r   option   was   specified   more   than   once.' % option)) 
      if (option == 'with'): 
         value = token_kwargs(remaining_bits, parser, support_legacy=True) 
         if (not value): 
            raise TemplateSyntaxError(('""with""   in   %r   tag   needs   at   least   one   keyword   argument.' % bits[0])) 
      elif (option == 'count'): 
         value = token_kwargs(remaining_bits, parser, support_legacy=True) 
         if (len(value) != 1): 
            raise TemplateSyntaxError(('""count""   in   %r   tag   expected   exactly   one   keyword   argument.' % bits[0])) 
      elif (option == 'context'): 
         try: 
            value = remaining_bits.pop(0) 
            value = parser.compile_filter(value) 
         except Exception: 
            raise TemplateSyntaxError(('""context""   in   %r   tag   expected   exactly   one   argument.' % bits[0])) 
      else: 
         raise TemplateSyntaxError(('Unknown   argument   for   %r   tag:   %r.' % (bits[0], option))) 
      options[option] = value 
   if ('count' in options): 
      (countervar, counter) = options['count'].items()[0] 
   else: 
      (countervar, counter) = (None, None) 
   if ('context' in options): 
      message_context = options['context'] 
   else: 
      message_context = None 
   extra_context = options.get('with', {}) 
   singular = [] 
   plural = [] 
   while parser.tokens: 
      token = parser.next_token() 
      if (token.token_type in (TOKEN_VAR, TOKEN_TEXT)): 
         singular.append(token) 
      else: 
         break 
   if (countervar and counter): 
      if (token.contents.strip() != 'plural'): 
         raise TemplateSyntaxError(""'blocktrans'   doesn't   allow   other   block   tags   inside   it"") 
      while parser.tokens: 
         token = parser.next_token() 
         if (token.token_type in (TOKEN_VAR, TOKEN_TEXT)): 
            plural.append(token) 
         else: 
            break 
   if (token.contents.strip() != 'endblocktrans'): 
      raise TemplateSyntaxError((""'blocktrans'   doesn't   allow   other   block   tags   (seen   %r)   inside   it"" % token.contents)) 
   return BlockTranslateNode(extra_context, singular, plural, countervar, counter, message_context)"," 'Render a list of items, each with a context and a count. 
 The context is passed to the template as a dictionary. 
 The count is passed as a variable name, with the count as a 
 value. 
 The template may render the singular form of the item, or the 
 plural form. 
 The template may render the singular form of the item, or the 
 plural form. 
 The template may render the singular form of the item, or the 
 plural form. 
 The template may render the singular form of the item, or the 
 plural form. 
 The template may render the singular form of the item, or the 
 plural form. 
 The template may render the singular form of the item, or the 
 plural form. 
 The template may render the singular form of the item, or the 
 plural form. 
 The template may render the singular form of the item, or the 
 plural form. 
 The template may render the singular form of the item, or the 
 plural form. 
 The template may render the singular form of the item, or the 
 plural","'This will translate a block of text with parameters. 
 Usage:: 
 {% blocktrans with bar=foo|filter boo=baz|filter %} 
 This is {{ bar }} and {{ boo }}. 
 {% endblocktrans %} 
 Additionally, this supports pluralization:: 
 {% blocktrans count count=var|length %} 
 There is {{ count }} object. 
 {% plural %} 
 There are {{ count }} objects. 
 {% endblocktrans %} 
 This is much like ngettext, only in template syntax. 
 The ""var as value"" legacy format is still supported:: 
 {% blocktrans with foo|filter as bar and baz|filter as boo %} 
 {% blocktrans count var|length as count %} 
 Contextual translations are supported:: 
 {% blocktrans with bar=foo|filter context ""greeting"" %} 
 This is {{ bar }}. 
 {% endblocktrans %} 
 This is equivalent to calling pgettext/npgettext instead of 
 (u)gettext/(u)ngettext.'"
"def setEntryText(entry, value): 
    if (entry == None): 
      return 
   entry.delete(0, Tkinter.END) 
   entry.insert(0, str(value))"," 'Sets the text in the given entry widget to the given value. 
 :param entry: Entry widget to set the text in. 
 :param value: Value to set the text to.'",'Set the entry text.'
"def levene(*args, **kwds): 
    center = 'median' 
   proportiontocut = 0.05 
   for (kw, value) in kwds.items(): 
      if (kw not in ['center', 'proportiontocut']): 
         raise TypeError((""levene()   got   an   unexpected   keyword   argument   '%s'"" % kw)) 
      if (kw == 'center'): 
         center = value 
      else: 
         proportiontocut = value 
   k = len(args) 
   if (k < 2): 
      raise ValueError('Must   enter   at   least   two   input   sample   vectors.') 
   Ni = zeros(k) 
   Yci = zeros(k, 'd') 
   if (center not in ['mean', 'median', 'trimmed']): 
      raise ValueError(""Keyword   argument   <center>   must   be   'mean',   'median'   or   'trimmed'."") 
   if (center == 'median'): 
      func = (lambda x: np.median(x, axis=0)) 
   elif (center == 'mean'): 
      func = (lambda x: np.mean(x, axis=0)) 
   else: 
      args = tuple((stats.trimboth(np.sort(arg), proportiontocut) for arg in args)) 
      func = (lambda x: np.mean(x, axis=0)) 
   for j in range(k): 
      Ni[j] = len(args[j]) 
      Yci[j] = func(args[j]) 
   Ntot = np.sum(Ni, axis=0) 
   Zij = ([None] * k) 
   for i in range(k): 
      Zij[i] = abs((asarray(args[i]) - Yci[i])) 
   Zbari = zeros(k, 'd') 
   Zbar = 0.0 
   for i in range(k): 
      Zbari[i] = np.mean(Zij[i], axis=0) 
      Zbar += (Zbari[i] * Ni[i]) 
   Zbar /= Ntot 
   numer = ((Ntot - k) * np.sum((Ni * ((Zbari - Zbar) ** 2)), axis=0)) 
   dvar = 0.0 
   for i in range(k): 
      dvar += np.sum(((Zij[i] - Zbari[i]) ** 2), axis=0) 
   denom = ((k - 1.0) * dvar) 
   W = (numer / denom) 
   pval = distributions.f.sf(W, (k - 1), (Ntot - k)) 
   return LeveneResult(W, pval)"," 'The Levene test of homogeneity of variances. 
 The Levene test of homogeneity of variances is a nonparametric test 
 of the null hypothesis that the variances of the population are equal. 
 The Levene test is a modification of the F-test, which is used to 
 test the equality of variances in a two-sample problem. The F-test 
 assumes that the variances are equal, but the Levene test does not. 
 The Levene test is a modification of the F-test, which is used to 
 test the equality of variances in a two-sample problem. The F-test 
 assumes that the variances are equal, but the Levene test does not. 
 Parameters 
 args : array_like 
 Array of samples to be compared. 
 center : {""mean"", ""median"", ""trimmed""} 
 Center of the distribution. 
 proportiontocut : float 
 Proportion of values to be trimmed from each distribution. 
 Returns 
 LeveneResult 
 A LeveneResult object.","'Perform Levene test for equal variances. 
 The Levene test tests the null hypothesis that all input samples 
 are from populations with equal variances.  Levene\'s test is an 
 alternative to Bartlett\'s test `bartlett` in the case where 
 there are significant deviations from normality. 
 Parameters 
 sample1, sample2, ... : array_like 
 The sample data, possibly with different lengths 
 center : {\'mean\', \'median\', \'trimmed\'}, optional 
 Which function of the data to use in the test.  The default 
 is \'median\'. 
 proportiontocut : float, optional 
 When `center` is \'trimmed\', this gives the proportion of data points 
 to cut from each end. (See `scipy.stats.trim_mean`.) 
 Default is 0.05. 
 Returns 
 statistic : float 
 The test statistic. 
 pvalue : float 
 The p-value for the test. 
 Notes 
 Three variations of Levene\'s test are possible.  The possibilities 
 and their recommended usages are: 
 * \'median\' : Recommended for skewed (non-normal) distributions> 
 * \'mean\' : Recommended for symmetric, moderate-tailed distributions. 
 * \'trimmed\' : Recommended for heavy-tailed distributions. 
 References 
 .. [1]  http://www.itl.nist.gov/div898/handbook/eda/section3/eda35a.htm 
 .. [2]   Levene, H. (1960). In Contributions to Probability and Statistics: 
 Essays in Honor of Harold Hotelling, I. Olkin et al. eds., 
 Stanford University Press, pp. 278-292. 
 .. [3]  Brown, M. B. and Forsythe, A. B. (1974), Journal of the American 
 Statistical Association, 69, 364-367'"
"def createExtraFillLoops(nestedRing, radius, radiusAround, shouldExtraLoopsBeAdded): 
    for innerNestedRing in nestedRing.innerNestedRings: 
      createFillForSurroundings(innerNestedRing.innerNestedRings, radius, radiusAround, shouldExtraLoopsBeAdded) 
   allFillLoops = intercircle.getInsetSeparateLoopsFromAroundLoops(nestedRing.getLoopsToBeFilled(), radius, max((1.4 * radius), radiusAround)) 
   if (len(allFillLoops) < 1): 
      return 
   if shouldExtraLoopsBeAdded: 
      nestedRing.extraLoops += allFillLoops 
      nestedRing.penultimateFillLoops = nestedRing.lastFillLoops 
   nestedRing.lastFillLoops = allFillLoops"," 'Add extra fill loops to the nested ring. 
 This method adds extra fill loops to the nested ring. 
 If the nested ring is not nested, no extra fill loops are added. 
 :param nestedRing: The nested ring to add extra fill loops to. 
 :param radius: The radius of the nested ring. 
 :param radiusAround: The radius of the circle around the nested ring. 
 :param shouldExtraLoopsBeAdded: If extra fill loops should be added. 
 :returns: The number of extra fill loops added.'",'Create extra fill loops.'
"def phone2numeric(phone): 
    letters = re.compile('[A-PR-Y]', re.I) 
   char2number = (lambda m: {'a': '2', 'c': '2', 'b': '2', 'e': '3', 'd': '3', 'g': '4', 'f': '3', 'i': '4', 'h': '4', 'k': '5', 'j': '5', 'm': '6', 'l': '5', 'o': '6', 'n': '6', 'p': '7', 's': '7', 'r': '7', 'u': '8', 't': '8', 'w': '9', 'v': '8', 'y': '9', 'x': '9'}.get(m.group(0).lower())) 
   return letters.sub(char2number, phone)"," 'Convert a phone number into its numeric equivalent. 
 :param phone: a phone number. 
 :type phone: str 
 :return: a number. 
 :rtype: int'",'Converts a phone number with letters into its numeric equivalent.'
"def p_unary_expression_4(t): 
    pass"," 'p_unary_expression_4 : 
 *expression 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
",'unary_expression : unary_operator cast_expression'
"def ensure_dirs(filename): 
    (dirname, _) = os.path.split(filename) 
   if (dirname and (not os.path.exists(dirname))): 
      os.makedirs(dirname)", 'Ensure that the given filename\'s directory exists.','Make sure the directories exist for `filename`.'
"def evaluateRegression(features, labels, nExp, MethodName, Params): 
    (featuresNorm, MEAN, STD) = normalizeFeatures([features]) 
   featuresNorm = featuresNorm[0] 
   nSamples = labels.shape[0] 
   partTrain = 0.9 
   ErrorsAll = [] 
   ErrorsTrainAll = [] 
   ErrorsBaselineAll = [] 
   for (Ci, C) in enumerate(Params): 
      Errors = [] 
      ErrorsTrain = [] 
      ErrorsBaseline = [] 
      for e in range(nExp): 
         randperm = numpy.random.permutation(range(nSamples)) 
         nTrain = int(round((partTrain * nSamples))) 
         featuresTrain = [featuresNorm[randperm[i]] for i in range(nTrain)] 
         featuresTest = [featuresNorm[randperm[(i + nTrain)]] for i in range((nSamples - nTrain))] 
         labelsTrain = [labels[randperm[i]] for i in range(nTrain)] 
         labelsTest = [labels[randperm[(i + nTrain)]] for i in range((nSamples - nTrain))] 
         featuresTrain = numpy.matrix(featuresTrain) 
         if (MethodName == 'svm'): 
            [Classifier, trainError] = trainSVMregression(featuresTrain, labelsTrain, C) 
         ErrorTest = [] 
         ErrorTestBaseline = [] 
         for (itest, fTest) in enumerate(featuresTest): 
            R = regressionWrapper(Classifier, MethodName, fTest) 
            Rbaseline = numpy.mean(labelsTrain) 
            ErrorTest.append(((R - labelsTest[itest]) * (R - labelsTest[itest]))) 
            ErrorTestBaseline.append(((Rbaseline - labelsTest[itest]) * (Rbaseline - labelsTest[itest]))) 
         Error = numpy.array(ErrorTest).mean() 
         ErrorBaseline = numpy.array(ErrorTestBaseline).mean() 
         Errors.append(Error) 
         ErrorsTrain.append(trainError) 
         ErrorsBaseline.append(ErrorBaseline) 
      ErrorsAll.append(numpy.array(Errors).mean()) 
      ErrorsTrainAll.append(numpy.array(ErrorsTrain).mean()) 
      ErrorsBaselineAll.append(numpy.array(ErrorsBaseline).mean()) 
   bestInd = numpy.argmin(ErrorsAll) 
   print '{0:s} DCTB  DCTB {1:s} DCTB  DCTB {2:s} DCTB  DCTB {3:s}'.format('Param', 'MSE', 'T-MSE', 'R-MSE') 
   for i in range(len(ErrorsAll)): 
      print '{0:.4f} DCTB  DCTB {1:.2f} DCTB  DCTB {2:.2f} DCTB  DCTB {3:.2f}'.format(Params[i], ErrorsAll[i], ErrorsTrainAll[i], ErrorsBaselineAll[i]), 
      if (i == bestInd): 
         print ' DCTB  DCTB    best', 
      print 
   return Params[bestInd]"," 'Evaluate the performance of the regression models. 
 Parameters 
 features : numpy.ndarray 
 Features of the dataset. 
 labels : numpy.ndarray 
 Labels of the dataset. 
 nExp : int 
 Number of models to evaluate. 
 MethodName : string 
 Name of the regression model. 
 Params : list 
 Parameters of the regression model. 
 Returns 
 Params : int 
 Parameter of the regression model that minimize the error. 
 Examples 
 >>> from sklearn.svm import svm 
 >>> from sklearn.datasets import make_regression 
 >>> X, y = make_regression(n_samples=100, n_features=2, n_informative=2, n_redundant=0, noise=0.1) 
 >>> C = [1, 10, 100] 
 >>> evaluateRegression(X, y, nExp=10, MethodName='svm', Params=C) 
 100'","'ARGUMENTS: 
 features:     numpy matrices of features [numOfSamples x numOfDimensions] 
 labels:       list of sample labels 
 nExp:         number of cross-validation experiments 
 MethodName:   ""svm"" or ""randomforest"" 
 Params:       list of classifier params to be evaluated 
 RETURNS: 
 bestParam:   the value of the input parameter that optimizes the selected performance measure'"
"def get_record(zone_id, record_id, profile): 
    conn = _get_driver(profile=profile) 
   return conn.get_record(zone_id, record_id)"," 'Retrieves the record with the specified ID from the zone. 
 :param zone_id: ID of the zone. 
 :param record_id: ID of the record. 
 :param profile: A dictionary of profile parameters. 
 :returns: The record object. 
 :rtype: :class:`~google.cloud.dns.v1beta1.Record`'","'Get record information for the given zone_id on the given profile 
 :param zone_id: Zone to export. 
 :type  zone_id: ``str`` 
 :param record_id: Record to delete. 
 :type  record_id: ``str`` 
 :param profile: The profile key 
 :type  profile: ``str`` 
 CLI Example: 
 .. code-block:: bash 
 salt myminion libcloud_dns.get_record google.com www profile1'"
"def get_restart_power_failure(): 
    ret = salt.utils.mac_utils.execute_return_result('systemsetup   -getrestartpowerfailure') 
   return (salt.utils.mac_utils.validate_enabled(salt.utils.mac_utils.parse_return(ret)) == 'on')"," 'Returns if the restart power failure is enabled. 
 :return: True if the restart power failure is enabled, False otherwise'","'Displays whether \'restart on power failure\' is on or off if supported 
 :return: A string value representing the ""restart on power failure"" settings 
 :rtype: string 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' power.get_restart_power_failure'"
"def _plot_ci_bars(ax, x, central_data, ci, color, err_kws, **kwargs): 
    for (x_i, y_i, (low, high)) in zip(x, central_data, ci.T): 
      ax.plot([x_i, x_i], [low, high], color=color, solid_capstyle='round', **err_kws)", 'Plot the confidence intervals as bars on the x-axis.','Plot error bars at each data point.'
"def get_hqe_percentage_complete(**filter_data): 
    query = models.HostQueueEntry.query_objects(filter_data) 
   complete_count = query.filter(complete=True).count() 
   total_count = query.count() 
   if (total_count == 0): 
      return 1 
   return (float(complete_count) / total_count)"," 'Returns the percentage of HostQueueEntries that are complete. 
 This is used to determine whether or not a HostQueueEntry should be 
 displayed on the dashboard.'","'Computes the fraction of host queue entries matching the given filter data 
 that are complete.'"
"def encrypt_stream(mode, in_stream, out_stream, block_size=BLOCK_SIZE, padding=PADDING_DEFAULT): 
    encrypter = Encrypter(mode, padding=padding) 
   _feed_stream(encrypter, in_stream, out_stream, block_size)"," 'Encrypts the contents of a stream using the given cipher mode. 
 :param mode: the cipher mode to use 
 :param in_stream: the input stream 
 :param out_stream: the output stream 
 :param block_size: the block size of the cipher 
 :param padding: the padding to use for the cipher 
 :return: the encrypted stream'",'Encrypts a stream of bytes from in_stream to out_stream using mode.'
"def build_query_rep(query, divider=u'   -   '): 
    return divider.join([el[0] for el in query])"," 'Build a query representation from a query object. 
 :param query: query object 
 :type query: Query 
 :param divider: string to use as a divider between query terms 
 :type divider: str 
 :return: a string representation of the query 
 :rtype: str'","'Build a string representation of a query, without metadata types'"
"def parent_dir(path): 
    return os.path.abspath(os.path.join(path, os.pardir))", 'Return the parent directory of a path.','Return the parent of a directory.'
"def load_meta_sources(): 
    meta_sources = {} 
   for (module_path, class_name) in SOURCES.items(): 
      module = import_module(((METASYNC_MODULE + '.') + module_path)) 
      meta_sources[class_name.lower()] = getattr(module, class_name) 
   return meta_sources"," 'Loads the meta sources. 
 Returns: 
 meta_sources: 
 A dictionary of meta source names as keys and the corresponding 
 meta source class as values.'","'Returns a dictionary of all the MetaSources 
 E.g., {\'itunes\': Itunes} with isinstance(Itunes, MetaSource) true'"
"def _get_resources(context, alias): 
    alias_sql = sqlalchemy.text(u'SELECT   alias_of   FROM   ""_table_metadata""\n                        WHERE   name   =   :alias   AND   alias_of   IS   NOT   NULL') 
   results = context['connection'].execute(alias_sql, alias=alias).fetchall() 
   return [x[0] for x in results]"," 'Return the list of resource names that are aliased to the given alias. 
 :param context: A dictionary of context values. 
 :param alias: The alias to check for. 
 :returns: A list of resource names.'","'Get a list of resources for an alias. There could be more than one alias 
 in a resource_dict.'"
"def shell_init_func(): 
    from simplewiki import database 
   wiki = make_wiki() 
   wiki.bind_to_context() 
   return {'wiki': wiki, 'db': database}"," 'Initializes the wiki, and the database.'",'Called on shell initialization.  Adds useful stuff to the namespace.'
"def maybe_download(filename, work_directory): 
    if (not os.path.exists(work_directory)): 
      os.mkdir(work_directory) 
   filepath = os.path.join(work_directory, filename) 
   if (not os.path.exists(filepath)): 
      print('Downloading   MNIST...') 
      (filepath, _) = urllib.request.urlretrieve((SOURCE_URL + filename), filepath) 
      statinfo = os.stat(filepath) 
      print('Succesfully   downloaded', filename, statinfo.st_size, 'bytes.') 
   return filepath"," 'Download the MNIST dataset to the given work directory. 
 Parameters 
 filename : str 
 The name of the MNIST dataset file to download. 
 work_directory : str 
 The path to the work directory where the MNIST dataset file will be 
 downloaded. 
 Returns 
 filepath : str 
 The path to the MNIST dataset file. 
 Raises 
 IOError 
 If the MNIST dataset file cannot be downloaded. 
 Notes 
 This function is used to download the MNIST dataset to the work directory 
 specified by `work_directory`. 
 If the MNIST dataset file does not exist in the work directory, it will 
 be downloaded from the Internet. 
 This function does not raise an exception if the MNIST dataset file cannot 
 be downloaded. 
 Examples 
 >>> from sklearn.datasets import fetch_mldata 
 >>> mnist = fetch_mldata(\'MNIST original\', download=True) 
 >>> mnist.data 
 array([[0, 0, 0, 0, 0, 0,","'Download the data from Yann\'s website, unless it\'s already here.'"
"def set_range_metadata(builder, load, lower_bound, upper_bound): 
    range_operands = [Constant.int(load.type, lower_bound), Constant.int(load.type, upper_bound)] 
   md = builder.module.add_metadata(range_operands) 
   load.set_metadata('range', md)"," 'Set metadata for a load to indicate the range of values it can 
 access.'","'Set the ""range"" metadata on a load instruction. 
 Note the interval is in the form [lower_bound, upper_bound).'"
"def get_log_for_pid(pid): 
    found_pid = False 
   pid_str = ('   PID:   %s   ' % pid) 
   for line in fileinput.input(glob.glob((static.DEBUG_FILE + '*'))): 
      if (pid_str in line): 
         (yield line) 
         found_pid = True 
      elif (found_pid and ('   PID:   ' not in line)): 
         (yield line) 
      else: 
         found_pid = False"," 'Returns a generator of lines from the debug file that contain the given PID. 
 If the PID is not found, the generator will return an empty generator.'",'Fetches the logs from the debug log file for a given StarCluster run by PID'
"def _layout_figure(params): 
    size = (params['fig'].get_size_inches() * params['fig'].dpi) 
   scroll_width = 25 
   hscroll_dist = 25 
   vscroll_dist = 10 
   l_border = 100 
   r_border = 10 
   t_border = 35 
   b_border = 40 
   if ((size[0] < (2 * scroll_width)) or (size[1] < ((2 * scroll_width) + hscroll_dist))): 
      return 
   scroll_width_x = (scroll_width / size[0]) 
   scroll_width_y = (scroll_width / size[1]) 
   vscroll_dist /= size[0] 
   hscroll_dist /= size[1] 
   l_border /= size[0] 
   r_border /= size[0] 
   t_border /= size[1] 
   b_border /= size[1] 
   ax_width = ((((1.0 - scroll_width_x) - l_border) - r_border) - vscroll_dist) 
   ax_y = ((hscroll_dist + scroll_width_y) + b_border) 
   ax_height = ((1.0 - ax_y) - t_border) 
   pos = [l_border, ax_y, ax_width, ax_height] 
   params['ax'].set_position(pos) 
   if ('ax2' in params): 
      params['ax2'].set_position(pos) 
   params['ax'].set_position(pos) 
   pos = [((ax_width + l_border) + vscroll_dist), ax_y, scroll_width_x, ax_height] 
   params['ax_vscroll'].set_position(pos) 
   pos = [l_border, b_border, ax_width, scroll_width_y] 
   params['ax_hscroll'].set_position(pos) 
   if ('ax_button' in params): 
      pos = [((l_border + ax_width) + vscroll_dist), b_border, scroll_width_x, scroll_width_y] 
      params['ax_button'].set_position(pos) 
   if ('ax_help_button' in params): 
      pos = [((l_border - vscroll_dist) - (scroll_width_x * 2)), b_border, (scroll_width_x * 2), scroll_width_y] 
      params['ax_help_button'].set_position(pos) 
   params['fig'].canvas.draw()"," 'Layout figure with scrollbars. 
 Parameters 
 fig : matplotlib.figure.Figure 
 ax : matplotlib.axes.Axes 
 ax2 : matplotlib.axes.Axes 
 ax_vscroll : matplotlib.axes.Axes 
 ax_hscroll : matplotlib.axes.Axes 
 ax_button : matplotlib.axes.Axes 
 ax_help_button : matplotlib.axes.Axes 
 Returns 
 None'",'Function for setting figure layout. Shared with raw and epoch plots.'
"@requires_good_network 
 def test_fetch_file_html(): 
    _test_fetch('http://google.com')", 'Test fetching an HTML file','Test file downloading over http.'
"def hadoop_fs_ls(stdout, stderr, environ, *args): 
    if (mock_hadoop_uses_yarn(environ) and args and (args[0] == '-R')): 
      path_args = args[1:] 
      recursive = True 
   else: 
      path_args = args 
      recursive = False 
   return _hadoop_fs_ls('ls', stdout, stderr, environ, path_args=path_args, recursive=recursive)"," 'Runs \'ls\' on HDFS. 
 :param stdout: a file-like object to write the output to 
 :param stderr: a file-like object to write the error output to 
 :param environ: a dictionary of environment variables 
 :param args: a list of arguments to pass to \'ls\' 
 :return: a dictionary of the results of the command'",'Implements hadoop fs -ls.'
"def _indent(s, indent=4): 
    return re.sub('(?m)^(?!$)', (indent * '   '), s)"," 'Indent a string, preserving leading and trailing whitespace.'","'Add the given number of space characters to the beginning every 
 non-blank line in `s`, and return the result.'"
"def calibrate_2d_polynomial(cal_pt_cloud, screen_size=(1, 1), threshold=35, binocular=False): 
    model_n = 7 
   if binocular: 
      model_n = 13 
   cal_pt_cloud = np.array(cal_pt_cloud) 
   (cx, cy, err_x, err_y) = fit_poly_surface(cal_pt_cloud, model_n) 
   (err_dist, err_mean, err_rms) = fit_error_screen(err_x, err_y, screen_size) 
   if cal_pt_cloud[(err_dist <= threshold)].shape[0]: 
      (cx, cy, new_err_x, new_err_y) = fit_poly_surface(cal_pt_cloud[(err_dist <= threshold)], model_n) 
      map_fn = make_map_function(cx, cy, model_n) 
      (new_err_dist, new_err_mean, new_err_rms) = fit_error_screen(new_err_x, new_err_y, screen_size) 
      logger.info('first   iteration.   root-mean-square   residuals:   {},   in   pixel'.format(err_rms)) 
      logger.info('second   iteration:   ignoring   outliers.   root-mean-square   residuals:   {}   in   pixel'.format(new_err_rms)) 
      logger.info('used   {}   data   points   out   of   the   full   dataset   {}:   subset   is   {}   percent'.format((cal_pt_cloud[(err_dist <= threshold)].shape[0], cal_pt_cloud.shape[0], ((100 * float(cal_pt_cloud[(err_dist <= threshold)].shape[0])) / cal_pt_cloud.shape[0])))) 
      return (map_fn, (err_dist <= threshold), (cx, cy, model_n)) 
   else: 
      map_fn = make_map_function(cx, cy, model_n) 
      logger.error('First   iteration.   root-mean-square   residuals:   {}   in   pixel,   this   is   bad!'.format(err_rms)) 
      logger.error('The   data   cannot   be   represented   by   the   model   in   a   meaningfull   way.') 
      return (map_fn, (err_dist <= threshold), (cx, cy, model_n))"," 'Perform a calibration of a 2D polynomial model. 
 :param cal_pt_cloud: A 2D point cloud with the calibration points. 
 :param screen_size: The screen size in pixels. 
 :param threshold: The threshold for the error in the screen size. 
 :param binocular: Whether to use a binocular model. 
 :return: The map function, the threshold, and the polynomial model. 
 :rtype: tuple(function, bool, tuple(float, float, int))'","'we do a simple two pass fitting to a pair of bi-variate polynomials 
 return the function to map vector'"
"@requires_version('scipy', '0.16') 
 def test_iir_stability(): 
    sig = np.empty(1000) 
   sfreq = 1000 
   assert_raises(RuntimeError, filter_data, sig, sfreq, 0.6, None, method='iir', iir_params=dict(ftype='butter', order=8, output='ba')) 
   filter_data(sig, sfreq, 0.6, None, method='iir', iir_params=dict(ftype='butter', order=8, output='sos')) 
   assert_raises(ValueError, filter_data, sig, sfreq, 0.6, None, method='iir', iir_params=dict(ftype='butter', order=8, output='foo')) 
   assert_raises(RuntimeError, filter_data, sig, sfreq, 0.6, None, method='iir', iir_params=dict(order=8, output='sos')) 
   assert_raises(RuntimeError, filter_data, sig, sfreq, 0.6, None, method='iir', iir_params=dict(order=8, ftype='foo', output='sos')) 
   assert_raises(RuntimeError, filter_data, sig, sfreq, 0.6, None, method='iir', iir_params=dict(gpass=0.5, output='sos')) 
   assert_raises(ValueError, filter_data, sig, sfreq, 0.1, None, method='fft', iir_params=dict(ftype='butter', order=2, output='sos')) 
   assert_raises(TypeError, filter_data, sig, sfreq, 0.1, None, method=1) 
   assert_raises(ValueError, filter_data, sig, sfreq, 0.1, None, method='blah') 
   assert_raises(TypeError, filter_data, sig, sfreq, 0.1, None, method='iir', iir_params='blah') 
   assert_raises(ValueError, filter_data, sig, sfreq, 0.1, None, method='fft', iir_params=dict()) 
   iir_params = dict(ftype='butter', order=2, output='sos') 
   x_sos = filter_data(sig, 250, 0.5, None, method='iir', iir_params=iir_params) 
   iir_params_sos = construct_iir_filter(iir_params, f_pass=0.5, sfreq=250, btype='highpass') 
   x_sos_2 = filter_data(sig, 250, 0.5, None, method='iir', iir_params=iir_params_sos) 
   assert_allclose(x_sos[100:(-100)], x_sos_2[100:(-100)]) 
   x_ba = filter_data(sig, 250, 0.5, None, method='iir', iir_params=dict(ftype='butter', order=2, output='ba')) 
   assert_allclose(x_sos[100:(-100)], x_ba[100:(-100)])", 'Test that the IIR filter works correctly.','Test IIR filter stability check.'
"def safe_extra(extra): 
    return re.sub('[^A-Za-z0-9.-]+', '_', extra).lower()"," 'Safely lowercase and underscore a string. 
 This is a safe way to do a case-insensitive lowercase. 
 This function will remove any non-alphanumeric characters from the string. 
 This is done to avoid conflicts with other modules. 
 Parameters 
 extra : string 
 The string to be lowercased and underscored. 
 Returns 
 lowered_string : string 
 The lowercased and underscored string.'","'Convert an arbitrary string to a standard \'extra\' name 
 Any runs of non-alphanumeric characters are replaced with a single \'_\', 
 and the result is always lowercased.'"
"def readSettingsFromText(repository, text): 
    lines = archive.getTextLines(text) 
   shortDictionary = {} 
   for setting in repository.preferences: 
      shortDictionary[getShortestUniqueSettingName(setting.name, repository.preferences)] = setting 
   for lineIndex in xrange(len(lines)): 
      setRepositoryToLine(lineIndex, lines, shortDictionary)"," 'Read settings from text file. 
 :param repository: Repository object. 
 :param text: Text file. 
 :return: 
 :rtype: 
 :raise: 
 :raises: 
 :seealso: 
 :maintainer: 
 :copyright: 
 :license: 
 :todo: 
 :version: 
 :bugs: 
 :status: 
 :release: 0.0.1""",'Read settings from a text.'
"def versions_report(): 
    return '\n'.join(salt.version.versions_report())", 'Returns a list of all the available versions of Salt',"'Returns versions of components used by salt 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' test.versions_report'"
"def runCalibration(eyegaze_control): 
    import subprocess, time 
   result = pEyeGaze.EgExit(byref(eyegaze_control)) 
   eyegaze_control = None 
   p = subprocess.Popen(('calibrate.exe', '')) 
   while (p.poll() is None): 
      time.sleep(0.05) 
   return initEyeGaze()"," 'Run the calibration program. 
 :param eyegaze_control: A reference to the EyeGaze control object 
 :return: A reference to the EyeGaze control object'","'Function to run the external calibrate.exe program. 
 Returns a new instance of _stEgControl (probably not \'necessary\').'"
"def run_mantel_correlogram(fps, distmats, num_perms, comment, alpha, sample_id_map=None, variable_size_distance_classes=False): 
    if (len(fps) != len(distmats)): 
      raise ValueError('Must   provide   the   same   number   of   filepaths   as   there   are   distance   matrices.') 
   if (comment is None): 
      comment = '' 
   result = ((((comment + 'DM1 DCTB DM2 DCTB Number   of   entries DCTB ') + 'Number   of   permutations DCTB Class   index DCTB ') + 'Number   of   distances DCTB Mantel   r   statistic DCTB ') + 'p-value DCTB p-value   (Bonferroni   corrected) DCTB Tail   type\n') 
   correlogram_fps = [] 
   correlograms = [] 
   for (i, (fp1, (dm1_labels, dm1_data))) in enumerate(zip(fps, distmats)): 
      for (fp2, (dm2_labels, dm2_data)) in zip(fps, distmats)[(i + 1):]: 
         ((dm1_labels, dm1_data), (dm2_labels, dm2_data)) = make_compatible_distance_matrices((dm1_labels, dm1_data), (dm2_labels, dm2_data), lookup=sample_id_map) 
         if (len(dm1_labels) < 3): 
            result += ('%s DCTB %s DCTB %d DCTB Too   few   samples\n' % (fp1, fp2, len(dm1_labels))) 
            continue 
         dm1 = DistanceMatrix(dm1_data, dm1_labels) 
         dm2 = DistanceMatrix(dm2_data, dm2_labels) 
         mc = MantelCorrelogram(dm1, dm2, alpha=alpha, variable_size_distance_classes=variable_size_distance_classes) 
         results = mc(num_perms) 
         dm1_name = path.basename(fp1) 
         dm2_name = path.basename(fp2) 
         correlogram_fps.append(('_'.join((dm1_name, 'AND', dm2_name, 'mantel_correlogram')) + '.')) 
         correlograms.append(results['correlogram_plot']) 
         first_time = True 
         for (class_idx, num_dist, r, p, p_corr) in zip(results['class_index'], results['num_dist'], results['mantel_r'], results['mantel_p'], results['mantel_p_corr']): 
            p_str = None 
            if (p is not None): 
               p_str = p_value_to_str(p, num_perms) 
            p_corr_str = None 
            if (p_corr is not None): 
               p_corr_str = p_value_to_str(p_corr, num_perms) 
            if (r is None): 
               tail_type = None 
            elif (r < 0): 
               tail_type = 'less' 
            else: 
               tail_type = 'greater' 
            if first_time: 
               result += ('%s DCTB %s DCTB %d DCTB %d DCTB %s DCTB %d DCTB %s DCTB %s DCTB %s DCTB %s\n' % (fp1, fp2, len(dm1_labels), num_perms, class_idx, num_dist, r, p_str, p_corr_str, tail_type)) 
               first_time = False 
            else: 
               result += (' DCTB  DCTB  DCTB  DCTB %s DCTB %d DCTB %s DCTB %s DCTB %s DCTB %s\n' % (class_idx, num_dist, r, p_str, p_corr_str, tail_type)) 
   return (result, correlogram_fps, correlograms)"," 'Computes correlograms for each pair of distance matrices in the input 
 distance matrices, and returns the results in a string. 
 Parameters 
 fps : list of filepaths 
 The filepaths to the distance matrices. 
 distmats : list of DistanceMatrix objects 
 The distance matrices. 
 num_perms : int 
 The number of permutations to use for the MantelCorrelogram. 
 comment : str 
 A comment to include at the top of the output. 
 alpha : float 
 The alpha value for the Bonferroni correction. 
 sample_id_map : dict, optional 
 A dictionary mapping sample IDs to indices in the distance matrices. 
 If provided, the distance matrices are assumed to be in the same order 
 as the sample IDs. 
 variable_size_distance_classes : bool, optional 
 If True, the distance classes are assumed to be variable size. 
 Returns 
 result : str 
 A string containing the Mantel correlograms for each pair of distance 
 matrices. 
 correlogram_fps : list of str 
 The filepaths to the","'Runs a Mantel correlogram analysis on all pairs of distance matrices. 
 Returns a string suitable for writing out to a file containing the results 
 of the test, a list of correlogram filepath names, and a list of matplotlib 
 Figure objects representing each correlogram. 
 The correlogram filepaths can have an extension string appended to the end 
 of them and then be used to save each of the correlogram Figures to a file. 
 Each correlogram filepath will be a combination of the two distance matrix 
 filepaths that were used to create it. 
 WARNING: Only symmetric, hollow distance matrices may be used as input. 
 Asymmetric distance matrices, such as those obtained by the UniFrac Gain 
 metric (i.e. beta_diversity.py -m unifrac_g), should not be used as input. 
 Arguments: 
 fps - list of filepaths of the distance matrices 
 distmats - list of tuples containing dm labels and dm data (i.e. the 
 output of parse_distmat) 
 num_perms - the number of permutations to use to calculate the 
 p-value(s) 
 comment - comment string to add to the beginning of the results string 
 alpha - the alpha value to use to determine significance in the 
 correlogram plots 
 sample_id_map - dict mapping sample IDs (i.e. what is expected by 
 make_compatible_distance_matrices) 
 variable_size_distance_classes - create distance classes that vary in 
 size (i.e. width) but have the same number of distances in each 
 class'"
"def launch(dpid, port, port_eth=None, name=None, __INSTANCE__=None): 
    if (port_eth in (True, None)): 
      pass 
   else: 
      port_eth = EthAddr(port_eth) 
   dpid = str_to_dpid(dpid) 
   try: 
      port = int(port) 
   except: 
      pass 
   def dhcpclient_init(): 
      n = name 
      if (n is None): 
         s = '' 
         while True: 
            if (not core.hasComponent(('DHCPClient' + s))): 
               n = ('DHCPClient' + s) 
               break 
            s = str((int(('0' + s)) + 1)) 
      elif core.hasComponent(n): 
         self.log.error('Already   have   component   %s', n) 
         return 
      client = DHCPClient(port=port, dpid=dpid, name=n, port_eth=port_eth) 
      core.register(n, client) 
   core.call_when_ready(dhcpclient_init, ['openflow'])", 'Launch a DHCP client.',"'Launch 
 port_eth unspecified: ""DPID MAC"" 
 port_eth enabled: Port MAC 
 port_eth specified: Use that'"
"def __validate__(config): 
    if (not isinstance(config, dict)): 
      return (False, 'Configuration   for   btmp   beacon   must   be   a   list   of   dictionaries.') 
   return (True, 'Valid   beacon   configuration')"," 'Validate the configuration for the btmp beacon. 
 :param config: The configuration to validate. 
 :type config: dict 
 :return: A tuple containing a boolean indicating if the configuration is 
 valid and an error message if it is not.'",'Validate the beacon configuration'
"def heatmap(data, vmin=None, vmax=None, cmap=None, center=None, robust=False, annot=None, fmt='.2g', annot_kws=None, linewidths=0, linecolor='white', cbar=True, cbar_kws=None, cbar_ax=None, square=False, ax=None, xticklabels=True, yticklabels=True, mask=None, **kwargs): 
    plotter = _HeatMapper(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, cbar, cbar_kws, xticklabels, yticklabels, mask) 
   kwargs['linewidths'] = linewidths 
   kwargs['edgecolor'] = linecolor 
   if (ax is None): 
      ax = plt.gca() 
   if square: 
      ax.set_aspect('equal') 
   plotter.plot(ax, cbar_ax, kwargs) 
   return ax"," 'Plot a heatmap of a 2-dimensional array. 
 Parameters 
 data : 2-dimensional array 
 The data to plot. 
 vmin : float, optional 
 The minimum value for the colorbar. Default is the minimum value of the data. 
 vmax : float, optional 
 The maximum value for the colorbar. Default is the maximum value of the data. 
 cmap : matplotlib colormap, optional 
 The colormap to use. If None, the default colormap is used. 
 center : float, optional 
 The center of the colorbar. Default is 0.5. 
 robust : bool, optional 
 If True, the colorbar is not clipped to the range of the data. 
 annot : dict, optional 
 A dictionary containing the annotations to be plotted. 
 fmt : str, optional 
 The format of the annotations. Default is `.2g`. 
 annot_kws : dict, optional 
 The keyword arguments to pass to the `annot` function. 
 linewidths : float, optional 
 The linewidths of the edges of the heatmap. Default is","'Plot rectangular data as a color-encoded matrix. 
 This function tries to infer a good colormap to use from the data, but 
 this is not guaranteed to work, so take care to make sure the kind of 
 colormap (sequential or diverging) and its limits are appropriate. 
 This is an Axes-level function and will draw the heatmap into the 
 currently-active Axes if none is provided to the ``ax`` argument.  Part of 
 this Axes space will be taken and used to plot a colormap, unless ``cbar`` 
 is False or a separate Axes is provided to ``cbar_ax``. 
 Parameters 
 data : rectangular dataset 
 2D dataset that can be coerced into an ndarray. If a Pandas DataFrame 
 is provided, the index/column information will be used to label the 
 columns and rows. 
 vmin, vmax : floats, optional 
 Values to anchor the colormap, otherwise they are inferred from the 
 data and other keyword arguments. When a diverging dataset is inferred, 
 one of these values may be ignored. 
 cmap : matplotlib colormap name or object, optional 
 The mapping from data values to color space. If not provided, this 
 will be either a cubehelix map (if the function infers a sequential 
 dataset) or ``RdBu_r`` (if the function infers a diverging dataset). 
 center : float, optional 
 The value at which to center the colormap. Passing this value implies 
 use of a diverging colormap. 
 robust : bool, optional 
 If True and ``vmin`` or ``vmax`` are absent, the colormap range is 
 computed with robust quantiles instead of the extreme values. 
 annot : bool or rectangular dataset, optional 
 If True, write the data value in each cell. If an array-like with the 
 same shape as ``data``, then use this to annotate the heatmap instead 
 of the raw data. 
 fmt : string, optional 
 String formatting code to use when adding annotations. 
 annot_kws : dict of key, value mappings, optional 
 Keyword arguments for ``ax.text`` when ``annot`` is True. 
 linewidths : float, optional 
 Width of the lines that will divide each cell. 
 linecolor : color, optional 
 Color of the lines that will divide each cell. 
 cbar : boolean, optional 
 Whether to draw a colorbar. 
 cbar_kws : dict of key, value mappings, optional 
 Keyword arguments for `fig.colorbar`. 
 cbar_ax : matplotlib Axes, optional 
 Axes in which to draw the colorbar, otherwise take space from the 
 main Axes. 
 square : boolean, optional 
 If True, set the Axes aspect to ""equal"" so each cell will be 
 square-shaped. 
 ax : matplotlib Axes, optional 
 Axes in which to draw the plot, otherwise use the currently-active 
 Axes. 
 xticklabels : list-like, int, or bool, optional 
 If True, plot the column names of the dataframe. If False, don\'t plot 
 the column names. If list-like, plot these alternate labels as the 
 xticklabels. If an integer, use the column names but plot only every 
 n label. 
 yticklabels : list-like, int, or bool, optional 
 If True, plot the row names of the dataframe. If False, don\'t plot 
 the row names. If list-like, plot these alternate labels as the 
 yticklabels. If an integer, use the index names but plot only every 
 n label. 
 mask : boolean array or DataFrame, optional 
 If passed, data will not be shown in cells where ``mask`` is True. 
 Cells with missing values are automatically masked. 
 kwargs : other keyword arguments 
 All other keyword arguments are passed to ``ax.pcolormesh``. 
 Returns 
 ax : matplotlib Axes 
 Axes object with the heatmap. 
 Examples 
 Plot a heatmap for a numpy array: 
 .. plot:: 
 :context: close-figs 
 >>> import numpy as np; np.random.seed(0) 
 >>> import seaborn as sns; sns.set() 
 >>> uniform_data = np.random.rand(10, 12) 
 >>> ax = sns.heatmap(uniform_data) 
 Change the limits of the colormap: 
 .. plot:: 
 :context: close-figs 
 >>> ax = sns.heatmap(uniform_data, vmin=0, vmax=1) 
 Plot a heatmap for data centered on 0: 
 .. plot:: 
 :context: close-figs 
 >>> normal_data = np.random.randn(10, 12) 
 >>> ax = sns.heatmap(normal_data) 
 Plot a dataframe with meaningful row and column labels: 
 .. plot:: 
 :context: close-figs 
 >>> flights = sns.load_dataset(""flights"") 
 >>> flights = flights.pivot(""month"", ""year"", ""passengers"") 
 >>> ax = sns.heatmap(flights) 
 Annotate each cell with the numeric value using integer formatting: 
 .. plot:: 
 :context: close-figs 
 >>> ax = sns.heatmap(flights, annot=True, fmt=""d"") 
 Add lines between each cell: 
 .. plot:: 
 :context: close-figs 
 >>> ax = sns.heatmap(flights, linewidths=.5) 
 Use a different colormap: 
 .. plot:: 
 :context: close-figs 
 >>> ax = sns.heatmap(flights, cmap=""YlGnBu"") 
 Center the colormap at a specific value: 
 .. plot:: 
 :context: close-figs 
 >>> ax = sns.heatmap(flights, center=flights.loc[""January"", 1955]) 
 Plot every other column label and don\'t plot row labels: 
 .. plot:: 
 :context: close-figs 
 >>> data = np.random.randn(50, 20) 
 >>> ax = sns.heatmap(data, xticklabels=2, yticklabels=False) 
 Don\'t draw a colorbar: 
 .. plot:: 
 :context: close-figs 
 >>> ax = sns.heatmap(flights, cbar=False) 
 Use different axes for the colorbar: 
 .. plot:: 
 :context: close-figs 
 >>> grid_kws = {""height_ratios"": (.9, .05), ""hspace"": .3} 
 >>> f, (ax, cbar_ax) = plt.subplots(2, gridspec_kw=grid_kws) 
 >>> ax = sns.heatmap(flights, ax=ax, 
 ...                  cbar_ax=cbar_ax, 
 ...                  cbar_kws={""orientation"": ""horizontal""}) 
 Use a mask to plot only part of a matrix 
 .. plot:: 
 :context: close-figs 
 >>> corr = np.corrcoef(np.random.randn(10, 200)) 
 >>> mask = np.zeros_like(corr) 
 >>> mask[np.triu_indices_from(mask)] = True 
 >>> with sns.axes_style(""white""): 
 ...     ax = sns.heatmap(corr, mask=mask, vmax=.3, square=True)'"
"def cached_property(fun): 
    @wraps(fun) 
   def get(self): 
      try: 
         return self._cache[fun] 
      except AttributeError: 
         self._cache = {} 
      except KeyError: 
         pass 
      ret = self._cache[fun] = fun(self) 
      return ret 
   return property(get)"," 'Decorator for a property that caches the value of the function on which 
 it\'s based. 
 :param fun: The function to cache. 
 :type fun: callable 
 :return: A property decorator. 
 :rtype: property'",'A memoize decorator for class properties.'
"def buggy_mkl_svd(function): 
    @wraps(function) 
   def dec(*args, **kwargs): 
      try: 
         return function(*args, **kwargs) 
      except np.linalg.LinAlgError as exp: 
         if ('SVD   did   not   converge' in str(exp)): 
            from nose.plugins.skip import SkipTest 
            msg = 'Intel   MKL   SVD   convergence   error   detected,   skipping   test' 
            warn(msg) 
            raise SkipTest(msg) 
         raise 
   return dec"," 'Decorator that skips tests if Intel MKL SVD is buggy. 
 The Intel MKL SVD implementation is buggy, as described in 
 http://software.intel.com/en-us/forums/topic/386225 
 This decorator skips the test if the SVD did not converge.'",'Decorator for tests that make calls to SVD and intermittently fail.'
"def _check_all_tasks(tasks): 
    running_tasks_data = [] 
   for task in tasks: 
      if task.isAlive(): 
         running_tasks_data.append(('      %s   (started   %s)' % (task.name, time.strftime('%H:%M:%S', time.localtime(task.start_time))))) 
      if task.exception: 
         ALL_ERRORS.append(task.exception) 
   if running_tasks_data: 
      log('----------------------------------------') 
      log('Tasks   still   running:') 
      for task_details in running_tasks_data: 
         log(task_details)", 'Checks all tasks and logs the results','Checks the results of all tasks.'
"def _option(value): 
    if (value in __opts__): 
      return __opts__[value] 
   master_opts = __pillar__.get('master', {}) 
   if (value in master_opts): 
      return master_opts[value] 
   if (value in __pillar__): 
      return __pillar__[value]", 'Returns the value of an option.','Look up the value for an option.'
"def socket_pair(): 
    port = socket() 
   port.bind(('', 0)) 
   port.listen(1) 
   client = socket() 
   client.setblocking(False) 
   client.connect_ex(('127.0.0.1', port.getsockname()[1])) 
   client.setblocking(True) 
   server = port.accept()[0] 
   server.send(b('x')) 
   assert (client.recv(1024) == b('x')) 
   client.send(b('y')) 
   assert (server.recv(1024) == b('y')) 
   server.setblocking(False) 
   client.setblocking(False) 
   return (server, client)"," 'Test socket_pair() function. 
 Tests the socket_pair() function to make sure that it works correctly. 
 This test is a bit of a hack, but it is the only way to test the 
 socket_pair() function. 
 The test requires two processes to communicate over a TCP socket. 
 The first process (server) listens for connections and the second 
 process (client) connects to the server and sends and receives data. 
 The client and server are created using the socket_pair() function. 
 The client and server are then connected and data is sent and received. 
 The test verifies that the data is sent and received correctly. 
 The test also verifies that the socket_pair() function returns two 
 sockets that are connected to each other. 
 The test also verifies that the client and server can be used as 
 normal sockets. 
 The test uses the following steps: 
 1. Create a server socket. 
 2. Create a client socket. 
 3. Connect the client to the server. 
 4. Send data to the server. 
 5. Receive data",'Establish and return a pair of network sockets connected to each other.'
"@manager.command 
 def deploy(): 
    from flask_migrate import upgrade 
   from app.models import Role, User 
   upgrade() 
   Role.insert_roles() 
   User.add_self_follows()", 'Deploy the application','Run deployment tasks.'
"def split_statements(hql): 
    statements = [] 
   current = '' 
   prev = '' 
   between_quotes = None 
   is_comment = None 
   start_row = 0 
   start_col = 0 
   end_row = 0 
   end_col = (len(hql) - 1) 
   if (hql.find(';') in ((-1), (len(hql) - 1))): 
      return [((start_row, start_col), (end_row, end_col), hql)] 
   lines = hql.splitlines() 
   for (row, line) in enumerate(lines): 
      end_col = 0 
      end_row = row 
      if ((start_row == row) and (line.strip() == '')): 
         start_row += 1 
      elif (current.strip() == ''): 
         start_row = row 
         start_col = 0 
      for (col, c) in enumerate(line): 
         current += c 
         if ((c in ('""', ""'"")) and (prev != '\\') and (is_comment is None)): 
            if (between_quotes == c): 
               between_quotes = None 
            elif (between_quotes is None): 
               between_quotes = c 
         elif ((c == '-') and (prev == '-') and (between_quotes is None) and (is_comment is None)): 
            is_comment = True 
         elif (c == ';'): 
            if ((between_quotes is None) and (is_comment is None)): 
               current = current.strip() 
               current = current[:(-1)] 
               if (len(current) > 1): 
                  statements.append(((start_row, start_col), (row, (col + 1)), current)) 
                  start_col = (col + 1) 
               current = '' 
         if ((prev == '\\') and (between_quotes is not None)): 
            c = '' 
         prev = c 
         end_col = col 
      is_comment = None 
      prev = os.linesep 
      if (current != ''): 
         current += os.linesep 
   if (current and (current != ';')): 
      current = current.strip() 
      statements.append(((start_row, start_col), (end_row, (end_col + 1)), current)) 
   return statements"," 'Split a HQL statement into a list of (row, col, statement) tuples. 
 :param hql: the HQL statement 
 :return: a list of (row, col, statement) tuples'","'Split statements at semicolons ignoring the ones inside quotes and comments. 
 The comment symbols that come inside quotes should be ignored.'"
"def _toUTF8(data, encoding): 
    if ((len(data) >= 4) and (data[:2] == _l2bytes([254, 255])) and (data[2:4] != _l2bytes([0, 0]))): 
      encoding = 'utf-16be' 
      data = data[2:] 
   elif ((len(data) >= 4) and (data[:2] == _l2bytes([255, 254])) and (data[2:4] != _l2bytes([0, 0]))): 
      encoding = 'utf-16le' 
      data = data[2:] 
   elif (data[:3] == _l2bytes([239, 187, 191])): 
      encoding = 'utf-8' 
      data = data[3:] 
   elif (data[:4] == _l2bytes([0, 0, 254, 255])): 
      encoding = 'utf-32be' 
      data = data[4:] 
   elif (data[:4] == _l2bytes([255, 254, 0, 0])): 
      encoding = 'utf-32le' 
      data = data[4:] 
   newdata = unicode(data, encoding) 
   declmatch = re.compile('^<\\?xml[^>]*?>') 
   newdecl = ""<?xml   version='1.0'   encoding='utf-8'?>"" 
   if declmatch.search(newdata): 
      newdata = declmatch.sub(newdecl, newdata) 
   else: 
      newdata = ((newdecl + u'\n') + newdata) 
   return newdata.encode('utf-8')"," 'Converts data to UTF-8. 
 :param data: data to convert. 
 :param encoding: encoding to use. 
 :returns: converted data. 
 :rtype: str'","'Changes an XML data stream on the fly to specify a new encoding 
 data is a raw sequence of bytes (not Unicode) that is presumed to be in %encoding already 
 encoding is a string recognized by encodings.aliases'"
"def cbServerGreeting(proto, username, password): 
    tp = TrivialPrompter() 
   stdio.StandardIO(tp) 
   proto.prompt = tp.prompt 
   proto.display = tp.display 
   return proto.authenticate(password).addCallback(cbAuthentication, proto).addErrback(ebAuthentication, proto, username, password)"," 'Prompt the user for a password and authenticate the user. 
 This is used to authenticate the user in the server. 
 :param proto: A ServerPrompter instance. 
 :param username: The username to authenticate. 
 :param password: The password to authenticate with. 
 :return: A Deferred that will fire when the authentication has completed. 
 :rtype: Deferred'",'Initial callback - invoked after the server sends us its greet message.'
"def Ql(filter_, thing): 
    res = Q(filter_, thing) 
   if isinstance(filter_, type({})): 
      for k in res: 
         res[k] = list(res[k]) 
      return res 
   else: 
      return list(res)"," 'Returns a list of all Q(filter_, thing) for a given thing. 
 If filter_ is a dictionary, it will be converted to a list of 
 Q(filter_, thing) and returned as a list. 
 If filter_ is not a dictionary, it will be converted to a list of 
 Q(filter_, thing) and returned as a list.'","'same as Q, but returns a list, not a generator'"
"def getLevel(level): 
    return _levelNames.get(level, ('Level   %s' % level))", 'Get a level name.',"'Return the textual representation of logging level \'level\'. 
 If the level is one of the predefined levels (CRITICAL, ERROR, WARNING, 
 INFO, DEBUG) then you get the corresponding string. If you have 
 associated levels with names using addLevelName then the name you have 
 associated with \'level\' is returned. 
 If a numeric value corresponding to one of the defined levels is passed 
 in, the corresponding string representation is returned. 
 Otherwise, the string ""Level %s"" % level is returned.'"
"def _delete_rpm_probes(probes): 
    return __salt__['probes.delete_probes'](_ordered_dict_to_dict(probes), commit=False)", 'Delete all RPM probes',"'Calls the Salt module ""probes"" to delete probes from the device.'"
"@login_required 
 def dissociate(request, template_name='authopenid/dissociate.html', dissociate_form=OpenidDissociateForm, redirect_field_name=REDIRECT_FIELD_NAME, default_redirect=settings.LOGIN_REDIRECT_URL, extra_context=None): 
    redirect_to = request.REQUEST.get(redirect_field_name, '') 
   if ((not redirect_to) or ('//' in redirect_to) or ('   ' in redirect_to)): 
      redirect_to = default_redirect 
   rels = UserAssociation.objects.filter(user__id=request.user.id) 
   associated_openids = [rel.openid_url for rel in rels] 
   if ((len(associated_openids) == 1) and (not request.user.has_usable_password())): 
      msg = _(""You   can't   remove   this   openid.   You   should   set   a   password   first."") 
      return HttpResponseRedirect(('%s?%s' % (redirect_to, urllib.urlencode({'msg': msg})))) 
   if request.POST: 
      form = dissociate_form(request.POST) 
      if form.is_valid(): 
         openid_url = form.cleaned_data['openid_url'] 
         msg = '' 
         if (openid_url not in associated_openids): 
            msg = (_('%s   is   not   associated   to   your   account') % openid_url) 
         if (not msg): 
            UserAssociation.objects.get(openid_url__exact=openid_url).delete() 
            if (openid_url == request.session.get('openid_url')): 
               del request.session['openid_url'] 
            msg = _('openid   removed.') 
         return HttpResponseRedirect(('%s?%s' % (redirect_to, urllib.urlencode({'msg': msg})))) 
   else: 
      openid_url = request.GET.get('openid_url', '') 
      if (not openid_url): 
         msg = _('Invalid   OpenID   url.') 
         return HttpResponseRedirect(('%s?%s' % (redirect_to, urllib.urlencode({'msg': msg})))) 
      form = dissociate_form(initial={'openid_url': openid_url}) 
   return render(template_name, {'form': form, 'openid_url': openid_url}, context_instance=_build_context(request, extra_context=extra_context))"," 'Dissociate an openid from a user. 
 This is the page where the user can remove an openid. 
 If the user has a password, he cannot remove the openid. 
 If the user does not have a password, he can remove any openid. 
 If the user has a password, he can only remove the first openid. 
 This is a security measure to prevent the user from accidentally 
 removing his only openid. 
 :param request: the current request 
 :param template_name: the name of the template to render 
 :param dissociate_form: the form to render 
 :param redirect_field_name: the name of the field to use to redirect 
 :param default_redirect: the default redirect url 
 :param extra_context: the context to pass to the template 
 :return: the rendered template'",'view used to dissociate an openid from an account'
"def check_complete(task, out_queue): 
    logger.debug('Checking   if   %s   is   complete', task) 
   try: 
      is_complete = task.complete() 
   except Exception: 
      is_complete = TracebackWrapper(traceback.format_exc()) 
   out_queue.put((task, is_complete))"," 'Check if a task is complete. 
 :param task: Task to check 
 :type task: Task 
 :param out_queue: Queue to put the result on 
 :type out_queue: Queue'","'Checks if task is complete, puts the result to out_queue.'"
"def remove(name=None, pkgs=None, **kwargs): 
    pkg2rm = '' 
   if pkgs: 
      for pkg in pkgs: 
         pkg2rm += '{0}   '.format(pkg) 
      log.debug('Installing   these   packages   instead   of   {0}:   {1}'.format(name, pkg2rm)) 
   else: 
      pkg2rm = '{0}'.format(name) 
   old = list_pkgs() 
   cmd = '/bin/pkg   uninstall   -v   {0}'.format(pkg2rm) 
   out = __salt__['cmd.run_all'](cmd, output_loglevel='trace') 
   __context__.pop('pkg.list_pkgs', None) 
   new = list_pkgs() 
   ret = salt.utils.compare_dicts(old, new) 
   if (out['retcode'] != 0): 
      raise CommandExecutionError('Error   occurred   removing   package(s)', info={'changes': ret, 'retcode': ips_pkg_return_values[out['retcode']], 'errors': [out['stderr']]}) 
   return ret"," 'Remove a package 
 This function removes a package from the system. 
 If pkgs is specified, it will remove these packages instead of the 
 specified package. 
 name: the name of the package to remove 
 pkgs: a list of packages to remove instead of the specified package 
 **kwargs: any other keyword arguments are passed to the :func:`pkg.list_pkgs` function'","'Remove specified package. Accepts full or partial FMRI. 
 In case of multiple match, the command fails and won\'t modify the OS. 
 name 
 The name of the package to be deleted. 
 Multiple Package Options: 
 pkgs 
 A list of packages to delete. Must be passed as a python list. The 
 ``name`` parameter will be ignored if this option is passed. 
 Returns a list containing the removed packages. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' pkg.remove <package name> 
 salt \'*\' pkg.remove tcsh 
 salt \'*\' pkg.remove pkg://solaris/shell/tcsh 
 salt \'*\' pkg.remove pkgs=\'[""foo"", ""bar""]\''"
"def create_test_db(): 
    from inbox.config import config 
   database_hosts = config.get_required('DATABASE_HOSTS') 
   schemas = [(shard['SCHEMA_NAME'], host['HOSTNAME']) for host in database_hosts for shard in host['SHARDS']] 
   assert all([('test' in s) for (s, h) in schemas]) 
   for (name, host) in schemas: 
      cmd = 'DROP   DATABASE   IF   EXISTS   {name};   CREATE   DATABASE   IF   NOT   EXISTS   {name}   DEFAULT   CHARACTER   SET   utf8mb4   DEFAULT   COLLATE   utf8mb4_general_ci'.format(name=name) 
      subprocess.check_call('mysql   -h   {}   -uinboxtest   -pinboxtest   -e   ""{}""'.format(host, cmd), shell=True)"," 'Create the test database. 
 This will create a database with a schema called \'test\'.'","'Creates new, empty test databases.'"
"def _get_encryption_headers(key, source=False): 
    if (key is None): 
      return {} 
   key = _to_bytes(key) 
   key_hash = hashlib.sha256(key).digest() 
   key_hash = base64.b64encode(key_hash).rstrip() 
   key = base64.b64encode(key).rstrip() 
   if source: 
      prefix = 'X-Goog-Copy-Source-Encryption-' 
   else: 
      prefix = 'X-Goog-Encryption-' 
   return {(prefix + 'Algorithm'): 'AES256', (prefix + 'Key'): _bytes_to_unicode(key), (prefix + 'Key-Sha256'): _bytes_to_unicode(key_hash)}"," 'Returns headers to be used for encryption. 
 :param key: Key to encrypt with 
 :param source: Whether the key is for source or destination 
 :returns: headers to be used for encryption'","'Builds customer encryption key headers 
 :type key: bytes 
 :param key: 32 byte key to build request key and hash. 
 :type source: bool 
 :param source: If true, return headers for the ""source"" blob; otherwise, 
 return headers for the ""destination"" blob. 
 :rtype: dict 
 :returns: dict of HTTP headers being sent in request.'"
"def _unpickle_appattr(reverse_name, args): 
    return get_current_app()._rgetattr(reverse_name)(*args)"," 'Unpickle the app attribute, and return the result.'",'Unpickle app.'
"def is_mobile_available_for_user(user, descriptor): 
    return (auth.user_has_role(user, CourseBetaTesterRole(descriptor.id)) or _has_staff_access_to_descriptor(user, descriptor, descriptor.id) or _is_descriptor_mobile_available(descriptor))"," 'Checks if a user can access a given descriptor. 
 This function checks if the user has the role of 
 CourseBetaTesterRole(descriptor.id) or if the user is a 
 staff member or if the descriptor is available for mobile 
 access.'","'Returns whether the given course is mobile_available for the given user. 
 Checks: 
 mobile_available flag on the course 
 Beta User and staff access overrides the mobile_available flag 
 Arguments: 
 descriptor (CourseDescriptor|CourseOverview): course or overview of course in question'"
"@conf.commands.register 
 def sr1(x, promisc=None, filter=None, iface=None, nofilter=0, *args, **kargs): 
    if (not kargs.has_key('timeout')): 
      kargs['timeout'] = (-1) 
   s = conf.L3socket(promisc=promisc, filter=filter, nofilter=nofilter, iface=iface) 
   (a, b) = sndrcv(s, x, *args, **kargs) 
   s.close() 
   if (len(a) > 0): 
      return a[0][1] 
   else: 
      return None"," 'Receive a single packet from the network. 
 This command will receive a single packet from the network. 
 The packet is received using the promiscuous mode of the network 
 interface. 
 The packet is received using the filter specified by the filter 
 argument. 
 If the iface argument is specified, the packet is received using 
 the interface specified. 
 If the nofilter argument is set to 1, the packet is received using 
 the default filter. 
 The timeout argument specifies the number of seconds to wait for a 
 packet. 
 The x argument is the data to send. 
 The *args and **kargs arguments are used to set the socket options. 
 See the L3socket class documentation for more information. 
 The return value is the first packet received. 
 If no packet is received, None is returned. 
 The return value is a tuple of the form (addr, data) where addr is the 
 source address of the packet and data is the data received. 
 If the source address is not available, the data is returned instead. 
 The data is returned as a string. 
 If the packet is received using","'Send packets at layer 3 and return only the first answer 
 nofilter: put 1 to avoid use of BPF filters 
 retry:    if positive, how many times to resend unanswered packets 
 if negative, how many times to retry when no more packets are answered 
 timeout:  how much time to wait after the last packet has been sent 
 verbose:  set verbosity level 
 multi:    whether to accept multiple answers for the same stimulus 
 filter:   provide a BPF filter 
 iface:    listen answers only on the given interface'"
"def clear_caches(): 
    global FS_CACHE, MR_CACHE 
   old = (FS_CACHE, MR_CACHE) 
   (FS_CACHE, MR_CACHE) = (None, None) 
   return old", 'Clear the filesystem cache and MR cache.',"'Clears cluster\'s internal caches.  Returns 
 something that can be given back to restore_caches.'"
"def length_is(value, arg): 
    return (len(value) == int(arg))"," 'Check if a value\'s length is equal to the argument. 
 Parameters 
 value : a string 
 arg : an integer 
 Returns 
 True if the length of the string is equal to the argument, False otherwise 
 Raises 
 ValueError if the length is not an integer'",'Returns a boolean of whether the value\'s length is the argument'
"def search(opts, returners, whitelist=None): 
    return LazyLoader(_module_dirs(opts, 'search', 'search'), opts, tag='search', whitelist=whitelist, pack={'__ret__': returners})"," 'Searches for a string in the current document. 
 :param opts: The search options 
 :param returners: The returners to use for search 
 :param whitelist: A list of searchable tags 
 :return: A list of matched tags'","'Returns the search modules 
 :param dict opts: The Salt options dictionary 
 :param returners: Undocumented 
 :param whitelist: Undocumented'"
"def base64_b64encode(instr): 
    if six.PY3: 
      b = salt.utils.to_bytes(instr) 
      b64 = base64.b64encode(b) 
      return salt.utils.to_str(b64) 
   return base64.b64encode(instr)"," 'Encode data to base64. 
 :param instr: data to be encoded 
 :type instr: str, bytes, or iterable 
 :return: base64 encoded data'","'Encode a string as base64 using the ""modern"" Python interface. 
 Among other possible differences, the ""modern"" encoder does not include 
 newline (\'\n\') characters in the encoded output. 
 .. versionadded:: 2016.3.0 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' hashutil.base64_b64encode \'get salted\''"
"def iteritems(d): 
    return getattr(d, _iteritems)()"," 'Returns an iterator over the key-value pairs in the dictionary. 
 This is a generator function, so it does not return a list. 
 >>> d = {1:2, 3:4} 
 >>> list(iteritems(d)) 
 [(1, 2), (3, 4)]'","'Return an iterator over the (key, value) pairs of a dictionary.'"
"def safe_encode(text, incoming=None, encoding='utf-8', errors='strict'): 
    if (not isinstance(text, basestring)): 
      raise TypeError((""%s   can't   be   encoded"" % type(text))) 
   if (not incoming): 
      incoming = (sys.stdin.encoding or sys.getdefaultencoding()) 
   if isinstance(text, unicode): 
      return text.encode(encoding, errors) 
   elif (text and (encoding != incoming)): 
      text = safe_decode(text, incoming, errors) 
      return text.encode(encoding, errors) 
   return text"," 'Safe encoding of text. 
 :param text: text to be encoded 
 :param incoming: encoding of input 
 :param encoding: encoding to use 
 :param errors: errors to use 
 :returns: encoded text'","'Encodes incoming str/unicode using `encoding`. If 
 incoming is not specified, text is expected to 
 be encoded with current python\'s default encoding. 
 (`sys.getdefaultencoding`) 
 :param incoming: Text\'s current encoding 
 :param encoding: Expected encoding for text (Default UTF-8) 
 :param errors: Errors handling policy. See here for valid 
 values http://docs.python.org/2/library/codecs.html 
 :returns: text or a bytestring `encoding` encoded 
 representation of it. 
 :raises TypeError: If text is not an isntance of basestring'"
"def get_storage_hash(storage): 
    if isinstance(storage, LazyObject): 
      if (storage._wrapped is None): 
         storage._setup() 
      storage = storage._wrapped 
   if (not isinstance(storage, six.string_types)): 
      storage_cls = storage.__class__ 
      storage = ('%s.%s' % (storage_cls.__module__, storage_cls.__name__)) 
   return hashlib.md5(storage.encode('utf8')).hexdigest()"," 'Return a hash of the storage object. 
 This is used to uniquely identify the storage object. 
 :param storage: The storage object. 
 :type storage: :class:`~.Storage` 
 :return: A hash of the storage object. 
 :rtype: str'","'Return a hex string hash for a storage object (or string containing 
 \'full.path.ClassName\' referring to a storage object).'"
"def test_get_debug_values_no_debugger(): 
    prev_value = config.compute_test_value 
   try: 
      config.compute_test_value = 'off' 
      x = T.vector() 
      for x_val in op.get_debug_values(x): 
         assert False 
   finally: 
      config.compute_test_value = prev_value"," 'Test that get_debug_values() returns empty list when no debugger is 
 active.'",'get_debug_values should return [] when debugger is off'
"def has_default_value(option, value): 
    return ((option in helpful_parser.defaults) and (helpful_parser.defaults[option] == value))"," 'Returns true if the option has a default value. 
 :param option: The option. 
 :param value: The value of the option. 
 :return: True if the option has a default value, false otherwise.'","'Does option have the default value? 
 If the default value of option is not known, False is returned. 
 :param str option: configuration variable being considered 
 :param value: value of the configuration variable named option 
 :returns: True if option has the default value, otherwise, False 
 :rtype: bool'"
"def rate_id(context, id_, val=3): 
    def rate_element(driver): 
      try: 
         inner_wrapper = find_id_with_wait(context, id_, wait_time=2) 
         els = inner_wrapper.find_elements_by_class_name(STAR_RATING_OPTION_CLASS) 
         rating_el = [el for el in filter((lambda x: (int(x.get_attribute('data-val')) == val)), els)][0] 
         rating_el.click() 
         return True 
      except (NoSuchElementException, StaleElementReferenceException, TimeoutException, IndexError): 
         return False 
   try: 
      WebDriverWait(context.browser, 30).until(rate_element) 
   except TimeoutException: 
      raise Exception(""Unable   to   enter   rating   for   container   with   id   '{id:s}'"".format(id=id_))"," 'Rates a container with id \'id\'. 
 :param context: A :py:class:`~selenium.webdriver.remote.RemoteWebDriver` 
 :param id: The id of the container 
 :param val: The rating to give (0-5)'","'Enter a star rating given the id of the container 
 :param context: behave context 
 :param id: id of the container element'"
"def _get_client(timeout=None): 
    if ('docker.client' not in __context__): 
      client_kwargs = {} 
      for (key, val) in (('base_url', 'docker.url'), ('version', 'docker.version')): 
         param = __salt__['config.get'](val, NOTSET) 
         if (param is not NOTSET): 
            client_kwargs[key] = param 
      if (('base_url' not in client_kwargs) and ('DOCKER_HOST' in os.environ)): 
         client_kwargs['base_url'] = os.environ.get('DOCKER_HOST') 
      if ('version' not in client_kwargs): 
         client_kwargs['version'] = 'auto' 
      docker_machine = __salt__['config.get']('docker.machine', NOTSET) 
      if (docker_machine is not NOTSET): 
         docker_machine_json = __salt__['cmd.run'](('docker-machine   inspect   ' + docker_machine)) 
         try: 
            docker_machine_json = json.loads(docker_machine_json) 
            docker_machine_tls = docker_machine_json['HostOptions']['AuthOptions'] 
            docker_machine_ip = docker_machine_json['Driver']['IPAddress'] 
            client_kwargs['base_url'] = (('https://' + docker_machine_ip) + ':2376') 
            client_kwargs['tls'] = docker.tls.TLSConfig(client_cert=(docker_machine_tls['ClientCertPath'], docker_machine_tls['ClientKeyPath']), ca_cert=docker_machine_tls['CaCertPath'], assert_hostname=False, verify=True) 
         except Exception as exc: 
            raise CommandExecutionError('Docker   machine   {0}   failed:   {1}'.format(docker_machine, exc)) 
      try: 
         __context__['docker.client'] = docker.Client(**client_kwargs) 
      except docker.errors.DockerException: 
         log.error('Could   not   initialize   Docker   client') 
         return False 
   if ((timeout is not None) and (__context__['docker.client'].timeout != timeout)): 
      __context__['docker.client'].timeout = timeout"," 'Get a docker client object. 
 If the docker client object is not already available in the context, 
 then this function will try to get one. 
 :param timeout: (optional) timeout for the client 
 :return: the docker client object'","'Obtains a connection to a docker API (socket or URL) based on config.get 
 mechanism (pillar -> grains) 
 By default it will use the base docker-py defaults which 
 at the time of writing are using the local socket and 
 the 1.4 API 
 Set those keys in your configuration tree somehow: 
 - docker.url: URL to the docker service 
 - docker.version: API version to use (default: ""auto"")'"
"def convert_to_RGB_255(colors): 
    rgb_components = [] 
   for component in colors: 
      rounded_num = decimal.Decimal(str((component * 255.0))).quantize(decimal.Decimal('1'), rounding=decimal.ROUND_HALF_EVEN) 
      rounded_num = int(rounded_num) 
      rgb_components.append(rounded_num) 
   return (rgb_components[0], rgb_components[1], rgb_components[2])", 'Convert the color components to RGB with 255 as the max value.',"'Multiplies each element of a triplet by 255 
 Each coordinate of the color tuple is rounded to the nearest float and 
 then is turned into an integer. If a number is of the form x.5, then 
 if x is odd, the number rounds up to (x+1). Otherwise, it rounds down 
 to just x. This is the way rounding works in Python 3 and in current 
 statistical analysis to avoid rounding bias 
 :param (list) rgb_components: grabs the three R, G and B values to be 
 returned as computed in the function'"
"def p_expr_list_2(t): 
    pass"," 'List of expressions. 
 exprs ::= expr_list 
 expr_list ::= exprs 
 exprs ::= exprs COMMA expr 
 expr ::= expr_list 
 expr_list ::= expr 
 expr ::= expr_list COMMA expr 
 expr ::= expr_list 
 expr_list ::= expr 
 expr ::= expr_list COMMA expr 
 expr ::= expr_list 
 expr ::= expr COMMA expr 
 expr ::= expr 
 expr ::= expr_list 
 expr ::= expr COMMA expr 
 expr ::= expr_list 
 expr ::= expr COMMA expr 
 expr ::= expr 
 expr ::= expr_list 
 expr ::= expr COMMA expr 
 expr ::= expr_list 
 expr ::= expr COMMA expr 
 expr ::= expr 
 expr ::= expr_list 
 expr ::= expr COMMA expr 
 expr ::= expr_list 
 expr ::= expr COMMA expr 
 expr ::= expr 
 expr ::= expr_list 
 expr ::= expr COMMA expr 
 expr ::= expr_list",'exprlist : expression'
"def p_relational_expression_5(t): 
    pass"," 'relational expression 
 relational expression 
 :type relational_expression_5: RelationalExpression'",'relational_expression : relational_expression GE shift_expression'
"def tryall(context, prefix=None): 
    context = context.copy() 
   results = {} 
   for (k, v) in context.iteritems(): 
      if (not hasattr(v, '__call__')): 
         continue 
      if (prefix and (not k.startswith(prefix))): 
         continue 
      print (k + ':'), 
      try: 
         r = v() 
         dictincr(results, r) 
         print r 
      except: 
         print 'ERROR' 
         dictincr(results, 'ERROR') 
         print ('         ' + '\n         '.join(traceback.format_exc().split('\n'))) 
   print ('-' * 40) 
   print 'results:' 
   for (k, v) in results.iteritems(): 
      print ('   ' * 2), (str(k) + ':'), v"," 'Try all the contexts and print the results. 
 :param context: 
 :param prefix: 
 :return: 
 :rtype: 
 :raise: 
 :example: 
 >>> from sympy.physics.mechanics import tryall 
 >>> tryall(a=1, b=2, c=3) 
 a: 1 
 b: 2 
 c: 3 
 >>> tryall(a=1, b=2, c=3, d=4) 
 a: 1 
 b: 2 
 c: 3 
 d: 4 
 >>> tryall(a=1, b=2, c=3, d=4, e=5) 
 a: 1 
 b: 2 
 c: 3 
 d: 4 
 e: 5 
 >>> tryall(a=1, b=2, c=3, d=4, e=5, f=6) 
 a: 1 
 b: 2 
 c: 3 
 d: 4 
 e: ","'Tries a series of functions and prints their results. 
 `context` is a dictionary mapping names to values; 
 the value will only be tried if it\'s callable. 
 For example, you might have a file `test/stuff.py` 
 with a series of functions testing various things in it. 
 At the bottom, have a line: 
 if __name__ == ""__main__"": tryall(globals()) 
 Then you can run `python test/stuff.py` and get the results of 
 all the tests.'"
"def getProfileDirectory(): 
    craftTypeName = getCraftTypeName() 
   return os.path.join(craftTypeName, getProfileName(craftTypeName))"," 'Returns the directory where the profile is stored. 
 :return: the profile directory'",'Get the profile directory.'
"def berp_zeros(nt): 
    if ((not isscalar(nt)) or (floor(nt) != nt) or (nt <= 0)): 
      raise ValueError('nt   must   be   positive   integer   scalar.') 
   return specfun.klvnzo(nt, 5)"," 'Return the zeros of the Berry-Esakov-Racah polynomial. 
 The Berry-Esakov-Racah polynomial is defined as 
 .. math:: P_n(z) = \frac{1}{2} (z + z^{-1})^{n} 
 \prod_{k=0}^{n-1} (1 - z^{2k+1}), 
 where n is a positive integer. 
 Parameters 
 n : integer 
 The order of the polynomial. 
 Returns 
 zeros : array_like 
 The zeros of the polynomial. 
 Examples 
 >>> from sympy.polys.polytools import berp_zeros 
 >>> berp_zeros(3) 
 [0, 1, 2]'","'Compute nt zeros of the Kelvin function ber\'(x). 
 References 
 .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special 
 Functions"", John Wiley and Sons, 1996. 
 http://jin.ece.illinois.edu/specfunc.html'"
"def referenced(word, article=INDEFINITE, gender=MALE, role=SUBJECT): 
    return ('%s   %s' % (_article(word, article, gender, role), word))"," 'Returns a reference to the word. 
 :param word: A word. 
 :param article: The article to use. 
 :param gender: The gender to use. 
 :param role: The role to use. 
 :return: A reference to the word.'",'Returns a string with the article + the word.'
"def nagios_from_file(results_file): 
    data = open(results_file).read().strip() 
   pieces = data.split('|') 
   if (not (len(pieces) == 4)): 
      state = 'UNKNOWN' 
      ret = 3 
      data = 'Results   file   malformed' 
   else: 
      timestamp = int(pieces[0]) 
      time_diff = (time.time() - timestamp) 
      if (time_diff > (60 * 2)): 
         ret = 3 
         state = 'UNKNOWN' 
         data = 'Results   file   is   stale' 
      else: 
         ret = int(pieces[1]) 
         state = pieces[2] 
         data = pieces[3] 
   return (ret, ('%s:   %s' % (state, data)))"," 'Parse a Nagios results file and return the state and a description 
 of the state. 
 :param results_file: Path to the results file 
 :return: state, description'","'Returns a nagios-appropriate string and return code obtained by 
 parsing the desired file on disk. The file on disk should be of format 
 %s|%s % (timestamp, nagios_string) 
 This file is created by various nagios checking cron jobs such as 
 check-rabbitmq-queues and check-rabbitmq-consumers'"
"@pytest.mark.django_db 
 def test_max_revision(revision, project0_nongnu, store0): 
    store0.sync() 
   store0.update(store0.file.store) 
   initial_max_revision = Unit.max_revision() 
   initial_revision = Revision.get() 
   assert (initial_max_revision == initial_revision) 
   for i in range(10): 
      _update_translation(store0, 0, {'target': str(i)}, sync=False) 
   end_max_revision = Unit.max_revision() 
   end_revision = Revision.get() 
   assert (end_max_revision == end_revision) 
   assert (end_max_revision != initial_max_revision) 
   assert (end_revision != initial_revision) 
   assert (end_revision == (10 + initial_revision))", 'Test that max revision is incremented when a new translation is added.','Tests `max_revision()` gets the latest revision.'
"def json_http_response(data): 
    return JsonResponse(data)"," 'Returns a json response for the given data. 
 :param data: The data to be returned. 
 :type data: dict or list 
 :return: JsonResponse object.'","'Return an HttpResponse with the data json-serialized and the right content 
 type header.'"
"def expect_mc(dist, func=(lambda x: 1), size=50000): 
    def fun(x): 
      return func(x) 
   rvs = dist.rvs(size=size) 
   return fun(rvs).mean(0)"," 'Expectation of a random variable with the given distribution. 
 Parameters 
 dist : :class:`Distribution` 
 The distribution to expect. 
 func : callable, optional 
 A function to apply to each sample. 
 size : int, optional 
 The number of samples to generate. 
 Returns 
 The expected value. 
 Examples 
 >>> from sympy.stats import Normal 
 >>> from sympy.stats.distributions import expect_mc 
 >>> Normal.pdf(0).expect_mc() 
 0 
 >>> Normal.pdf(1).expect_mc() 
 1'","'calculate expected value of function by Monte Carlo integration 
 Parameters 
 dist : distribution instance 
 needs to have rvs defined as a method for drawing random numbers 
 func : callable 
 function for which expectation is calculated, this function needs to 
 be vectorized, integration is over axis=0 
 size : int 
 number of random samples to use in the Monte Carlo integration, 
 Notes 
 this doesn\'t batch 
 Returns 
 expected value : ndarray 
 return of function func integrated over axis=0 by MonteCarlo, this will 
 have the same shape as the return of func without axis=0 
 Examples 
 integrate probability that both observations are negative 
 >>> mvn = mve.MVNormal([0,0],2.) 
 >>> mve.expect_mc(mvn, lambda x: (x<np.array([0,0])).all(-1), size=100000) 
 0.25306000000000001 
 get tail probabilities of marginal distribution (should be 0.1) 
 >>> c = stats.norm.isf(0.05, scale=np.sqrt(2.)) 
 >>> expect_mc(mvn, lambda x: (np.abs(x)>np.array([c, c])), size=100000) 
 array([ 0.09969,  0.0986 ]) 
 or calling the method 
 >>> mvn.expect_mc(lambda x: (np.abs(x)>np.array([c, c])), size=100000) 
 array([ 0.09937,  0.10075])'"
"def overrideRootMenu(root, flist): 
    from Tkinter import Menu, Text, Text 
   from idlelib.EditorWindow import prepstr, get_accelerator 
   from idlelib import Bindings 
   from idlelib import WindowList 
   from idlelib.MultiCall import MultiCallCreator 
   closeItem = Bindings.menudefs[0][1][(-2)] 
   del Bindings.menudefs[0][1][(-3):] 
   Bindings.menudefs[0][1].insert(6, closeItem) 
   del Bindings.menudefs[(-1)][1][0:2] 
   del Bindings.menudefs[(-2)][1][0:2] 
   menubar = Menu(root) 
   root.configure(menu=menubar) 
   menudict = {} 
   menudict['windows'] = menu = Menu(menubar, name='windows') 
   menubar.add_cascade(label='Window', menu=menu, underline=0) 
   def postwindowsmenu(menu=menu): 
      end = menu.index('end') 
      if (end is None): 
         end = (-1) 
      if (end > 0): 
         menu.delete(0, end) 
      WindowList.add_windows_to_menu(menu) 
   WindowList.register_callback(postwindowsmenu) 
   def about_dialog(event=None): 
      from idlelib import aboutDialog 
      aboutDialog.AboutDialog(root, 'About   IDLE') 
   def config_dialog(event=None): 
      from idlelib import configDialog 
      root.instance_dict = flist.inversedict 
      configDialog.ConfigDialog(root, 'Settings') 
   def help_dialog(event=None): 
      from idlelib import textView 
      fn = path.join(path.abspath(path.dirname(__file__)), 'help.txt') 
      textView.view_file(root, 'Help', fn) 
   root.bind('<<about-idle>>', about_dialog) 
   root.bind('<<open-config-dialog>>', config_dialog) 
   root.createcommand('::tk::mac::ShowPreferences', config_dialog) 
   if flist: 
      root.bind('<<close-all-windows>>', flist.close_all_callback) 
      root.createcommand('exit', flist.close_all_callback) 
   if isCarbonTk(): 
      menudict['application'] = menu = Menu(menubar, name='apple') 
      menubar.add_cascade(label='IDLE', menu=menu) 
      Bindings.menudefs.insert(0, ('application', [('About   IDLE', '<<about-idle>>'), None])) 
      tkversion = root.tk.eval('info   patchlevel') 
      if (tuple(map(int, tkversion.split('.'))) < (8, 4, 14)): 
         Bindings.menudefs[0][1].append(('_Preferences....', '<<open-config-dialog>>')) 
   if isCocoaTk(): 
      root.createcommand('tkAboutDialog', about_dialog) 
      root.createcommand('::tk::mac::ShowHelp', help_dialog) 
      del Bindings.menudefs[(-1)][1][0]"," 'Adds the flist\'s windows to the root menu. 
 This is done by adding the flist\'s windows to the root menu. 
 This is done by adding the flist\'s windows to the root menu. 
 :param root: The root window 
 :param flist: The flist to add the windows to'","'Replace the Tk root menu by something that is more appropriate for 
 IDLE with an Aqua Tk.'"
"def limitedTime(second, func, *args, **kw): 
    return func(*args, **kw)"," 'Limits the time a function is executed. 
 Example:: 
 limitedTime(1, print, \'Hello\') 
 Limited time is reset when the function returns. 
 Example:: 
 limitedTime(1, print, \'Hello\') 
 Hello 
 limitedTime(1, print, \'Hello\') 
 Hello 
 limitedTime(1, print, \'Hello\') 
 Hello 
 limitedTime(1, print, \'Hello\') 
 Hello 
 limitedTime(1, print, \'Hello\') 
 Hello 
 limitedTime(1, print, \'Hello\') 
 Hello 
 limitedTime(1, print, \'Hello\') 
 Hello 
 limitedTime(1, print, \'Hello\') 
 Hello 
 limitedTime(1, print, \'Hello\') 
 Hello 
 limitedTime(1, print, \'Hello\') 
 Hello 
 limitedTime(1, print, \'Hello\') 
 Hello 
 limitedTime(1, print, \'Hello\') 
 Hello 
 limitedTime(1, print","'Call func(*args, **kw) with a timeout of second seconds.'"
"def rnn_helper(inp, length, cell_type=None, direction='forward', name=None, *args, **kwargs): 
    assert (cell_type is not None) 
   rnn_func = None 
   if (cell_type == 'lstm'): 
      rnn_func = lstm_layer 
   assert (rnn_func is not None) 
   assert (direction in ['forward', 'backward', 'bidirectional']) 
   with tf.variable_scope(name): 
      if (direction in ['forward', 'bidirectional']): 
         forward = rnn_func(inp=inp, length=length, backward=False, name='forward', *args, **kwargs) 
         if isinstance(forward, tuple): 
            forward = forward[0] 
      if (direction in ['backward', 'bidirectional']): 
         backward = rnn_func(inp=inp, length=length, backward=True, name='backward', *args, **kwargs) 
         if isinstance(backward, tuple): 
            backward = backward[0] 
      if (direction == 'forward'): 
         out = forward 
      elif (direction == 'backward'): 
         out = backward 
      else: 
         out = tf.concat(2, [forward, backward]) 
   return out"," 'Helper function for recurrent neural network layers. 
 Parameters 
 inp : tensor 
 Input tensor. 
 length : int 
 Length of input sequence. 
 cell_type : string, optional 
 Type of recurrent cell. 
 direction : string, optional 
 Direction of the recurrent neural network. 
 It can be either \'forward\', \'backward\' or \'bidirectional\'. 
 If \'forward\' or \'backward\' is chosen, the output is a tensor 
 of shape [batch_size, length, output_size]. 
 If \'bidirectional\' is chosen, the output is a tensor of shape 
 [batch_size, length, 2 * output_size]. 
 name : string, optional 
 Optional name for the operation. 
 *args : tuple, optional 
 Optional positional arguments for the recurrent layer. 
 *kwargs : dict, optional 
 Optional keyword arguments for the recurrent layer. 
 Returns 
 out : tensor 
 Output tensor. 
 Examples 
 >>> inp = tf.placeholder(tf.float32, shape","'Adds ops for a recurrent neural network layer. 
 This function calls an actual implementation of a recurrent neural network 
 based on `cell_type`. 
 There are three modes depending on the value of `direction`: 
 forward: Adds a forward RNN. 
 backward: Adds a backward RNN. 
 bidirectional: Adds both forward and backward RNNs and creates a 
 bidirectional RNN. 
 Args: 
 inp: A 3-D tensor of shape [`batch_size`, `max_length`, `feature_dim`]. 
 length: A 1-D tensor of shape [`batch_size`] and type int64. Each element 
 represents the length of the corresponding sequence in `inp`. 
 cell_type: Cell type of RNN. Currently can only be ""lstm"". 
 direction: One of ""forward"", ""backward"", ""bidirectional"". 
 name: Name of the op. 
 *args: Other arguments to the layer. 
 **kwargs: Keyword arugments to the layer. 
 Returns: 
 A 3-D tensor of shape [`batch_size`, `max_length`, `num_nodes`].'"
"def generate_random_alphanumeric(length): 
    return ''.join((random.choice((string.ascii_uppercase + string.digits)) for _x in range(length)))"," 'Generates a random string of length `length` 
 :param length: The length of the string to generate 
 :type length: int 
 :return: A random string of length `length` 
 :rtype: str'",'Creates a random alphanumeric string of specified length.'
"def scheme_node_from_element(node_el, registry): 
    try: 
      widget_desc = registry.widget(node_el.get('qualified_name')) 
   except KeyError as ex: 
      raise UnknownWidgetDefinition(*ex.args) 
   title = node_el.get('title') 
   pos = node_el.get('position') 
   if (pos is not None): 
      pos = tuple_eval(pos) 
   return SchemeNode(widget_desc, title=title, position=pos)"," 'Creates a SchemeNode from a DOM element. 
 :param node_el: The DOM element to create a SchemeNode from. 
 :type node_el: Element 
 :param registry: The registry that contains the widget definitions. 
 :type registry: Registry 
 :return: The SchemeNode.'",'Create a SchemeNode from an `Element` instance.'
"def canonicalize_emails(changelog, mapping): 
    for (alias, email_address) in mapping.iteritems(): 
      changelog = changelog.replace(alias, email_address) 
   return changelog", 'Replace the aliases in the changelog with their canonical email addresses.',"'Takes in a string and an email alias mapping and replaces all 
 instances of the aliases in the string with their real email.'"
"def _ordered_count(iterable): 
    c = OrderedDict() 
   for elem in iterable: 
      c[elem] = (c.get(elem, 0) + 1) 
   return c"," 'Counts the number of elements in a sequence, ordered by value. 
 >>> _ordered_count(range(10)) 
 {0: 5, 1: 2, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1} 
 >>> _ordered_count(range(10)) == _ordered_count(range(10)[::-1]) 
 True 
 >>> _ordered_count(range(10)) == _ordered_count(range(10)[::-1]) 
 False 
 >>> _ordered_count(range(10)) == _ordered_count(range(10)[::-1]) 
 True'","'Return dict of element counts, in the order they were first seen'"
"def pull_dkr(url, name, index): 
    return _pull_image('dkr', url, name, index=index)"," 'Pulls a Docker image from a registry. 
 :param url: The registry URL. 
 :param name: The image name. 
 :param index: The image index. 
 :returns: A dictionary with the image info.'","'Execute a ``machinectl pull-dkr`` to download a docker image and add it to 
 /var/lib/machines as a new container. 
 .. note:: 
 **Requires systemd >= 219** 
 url 
 URL from which to download the container 
 name 
 Name for the new container 
 index 
 URL of the Docker index server from which to pull (must be an 
 ``http://`` or ``https://`` URL). 
 CLI Examples: 
 .. code-block:: bash 
 salt myminion nspawn.pull_dkr centos/centos6 cent6 index=https://get.docker.com 
 salt myminion nspawn.pull_docker centos/centos6 cent6 index=https://get.docker.com'"
"def managedcloud(vm_): 
    return config.get_cloud_config_value('managedcloud', vm_, __opts__, default='False', search_global=False)", 'Returns the managedcloud setting for the vm.',"'Determine if we should wait for the managed cloud automation before 
 running. Either \'False\' (default) or \'True\'.'"
"def assert_raises_regex(exception_class, expected_regexp, callable_obj=None, *args, **kwargs): 
    __tracebackhide__ = True 
   nose = import_nose() 
   if (sys.version_info.major >= 3): 
      funcname = nose.tools.assert_raises_regex 
   else: 
      funcname = nose.tools.assert_raises_regexp 
   return funcname(exception_class, expected_regexp, callable_obj, *args, **kwargs)"," 'Raises an exception if an exception with a given regex is raised. 
 :param exception_class: The exception class to raise. 
 :param expected_regexp: The expected regex to match the exception. 
 :param callable_obj: The callable to be tested. 
 :param args: Arguments to pass to the callable. 
 :param kwargs: Keyword arguments to pass to the callable. 
 :returns: The value returned by the callable. 
 :raises: The exception class passed in the first argument. 
 :raises: An exception with the same message as the exception raised in 
 the callable. 
 :raises: An exception with the same traceback as the exception raised in 
 the callable. 
 :raises: An exception with the same stack level as the exception raised in 
 the callable. 
 :raises: An exception with the same frame as the exception raised in the 
 callable. 
 :raises: An exception with the same line number as the exception raised in 
 the callable. 
 :raises: An exception with the same file as the exception","'Fail unless an exception of class exception_class and with message that 
 matches expected_regexp is thrown by callable when invoked with arguments 
 args and keyword arguments kwargs. 
 Name of this function adheres to Python 3.2+ reference, but should work in 
 all versions down to 2.6. 
 Notes 
 .. versionadded:: 1.8.0'"
"def delete_files(processPath, notwantedFiles, result, force=False): 
    if ((not result.result) and force): 
      result.output += logHelper(u'Forcing   deletion   of   files,   even   though   last   result   was   not   success', sickrage.srCore.srLogger.DEBUG) 
   elif (not result.result): 
      return 
   for cur_file in notwantedFiles: 
      cur_file_path = os.path.join(processPath, cur_file) 
      if (not os.path.isfile(cur_file_path)): 
         continue 
      result.output += logHelper((u'Deleting   file   %s' % cur_file), sickrage.srCore.srLogger.DEBUG) 
      file_attribute = os.stat(cur_file_path)[0] 
      if (not (file_attribute & stat.S_IWRITE)): 
         result.output += logHelper((u'Changing   ReadOnly   Flag   for   file   %s' % cur_file), sickrage.srCore.srLogger.DEBUG) 
         try: 
            os.chmod(cur_file_path, stat.S_IWRITE) 
         except OSError as e: 
            result.output += logHelper((u'Cannot   change   permissions   of   %s:   %s' % (cur_file, str(e.strerror).decode(sickrage.SYS_ENCODING))), sickrage.srCore.srLogger.DEBUG) 
      try: 
         os.remove(cur_file_path) 
      except OSError as e: 
         result.output += logHelper((u'Unable   to   delete   file   %s:   %s' % (cur_file, str(e.strerror).decode(sickrage.SYS_ENCODING))), sickrage.srCore.srLogger.DEBUG)"," 'Delete files from the process\'s temp directory 
 :param processPath: Path to the process\'s temp directory 
 :param notwantedFiles: List of files to be deleted 
 :param result: Result object to append to 
 :param force: Whether or not to force the deletion of files, even if the last result was not successful 
 :return: None'","'Remove files from filesystem 
 :param processPath: path to process 
 :param notwantedFiles: files we do not want 
 :param result: Processor results 
 :param force: Boolean, force deletion, defaults to false'"
"def confirmation_option(*param_decls, **attrs): 
    def decorator(f): 
      def callback(ctx, param, value): 
         if (not value): 
            ctx.abort() 
      attrs.setdefault('is_flag', True) 
      attrs.setdefault('callback', callback) 
      attrs.setdefault('expose_value', False) 
      attrs.setdefault('prompt', 'Do   you   want   to   continue?') 
      attrs.setdefault('help', 'Confirm   the   action   without   prompting.') 
      return option(*(param_decls or ('--yes',)), **attrs)(f) 
   return decorator", 'Decorator for a command option that confirms the action without prompting.',"'Shortcut for confirmation prompts that can be ignored by passing 
 ``--yes`` as parameter. 
 This is equivalent to decorating a function with :func:`option` with 
 the following parameters:: 
 def callback(ctx, param, value): 
 if not value: 
 ctx.abort() 
 @click.command() 
 @click.option(\'--yes\', is_flag=True, callback=callback, 
 expose_value=False, prompt=\'Do you want to continue?\') 
 def dropdb(): 
 pass'"
"@inspect_command(alias=u'dump_schedule') 
 def scheduled(state, **kwargs): 
    return list(_iter_schedule_requests(state.consumer.timer))", 'List scheduled tasks.','List of currently scheduled ETA/countdown tasks.'
"def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None, input_data=None, expected_output=None, expected_output_dtype=None, fixed_batch_size=False): 
    if (input_data is None): 
      assert input_shape 
      if (not input_dtype): 
         input_dtype = K.floatx() 
      input_data_shape = list(input_shape) 
      for (i, e) in enumerate(input_data_shape): 
         if (e is None): 
            input_data_shape[i] = np.random.randint(1, 4) 
      input_data = (10 * np.random.random(input_data_shape)) 
      input_data = input_data.astype(input_dtype) 
   elif (input_shape is None): 
      input_shape = input_data.shape 
   if (expected_output_dtype is None): 
      expected_output_dtype = input_dtype 
   layer = layer_cls(**kwargs) 
   weights = layer.get_weights() 
   layer.set_weights(weights) 
   if ('weights' in inspect.getargspec(layer_cls.__init__)): 
      kwargs['weights'] = weights 
      layer = layer_cls(**kwargs) 
   if fixed_batch_size: 
      x = Input(batch_shape=input_shape, dtype=input_dtype) 
   else: 
      x = Input(shape=input_shape[1:], dtype=input_dtype) 
   y = layer(x) 
   assert (K.dtype(y) == expected_output_dtype) 
   model = Model(input=x, output=y) 
   model.compile('rmsprop', 'mse') 
   expected_output_shape = layer.get_output_shape_for(input_shape) 
   actual_output = model.predict(input_data) 
   actual_output_shape = actual_output.shape 
   for (expected_dim, actual_dim) in zip(expected_output_shape, actual_output_shape): 
      if (expected_dim is not None): 
         assert (expected_dim == actual_dim) 
   if (expected_output is not None): 
      assert_allclose(actual_output, expected_output, rtol=0.001) 
   model_config = model.get_config() 
   model = Model.from_config(model_config) 
   model.compile('rmsprop', 'mse') 
   layer_config = layer.get_config() 
   layer_config['batch_input_shape'] = input_shape 
   layer = layer.__class__.from_config(layer_config) 
   model = Sequential() 
   model.add(layer) 
   model.compile('rmsprop', 'mse') 
   actual_output = model.predict(input_data) 
   actual_output_shape = actual_output.shape 
   for (expected_dim, actual_dim) in zip(expected_output_shape, actual_output_shape): 
      if (expected_dim is not None): 
         assert (expected_dim == actual_dim) 
   if (expected_output is not None): 
      assert_allclose(actual_output, expected_output, rtol=0.001) 
   json_model = model.to_json() 
   model = model_from_json(json_model) 
   return actual_output"," 'Layer test. 
 Parameters 
 layer_cls : class 
 The class of the layer to test. 
 kwargs : dict 
 Keyword arguments to pass to the layer\'s constructor. 
 input_shape : tuple 
 The shape of the input. 
 input_dtype : dtype 
 The dtype of the input. 
 input_data : ndarray 
 The data to pass to the layer. 
 expected_output : ndarray 
 The expected output. 
 expected_output_dtype : dtype 
 The dtype of the expected output. 
 fixed_batch_size : bool 
 If True, the batch size will be fixed to the input shape. 
 Returns 
 actual_output : ndarray 
 The output of the layer. 
 actual_output_shape : tuple 
 The shape of the actual output. 
 Raises 
 ValueError 
 If the expected output is not of the correct shape.'","'Test routine for a layer with a single input tensor 
 and single output tensor.'"
"@instrumented_task(name='sentry.tasks.post_process.plugin_post_process_group', stat_suffix=(lambda plugin_slug, *a, **k: plugin_slug)) 
 def plugin_post_process_group(plugin_slug, event, **kwargs): 
    Raven.tags_context({'project': event.project_id}) 
   plugin = plugins.get(plugin_slug) 
   safe_execute(plugin.post_process, event=event, group=event.group, **kwargs)"," 'Post-processes a group of events. 
 This is the default plugin post-processing task. 
 It is called after the group of events has been processed, but before 
 the group is sent to the backend. 
 :param plugin_slug: The slug of the plugin to run. 
 :param event: The event to post-process. 
 :param group: The group of events to post-process. 
 :param kwargs: Any additional keyword arguments passed to the plugin. 
 :returns: None.'",'Fires post processing hooks for a group.'
"def is_valid_asn(asn): 
    return (isinstance(asn, numbers.Integral) and (0 <= asn <= 4294967295))"," 'Checks if an ASN is valid. 
 :param asn: ASN to check. 
 :type asn: int 
 :returns: True if the ASN is valid, False otherwise. 
 :rtype: bool'",'Returns True if the given AS number is Two or Four Octet.'
"def bin(number): 
    tmp = [BIN_HEX_DICT[hstr] for hstr in hex(number)[2:]] 
   return BIN_ZSTRIP.sub('0b', ''.join(tmp))", 'Convert a number to binary string',"'Adapted from http://code.activestate.com/recipes/576847/ 
 :codeauthor: Vishal Sapre 
 :license: MIT 
 A foolishly simple look-up method of getting binary string from an integer 
 This happens to be faster than all other ways!!!'"
"def parseTargetDirect(): 
    if (not conf.direct): 
      return 
   details = None 
   remote = False 
   for dbms in SUPPORTED_DBMS: 
      details = re.search(('^(?P<dbms>%s)://(?P<credentials>(?P<user>.+?)\\:(?P<pass>.*)\\@)?(?P<remote>(?P<hostname>.+?)\\:(?P<port>[\\d]+)\\/)?(?P<db>[\\w\\d\\   \\:\\.\\_\\-\\/\\\\]+?)$' % dbms), conf.direct, re.I) 
      if details: 
         conf.dbms = details.group('dbms') 
         if details.group('credentials'): 
            conf.dbmsUser = details.group('user') 
            conf.dbmsPass = details.group('pass') 
         elif conf.dbmsCred: 
            (conf.dbmsUser, conf.dbmsPass) = conf.dbmsCred.split(':') 
         else: 
            conf.dbmsUser = unicode() 
            conf.dbmsPass = unicode() 
         if (not conf.dbmsPass): 
            conf.dbmsPass = None 
         if details.group('remote'): 
            remote = True 
            conf.hostname = details.group('hostname').strip() 
            conf.port = int(details.group('port')) 
         else: 
            conf.hostname = 'localhost' 
            conf.port = 0 
         conf.dbmsDb = details.group('db') 
         conf.parameters[None] = 'direct   connection' 
         break 
   if (not details): 
      errMsg = 'invalid   target   details,   valid   syntax   is   for   instance   ' 
      errMsg += ""'mysql://USER:PASSWORD@DBMS_IP:DBMS_PORT/DATABASE_NAME'   "" 
      errMsg += ""or   'access://DATABASE_FILEPATH'"" 
      raise SqlmapSyntaxException(errMsg) 
   for (dbmsName, data) in DBMS_DICT.items(): 
      if ((dbmsName == conf.dbms) or (conf.dbms.lower() in data[0])): 
         try: 
            if (dbmsName in (DBMS.ACCESS, DBMS.SQLITE, DBMS.FIREBIRD)): 
               if remote: 
                  warnMsg = 'direct   connection   over   the   network   for   ' 
                  warnMsg += ('%s   DBMS   is   not   supported' % dbmsName) 
                  logger.warn(warnMsg) 
                  conf.hostname = 'localhost' 
                  conf.port = 0 
            elif (not remote): 
               errMsg = 'missing   remote   connection   details   (e.g.   ' 
               errMsg += ""'mysql://USER:PASSWORD@DBMS_IP:DBMS_PORT/DATABASE_NAME'   "" 
               errMsg += ""or   'access://DATABASE_FILEPATH')"" 
               raise SqlmapSyntaxException(errMsg) 
            if (dbmsName in (DBMS.MSSQL, DBMS.SYBASE)): 
               import _mssql 
               import pymssql 
               if ((not hasattr(pymssql, '__version__')) or (pymssql.__version__ < '1.0.2')): 
                  errMsg = (""'%s'   third-party   library   must   be   "" % data[1]) 
                  errMsg += 'version   >=   1.0.2   to   work   properly.   ' 
                  errMsg += (""Download   from   '%s'"" % data[2]) 
                  raise SqlmapMissingDependence(errMsg) 
            elif (dbmsName == DBMS.MYSQL): 
               import pymysql 
            elif (dbmsName == DBMS.PGSQL): 
               import psycopg2 
            elif (dbmsName == DBMS.ORACLE): 
               import cx_Oracle 
            elif (dbmsName == DBMS.SQLITE): 
               import sqlite3 
            elif (dbmsName == DBMS.ACCESS): 
               import pyodbc 
            elif (dbmsName == DBMS.FIREBIRD): 
               import kinterbasdb 
         except ImportError: 
            if (_sqlalchemy and (data[3] in _sqlalchemy.dialects.__all__)): 
               pass 
            else: 
               errMsg = (""sqlmap   requires   '%s'   third-party   library   "" % data[1]) 
               errMsg += 'in   order   to   directly   connect   to   the   DBMS   ' 
               errMsg += (""%s.   You   can   download   it   from   '%s'"" % (dbmsName, data[2])) 
               errMsg += "".   Alternative   is   to   use   a   package   'python-sqlalchemy'   "" 
               errMsg += (""with   support   for   dialect   '%s'   installed"" % data[3]) 
               raise SqlmapMissingDependence(errMsg)", 'Parses direct connection details from the config file','Parse target dbms and set some attributes into the configuration singleton.'
"def create_access_key(user_name, region=None, key=None, keyid=None, profile=None): 
    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) 
   try: 
      return conn.create_access_key(user_name) 
   except boto.exception.BotoServerError as e: 
      log.debug(e) 
      log.error('Failed   to   create   access   key.') 
      return str(e)"," 'Creates an access key for the given user. 
 :param user_name: Name of the user. 
 :type user_name: str 
 :param region: Region to use. 
 :type region: str 
 :param key: Key to use. 
 :type key: str 
 :param keyid: Key id to use. 
 :type keyid: str 
 :param profile: Profile to use. 
 :type profile: str 
 :return: Access key object. 
 :rtype: :class:`boto.ec2.access_key.AccessKey`'","'Create access key id for a user. 
 .. versionadded:: 2015.8.0 
 CLI Example: 
 .. code-block:: bash 
 salt myminion boto_iam.create_access_key myuser'"
"def _eintr_retry(func, *args): 
    while True: 
      try: 
         return func(*args) 
      except (OSError, select.error) as e: 
         if (e.args[0] != errno.EINTR): 
            raise", 'Wrapper around a function to retry on EINTR.','restart a system call interrupted by EINTR'
"def validate_color(s): 
    if (s.lower() == 'none'): 
      return 'None' 
   if is_color_like(s): 
      return s 
   stmp = ('#' + s) 
   if is_color_like(stmp): 
      return stmp 
   colorarg = s 
   msg = '' 
   if (s.find(',') >= 0): 
      stmp = ''.join([c for c in s if (c.isdigit() or (c == '.') or (c == ','))]) 
      vals = stmp.split(',') 
      if (len(vals) != 3): 
         msg = '\nColor   tuples   must   be   length   3' 
      else: 
         try: 
            colorarg = [float(val) for val in vals] 
         except ValueError: 
            msg = '\nCould   not   convert   all   entries   to   floats' 
   if ((not msg) and is_color_like(colorarg)): 
      return colorarg 
   raise ValueError(('%s   does   not   look   like   a   color   arg%s' % (s, msg)))"," 'Validate a color string. 
 If the string is a valid color, return it.  Otherwise, raise a ValueError.'",'return a valid color arg'
"def find_tests(testdir, prefixes=DEFAULT_PREFIXES, suffix='.py', excludes=(), remove_suffix=True): 
    tests = [] 
   for name in os.listdir(testdir): 
      if ((not suffix) or name.endswith(suffix)): 
         for prefix in prefixes: 
            if name.startswith(prefix): 
               if (remove_suffix and name.endswith(suffix)): 
                  name = name[:(- len(suffix))] 
               if (name not in excludes): 
                  tests.append(name) 
   tests.sort() 
   return tests"," 'Returns a list of all the tests in the given test directory. 
 If remove_suffix is True, the suffix will be removed from the names. 
 If prefixes is a list of prefixes, only tests with names that start with 
 one of the prefixes will be returned. 
 If excludes is a list of names, only tests that are not in that list will 
 be returned. 
 :param testdir: the directory containing the tests to find 
 :type testdir: str 
 :param prefixes: list of prefixes to match 
 :type prefixes: list 
 :param suffix: suffix to match 
 :type suffix: str 
 :param excludes: list of names to exclude 
 :type excludes: list 
 :param remove_suffix: remove suffix from names 
 :type remove_suffix: bool 
 :returns: list of names of the tests found'",'Return a list of all applicable test modules.'
"@lockutils.synchronized('storage-registry-lock', 'nova-', external=True) 
 def get_storage_users(storage_path): 
    d = {} 
   id_path = os.path.join(storage_path, 'compute_nodes') 
   if os.path.exists(id_path): 
      with open(id_path) as f: 
         d = json.loads(f.read()) 
   recent_users = [] 
   for node in d: 
      if ((time.time() - d[node]) < TWENTY_FOUR_HOURS): 
         recent_users.append(node) 
   return recent_users", 'Returns the list of compute nodes that have been recently used.','Get a list of all the users of this storage path.'
"def tob(data, enc='utf8'): 
    return (data.encode(enc) if isinstance(data, unicode) else bytes(data))", 'Encode a string to bytes.','Convert anything to bytes'
"@raise_if_offline 
 def schema_has_table(table_name): 
    bind = op.get_bind() 
   insp = sa.engine.reflection.Inspector.from_engine(bind) 
   return (table_name in insp.get_table_names())"," 'Check if a table exists in the current schema. 
 :param table_name: name of the table 
 :type table_name: str 
 :return: True if the table exists, False otherwise 
 :rtype: bool'","'Check whether the specified table exists in the current schema. 
 This method cannot be executed in offline mode.'"
"@event(u'manager.startup') 
 def init_parsers(manager): 
    for parser_type in PARSER_TYPES: 
      parsers[parser_type] = {} 
      for p in plugin.get_plugins(interface=(parser_type + u'_parser')): 
         parsers[parser_type][p.name.replace(u'parser_', u'')] = p.instance 
      func_name = (u'parse_' + parser_type) 
      default_parsers[parser_type] = max(iter(parsers[parser_type].items()), key=(lambda p: getattr(getattr(p[1], func_name), u'priority', 0)))[0] 
      log.debug((u'setting   default   %s   parser   to   %s.   (options:   %s)' % (parser_type, default_parsers[parser_type], parsers[parser_type])))"," 'Initialize the parsers. 
 This is called at startup. 
 :param manager: the manager instance 
 :type manager: Manager'",'Prepare our list of parsing plugins and default parsers.'
"def parseline(line): 
    fields = [] 
   (i, n) = (0, len(line)) 
   while (i < n): 
      (field, i) = parsefield(line, i, n) 
      fields.append(field) 
      i = (i + 1) 
   if (len(fields) < 2): 
      return (None, None) 
   (key, view, rest) = (fields[0], fields[1], fields[2:]) 
   fields = {'view': view} 
   for field in rest: 
      i = field.find('=') 
      if (i < 0): 
         fkey = field 
         fvalue = '' 
      else: 
         fkey = field[:i].strip() 
         fvalue = field[(i + 1):].strip() 
      if (fkey in fields): 
         pass 
      else: 
         fields[fkey] = fvalue 
   return (key, fields)"," 'Parse a line from the input file and return the key and the fields 
 dictionary. 
 The input line can be either a single key-value pair or a list of 
 key-value pairs. 
 :param line: input line 
 :return: (key, fields) tuple 
 :rtype: tuple'","'Parse one entry in a mailcap file and return a dictionary. 
 The viewing command is stored as the value with the key ""view"", 
 and the rest of the fields produce key-value pairs in the dict.'"
"def _qsturng(p, r, v): 
    global A, p_keys, v_keys 
   if ((p < 0.1) or (p > 0.999)): 
      raise ValueError('p   must   be   between   .1   and   .999') 
   if (p < 0.9): 
      if (v < 2): 
         raise ValueError('v   must   be   >   2   when   p   <   .9') 
   elif (v < 1): 
      raise ValueError('v   must   be   >   1   when   p   >=   .9') 
   p = float(p) 
   if isinstance(v, np.ndarray): 
      v = v.item() 
   if ((p, v) in A): 
      y = (_func(A[(p, v)], p, r, v) + 1.0) 
   elif ((p not in p_keys) and (v not in (v_keys + ([], [1])[(p >= 0.9)]))): 
      (v0, v1, v2) = _select_vs(v, p) 
      (p0, p1, p2) = _select_ps(p) 
      r0_sq = (_interpolate_p(p, r, v0) ** 2) 
      r1_sq = (_interpolate_p(p, r, v1) ** 2) 
      r2_sq = (_interpolate_p(p, r, v2) ** 2) 
      (v_, v0_, v1_, v2_) = ((1.0 / v), (1.0 / v0), (1.0 / v1), (1.0 / v2)) 
      d2 = ((2.0 * (((r2_sq - r1_sq) / (v2_ - v1_)) - ((r0_sq - r1_sq) / (v0_ - v1_)))) / (v2_ - v0_)) 
      if ((v2_ + v0_) >= (v1_ + v1_)): 
         d1 = (((r2_sq - r1_sq) / (v2_ - v1_)) - ((0.5 * d2) * (v2_ - v1_))) 
      else: 
         d1 = (((r1_sq - r0_sq) / (v1_ - v0_)) + ((0.5 * d2) * (v1_ - v0_))) 
      d0 = r1_sq 
      y = math.sqrt(((((d2 / 2.0) * ((v_ - v1_) ** 2.0)) + (d1 * (v_ - v1_))) + d0)) 
   elif (v not in (v_keys + ([], [1])[(p >= 0.9)])): 
      y = _interpolate_v(p, r, v) 
   elif (p not in p_keys): 
      y = _interpolate_p(p, r, v) 
   return ((math.sqrt(2) * (- y)) * scipy.stats.t.isf(((1.0 + p) / 2.0), (v, 1e+38)[(v > 1e+38)]))"," 'Returns the value of the t-distribution at p and v with df = r. 
 Parameters 
 p : float 
 The probability that the t-distribution is greater than the given 
 value. 
 r : float 
 The degrees of freedom of the t-distribution. 
 v : float 
 The value of the t-distribution. 
 Returns 
 y : float 
 The value of the t-distribution. 
 Examples 
 >>> from scipy.stats import t 
 >>> t.qsturng(0.9, 2, 1) 
 0.10665665665665667 
 >>> t.qsturng(0.9, 2, 2) 
 0.000000000000000001 
 >>> t.qsturng(0.9, 2, 3) 
 -0.000000000000000001 
 >>> t.qsturng(0.9, 2, ",'scalar version of qsturng'
"def delete(blob_key, **options): 
    fut = delete_async(blob_key, **options) 
   return fut.get_result()"," 'Deletes a blob. 
 :param blob_key: The key under which the blob is stored. 
 :type blob_key: str 
 :param options: Options for the request. 
 :type options: dict 
 :return: The response object. 
 :rtype: :class:`~azure.storage.blob.models.DeleteResponse`'","'Delete a blob from Blobstore. 
 Args: 
 blob_key: A blob key.  This may be a str, unicode or BlobKey instance. 
 **options: Options for create_rpc().'"
"def TRANGE(barDs, count): 
    return call_talib_with_hlc(barDs, count, talib.TRANGE)"," 'Returns the TRANGE(count) value for each bar in barDs. 
 The TRANGE(count) value is the difference between the highest high and 
 the lowest low for the last count bars. 
 Example: 
 >>> barDs = ta.bar_data(close, \'19990101\', \'20000101\') 
 >>> TRANGE(barDs, 10) 
 {19990101: 0, 19990111: 0, 19990201: 0, 19990211: 0, 19990301: 0, 19990311: 0, 19990401: 0, 19990411: 0, 19990501: 0, 19990511: 0, 19990601: 0, 1999061",'True Range'
"def valid_year(year): 
    return (1920 <= year < 2030)", 'Valid year must be between 1920 and 2030.','Check if number is a valid year'
"def orchestrate_high(data, test=None, queue=False, pillar=None, **kwargs): 
    if ((pillar is not None) and (not isinstance(pillar, dict))): 
      raise SaltInvocationError('Pillar   data   must   be   formatted   as   a   dictionary') 
   __opts__['file_client'] = 'local' 
   minion = salt.minion.MasterMinion(__opts__) 
   running = minion.functions['state.high'](data, test=None, queue=False, pillar=pillar, **kwargs) 
   ret = {minion.opts['id']: running} 
   __jid_event__.fire_event({'data': ret, 'outputter': 'highstate'}, 'progress') 
   return ret"," 'Orchestrate the high state of a minion. 
 This function orchestrates the high state of a minion. 
 It will run all the state functions that are specified in the ``data`` 
 dictionary. 
 :param data: 
 The dictionary of state functions to run. 
 :param test: 
 If ``True`` then the state functions will be run in test mode. 
 :param queue: 
 If ``True`` then the state functions will be run in queue mode. 
 :param pillar: 
 A dictionary of pillar data to use for state functions. 
 :param kwargs: 
 Additional arguments to pass to the state functions. 
 :return: 
 The dictionary of state results. 
 :rtype: dict'","'Execute a single state orchestration routine 
 .. versionadded:: 2015.5.0 
 CLI Example: 
 .. code-block:: bash 
 salt-run state.orchestrate_high \'{ 
 stage_one: 
 {salt.state: [{tgt: ""db*""}, {sls: postgres_setup}]}, 
 stage_two: 
 {salt.state: [{tgt: ""web*""}, {sls: apache_setup}, { 
 require: [{salt: stage_one}],'"
"def flatten(a): 
    if isinstance(a, (tuple, list, set)): 
      l = [] 
      for item in a: 
         l.extend(flatten(item)) 
      return l 
   else: 
      return [a]"," 'flatten a list of lists or tuples into a list of items 
 >>> flatten([[1,2,3], [4,5,6]]) 
 [1, 2, 3, 4, 5, 6] 
 >>> flatten([1,2,3,4,5,6]) 
 [1, 2, 3, 4, 5, 6] 
 >>> flatten([]) 
 []'","'Recursively flatten tuple, list and set in a list.'"
"def run(): 
    all_cats = Category.objects.filter(type=amo.ADDON_WEBAPP) 
   try: 
      entertainment = all_cats.filter(slug='entertainment-sports')[0] 
   except IndexError: 
      print 'Could   not   find   Category   with   slug=""entertainment-sports""' 
   else: 
      entertainment.name = 'Entertainment' 
      entertainment.slug = 'entertainment' 
      entertainment.save() 
      print 'Renamed   ""Entertainment   &   Sports""   to   ""Entertainment""' 
   Category.objects.create(type=amo.ADDON_WEBAPP, slug='sports', name='Sports') 
   print 'Created   ""Sports""' 
   try: 
      music = all_cats.filter(slug='music')[0] 
   except IndexError: 
      print 'Could   not   find   Category   with   slug=""music""' 
   else: 
      music.name = 'Music' 
      music.save() 
      print 'Renamed   ""Music   &   Audio""   to   ""Music""' 
   try: 
      social = all_cats.filter(slug='social')[0] 
   except IndexError: 
      print 'Could   not   find   Category   with   slug=""social""' 
   else: 
      social.name = 'Social' 
      social.save() 
      print 'Renamed   ""Social   &   Communication""   to   ""Social""' 
   try: 
      books = all_cats.filter(slug='books-reference')[0] 
   except IndexError: 
      print 'Could   not   find   Category   with   slug=""books-reference""' 
   else: 
      books.name = 'Books' 
      books.slug = 'books' 
      books.save() 
      print 'Renamed   ""Books   &   Reference""   to   ""Books""' 
   Category.objects.create(type=amo.ADDON_WEBAPP, slug='reference', name='Reference') 
   print 'Created   ""Reference""' 
   try: 
      photos = all_cats.filter(slug='photos-media')[0] 
   except IndexError: 
      print 'Could   not   find   Category   with   slug=""photos-media""' 
   else: 
      photos.name = 'Photo   &   Video' 
      photos.slug = 'photo-video' 
      photos.save() 
      print 'Renamed   ""Photos   &   Media""   to   ""Photo   &   Video""' 
   Category.objects.create(type=amo.ADDON_WEBAPP, slug='maps-navigation', name='Maps   &   Navigation') 
   print 'Created   ""Maps   &   Navigation""'", 'Renames the old categories to their new names.',"'We reorganized our categories: 
 https://bugzilla.mozilla.org/show_bug.cgi?id=854499 
 Usage:: 
 python -B manage.py runscript migrations.575-reorganize-cats'"
"def load_class(dotted_path): 
    dotted_path_split = dotted_path.split('.') 
   if (len(dotted_path_split) > 1): 
      klass_name = dotted_path_split[(-1)] 
      module_name = '.'.join(dotted_path_split[:(-1)]) 
      module = load_module(module_name) 
      if has_attribute(module, klass_name): 
         klass = getattr(module, klass_name) 
         return klass 
      else: 
         raise AttributeError(('Module   %s   does   not   have   class   attribute   %s' % (module_name, klass_name))) 
   else: 
      raise ValueError(('Dotted   module   path   %s   must   contain   a   module   name   and   a   classname' % dotted_path))"," 'Loads a class from a dotted path, e.g. 
 load_class(\'mypackage.myclass\') 
 load_class(\'mypackage.myclass.subclass\') 
 load_class(\'mypackage.myclass.subclass.subsubclass\') 
 load_class(\'mypackage.myclass.subclass.subsubclass.subsubsubclass\') 
 load_class(\'mypackage.myclass.subclass.subsubclass.subsubsubclass.subsubsubsubclass\') 
 load_class(\'mypackage.myclass.subclass.subsubclass.subsubsubclass.subsubsubsubclass.subsubsubsubsubclass\') 
 load_class(\'mypackage.myclass.subclass.subsubclass.subsubsubclass.subsubsubsubclass.subsubsubsubsubclass.subsubsubsubsubsubclass\') 
 load_class(\'mypackage.myclass.subclass.subsubclass.subsubsubclass.subsubsubsubclass.subsubsubsubsubclass.subsubsubsubsubsubclass","'Loads and returns a class definition provided a dotted path 
 specification the last part of the dotted path is the class name 
 and there is at least one module name preceding the class name. 
 Notes: 
 You will need to ensure that the module you are trying to load 
 exists in the Python path. 
 Examples: 
 - module.name.ClassName    # Provided module.name is in the Python path. 
 - module.ClassName         # Provided module is in the Python path. 
 What won\'t work: 
 - ClassName 
 - modle.name.ClassName     # Typo in module name. 
 - module.name.ClasNam      # Typo in classname.'"
"def resolve_link(stats): 
    return stats", 'Resolve the link for a stats object.','Passes through the projected Files/Dirs for link resolution.'
"def deconv(X, w, subsample=(1, 1), border_mode=(0, 0), conv_mode='conv'): 
    img = gpu_contiguous(X) 
   kerns = gpu_contiguous(w) 
   desc = GpuDnnConvDesc(border_mode=border_mode, subsample=subsample, conv_mode=conv_mode)(gpu_alloc_empty(img.shape[0], kerns.shape[1], (img.shape[2] * subsample[0]), (img.shape[3] * subsample[1])).shape, kerns.shape) 
   out = gpu_alloc_empty(img.shape[0], kerns.shape[1], (img.shape[2] * subsample[0]), (img.shape[3] * subsample[1])) 
   d_img = GpuDnnConvGradI()(kerns, img, out, desc) 
   return d_img"," 'Deconvolution 
 Parameters 
 X : (N, C, H, W) ndarray 
 Input image 
 w : (C, K, K) ndarray 
 Weights 
 subsample : (2,) tuple 
 Subsample size 
 border_mode : (2,) tuple 
 Border mode 
 conv_mode : \'conv\' or \'conv_direct\' 
 Returns 
 out : (N, C, H, W) ndarray 
 Deconvolved image'","'sets up dummy convolutional forward pass and uses its grad as deconv 
 currently only tested/working with same padding'"
"def raw_command(cmd, capture=False, env=None, data=None, cwd=None, explain=False, stdin=None, stdout=None): 
    if (not cwd): 
      cwd = os.getcwd() 
   if (not env): 
      env = common_environment() 
   cmd = list(cmd) 
   escaped_cmd = '   '.join((pipes.quote(c) for c in cmd)) 
   display.info(('Run   command:   %s' % escaped_cmd), verbosity=1) 
   display.info(('Working   directory:   %s' % cwd), verbosity=2) 
   program = find_executable(cmd[0], cwd=cwd, path=env['PATH'], required='warning') 
   if program: 
      display.info(('Program   found:   %s' % program), verbosity=2) 
   for key in sorted(env.keys()): 
      display.info(('%s=%s' % (key, env[key])), verbosity=2) 
   if explain: 
      return (None, None) 
   communicate = False 
   if (stdin is not None): 
      data = None 
      communicate = True 
   elif (data is not None): 
      stdin = subprocess.PIPE 
      communicate = True 
   if stdout: 
      communicate = True 
   if capture: 
      stdout = (stdout or subprocess.PIPE) 
      stderr = subprocess.PIPE 
      communicate = True 
   else: 
      stderr = None 
   start = time.time() 
   try: 
      process = subprocess.Popen(cmd, env=env, stdin=stdin, stdout=stdout, stderr=stderr, cwd=cwd) 
   except OSError as ex: 
      if (ex.errno == errno.ENOENT): 
         raise ApplicationError(('Required   program   ""%s""   not   found.' % cmd[0])) 
      raise 
   if communicate: 
      (stdout, stderr) = process.communicate(data) 
   else: 
      process.wait() 
      (stdout, stderr) = (None, None) 
   status = process.returncode 
   runtime = (time.time() - start) 
   display.info(('Command   exited   with   status   %s   after   %s   seconds.' % (status, runtime)), verbosity=4) 
   if (status == 0): 
      return (stdout, stderr) 
   raise SubprocessError(cmd, status, stdout, stderr, runtime)"," 'Run a command. 
 :param cmd: 
 :param capture: 
 :param env: 
 :param data: 
 :param cwd: 
 :param explain: 
 :param stdin: 
 :param stdout: 
 :return: 
 :raises: 
 :rtype: 
 :see: subprocess'","':type cmd: collections.Iterable[str] 
 :type capture: bool 
 :type env: dict[str, str] | None 
 :type data: str | None 
 :type cwd: str | None 
 :type explain: bool 
 :type stdin: file | None 
 :type stdout: file | None 
 :rtype: str | None, str | None'"
"def getAlterationLines(fileName): 
    return archive.getTextLines(getAlterationFile(fileName))"," 'Returns a list of lines in the alteration file. 
 :param fileName: the name of the file to get the alteration lines from 
 :type fileName: str 
 :return: a list of lines from the alteration file 
 :rtype: list'",'Get the text lines from the fileName in the alterations directories.'
"def isFileLocked(checkfile, writeLockCheck=False): 
    checkfile = ek(os.path.abspath, checkfile) 
   if (not ek(os.path.exists, checkfile)): 
      return True 
   try: 
      f = ek(io.open, checkfile, u'rb') 
      f.close() 
   except IOError: 
      return True 
   if writeLockCheck: 
      lockFile = (checkfile + u'.lckchk') 
      if ek(os.path.exists, lockFile): 
         ek(os.remove, lockFile) 
      try: 
         ek(os.rename, checkfile, lockFile) 
         time.sleep(1) 
         ek(os.rename, lockFile, checkfile) 
      except (OSError, IOError): 
         return True 
   return False"," 'Check if a file is locked. 
 If writeLockCheck is True, the lock is checked by renaming the file to a temp 
 file and back. 
 :param checkfile: the file to check 
 :type checkfile: str 
 :param writeLockCheck: if True, a write lock check is performed 
 :type writeLockCheck: bool 
 :return: True if the file is locked, False otherwise'","'Checks to see if a file is locked. Performs three checks 
 1. Checks if the file even exists 
 2. Attempts to open the file for reading. This will determine if the file has a write lock. 
 Write locks occur when the file is being edited or copied to, e.g. a file copy destination 
 3. If the readLockCheck parameter is True, attempts to rename the file. If this fails the 
 file is open by some other process for reading. The file can be read, but not written to 
 or deleted. 
 :param file: the file being checked 
 :param writeLockCheck: when true will check if the file is locked for writing (prevents move operations)'"
"def get_era(): 
    return UUID(hex=_BOOT_ID.getContent().strip())"," 'Returns the era of the system. 
 :returns: the era of the system 
 :rtype: str'",':return UUID: A node- and boot-specific globally unique id.'
"def build_pdf(branch): 
    os.chdir(os.path.join(gitdname, 'statsmodels', 'docs')) 
   sphinx_dir = os.path.join(virtual_dir, 'bin') 
   retcode = subprocess.call('   '.join(['make', 'latexpdf', (('SPHINXBUILD=' + sphinx_dir) + '/sphinx-build')]), shell=True) 
   if (retcode != 0): 
      msg = ('Could   not   build   the   pdf   docs   for   branch   %s' % branch) 
      raise Exception(msg) 
   os.chdir(dname)"," 'Build the pdf docs for the given branch. 
 Parameters 
 branch : str 
 The name of the branch to build docs for. 
 Returns 
 None'","'Changes into new_branch_dir and builds the docs using sphinx in the 
 BUILDENV virtualenv'"
"def attach_ordered_steps(workflow, steps): 
    ordered_steps = order_workflow_steps(steps) 
   workflow.has_cycles = (not bool(ordered_steps)) 
   for (i, step) in enumerate((ordered_steps or steps)): 
      step.order_index = i 
      workflow.steps.append(step)"," 'Attach ordered steps to a workflow. 
 :param workflow: The workflow. 
 :param steps: The steps to attach. 
 :type steps: list 
 :return: The workflow with the steps attached. 
 :rtype: :class:`Workflow` 
 :raises: ValueError if the workflow already has ordered steps.'","'Attempt to topologically order steps and attach to workflow. If this 
 fails - the workflow contains cycles so it mark it as such.'"
"def check_message(keywords, message): 
    (exc_type, exc_value, exc_traceback) = sys.exc_info() 
   if set(str(exc_value).split('   ')).issuperset(set(keywords)): 
      exc_value._safe_message = message 
      raise"," 'Check if a message is in the given keywords, and if so, add it to the 
 exception object.'","'Checks an exception for given keywords and raises a new ``ActionError`` 
 with the desired message if the keywords are found. This allows selective 
 control over API error messages.'"
"def _fix_global_ids(html): 
    html = re.sub('id=""\\d+""', 'id=""###""', html) 
   global_id = 1 
   while (len(re.findall('id=""###""', html)) > 0): 
      html = re.sub('id=""###""', ('id=""%s""' % global_id), html, count=1) 
      global_id += 1 
   return html", 'Fixes all ids in the HTML so that they are unique.','Fix the global_ids after reordering in _render_toc().'
"def test_scenario_has_name(): 
    scenario = Scenario.from_string(SCENARIO1) 
   assert isinstance(scenario, Scenario) 
   assert_equals(scenario.name, 'Adding   some   students   to   my   university   database')", 'Test that a scenario has a name','It should extract the name of the scenario'
"def volumedriver(cls): 
    _volume_register.append(cls) 
   return cls"," 'Register a volume driver. 
 This function is used to register a new volume driver. 
 It is used to register drivers that are not part of the 
 ``glance`` library. 
 :param cls: The volume driver to register.'",'Decorator for concrete volume driver implementations.'
"def _matchingString(constantString, inputString): 
    if isinstance(constantString, bytes): 
      otherType = constantString.decode('ascii') 
   else: 
      otherType = constantString.encode('ascii') 
   if (type(constantString) == type(inputString)): 
      return constantString 
   else: 
      return otherType"," 'Matches a string constant with the input string. 
 :param constantString: The string constant. 
 :type constantString: str 
 :param inputString: The input string. 
 :type inputString: str 
 :return: The matched string. 
 :rtype: str'","'Some functions, such as C{os.path.join}, operate on string arguments which 
 may be bytes or text, and wish to return a value of the same type.  In 
 those cases you may wish to have a string constant (in the case of 
 C{os.path.join}, that constant would be C{os.path.sep}) involved in the 
 parsing or processing, that must be of a matching type in order to use 
 string operations on it.  L{_matchingString} will take a constant string 
 (either L{bytes} or L{unicode}) and convert it to the same type as the 
 input string.  C{constantString} should contain only characters from ASCII; 
 to ensure this, it will be encoded or decoded regardless. 
 @param constantString: A string literal used in processing. 
 @type constantString: L{unicode} or L{bytes} 
 @param inputString: A byte string or text string provided by the user. 
 @type inputString: L{unicode} or L{bytes} 
 @return: C{constantString} converted into the same type as C{inputString} 
 @rtype: the type of C{inputString}'"
"def make_tarball(base_name, base_dir, compress='gzip', verbose=0, dry_run=0, owner=None, group=None): 
    tar_compression = {'gzip': 'gz', 'bzip2': 'bz2', None: '', 'compress': ''} 
   compress_ext = {'gzip': '.gz', 'bzip2': '.bz2', 'compress': '.Z'} 
   if ((compress is not None) and (compress not in compress_ext.keys())): 
      raise ValueError, ""bad   value   for   'compress':   must   be   None,   'gzip',   'bzip2'   or   'compress'"" 
   archive_name = (base_name + '.tar') 
   if (compress != 'compress'): 
      archive_name += compress_ext.get(compress, '') 
   mkpath(os.path.dirname(archive_name), dry_run=dry_run) 
   import tarfile 
   log.info('Creating   tar   archive') 
   uid = _get_uid(owner) 
   gid = _get_gid(group) 
   def _set_uid_gid(tarinfo): 
      if (gid is not None): 
         tarinfo.gid = gid 
         tarinfo.gname = group 
      if (uid is not None): 
         tarinfo.uid = uid 
         tarinfo.uname = owner 
      return tarinfo 
   if (not dry_run): 
      tar = tarfile.open(archive_name, ('w|%s' % tar_compression[compress])) 
      try: 
         tar.add(base_dir, filter=_set_uid_gid) 
      finally: 
         tar.close() 
   if (compress == 'compress'): 
      warn(""'compress'   will   be   deprecated."", PendingDeprecationWarning) 
      compressed_name = (archive_name + compress_ext[compress]) 
      if (sys.platform == 'win32'): 
         cmd = [compress, archive_name, compressed_name] 
      else: 
         cmd = [compress, '-f', archive_name] 
      spawn(cmd, dry_run=dry_run) 
      return compressed_name 
   return archive_name"," 'Make a tarball of the given directory. 
 :param base_name: The name of the tarball. 
 :param base_dir: The directory to make the tarball from. 
 :param compress: The compression method to use. 
 :param verbose: If 1, print the command line used to make the tarball. 
 :param dry_run: If 1, don\'t actually make the tarball. 
 :param owner: The owner of the tarball. 
 :param group: The group of the tarball. 
 :returns: The name of the tarball.'","'Create a (possibly compressed) tar file from all the files under 
 \'base_dir\'. 
 \'compress\' must be ""gzip"" (the default), ""compress"", ""bzip2"", or None. 
 (compress will be deprecated in Python 3.2) 
 \'owner\' and \'group\' can be used to define an owner and a group for the 
 archive that is being built. If not provided, the current owner and group 
 will be used. 
 The output tar file will be named \'base_dir\' +  "".tar"", possibly plus 
 the appropriate compression extension ("".gz"", "".bz2"" or "".Z""). 
 Returns the output filename.'"
"def select(rlist, wlist, xlist, timeout=None): 
    allevents = [] 
   timeout = Timeout.start_new(timeout) 
   result = SelectResult() 
   try: 
      try: 
         for readfd in rlist: 
            allevents.append(core.read_event(get_fileno(readfd), result.update, arg=readfd)) 
         for writefd in wlist: 
            allevents.append(core.write_event(get_fileno(writefd), result.update, arg=writefd)) 
      except IOError as ex: 
         raise error(*ex.args) 
      result.event.wait(timeout=timeout) 
      return (result.read, result.write, []) 
   finally: 
      for evt in allevents: 
         evt.cancel() 
      timeout.cancel()"," 'Select on read, write and exception events. 
 :param rlist: A list of file descriptors to read on 
 :param wlist: A list of file descriptors to write on 
 :param xlist: A list of file descriptors to wait for exceptions on 
 :param timeout: The timeout for the operation 
 :return: The read, write and exception events that occurred'","'An implementation of :meth:`select.select` that blocks only the current greenlet. 
 Note: *xlist* is ignored.'"
"def quote_chinese(url, encodeing='utf-8'): 
    if isinstance(url, six.text_type): 
      return quote_chinese(url.encode(encodeing)) 
   if six.PY3: 
      res = [(six.int2byte(b).decode('latin-1') if (b < 128) else ('%%%02X' % b)) for b in url] 
   else: 
      res = [(b if (ord(b) < 128) else ('%%%02X' % ord(b))) for b in url] 
   return ''.join(res)"," 'Quote a URL that contains Chinese characters. 
 :param url: The URL to be quoted 
 :param encodeing: The encoding to use when encoding the URL. 
 :return: The quoted URL'",'Quote non-ascii characters'
"@task(base=BaseInstructorTask, routing_key=settings.GRADES_DOWNLOAD_ROUTING_KEY) 
 def generate_certificates(entry_id, xmodule_instance_args): 
    action_name = ugettext_noop('certificates   generated') 
   TASK_LOG.info(u'Task:   %s,   InstructorTask   ID:   %s,   Task   type:   %s,   Preparing   for   task   execution', xmodule_instance_args.get('task_id'), entry_id, action_name) 
   task_fn = partial(generate_students_certificates, xmodule_instance_args) 
   return run_main_task(entry_id, task_fn, action_name)", 'Generate student certificates for the course.','Grade students and generate certificates.'
"@gzip_page 
 @cache_control(max_age=settings.CACHE_MIDDLEWARE_SECONDS) 
 def commonplace(request, repo, **kwargs): 
    if (repo not in settings.FRONTEND_REPOS): 
      raise Http404 
   BUILD_ID = get_build_id(repo) 
   ua = request.META.get('HTTP_USER_AGENT', '').lower() 
   include_splash = False 
   detect_region_with_geoip = False 
   if (repo == 'fireplace'): 
      include_splash = True 
      has_sim_info_in_query = (('mccs' in request.GET) or (('mcc' in request.GET) and ('mnc' in request.GET))) 
      if (not has_sim_info_in_query): 
         detect_region_with_geoip = True 
   (fxa_auth_state, fxa_auth_url) = fxa_auth_info() 
   site_settings = {'dev_pay_providers': settings.DEV_PAY_PROVIDERS, 'fxa_auth_state': fxa_auth_state, 'fxa_auth_url': fxa_auth_url} 
   ctx = {'BUILD_ID': BUILD_ID, 'LANG': request.LANG, 'langdir': lang_dir(request.LANG), 'include_splash': include_splash, 'repo': repo, 'robots': ('googlebot' in ua), 'site_settings': site_settings, 'newrelic_header': newrelic.agent.get_browser_timing_header, 'newrelic_footer': newrelic.agent.get_browser_timing_footer} 
   if (repo == 'fireplace'): 
      resolved_url = resolve(request.path) 
      if (resolved_url.url_name == 'detail'): 
         ctx = add_app_ctx(ctx, resolved_url.kwargs['app_slug']) 
   ctx['waffle_switches'] = list(waffle.models.Switch.objects.filter(active=True).values_list('name', flat=True)) 
   media_url = urlparse(settings.MEDIA_URL) 
   if media_url.netloc: 
      ctx['media_origin'] = ((media_url.scheme + '://') + media_url.netloc) 
   if detect_region_with_geoip: 
      region_middleware = RegionMiddleware() 
      ctx['geoip_region'] = region_middleware.region_from_request(request) 
   if (repo == 'marketplace-tv-front-end'): 
      return render(request, 'commonplace/index_tv.html', ctx) 
   elif (repo in settings.REACT_REPOS): 
      return render(request, 'commonplace/index_react.html', ctx) 
   elif (repo in settings.COMMONPLACE_REPOS): 
      return render(request, 'commonplace/index.html', ctx)", 'Commonplace index page.','Serves the frontend single-page apps.'
"def _find_image_files(data_dir, labels_file): 
    print(('Determining   list   of   input   files   and   labels   from   %s.' % data_dir)) 
   challenge_synsets = [l.strip() for l in tf.gfile.FastGFile(labels_file, 'r').readlines()] 
   labels = [] 
   filenames = [] 
   synsets = [] 
   label_index = 1 
   for synset in challenge_synsets: 
      jpeg_file_path = ('%s/%s/*.JPEG' % (data_dir, synset)) 
      matching_files = tf.gfile.Glob(jpeg_file_path) 
      labels.extend(([label_index] * len(matching_files))) 
      synsets.extend(([synset] * len(matching_files))) 
      filenames.extend(matching_files) 
      if (not (label_index % 100)): 
         print(('Finished   finding   files   in   %d   of   %d   classes.' % (label_index, len(challenge_synsets)))) 
      label_index += 1 
   shuffled_index = list(range(len(filenames))) 
   random.seed(12345) 
   random.shuffle(shuffled_index) 
   filenames = [filenames[i] for i in shuffled_index] 
   synsets = [synsets[i] for i in shuffled_index] 
   labels = [labels[i] for i in shuffled_index] 
   print(('Found   %d   JPEG   files   across   %d   labels   inside   %s.' % (len(filenames), len(challenge_synsets), data_dir))) 
   return (filenames, synsets, labels)"," 'Returns the filenames and labels for the input data. 
 This function uses the labels file to determine the filenames and labels 
 for the input data. 
 Args: 
 data_dir: Path to the directory containing the input data. 
 labels_file: Path to the labels file. 
 Returns: 
 filenames: List of filenames of the input images. 
 synsets: List of synsets for the input images. 
 labels: List of labels for the input images. 
 Examples: 
 >>> data_dir = \'/tmp/data\' 
 >>> labels_file = \'/tmp/labels.txt\' 
 >>> filenames, synsets, labels = _find_image_files(data_dir, labels_file) 
 >>> print(filenames) 
 [/tmp/data/00000000000000000000000000000000000000000000000000000000000000","'Build a list of all images files and labels in the data set. 
 Args: 
 data_dir: string, path to the root directory of images. 
 Assumes that the ImageNet data set resides in JPEG files located in 
 the following directory structure. 
 data_dir/n01440764/ILSVRC2012_val_00000293.JPEG 
 data_dir/n01440764/ILSVRC2012_val_00000543.JPEG 
 where \'n01440764\' is the unique synset label associated with these images. 
 labels_file: string, path to the labels file. 
 The list of valid labels are held in this file. Assumes that the file 
 contains entries as such: 
 n01440764 
 n01443537 
 n01484850 
 where each line corresponds to a label expressed as a synset. We map 
 each synset contained in the file to an integer (based on the alphabetical 
 ordering) starting with the integer 1 corresponding to the synset 
 contained in the first line. 
 The reason we start the integer labels at 1 is to reserve label 0 as an 
 unused background class. 
 Returns: 
 filenames: list of strings; each string is a path to an image file. 
 synsets: list of strings; each string is a unique WordNet ID. 
 labels: list of integer; each integer identifies the ground truth.'"
"def order_by_precedence(media_type_lst): 
    ret = [set(), set(), set(), set()] 
   for media_type in media_type_lst: 
      precedence = _MediaType(media_type).precedence 
      ret[(3 - precedence)].add(media_type) 
   return [media_types for media_types in ret if media_types]"," 'Returns a list of media types sorted by precedence. 
 The order of precedence is: 
 - 0: text/plain 
 - 1: text/html 
 - 2: application/xml 
 - 3: application/json'","'Returns a list of sets of media type strings, ordered by precedence. 
 Precedence is determined by how specific a media type is: 
 3. \'type/subtype; param=val\' 
 2. \'type/subtype\' 
 1. \'type/*\' 
 0. \'*/*\''"
"def filter_label_1(context, label): 
    return False", 'Filter the first label of a context.','Test Filter Label 1'
"def run(*arg, **kw): 
    kw['exit'] = False 
   return TestProgram(*arg, **kw).success"," 'Runs the test suite, returning True if it succeeds.'","'Collect and run tests, returning success or failure. 
 The arguments to `run()` are the same as to `main()`: 
 * module: All tests are in this module (default: None) 
 * defaultTest: Tests to load (default: \'.\') 
 * argv: Command line arguments (default: None; sys.argv is read) 
 * testRunner: Test runner instance (default: None) 
 * testLoader: Test loader instance (default: None) 
 * env: Environment; ignored if config is provided (default: None; 
 os.environ is read) 
 * config: :class:`nose.config.Config` instance (default: None) 
 * suite: Suite or list of tests to run (default: None). Passing a 
 suite or lists of tests will bypass all test discovery and 
 loading. *ALSO NOTE* that if you pass a unittest.TestSuite 
 instance as the suite, context fixtures at the class, module and 
 package level will not be used, and many plugin hooks will not 
 be called. If you want normal nose behavior, either pass a list 
 of tests, or a fully-configured :class:`nose.suite.ContextSuite`. 
 * plugins: List of plugins to use; ignored if config is provided 
 (default: load plugins with DefaultPluginManager) 
 * addplugins: List of **extra** plugins to use. Pass a list of plugin 
 instances in this argument to make custom plugins available while 
 still using the DefaultPluginManager. 
 With the exception that the ``exit`` argument is always set 
 to False.'"
"def assert_samelines(testcase, text1, text2, msg=None): 
    testcase.assertEqual(text1.splitlines(), text2.splitlines(), msg)"," 'Assert that two strings are equal, ignoring line endings. 
 :param testcase: The testcase to use. 
 :param text1: The first string to compare. 
 :param text2: The second string to compare. 
 :param msg: A message to use if an exception is raised.'","'Asserts text1 and text2 have the same lines, ignoring differences in 
 line endings between platforms'"
"def main(*args): 
    if (not args): 
      args = sys.argv[1:] 
   logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='%(message)s') 
   for gt in args: 
      load_grammar(gt, save=True, force=True) 
   return True"," 'Main function to run the grammar loader. 
 :param args: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype","'Main program, when run as a script: produce grammar pickle files. 
 Calls load_grammar for each argument, a path to a grammar text file.'"
"def getProcessOutput(executable, args=(), env={}, path=None, reactor=None, errortoo=0): 
    return _callProtocolWithDeferred((lambda d: _BackRelay(d, errortoo=errortoo)), executable, args, env, path, reactor)"," 'Returns the process output as a string. 
 The process is executed in a separate thread. 
 The process will be killed if the thread is interrupted. 
 This method will not block the calling thread. 
 Arguments: 
 executable(str): the executable to be executed. 
 args(list): the arguments to be passed to the executable. 
 env(dict): the environment to be used. 
 path(str): the path to be used to find the executable. 
 reactor(Reactor): the reactor to be used. 
 errortoo(int): if the process is killed, the error code is returned as well. 
 Returns: 
 the process output as a string. 
 Examples: 
 >>> from twisted.internet import reactor 
 >>> from twisted.internet.process import getProcessOutput 
 >>> def printHello(output): 
 ...     print output 
 >>> reactor.callLater(0, printHello, ""Hello"") 
 >>> print ""Hello"" 
 Hello 
 >>> print getProcessOutput(\'/bin/echo\', \'Hello\') 
 Hello 
 >>>","'Spawn a process and return its output as a deferred returning a string. 
 @param executable: The file name to run and get the output of - the 
 full path should be used. 
 @param args: the command line arguments to pass to the process; a 
 sequence of strings. The first string should *NOT* be the 
 executable\'s name. 
 @param env: the environment variables to pass to the processs; a 
 dictionary of strings. 
 @param path: the path to run the subprocess in - defaults to the 
 current directory. 
 @param reactor: the reactor to use - defaults to the default reactor 
 @param errortoo: If true, include stderr in the result.  If false, if 
 stderr is received the returned L{Deferred} will errback with an 
 L{IOError} instance with a C{processEnded} attribute.  The 
 C{processEnded} attribute refers to a L{Deferred} which fires when the 
 executed process ends.'"
"def is_sorted(exp): 
    return _contains(exp, Sorted)", 'Returns true if the expression is a sorted expression.','Does `exp` contain a `Reduced` node.'
"def extra_job_filters(not_yet_run=False, running=False, finished=False): 
    assert (not ((not_yet_run and running) or (not_yet_run and finished) or (running and finished))), 'Cannot   specify   more   than   one   filter   to   this   function' 
   not_queued = ('(SELECT   job_id   FROM   afe_host_queue_entries   WHERE   status   !=   ""%s"")' % models.HostQueueEntry.Status.QUEUED) 
   not_finished = '(SELECT   job_id   FROM   afe_host_queue_entries   WHERE   not   complete)' 
   if not_yet_run: 
      where = [('id   NOT   IN   ' + not_queued)] 
   elif running: 
      where = [('(id   IN   %s)   AND   (id   IN   %s)' % (not_queued, not_finished))] 
   elif finished: 
      where = [('id   NOT   IN   ' + not_finished)] 
   else: 
      return {} 
   return {'where': where}"," 'Returns a dict of filters to apply to the job list. 
 :param not_yet_run: If True, only jobs that have not yet run are returned. 
 :param running: If True, only jobs that are currently running are returned. 
 :param finished: If True, only jobs that have finished are returned. 
 :returns: A dict of filters to apply to the job list. 
 :rtype: dict'","'Generate a SQL WHERE clause for job status filtering, and return it in 
 a dict of keyword args to pass to query.extra().  No more than one of 
 the parameters should be passed as True. 
 * not_yet_run: all HQEs are Queued 
 * finished: all HQEs are complete 
 * running: everything else'"
"@contextmanager 
 def _latextools_module_hack(): 
    plugins_whitelist = get_setting('plugins_whitelist', ['external', 'getTeXRoot', 'latextools_utils']) 
   plugins_whitelist.append('latextools_plugin') 
   overwritten_modules = {} 
   whitelist = [(name, None) for name in plugins_whitelist] 
   whitelist.extend(internal._WHITELIST_ADDED) 
   __dir__ = os.path.dirname(__file__) 
   if (__dir__ == '.'): 
      __dir__ = os.path.join(sublime.packages_path(), 'LaTeXTools') 
   sys.path.insert(0, __dir__) 
   for (name, module) in whitelist: 
      if callable(module): 
         module = module() 
      if (name in sys.modules): 
         overwritten_modules[name] = sys.modules[name] 
      if (module is None): 
         latextools_module_name = _get_sublime_module_name(__dir__, name) 
         if (latextools_module_name in sys.modules): 
            sys.modules[name] = sys.modules[latextools_module_name] 
         else: 
            try: 
               sys.modules[name] = _load_module(name, name, __dir__) 
            except ImportError: 
               print('An   error   occurred   while   trying   to   load   white-listed   module   {0}'.format(name)) 
               traceback.print_exc() 
      else: 
         sys.modules[name] = module 
   sys.path.pop(0) 
   (yield) 
   for module in plugins_whitelist: 
      if (_get_sublime_module_name(__dir__, module) != module): 
         del sys.modules[module] 
      if (module in overwritten_modules): 
         sys.modules[module] = overwritten_modules[module]"," 'Hack around the fact that sublime.load_package() doesn\'t support 
 module-level whitelisting. 
 This is a temporary hack, which will be removed once the package loading 
 system is fixed.'","'Context manager to ensure sys.modules has certain white-listed modules, 
 most especially latextools_plugins. This exposes some of the modules in 
 LaTeXTools to plugins. It is intended primarily to expose library-esque 
 functionality, such as the getTeXRoot module, but can be configured by 
 the user as-needed.'"
"def not_friends(user, other_user): 
    return all((not_the_same(friend, other_user) for friend in user['friends']))"," 'Return True if the given user is not friends with the other user. 
 This is a convenience function that returns True if the given user is not friends 
 with the other user. It is a shortcut for checking if the given user is friends 
 with any of the other user\'s friends. 
 :param user: A user object. 
 :param other_user: A user object. 
 :return: True if the given user is not friends with the other user, False 
 otherwise.'","'other_user is not a friend if he\'s not in user[""friends""]; 
 that is, if he\'s not_the_same as all the people in user[""friends""]'"
"def writeOutput(fileName=''): 
    fileName = fabmetheus_interpret.getFirstTranslatorFileNameUnmodified(fileName) 
   if (fileName != ''): 
      skeinforge_craft.writeChainTextWithNounMessage(fileName, 'splodge')"," 'Write the output file. 
 This function writes the output file to the specified file name. 
 This function does not write the output file if the file name is 
 not specified. 
 @param fileName: the name of the output file. 
 @type fileName: string 
 @return: the name of the output file. 
 @rtype: string'",'Splodge a gcode linear move file.'
"def main(): 
    module = AnsibleModule(argument_spec=dict(name=dict(type='str', required=True), new_name=dict(type='str'), config=dict(type='dict'), description=dict(type='str'), devices=dict(type='dict'), state=dict(choices=PROFILES_STATES, default='present'), url=dict(type='str', default='unix:/var/lib/lxd/unix.socket'), key_file=dict(type='str', default='{}/.config/lxc/client.key'.format(os.environ['HOME'])), cert_file=dict(type='str', default='{}/.config/lxc/client.crt'.format(os.environ['HOME'])), trust_password=dict(type='str')), supports_check_mode=False) 
   lxd_manage = LXDProfileManagement(module=module) 
   lxd_manage.run()", 'Manage LXD profiles.','Ansible Main module.'
"def _LC(f): 
    ring = f.ring 
   k = ring.ngens 
   yring = ring.clone(symbols=ring.symbols[(k - 1)]) 
   y = yring.gens[0] 
   degf = _deg(f) 
   lcf = yring.zero 
   for (monom, coeff) in f.iterterms(): 
      if (monom[:(-1)] == degf): 
         lcf += (coeff * (y ** monom[(-1)])) 
   return lcf"," 'Compute the leading coefficient of a polynomial in `ring`. 
 Examples 
 >>> from sympy.polys.ring import ZZ 
 >>> from sympy.polys.densebasic import _LC 
 >>> _LC(ZZ(2, 3, 4)) 
 2'","'Compute the leading coefficient of a multivariate polynomial 
 `f \in K[x_0, \ldots, x_{k-2}, y] \cong K[y][x_0, \ldots, x_{k-2}]`. 
 Parameters 
 f : PolyElement 
 polynomial in `K[x_0, \ldots, x_{k-2}, y]` 
 Returns 
 lcf : PolyElement 
 polynomial in `K[y]`, leading coefficient of `f` 
 Examples 
 >>> from sympy.polys.modulargcd import _LC 
 >>> from sympy.polys import ring, ZZ 
 >>> R, x, y = ring(""x, y"", ZZ) 
 >>> f = x**2*y**2 + x**2*y - 1 
 >>> _LC(f) 
 y**2 + y 
 >>> R, x, y, z = ring(""x, y, z"", ZZ) 
 >>> f = x**2*y**2 + x**2*y - 1 
 >>> _LC(f) 
 1 
 >>> f = x*y*z - y**2*z**2 
 >>> _LC(f) 
 z'"
"def initialize_plugin(pelican_obj): 
    if (_MAIN_SETTINGS is None): 
      initialize_dbs(pelican_obj.settings) 
      subscribe_filter_to_signals(pelican_obj.settings)", 'Initialize the plugin.','Initialize plugin variables and Pelican settings'
"def shutdown_datastore(): 
    logging.info('Shutting   down   Cassandra.') 
   monit_interface.stop(cassandra_interface.CASSANDRA_MONIT_WATCH_NAME, is_group=False) 
   logging.warning('Done!') 
   return True", 'Shutdown the Cassandra instance.',"'Top level function for bringing down Cassandra. 
 Returns: 
 True on success, False otherwise.'"
"def find_lemmata(tokens): 
    for token in tokens: 
      (word, pos, lemma) = (token[0], token[1], token[0]) 
      if pos.startswith(('DT',)): 
         lemma = singularize(word, pos='DT') 
      if pos.startswith('JJ'): 
         lemma = predicative(word) 
      if (pos == 'NNS'): 
         lemma = singularize(word) 
      if pos.startswith(('VB', 'MD')): 
         lemma = (conjugate(word, INFINITIVE) or word) 
      token.append(lemma.lower()) 
   return tokens"," 'Adds lemma information to each token. 
 :param tokens: list of (word, pos, lemma) tuples 
 :return: list of (word, pos, lemma) tuples'","'Annotates the tokens with lemmata for plural nouns and conjugated verbs, 
 where each token is a [word, part-of-speech] list.'"
"def test_raises_value_error_non_sym(): 
    P = np.array([[0.4, 0.6]]) 
   assert_raises(ValueError, MarkovChain, P) 
   assert_raises(ValueError, MarkovChain, sparse.csr_matrix(P))"," 'Test that ValueError is raised when MarkovChain is created with 
 non-symmetric matrix'",'Test with non symmetric input'
"def inherit_from_std_ex(node): 
    if ((node.name in ('Exception', 'BaseException')) and (node.root().name == EXCEPTIONS_MODULE)): 
      return True 
   return any((inherit_from_std_ex(parent) for parent in node.ancestors(recurs=False)))", 'Checks whether node is a standard exception.',"'Return true if the given class node is subclass of 
 exceptions.Exception.'"
"def build_arg_list(fn, env): 
    kw = {} 
   argspec = inspect.getargspec(fn) 
   if argspec[2]: 
      kw = env 
   else: 
      argnames = argspec[0][1:] 
      for name in argnames: 
         if (name in env): 
            kw[name] = env[name] 
   return kw"," 'Build a keyword argument list for the given function. 
 This is used to pass in environment variables to the function. 
 :param fn: The function to build the argument list for. 
 :param env: The environment to use to build the argument list. 
 :returns: A dictionary of keyword arguments to pass to the function. 
 :rtype: dict'","'given a fn and and environment the builds a keyword argument list 
 for fn'"
"def is_python_proxy(parameter): 
    try: 
      is_proxy = (len(parameter.Java.implements) > 0) 
   except Exception: 
      is_proxy = False 
   return is_proxy"," 'Returns True if the parameter is a python proxy, False otherwise.'","'Determines whether parameter is a Python Proxy, i.e., it has a Java 
 internal class with an `implements` member. 
 :param parameter: the object to check. 
 :rtype: True if the parameter is a Python Proxy'"
"def interpret_size(si): 
    m = re.match(u'\\s*(\\d+)\\s*([ac-z]?)(b?)\\s*$', str(si), re.I) 
   if m: 
      if ((not m.group(2)) and m.group(3)): 
         times = 1 
      else: 
         times = (const.SIPrefixTimes[m.group(2).upper()] if m.group(2) else 1) 
      return (int(m.group(1)) * times) 
   else: 
      raise ValueError"," 'Interpret a size in bytes as a SI prefixed value. 
 :param si: A string in the form \'1k\', \'2M\', \'3G\', \'4T\', \'5P\', \'6E\', 
 \'7Z\'. 
 :return: The corresponding value in bytes. 
 :raises ValueError: If the string does not match the expected format. 
 :rtype: int'","'>>> interpret_size(10) 
 10 
 >>> interpret_size(\'10\') 
 10 
 >>> interpret_size(\'10b\') 
 10 
 >>> interpret_size(\'10k\') 
 10240 
 >>> interpret_size(\'10K\') 
 10240 
 >>> interpret_size(\'10kb\') 
 10240 
 >>> interpret_size(\'10kB\') 
 10240 
 >>> interpret_size(\'a10\') 
 Traceback (most recent call last): 
 ValueError 
 >>> interpret_size(\'10a\') 
 Traceback (most recent call last): 
 KeyError: \'A\''"
"def main(): 
    salt_vars = get_salt_vars() 
   def salt_outputter(value): 
      ""\n                        Use   Salt's   outputters   to   print   values   to   the   shell\n                        "" 
      if (value is not None): 
         builtins._ = value 
         salt.output.display_output(value, '', salt_vars['__opts__']) 
   sys.displayhook = salt_outputter 
   readline.set_history_length(300) 
   if os.path.exists(HISTFILE): 
      readline.read_history_file(HISTFILE) 
   atexit.register(savehist) 
   atexit.register((lambda : sys.stdout.write('Salt   you   later!\n'))) 
   saltrepl = InteractiveConsole(locals=salt_vars) 
   saltrepl.interact(banner=__doc__)", 'Runs the interactive repl.','The main entry point'
"def node_degree_xy(G, x='out', y='in', weight=None, nodes=None): 
    if (nodes is None): 
      nodes = set(G) 
   else: 
      nodes = set(nodes) 
   xdeg = G.degree 
   ydeg = G.degree 
   if G.is_directed(): 
      direction = {'out': G.out_degree, 'in': G.in_degree} 
      xdeg = direction[x] 
      ydeg = direction[y] 
   for (u, degu) in xdeg(nodes, weight=weight): 
      neighbors = (nbr for (_, nbr) in G.edges(u) if (nbr in nodes)) 
      for (v, degv) in ydeg(neighbors, weight=weight): 
         (yield (degu, degv))"," 'Return a list of tuples of the form (degu, degv) where degu is the 
 out-degree of node u and degv is the in-degree of node v. 
 Parameters 
 G : NetworkX graph 
 The graph to analyze. 
 x : str 
 The string that specifies the name of the out-degree attribute. 
 y : str 
 The string that specifies the name of the in-degree attribute. 
 weight : function 
 A function that returns the weight of an edge. 
 nodes : set 
 A set of nodes to use in the computation. 
 Examples 
 >>> G = nx.path_graph(4) 
 >>> node_degree_xy(G) 
 [(0, 0), (0, 1), (0, 2), (0, 3), (1, 0), (1, 2), (2, 0), (2, 1), (2, 3), (3, 0), (3, 1), (3, 2)] 
 >>> G = nx.path_graph(","'Generate node degree-degree pairs for edges in G. 
 Parameters 
 G: NetworkX graph 
 x: string (\'in\',\'out\') 
 The degree type for source node (directed graphs only). 
 y: string (\'in\',\'out\') 
 The degree type for target node (directed graphs only). 
 weight: string or None, optional (default=None) 
 The edge attribute that holds the numerical value used 
 as a weight.  If None, then each edge has weight 1. 
 The degree is the sum of the edge weights adjacent to the node. 
 nodes: list or iterable (optional) 
 Use only edges that are adjacency to specified nodes. 
 The default is all nodes. 
 Returns 
 (x,y): 2-tuple 
 Generates 2-tuple of (degree,degree) values. 
 Examples 
 >>> G = nx.DiGraph() 
 >>> G.add_edge(1,2) 
 >>> list(nx.node_degree_xy(G,x=\'out\',y=\'in\')) 
 [(1, 1)] 
 >>> list(nx.node_degree_xy(G,x=\'in\',y=\'out\')) 
 [(0, 0)] 
 Notes 
 For undirected graphs each edge is produced twice, once for each edge 
 representation (u,v) and (v,u), with the exception of self-loop edges 
 which only appear once.'"
"def setup_platform(hass, config, add_devices, discovery_info=None): 
    if (discovery_info is None): 
      _LOGGER.error('Configure   Qwikswitch   component') 
      return False 
   add_devices(qwikswitch.QSUSB['switch']) 
   return True", 'Setup the Qwikswitch component.','Add switched from the main Qwikswitch component.'
"def get_asset_dir_prefix(): 
    asset_dir_prefix = '' 
   if (feconf.IS_MINIFIED or (not feconf.DEV_MODE)): 
      cache_slug = get_cache_slug() 
      asset_dir_prefix = ('/build/%s' % cache_slug) 
   return asset_dir_prefix"," 'Returns the absolute path to the asset directory. 
 This is the directory where all of the static assets are stored. 
 If the application is in dev mode, then the path is relative to the 
 directory where the source code is located.  Otherwise, it is relative 
 to the directory where the build is located. 
 @return: The absolute path to the asset directory.'","'Returns prefix for asset directory depending whether dev or prod. 
 It is used as a prefix in urls for images, css and script files.'"
"def register(dmm, typecls): 
    def wraps(fn): 
      dmm.register(typecls, fn) 
      return fn 
   return wraps"," 'Registers a function with the given type. 
 :param dmm: the dmm object 
 :param typecls: the type to register'","'Used as decorator to simplify datamodel registration. 
 Returns the object being decorated so that chaining is possible.'"
"def load_meta(prefix, dist): 
    return linked_data(prefix).get(dist)"," 'Load metadata for a distribution. 
 :param prefix: a prefix to use for the metadata 
 :param dist: the distribution to load metadata for'","'Return the install meta-data for a linked package in a prefix, or None 
 if the package is not linked in the prefix.'"
"def downgrade(migrate_engine): 
    dialect = migrate_engine.url.get_dialect() 
   if (dialect is not postgresql.dialect): 
      meta = MetaData(bind=migrate_engine) 
      for (table, column) in TABLE_COLUMNS: 
         t = Table(table, meta, autoload=True) 
         getattr(t.c, column).alter(type=String(43))", 'Downgrade the migration table to use VARCHAR(43) instead of VARCHAR(255).','Convert columns back to the larger String(43) defined in version 149.'
"def condense_zero_units(css): 
    return re.sub('([\\s:])(0)(px|em|%|in|cm|mm|pc|pt|ex)', '\\1\\2', css)", 'Condense zero-units into their base units.',"'Replace `0(px, em, %, etc)` with `0`.'"
"def get_file_for_svn_txn(repo_path, txn_id, filename): 
    return subproc_check_output(['svnlook', 'cat', repo_path, '-t', txn_id, filename])"," 'Return the contents of a file from an svn transaction. 
 :param repo_path: Path to the repository. 
 :param txn_id: The id of the transaction. 
 :param filename: The filename to retrieve. 
 :returns: The contents of the file.'","'Returns file in an svn transaction. 
 Helper function for hook scripts in the svn commit mission. 
 This function may be mocked in the tests.'"
"def GetReportByName(name): 
    report_class = REGISTRY.GetRegisteredPlugins()[name] 
   report_object = report_class() 
   return report_object"," 'Return a report object by name. 
 This is a convenience function that returns a report object by name. 
 :param name: The name of the report object to return 
 :type name: str 
 :return: A report object 
 :rtype: Report'","'Maps report plugin names to report objects. 
 Args: 
 name: The name of a plugin class. Also the name field of 
 ApiGetReportArgs and ApiReportDescriptor. 
 Returns: 
 Report plugin object of class corresponding to the given name.'"
"def save_load(jid, clear_load, minions=None, recurse_count=0): 
    if (recurse_count >= 5): 
      err = 'save_load   could   not   write   job   cache   file   after   {0}   retries.'.format(recurse_count) 
      log.error(err) 
      raise salt.exceptions.SaltCacheError(err) 
   jid_dir = salt.utils.jid.jid_dir(jid, _job_dir(), __opts__['hash_type']) 
   serial = salt.payload.Serial(__opts__) 
   try: 
      if (not os.path.exists(jid_dir)): 
         os.makedirs(jid_dir) 
   except OSError as exc: 
      if (exc.errno == errno.EEXIST): 
         pass 
      else: 
         raise 
   try: 
      serial.dump(clear_load, salt.utils.fopen(os.path.join(jid_dir, LOAD_P), 'w+b')) 
   except IOError as exc: 
      log.warning('Could   not   write   job   invocation   cache   file:   %s', exc) 
      time.sleep(0.1) 
      return save_load(jid=jid, clear_load=clear_load, recurse_count=(recurse_count + 1)) 
   if (('tgt' in clear_load) and (clear_load['tgt'] != '')): 
      if (minions is None): 
         ckminions = salt.utils.minions.CkMinions(__opts__) 
         minions = ckminions.check_minions(clear_load['tgt'], clear_load.get('tgt_type', 'glob')) 
      save_minions(jid, minions)"," 'Save the job cache to disk. 
 :param jid: the job ID 
 :param clear_load: the job invocation 
 :param minions: a list of minions to save to disk 
 :param recurse_count: the number of times we have tried to save the job cache 
 :return: None'","'Save the load to the specified jid 
 minions argument is to provide a pre-computed list of matched minions for 
 the job, for cases when this function can\'t compute that list itself (such 
 as for salt-ssh)'"
"def _get_service_user(user, service_username): 
    if ((not user) or user.is_anonymous()): 
      try: 
         user = User.objects.get(username=service_username) 
      except User.DoesNotExist: 
         user = None 
   return user"," 'Get the user for a service, if the service_username is set. 
 :param user: User object to use if not set. 
 :param service_username: Service username to use if not set. 
 :return: User object or None.'","'Retrieve and return the Catalog Integration Service User Object 
 if the passed user is None or anonymous'"
"@jit(nopython=True) 
 def _initialize_tableaux_ig(X, Y, tableaux, bases): 
    m = X.shape[0] 
   min_ = np.zeros(m) 
   for i in range(m): 
      for j in range((2 * m)): 
         if ((j == i) or (j == (i + m))): 
            tableaux[0][(i, j)] = 1 
         else: 
            tableaux[0][(i, j)] = 0 
      tableaux[0][(i, (2 * m))] = 1 
   for i in range(m): 
      for j in range(m): 
         if (j == i): 
            tableaux[1][(i, j)] = 1 
         else: 
            tableaux[1][(i, j)] = 0 
      for j in range(m): 
         d = (X[i] - Y[j]) 
         tableaux[1][(i, (m + j))] = (_square_sum(d) * (-1)) 
         if (tableaux[1][(i, (m + j))] < min_[j]): 
            min_[j] = tableaux[1][(i, (m + j))] 
      tableaux[1][(i, (2 * m))] = 1 
   for i in range(m): 
      for j in range(m): 
         tableaux[1][(i, (m + j))] -= min_[j] 
         tableaux[1][(i, (m + j))] += 1 
   for (pl, start) in enumerate([m, 0]): 
      for i in range(m): 
         bases[pl][i] = (start + i) 
   return (tableaux, bases)"," 'Initializes the tableaux for the IG algorithm. 
 Parameters 
 X : ndarray 
 The input data. 
 Y : ndarray 
 The target data. 
 tableaux : ndarray 
 The tableaux. 
 bases : ndarray 
 The bases. 
 Returns 
 tableaux : ndarray 
 The tableaux. 
 bases : ndarray 
 The bases. 
 Examples 
 >>> X = np.array([[0, 0, 0], [1, 1, 1], [2, 2, 2]]) 
 >>> Y = np.array([[0, 0, 0], [1, 1, 1], [2, 2, 2]]) 
 >>> tableaux, bases = _initialize_tableaux_ig(X, Y) 
 >>> tableaux 
 array([[0, 0, 0, 0, 0], 
 [0, 0, 0, 0, 0], 
 [0, 0, 0, 0, 0], 
 [0,","'Given sequences `X` and `Y` of ndarrays, initialize the tableau and 
 basis arrays in place for the ""geometric"" imitation game as defined 
 in McLennan and Tourky (2006), to be passed to `lemke_howson_tbl`. 
 Parameters 
 X, Y : ndarray(float) 
 Arrays of the same shape (m, n). 
 tableaux : tuple(ndarray(float, ndim=2)) 
 Tuple of two arrays to be used to store the tableaux, of shape 
 (2m, 2m). Modified in place. 
 bases : tuple(ndarray(int, ndim=1)) 
 Tuple of two arrays to be used to store the bases, of shape 
 (m,). Modified in place. 
 Returns 
 tableaux : tuple(ndarray(float, ndim=2)) 
 View to `tableaux`. 
 bases : tuple(ndarray(int, ndim=1)) 
 View to `bases`.'"
"def sobel_v(image, mask=None): 
    assert_nD(image, 2) 
   image = img_as_float(image) 
   result = convolve(image, VSOBEL_WEIGHTS) 
   return _mask_filter_result(result, mask)"," 'Compute the vertical Sobel operator on an image. 
 Parameters 
 image : 2-D array 
 Image to apply the filter to. 
 mask : 2-D array, optional 
 Mask array used to restrict the filter operation to a neighborhood 
 of a certain size. If `mask` is not given, the filter is applied to the 
 entire image. 
 Returns 
 out : 2-D array 
 The filtered image. 
 Notes 
 This filter is a horizontal derivative of the image. 
 Examples 
 >>> from skimage import data 
 >>> from skimage.filters.rank import sobel_v 
 >>> img = data.camera() 
 >>> sobel_v(img) 
 array([[[ 0.00000000e+00,  0.00000000e+00], 
        [ 0.00000000e+00,  0.00000000e+00], 
        [ 0.00000000e+00","'Find the vertical edges of an image using the Sobel transform. 
 Parameters 
 image : 2-D array 
 Image to process. 
 mask : 2-D array, optional 
 An optional mask to limit the application to a certain area. 
 Note that pixels surrounding masked regions are also masked to 
 prevent masked regions from affecting the result. 
 Returns 
 output : 2-D array 
 The Sobel edge map. 
 Notes 
 We use the following kernel:: 
 1   0  -1 
 2   0  -2 
 1   0  -1'"
"def _estimate_log_gaussian_prob(X, means, precisions_chol, covariance_type): 
    (n_samples, n_features) = X.shape 
   (n_components, _) = means.shape 
   log_det = _compute_log_det_cholesky(precisions_chol, covariance_type, n_features) 
   if (covariance_type == 'full'): 
      log_prob = np.empty((n_samples, n_components)) 
      for (k, (mu, prec_chol)) in enumerate(zip(means, precisions_chol)): 
         y = (np.dot(X, prec_chol) - np.dot(mu, prec_chol)) 
         log_prob[:, k] = np.sum(np.square(y), axis=1) 
   elif (covariance_type == 'tied'): 
      log_prob = np.empty((n_samples, n_components)) 
      for (k, mu) in enumerate(means): 
         y = (np.dot(X, precisions_chol) - np.dot(mu, precisions_chol)) 
         log_prob[:, k] = np.sum(np.square(y), axis=1) 
   elif (covariance_type == 'diag'): 
      precisions = (precisions_chol ** 2) 
      log_prob = ((np.sum(((means ** 2) * precisions), 1) - (2.0 * np.dot(X, (means * precisions).T))) + np.dot((X ** 2), precisions.T)) 
   elif (covariance_type == 'spherical'): 
      precisions = (precisions_chol ** 2) 
      log_prob = (((np.sum((means ** 2), 1) * precisions) - (2 * np.dot(X, (means.T * precisions)))) + np.outer(row_norms(X, squared=True), precisions)) 
   return (((-0.5) * ((n_features * np.log((2 * np.pi))) + log_prob)) + log_det)"," 'Estimate the log-probability of the Gaussian distribution 
 Parameters 
 X : ndarray 
 Input data, shape = [n_samples, n_features] 
 means : ndarray 
 Means of the Gaussian distribution, shape = [n_components, n_features] 
 precisions_chol : ndarray 
 Covariance matrix of the Gaussian distribution, shape = [n_components, n_features] 
 covariance_type : str 
 Type of covariance matrix. 
 \'full\', \'tied\', \'diag\', \'spherical\'. 
 Returns 
 log_prob : ndarray 
 Log-probability of the Gaussian distribution. 
 Notes 
 This function estimates the log-probability of the Gaussian distribution 
 using the Cholesky decomposition of the covariance matrix. 
 References 
 .. [1] ""Estimating the log-probability of the Gaussian distribution"", 
 NIPS 2008 
 Examples 
 >>> X = np.array([[1, 2, 3], [4,","'Estimate the log Gaussian probability. 
 Parameters 
 X : array-like, shape (n_samples, n_features) 
 means : array-like, shape (n_components, n_features) 
 precisions_chol : array-like, 
 Cholesky decompositions of the precision matrices. 
 \'full\' : shape of (n_components, n_features, n_features) 
 \'tied\' : shape of (n_features, n_features) 
 \'diag\' : shape of (n_components, n_features) 
 \'spherical\' : shape of (n_components,) 
 covariance_type : {\'full\', \'tied\', \'diag\', \'spherical\'} 
 Returns 
 log_prob : array, shape (n_samples, n_components)'"
"def gen_resource(ob, perm=None): 
    res = [] 
   if isinstance(ob, dict): 
      role = ob.get('role') 
      asset_r = ob.get('asset') 
      user = ob.get('user') 
      if (not perm): 
         perm = get_group_user_perm(user) 
      if role: 
         roles = perm.get('role', {}).keys() 
         if (role not in roles): 
            return {} 
         role_assets_all = perm.get('role').get(role).get('asset') 
         assets = (set(role_assets_all) & set(asset_r)) 
         for asset in assets: 
            asset_info = get_asset_info(asset) 
            role_key = get_role_key(user, role) 
            info = {'hostname': asset.hostname, 'ip': asset.ip, 'port': asset_info.get('port', 22), 'ansible_ssh_private_key_file': role_key, 'username': role.name} 
            if os.path.isfile(role_key): 
               info['ssh_key'] = role_key 
            res.append(info) 
      else: 
         for (asset, asset_info) in perm.get('asset').items(): 
            if (asset not in asset_r): 
               continue 
            asset_info = get_asset_info(asset) 
            try: 
               role = sorted(list(perm.get('asset').get(asset).get('role')))[0] 
            except IndexError: 
               continue 
            role_key = get_role_key(user, role) 
            info = {'hostname': asset.hostname, 'ip': asset.ip, 'port': asset_info.get('port', 22), 'username': role.name, 'password': CRYPTOR.decrypt(role.password)} 
            if os.path.isfile(role_key): 
               info['ssh_key'] = role_key 
            res.append(info) 
   elif isinstance(ob, User): 
      if (not perm): 
         perm = get_group_user_perm(ob) 
      for (asset, asset_info) in perm.get('asset').items(): 
         asset_info = get_asset_info(asset) 
         info = {'hostname': asset.hostname, 'ip': asset.ip, 'port': asset_info.get('port', 22)} 
         try: 
            role = sorted(list(perm.get('asset').get(asset).get('role')))[0] 
         except IndexError: 
            continue 
         info['username'] = role.name 
         info['password'] = CRYPTOR.decrypt(role.password) 
         role_key = get_role_key(ob, role) 
         if os.path.isfile(role_key): 
            info['ssh_key'] = role_key 
         res.append(info) 
   elif isinstance(ob, (list, QuerySet)): 
      for asset in ob: 
         info = get_asset_info(asset) 
         res.append(info) 
   logger.debug(('\xe7\x94\x9f\xe6\x88\x90res:   %s' % res)) 
   return res"," 'Generate resource list for user/group/asset. 
 :param ob: user/group/asset 
 :param perm: permission 
 :return: list of dict'","'obä¸ºç¨æ·æèµäº§åè¡¨æèµäº§queryset, å¦æåæ¶è¾å¥ç¨æ·å{\'role\': role1, \'asset\': []}ï¼åè·åç¨æ·å¨è¿äºèµäº§ä¸çä¿¡æ¯ 
 çæMyInventoryéè¦ç resourceæä»¶'"
"def get_filter(doctype, f): 
    from frappe.model import default_fields, optional_fields 
   if isinstance(f, dict): 
      (key, value) = f.items()[0] 
      f = make_filter_tuple(doctype, key, value) 
   if (not isinstance(f, (list, tuple))): 
      frappe.throw(u'Filter   must   be   a   tuple   or   list   (in   a   list)') 
   if (len(f) == 3): 
      f = (doctype, f[0], f[1], f[2]) 
   elif (len(f) != 4): 
      frappe.throw(u'Filter   must   have   4   values   (doctype,   fieldname,   operator,   value):   {0}'.format(str(f))) 
   f = frappe._dict(doctype=f[0], fieldname=f[1], operator=f[2], value=f[3]) 
   if (not f.operator): 
      f.operator = u'=' 
   valid_operators = (u'=', u'!=', u'>', u'<', u'>=', u'<=', u'like', u'not   like', u'in', u'not   in', u'Between') 
   if (f.operator not in valid_operators): 
      frappe.throw(u'Operator   must   be   one   of   {0}'.format(u',   '.join(valid_operators))) 
   if (f.doctype and (f.fieldname not in (default_fields + optional_fields))): 
      meta = frappe.get_meta(f.doctype) 
      if (not meta.has_field(f.fieldname)): 
         for df in meta.get_table_fields(): 
            if frappe.get_meta(df.options).has_field(f.fieldname): 
               f.doctype = df.options 
               break 
   return f"," 'Return a filter tuple (doctype, fieldname, operator, value) 
 :param doctype: doctype 
 :param f: filter 
 :returns: filter tuple (doctype, fieldname, operator, value)'","'Returns a _dict like 
 ""doctype"": 
 ""fieldname"": 
 ""operator"": 
 ""value"":'"
"def single_line(text): 
    return re.sub('   +', '   ', normalize_newlines(text).replace('\n', '')).strip()"," 'Remove all multiple spaces in a single line and return the result. 
 >>> single_line(\'hello\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\",'Quick utility to make comparing template output easier.'
"@RegisterWithArgChecks(name='neighbor.attribute_map.get', req_args=[neighbors.IP_ADDRESS], opt_args=[ROUTE_DISTINGUISHER, VRF_RF]) 
 def get_neighbor_attribute_map(neigh_ip_address, route_dist=None, route_family=VRF_RF_IPV4): 
    core = CORE_MANAGER.get_core_service() 
   peer = core.peer_manager.get_by_addr(neigh_ip_address) 
   at_maps_key = const.ATTR_MAPS_LABEL_DEFAULT 
   if (route_dist is not None): 
      at_maps_key = ':'.join([route_dist, route_family]) 
   at_maps = peer.attribute_maps.get(at_maps_key) 
   if at_maps: 
      return at_maps.get(const.ATTR_MAPS_ORG_KEY) 
   else: 
      return []"," 'Returns the list of attribute maps for a given neighbor. 
 The attribute maps are used to populate the neighbor\'s attributes 
 in the NeighborTable. 
 @param neighbors.IP_ADDRESS: IP address of the neighbor 
 @param route_dist: Route Distinguisher 
 @param route_family: Route family 
 @returns: list of attribute maps'",'Returns a neighbor attribute_map for given ip address if exists.'
"def _get_conn(ret=None): 
    _options = _get_options(ret) 
   dsn = _options.get('dsn') 
   user = _options.get('user') 
   passwd = _options.get('passwd') 
   return pyodbc.connect('DSN={0};UID={1};PWD={2}'.format(dsn, user, passwd))", 'Get a connection object for the given options.','Return a MSSQL connection.'
"def validate(tax_number): 
    try: 
      verify_vat(tax_number) 
      return u'vat' 
   except VatCannotIdentifyValidationError: 
      pass 
   return u'unknown'"," 'Validate a tax number. 
 :param tax_number: A tax number to validate. 
 :return: A string indicating the type of tax number. 
 :rtype: str'","'Validate a tax number. 
 :param tax_number: Tax number to validate 
 :type tax_number: str 
 :return: 
 Type identifier of the tax number, if detected.  Possible 
 values for now are either ""vat"" or ""unknown"". 
 :rtype: str 
 :raise: 
 `ValidationError` if tax number type was detected, but it is 
 somehow malformed.'"
"def get_file_title(files_path_list, filename): 
    fname = os.path.basename(filename) 
   same_name_files = get_same_name_files(files_path_list, fname) 
   if (len(same_name_files) > 1): 
      compare_path = shortest_path(same_name_files) 
      if (compare_path == filename): 
         same_name_files.remove(path_components(filename)) 
         compare_path = shortest_path(same_name_files) 
      diff_path = differentiate_prefix(path_components(filename), path_components(compare_path)) 
      diff_path_length = len(diff_path) 
      path_component = path_components(diff_path) 
      if ((diff_path_length > 20) and (len(path_component) > 2)): 
         if ((path_component[0] != '/') and (path_component[0] != '')): 
            path_component = [path_component[0], '...', path_component[(-1)]] 
         else: 
            path_component = [path_component[2], '...', path_component[(-1)]] 
         diff_path = os.path.join(*path_component) 
      fname = ((fname + '   -   ') + diff_path) 
   return fname"," 'Return the file title of the file with the given name. 
 The file title is the filename with the prefix removed, if there is a 
 prefix. 
 :param files_path_list: A list of file paths. 
 :param filename: The file path to the file. 
 :return: The file title.'",'Get tab title without ambiguation.'
"def query_chooser(query): 
    ids = [] 
   for (column, operator, value) in _get_query_comparisons(query): 
      if column.shares_lineage(weather_locations.c.continent): 
         if (operator == operators.eq): 
            ids.append(shard_lookup[value]) 
         elif (operator == operators.in_op): 
            ids.extend((shard_lookup[v] for v in value)) 
   if (len(ids) == 0): 
      return ['north_america', 'asia', 'europe', 'south_america'] 
   else: 
      return ids"," 'Returns a list of ids that are associated with the given query. 
 :param query: The query that was used to generate the list of ids. 
 :returns: A list of ids that are associated with the query. 
 :rtype: list'","'query chooser. 
 this also returns a list of shard ids, which can 
 just be all of them.  but here we\'ll search into the Query in order 
 to try to narrow down the list of shards to query.'"
"def remove_ignorable_whitespace(node): 
    if (node.tail and (node.tail.strip() == '')): 
      node.tail = None 
   for child in node: 
      if (node.text and (node.text.strip() == '')): 
         node.text = None 
      remove_ignorable_whitespace(child)"," 'Removes all whitespace that is ignored by the XML parser. 
 This is useful for testing with XMLUnit.'","'Remove insignificant whitespace from XML nodes 
 It should only remove whitespace in between elements and sub elements. 
 This should be safe for Jenkins due to how it\'s XML serialization works 
 but may not be valid for other XML documents. So use this method with 
 caution outside of this specific library.'"
"def nopackages(pkg_list): 
    pkg_list = [pkg for pkg in pkg_list if is_installed(pkg)] 
   if pkg_list: 
      uninstall(pkg_list)", 'Uninstall packages that are not installed.',"'Require several opkg packages to be uninstalled. 
 Example:: 
 from fabtools import require 
 require.opkg.nopackages([ 
 \'perl\', 
 \'php5\', 
 \'ruby\','"
"def get_colors(palette, funcs): 
    palettes = import_required('bokeh.palettes', _BOKEH_MISSING_MSG) 
   tz = import_required('toolz', _TOOLZ_MISSING_MSG) 
   unique_funcs = list(sorted(tz.unique(funcs))) 
   n_funcs = len(unique_funcs) 
   palette_lookup = palettes.all_palettes[palette] 
   keys = list(sorted(palette_lookup.keys())) 
   index = keys[min(bisect_left(keys, n_funcs), (len(keys) - 1))] 
   palette = palette_lookup[index] 
   palette = list(tz.unique(palette)) 
   if (len(palette) > n_funcs): 
      random.Random(42).shuffle(palette) 
   color_lookup = dict(zip(unique_funcs, cycle(palette))) 
   return [color_lookup[n] for n in funcs]"," 'Returns a list of colors for a given palette and a list of 
 functions to be mapped to the colors.'","'Get a dict mapping funcs to colors from palette. 
 Parameters 
 palette : string 
 Name of the bokeh palette to use, must be a member of 
 bokeh.palettes.all_palettes. 
 funcs : iterable 
 Iterable of function names'"
"def get_time_format(format='medium', locale=LC_TIME): 
    return Locale.parse(locale).time_formats[format]"," 'Return a time format string for the given format. 
 :param format: A time format string, such as \'short\', \'medium\', or \'full\'. 
 :param locale: The locale to use for formatting. 
 :return: A time format string.'","'Return the time formatting patterns used by the locale for the specified 
 format. 
 >>> get_time_format(locale=\'en_US\') 
 <DateTimePattern u\'h:mm:ss a\'> 
 >>> get_time_format(\'full\', locale=\'de_DE\') 
 <DateTimePattern u\'HH:mm:ss v\'> 
 :param format: the format to use, one of ""full"", ""long"", ""medium"", or 
 ""short"" 
 :param locale: the `Locale` object, or a locale string 
 :return: the time format pattern 
 :rtype: `DateTimePattern`'"
"def alias(selectable, name=None, flat=False): 
    return _interpret_as_from(selectable).alias(name=name, flat=flat)"," 'Create an alias for a selectable. 
 :param selectable: The selectable to alias. 
 :type selectable: :class:`~sqlalchemy.schema.Selectable` 
 :param name: The name of the alias. 
 :type name: str 
 :param flat: If True, the alias will be flattened into the selectable\'s 
 output. 
 :type flat: bool 
 :return: The alias.'","'Return an :class:`.Alias` object. 
 An :class:`.Alias` represents any :class:`.FromClause` 
 with an alternate name assigned within SQL, typically using the ``AS`` 
 clause when generated, e.g. ``SELECT * FROM table AS aliasname``. 
 Similar functionality is available via the 
 :meth:`~.FromClause.alias` method 
 available on all :class:`.FromClause` subclasses. 
 When an :class:`.Alias` is created from a :class:`.Table` object, 
 this has the effect of the table being rendered 
 as ``tablename AS aliasname`` in a SELECT statement. 
 For :func:`.select` objects, the effect is that of creating a named 
 subquery, i.e. ``(select ...) AS aliasname``. 
 The ``name`` parameter is optional, and provides the name 
 to use in the rendered SQL.  If blank, an ""anonymous"" name 
 will be deterministically generated at compile time. 
 Deterministic means the name is guaranteed to be unique against 
 other constructs used in the same statement, and will also be the 
 same name for each successive compilation of the same statement 
 object. 
 :param selectable: any :class:`.FromClause` subclass, 
 such as a table, select statement, etc. 
 :param name: string name to be assigned as the alias. 
 If ``None``, a name will be deterministically generated 
 at compile time. 
 :param flat: Will be passed through to if the given selectable 
 is an instance of :class:`.Join` - see :meth:`.Join.alias` 
 for details. 
 .. versionadded:: 0.9.0'"
"def clean_html(buf): 
    buf = buf.strip() 
   if (not buf): 
      return buf 
   html_parser = html5lib.HTMLParser(tree=treebuilders.getTreeBuilder('dom'), tokenizer=HTMLSanitizer) 
   dom_tree = html_parser.parseFragment(buf) 
   walker = treewalkers.getTreeWalker('dom') 
   stream = walker(dom_tree) 
   s = serializer.htmlserializer.HTMLSerializer(omit_optional_tags=False, quote_attr_values=True) 
   output = s.render(stream, 'utf-8') 
   while ('toberemoved' in output): 
      oldoutput = output 
      matches = re.findall('&lt;toberemoved.*?&gt;.*?&lt;/toberemoved&gt;', output, re.DOTALL) 
      for s in matches: 
         output = output.replace(s, '') 
      matches = re.findall('&lt;/toberemoved&gt;', output, re.DOTALL) 
      for s in matches: 
         output = output.replace(s, '') 
      matches = re.findall('&lt;toberemoved.*?&gt;', output, re.DOTALL) 
      for s in matches: 
         output = output.replace(s, '') 
      if (output == oldoutput): 
         break 
   return output"," 'Returns a sanitized version of the given string, stripping out all 
 HTML tags and attributes that are not allowed in a wiki. 
 If the string is empty, it is returned unchanged. 
 This function is used by the HTML parser to clean up HTML before 
 rendering it into a wiki-friendly format.'",'Cleans HTML of dangerous tags and content.'
"def getDisplayedDialogFromConstructor(repository): 
    try: 
      getReadRepository(repository) 
      return RepositoryDialog(repository, Tkinter.Tk()) 
   except: 
      print 'this   should   never   happen,   getDisplayedDialogFromConstructor   in   settings   could   not   open' 
      print repository 
      traceback.print_exc(file=sys.stdout) 
      return None"," 'getDisplayedDialogFromConstructor(repository) 
 Returns the displayed dialog for the repository passed in. 
 This is used by the settings dialog to determine if the repository 
 is open or not.'",'Display the repository dialog.'
"@cors_enabled('*') 
 def serve_cors(*args, **kwargs): 
    if (not settings.DEBUG): 
      raise RuntimeError(""Don't   use   kitsune.sumo.views.serve_cors   in   production."") 
   from django.views.static import serve 
   return serve(*args, **kwargs)"," 'Serve static files using CORS. 
 This is a wrapper around django\'s serve_static that allows CORS headers to 
 be set. 
 See: 
 http://docs.djangoproject.com/en/dev/ref/contrib/cors/#django.contrib.cors.views.cors_headers 
 http://docs.djangoproject.com/en/dev/ref/contrib/cors/#django.contrib.cors.views.cors_headers 
 http://docs.djangoproject.com/en/dev/ref/contrib/cors/#django.contrib.cors.views.cors_headers 
 http://docs.djangoproject.com/en/dev/ref/contrib/cors/#django.contrib.cors.views.cors_headers'",'A wrapper around django.views.static.serve that adds CORS headers.'
"def getRoundedToThreePlaces(number): 
    return str(round(number, 3))", 'Returns the number rounded to three places','Get number rounded to three places as a string.'
"def create_read_replica(name, source_name, db_instance_class=None, availability_zone=None, port=None, auto_minor_version_upgrade=None, iops=None, option_group_name=None, publicly_accessible=None, tags=None, db_subnet_group_name=None, storage_type=None, copy_tags_to_snapshot=None, monitoring_interval=None, monitoring_role_arn=None, region=None, key=None, keyid=None, profile=None): 
    if (not backup_retention_period): 
      raise SaltInvocationError('backup_retention_period   is   required') 
   res = __salt__['boto_rds.exists'](source_name, tags, region, key, keyid, profile) 
   if (not res.get('exists')): 
      return {'exists': bool(res), 'message': 'RDS   instance   source   {0}   does   not   exists.'.format(source_name)} 
   res = __salt__['boto_rds.exists'](name, tags, region, key, keyid, profile) 
   if res.get('exists'): 
      return {'exists': bool(res), 'message': 'RDS   replica   instance   {0}   already   exists.'.format(name)} 
   try: 
      conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) 
      kwargs = {} 
      for key in ('OptionGroupName', 'MonitoringRoleArn'): 
         if (locals()[key] is not None): 
            kwargs[key] = str(locals()[key]) 
      for key in ('MonitoringInterval', 'Iops', 'Port'): 
         if (locals()[key] is not None): 
            kwargs[key] = int(locals()[key]) 
      for key in ('CopyTagsToSnapshot', 'AutoMinorVersionUpgrade'): 
         if (locals()[key] is not None): 
            kwargs[key] = bool(locals()[key]) 
      taglist = _tag_doc(tags) 
      rds_replica = conn.create_db_instance_read_replica(DBInstanceIdentifier=name, SourceDBInstanceIdentifier=source_name, DBInstanceClass=db_instance_class, AvailabilityZone=availability_zone, PubliclyAccessible=publicly_accessible, Tags=taglist, DBSubnetGroupName=db_subnet_group_name, StorageType=storage_type, **kwargs) 
      return {'exists': bool(rds_replica)} 
   except ClientError as e: 
      return {'error': salt.utils.boto3.get_error(e)}"," 'Create a read replica in a given region. 
 This function will create a read replica of a source instance. 
 :param name: Name of the new RDS instance. 
 :param source_name: Name of the source instance. 
 :param db_instance_class: DB instance class. 
 :param availability_zone: Availability zone. 
 :param port: Port. 
 :param auto_minor_version_upgrade: Auto minor version upgrade. 
 :param iops: IOPS. 
 :param option_group_name: Option group name. 
 :param publicly_accessible: Publicly accessible. 
 :param tags: Tags. 
 :param db_subnet_group_name: DB subnet group name. 
 :param storage_type: Storage type. 
 :param copy_tags_to_snapshot: Copy tags to snapshot. 
 :param monitoring_interval: Monitoring interval. 
 :param monitoring_role_arn: Monitoring role arn. 
 :param region: Region. 
 :return: True if the instance was created, False otherwise","'Create an RDS read replica 
 CLI example to create an RDS  read replica:: 
 salt myminion boto_rds.create_read_replica replicaname source_name'"
"@task(ignore_result=True) 
 def email_membership_change(group_pk, user_pk, old_status, new_status): 
    from mozillians.groups.models import Group, GroupMembership 
   group = Group.objects.get(pk=group_pk) 
   user = User.objects.get(pk=user_pk) 
   activate('en-us') 
   if (old_status in [GroupMembership.PENDING, GroupMembership.PENDING_TERMS]): 
      if (new_status == GroupMembership.MEMBER): 
         subject = (_('Accepted   to   Mozillians   group   ""%s""') % group.name) 
         template_name = 'groups/email/accepted.txt' 
      elif (new_status is None): 
         subject = (_('Not   accepted   to   Mozillians   group   ""%s""') % group.name) 
         template_name = 'groups/email/rejected.txt' 
      else: 
         raise ValueError('BAD   ARGS   TO   email_membership_change') 
   else: 
      raise ValueError('BAD   ARGS   TO   email_membership_change') 
   context = {'group': group, 'user': user} 
   template = get_template(template_name) 
   body = template.render(context) 
   send_mail(subject, body, settings.FROM_NOREPLY, [user.email], fail_silently=False)", 'Emails a user when their group membership changes.',"'Email user that their group membership status has changed. 
 old_status and new_status can either be a valid value for GroupMembership.status, 
 or None if we\'re going from or to a state where there is no GroupMembership 
 record (e.g. if they\'re being removed from a group). 
 This is queued from Group.add_member() and Group.remove_member().'"
"def cyclic(length=None, alphabet=string.ascii_lowercase, n=None): 
    if (n is None): 
      n = 4 
   if ((len(alphabet) ** n) < length): 
      log.error((""Can't   create   a   pattern   length=%i   with   len(alphabet)==%i   and   n==%i"" % (length, len(alphabet), n))) 
   out = [] 
   for (ndx, c) in enumerate(de_bruijn(alphabet, n)): 
      if ((length != None) and (ndx >= length)): 
         break 
      else: 
         out.append(c) 
   if isinstance(alphabet, str): 
      return ''.join(out) 
   else: 
      return out"," 'Returns a cyclic permutation of a string 
 Parameters 
 length : int 
 The length of the cyclic permutation. 
 alphabet : str 
 The alphabet to use. 
 n : int 
 The number of permutations to generate. 
 Returns 
 out : str 
 A cyclic permutation of the input string. 
 Examples 
 >>> cyclic(""abc"", n=3) 
 \'aaa\''","'cyclic(length = None, alphabet = string.ascii_lowercase, n = 4) -> list/str 
 A simple wrapper over :func:`de_bruijn`. This function returns at most 
 `length` elements. 
 If the given alphabet is a string, a string is returned from this function. Otherwise 
 a list is returned. 
 Arguments: 
 length: The desired length of the list or None if the entire sequence is desired. 
 alphabet: List or string to generate the sequence over. 
 n(int): The length of subsequences that should be unique. 
 Example: 
 >>> cyclic(alphabet = ""ABC"", n = 3) 
 \'AAABAACABBABCACBACCBBBCBCCC\' 
 >>> cyclic(20) 
 \'aaaabaaacaaadaaaeaaa\' 
 >>> alphabet, n = range(30), 3 
 >>> len(alphabet)**n, len(cyclic(alphabet = alphabet, n = n)) 
 (27000, 27000)'"
"def _process_dataset(name, images, vocab, num_shards): 
    images = [ImageMetadata(image.image_id, image.filename, [caption]) for image in images for caption in image.captions] 
   random.seed(12345) 
   random.shuffle(images) 
   num_threads = min(num_shards, FLAGS.num_threads) 
   spacing = np.linspace(0, len(images), (num_threads + 1)).astype(np.int) 
   ranges = [] 
   threads = [] 
   for i in xrange((len(spacing) - 1)): 
      ranges.append([spacing[i], spacing[(i + 1)]]) 
   coord = tf.train.Coordinator() 
   decoder = ImageDecoder() 
   print(('Launching   %d   threads   for   spacings:   %s' % (num_threads, ranges))) 
   for thread_index in xrange(len(ranges)): 
      args = (thread_index, ranges, name, images, decoder, vocab, num_shards) 
      t = threading.Thread(target=_process_image_files, args=args) 
      t.start() 
      threads.append(t) 
   coord.join(threads) 
   print((""%s:   Finished   processing   all   %d   image-caption   pairs   in   data   set   '%s'."" % (datetime.now(), len(images), name)))"," 'Processes a dataset, dividing it into chunks for parallel processing. 
 Args: 
 name: name of the dataset. 
 images: list of ImageMetadata objects. 
 vocab: vocabulary object. 
 num_shards: number of shards. 
 Returns: 
 list of ImageMetadata objects.'","'Processes a complete data set and saves it as a TFRecord. 
 Args: 
 name: Unique identifier specifying the dataset. 
 images: List of ImageMetadata. 
 vocab: A Vocabulary object. 
 num_shards: Integer number of shards for the output files.'"
"def detach_user_policy(policy_name, user_name, region=None, key=None, keyid=None, profile=None): 
    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) 
   policy_arn = _get_policy_arn(policy_name, region, key, keyid, profile) 
   try: 
      conn.detach_user_policy(policy_arn, user_name) 
      log.info('Detached   {0}   policy   to   user   {1}.'.format(policy_name, user_name)) 
   except boto.exception.BotoServerError as e: 
      log.debug(e) 
      msg = 'Failed   to   detach   {0}   policy   to   user   {1}.' 
      log.error(msg.format(policy_name, user_name)) 
      return False 
   return True"," 'Detach a policy from a user. 
 :param policy_name: The name of the policy to detach. 
 :type policy_name: str 
 :param user_name: The name of the user to detach the policy from. 
 :type user_name: str 
 :param region: The region to connect to. 
 :type region: str 
 :param key: The access key to use. 
 :type key: str 
 :param keyid: The access key ID to use. 
 :type keyid: str 
 :param profile: The profile to use. 
 :type profile: str 
 :return: True if the policy was detached, False otherwise.'","'Detach a managed policy to a user. 
 CLI Example: 
 .. code-block:: bash 
 salt myminion boto_iam.detach_user_policy mypolicy myuser'"
"def scan(): 
    ret = [] 
   devices = bluetooth.discover_devices(lookup_names=True) 
   for device in devices: 
      ret.append({device[0]: device[1]}) 
   return ret"," 'Scan for bluetooth devices and return a dictionary of their names and 
 addresses'","'Scan for bluetooth devices in the area 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' bluetooth.scan'"
"def from_castra(x, columns=None, index=False): 
    from castra import Castra 
   if (not isinstance(x, Castra)): 
      x = Castra(x, readonly=True) 
   elif (not x._readonly): 
      x = Castra(x.path, readonly=True) 
   if (columns is None): 
      columns = x.columns 
   name = ('from-castra-' + tokenize(os.path.getmtime(x.path), x.path, columns, index)) 
   dsk = dict((((name, i), (load_castra_partition, x, part, columns, index)) for (i, part) in enumerate(x.partitions))) 
   return Bag(dsk, name, len(x.partitions))"," 'Construct a Bag from a Castra object. 
 Parameters 
 x : Castra 
 The Castra object. 
 columns : list of str 
 A list of columns to load. 
 index : bool 
 If True, load the index. 
 Returns 
 Bag 
 A Bag object. 
 Examples 
 >>> from sklearn.datasets import make_blobs 
 >>> from sklearn.datasets import make_classification 
 >>> from sklearn.datasets import make_regression 
 >>> from sklearn.datasets import make_multilabel_classification 
 >>> from sklearn.datasets import make_multinomial 
 >>> from sklearn.datasets import make_multinomial 
 >>> from sklearn.datasets import make_multinomial 
 >>> from sklearn.datasets import make_multinomial 
 >>> from sklearn.datasets import make_multinomial 
 >>> from sklearn.datasets import make_multinomial 
 >>> from sklearn.datasets import make_multinomial 
 >>> from sklearn.datasets import make_multinomial 
","'Load a dask Bag from a Castra. 
 Parameters 
 x : filename or Castra 
 columns: list or string, optional 
 The columns to load. Default is all columns. 
 index: bool, optional 
 If True, the index is included as the first element in each tuple. 
 Default is False.'"
"def execute_manager(settings_mod, argv=None): 
    warnings.warn(""The   'execute_manager'   function   is   deprecated,   you   likely   need   to   update   your   'manage.py';   please   see   the   Django   1.4   release   notes   (https://docs.djangoproject.com/en/dev/releases/1.4/)."", PendingDeprecationWarning) 
   setup_environ(settings_mod) 
   utility = ManagementUtility(argv) 
   utility.execute()", 'Executes the Django command line management utility.',"'Like execute_from_command_line(), but for use by manage.py, a 
 project-specific django-admin.py utility.'"
"def _DefaultValueConstructorForField(field): 
    if (field.label == _FieldDescriptor.LABEL_REPEATED): 
      if (field.has_default_value and (field.default_value != [])): 
         raise ValueError(('Repeated   field   default   value   not   empty   list:   %s' % field.default_value)) 
      if (field.cpp_type == _FieldDescriptor.CPPTYPE_MESSAGE): 
         message_type = field.message_type 
         def MakeRepeatedMessageDefault(message): 
            return containers.RepeatedCompositeFieldContainer(message._listener_for_children, field.message_type) 
         return MakeRepeatedMessageDefault 
      else: 
         type_checker = type_checkers.GetTypeChecker(field) 
         def MakeRepeatedScalarDefault(message): 
            return containers.RepeatedScalarFieldContainer(message._listener_for_children, type_checker) 
         return MakeRepeatedScalarDefault 
   if (field.cpp_type == _FieldDescriptor.CPPTYPE_MESSAGE): 
      message_type = field.message_type 
      def MakeSubMessageDefault(message): 
         result = message_type._concrete_class() 
         result._SetListener(message._listener_for_children) 
         return result 
      return MakeSubMessageDefault 
   def MakeScalarDefault(message): 
      return field.default_value 
   return MakeScalarDefault"," 'Returns a function that can be used to construct default values for 
 fields of type message or repeated message. 
 :param field: The field to construct the default value for. 
 :return: A function that can be used to construct the default value.'","'Returns a function which returns a default value for a field. 
 Args: 
 field: FieldDescriptor object for this field. 
 The returned function has one argument: 
 message: Message instance containing this field, or a weakref proxy 
 of same. 
 That function in turn returns a default value for this field.  The default 
 value may refer back to |message| via a weak reference.'"
"def new_figure_manager_given_figure(num, figure): 
    canvas = FigureCanvasWebAgg(figure) 
   manager = core.FigureManagerWebAgg(canvas, num) 
   return manager"," 'Create a new FigureManager for a figure. 
 This function is used by the FigureManager class to create a new 
 FigureManager for a given figure. 
 Parameters 
 num : int 
 The number of the figure manager. 
 figure : Figure 
 The figure to be managed. 
 Returns 
 FigureManager 
 The new FigureManager.'",'Create a new figure manager instance for the given figure.'
"def tetrahedral_graph(create_using=None): 
    G = complete_graph(4, create_using) 
   G.name = 'Platonic   Tetrahedral   graph' 
   return G"," 'Returns a Tetrahedral graph, a Platonic graph. 
 Parameters 
 create_using : str 
 Name of the Graph class to use. 
 Returns 
 A Tetrahedral graph. 
 Examples 
 >>> from sympy.combinatorics.graphs import tetrahedral_graph 
 >>> G = tetrahedral_graph() 
 >>> G 
 Tetrahedral graph 
 >>> G.name 
 \'Platonic Tetrahedral graph\' 
 >>> G.adjacency_matrix 
 Matrix([[0, 1, 1, 1], 
 [1, 0, 1, 1], 
 [1, 1, 0, 1], 
 [1, 1, 1, 0]])'",'Return the 3-regular Platonic Tetrahedral graph.'
"def get_data_path(f=''): 
    (_, filename, _, _, _, _) = inspect.getouterframes(inspect.currentframe())[1] 
   base_dir = os.path.abspath(os.path.dirname(filename)) 
   return os.path.join(base_dir, 'data', f)"," 'Get the data path from the current file. 
 Parameters 
 f : str 
 The name of the file without extension. 
 Returns 
 str 
 The absolute path of the data folder.'","'Return the path of a data file, these are relative to the current test 
 directory.'"
"def statusEnquiry(): 
    a = TpPd(pd=3) 
   b = MessageType(mesType=52) 
   packet = (a / b) 
   return packet", 'Returns a packet with a status enquiry message.','STATUS ENQUIRY Section 9.3.28'
"def encryptMessage(key, message): 
    return translateMessage(key, message, 'encrypt')"," 'Encrypts a message using the provided key. 
 :param key: The key to use to encrypt the message. 
 :param message: The message to encrypt. 
 :return: The encrypted message.'","'>>> encryptMessage(\'LFWOAYUISVKMNXPBDCRJTQEGHZ\', \'Harshil Darji\') 
 \'Ilcrism Olcvs\''"
"def python_version(): 
    return _sys_version()[1]", 'Returns the Python version as a string.',"'Returns the Python version as string \'major.minor.patchlevel\' 
 Note that unlike the Python sys.version, the returned value 
 will always include the patchlevel (it defaults to 0).'"
"def _trial(factors, n, candidates, verbose=False): 
    if verbose: 
      factors0 = list(factors.keys()) 
   nfactors = len(factors) 
   for d in candidates: 
      if ((n % d) == 0): 
         m = multiplicity(d, n) 
         n //= (d ** m) 
         factors[d] = m 
   if verbose: 
      for k in sorted(set(factors).difference(set(factors0))): 
         print((factor_msg % (k, factors[k]))) 
   return (int(n), (len(factors) != nfactors))"," 'Returns the number of factors and the number of factors not in the 
 list of candidates. 
 If verbose is True, prints the factors. 
 :param factors: A dictionary of factors of n. 
 :type factors: dict 
 :param n: The number to factor. 
 :type n: int 
 :param candidates: The list of candidates to test. 
 :type candidates: list 
 :param verbose: If True, prints the factors. 
 :type verbose: bool 
 :returns: The number of factors and the number of factors not in the 
 list of candidates. 
 :rtype: tuple'","'Helper function for integer factorization. Trial factors ``n` 
 against all integers given in the sequence ``candidates`` 
 and updates the dict ``factors`` in-place. Returns the reduced 
 value of ``n`` and a flag indicating whether any factors were found.'"
"def tokenize_wrapper(input): 
    skip = set((token.NEWLINE, token.INDENT, token.DEDENT)) 
   tokens = tokenize.generate_tokens(StringIO.StringIO(input).readline) 
   for quintuple in tokens: 
      (type, value, start, end, line_text) = quintuple 
      if (type not in skip): 
         (yield quintuple)", 'Tokenize the given input string.','Tokenizes a string suppressing significant whitespace.'
"@mock_autoscaling 
 def test_execute_policy_small_percent_change_in_capacity(): 
    setup_autoscale_group() 
   conn = boto.connect_autoscale() 
   policy = ScalingPolicy(name=u'ScaleUp', adjustment_type=u'PercentChangeInCapacity', as_name=u'tester_group', scaling_adjustment=1) 
   conn.create_scaling_policy(policy) 
   conn.execute_policy(u'ScaleUp') 
   instances = list(conn.get_all_autoscaling_instances()) 
   instances.should.have.length_of(3)"," 'Test that a scaling policy with a small adjustment in capacity will execute 
 correctly'","'http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/as-scale-based-on-demand.html 
 If PercentChangeInCapacity returns a value between 0 and 1, 
 Auto Scaling will round it off to 1.'"
"def get_prediction(self, exog=None, transform=True, weights=None, row_labels=None, pred_kwds=None): 
    if (transform and hasattr(self.model, 'formula') and (exog is not None)): 
      from patsy import dmatrix 
      exog = dmatrix(self.model.data.design_info.builder, exog) 
   if (exog is not None): 
      if (row_labels is None): 
         if hasattr(exog, 'index'): 
            row_labels = exog.index 
         else: 
            row_labels = None 
      exog = np.asarray(exog) 
      if ((exog.ndim == 1) and ((self.model.exog.ndim == 1) or (self.model.exog.shape[1] == 1))): 
         exog = exog[:, None] 
      exog = np.atleast_2d(exog) 
   else: 
      exog = self.model.exog 
      if (weights is None): 
         weights = getattr(self.model, 'weights', None) 
      if (row_labels is None): 
         row_labels = getattr(self.model.data, 'row_labels', None) 
   if (weights is not None): 
      weights = np.asarray(weights) 
      if ((weights.size > 1) and ((weights.ndim != 1) or (weights.shape[0] == exog.shape[1]))): 
         raise ValueError('weights   has   wrong   shape') 
   if (pred_kwds is None): 
      pred_kwds = {} 
   predicted_mean = self.model.predict(self.params, exog, **pred_kwds) 
   covb = self.cov_params() 
   var_pred_mean = (exog * np.dot(covb, exog.T).T).sum(1) 
   var_resid = (self.scale / weights) 
   if (self.cov_type == 'fixed   scale'): 
      var_resid = (self.cov_kwds['scale'] / weights) 
   dist = ['norm', 't'][self.use_t] 
   return PredictionResults(predicted_mean, var_pred_mean, var_resid, df=self.df_resid, dist=dist, row_labels=row_labels)"," 'Returns the prediction results for the model. 
 Parameters 
 exog : array-like 
 The exogenous variables for the model. 
 transform : bool 
 If True, transform the exogenous variables to the model space. 
 weights : array-like 
 The weights for the model. 
 row_labels : array-like 
 The row labels for the exogenous variables. 
 pred_kwds : dict 
 A dictionary of keyword arguments to pass to the model. 
 Returns 
 PredictionResults 
 The prediction results for the model. 
 Notes 
 This function is only called when the model is not fitted. 
 References 
 .. [1] http://en.wikipedia.org/wiki/Prediction_interval'","'compute prediction results 
 Parameters 
 exog : array-like, optional 
 The values for which you want to predict. 
 transform : bool, optional 
 If the model was fit via a formula, do you want to pass 
 exog through the formula. Default is True. E.g., if you fit 
 a model y ~ log(x1) + log(x2), and transform is True, then 
 you can pass a data structure that contains x1 and x2 in 
 their original form. Otherwise, you\'d need to log the data 
 first. 
 weights : array_like, optional 
 Weights interpreted as in WLS, used for the variance of the predicted 
 residual. 
 args, kwargs : 
 Some models can take additional arguments or keywords, see the 
 predict method of the model for the details. 
 Returns 
 prediction_results : instance 
 The prediction results instance contains prediction and prediction 
 variance and can on demand calculate confidence intervals and summary 
 tables for the prediction of the mean and of new observations.'"
"def list_bucket(bucket_name): 
    url = 'https://www.googleapis.com/storage/v1/b/{0}/o'.format(bucket_name) 
   try: 
      response = gcs_get_request(url) 
      if (response.status_code != HTTP_OK): 
         logging.error('Error   on   listing   objects   in   GCS   bucket:   {0}.   Error:   {1}'.format(bucket_name, response.status_code)) 
         return [] 
      content = json.loads(response.content) 
   except requests.HTTPError as error: 
      logging.error('Error   on   listing   objects   in   GCS   bucket:   {0}.   Error:   {1}'.format(bucket_name, error)) 
      return [] 
   if ('items' not in content.keys()): 
      return [] 
   objects = [] 
   for item in content['items']: 
      objects.append(item['name']) 
   logging.debug('Bucket   contents:   {0}'.format(objects)) 
   return objects"," 'Lists the objects in a bucket. 
 Args: 
 bucket_name (str): The name of the bucket. 
 Returns: 
 list: The list of objects in the bucket.'","'Lists all the files that are in the designated GCS bucket. 
 Args: 
 bucket_name: A str, the name of the GCS bucket to look up. 
 Returns: 
 A list of str, the names of the files in the bucket.'"
"def liveobj_changed(obj, other): 
    return ((obj != other) or (type(obj) != type(other)))"," 'Returns True if obj and other are different objects, or if obj and other 
 are the same object but have different types.'","'Check whether obj and other are not equal, properly handling lost weakrefs. 
 Use this whenever you cache a Live API object in some variable, and want to check 
 whether you need to update the cached object.'"
"def site_enabled(config): 
    enable_site(config) 
   reload_service('apache2')", 'Enable the site in Apache.',"'Require an Apache site to be enabled. 
 This will cause Apache to reload its configuration. 
 from fabtools import require 
 require.apache.site_enabled(\'mysite\')'"
"def read_weighted_edgelist(path, comments='#', delimiter=None, create_using=None, nodetype=None, encoding='utf-8'): 
    return read_edgelist(path, comments=comments, delimiter=delimiter, create_using=create_using, nodetype=nodetype, data=(('weight', float),), encoding=encoding)"," 'Reads a weighted edge list. 
 :param path: path to the file to read 
 :param comments: string used to identify comments in the file 
 :param delimiter: string used to separate fields in the file 
 :param create_using: function used to create a node 
 :param nodetype: nodetype to use for nodes 
 :param encoding: encoding to use for the file 
 :returns: dictionary of nodes with edge weights'","'Read a graph as list of edges with numeric weights. 
 Parameters 
 path : file or string 
 File or filename to read. If a file is provided, it must be 
 opened in \'rb\' mode. 
 Filenames ending in .gz or .bz2 will be uncompressed. 
 comments : string, optional 
 The character used to indicate the start of a comment. 
 delimiter : string, optional 
 The string used to separate values.  The default is whitespace. 
 create_using : Graph container, optional, 
 Use specified container to build graph.  The default is networkx.Graph, 
 an undirected graph. 
 nodetype : int, float, str, Python type, optional 
 Convert node data from strings to specified type 
 encoding: string, optional 
 Specify which encoding to use when reading file. 
 Returns 
 G : graph 
 A networkx Graph or other type specified with create_using 
 Notes 
 Since nodes must be hashable, the function nodetype must return hashable 
 types (e.g. int, float, str, frozenset - or tuples of those, etc.) 
 Example edgelist file format. 
 With numeric edge data:: 
 # read with 
 # >>> G=nx.read_weighted_edgelist(fh) 
 # source target data 
 a b 1 
 a c 3.14159 
 d e 42'"
"def _d(n, j, prec, sq23pi, sqrt8): 
    j = from_int(j) 
   pi = mpf_pi(prec) 
   a = mpf_div(sq23pi, j, prec) 
   b = mpf_sub(from_int(n), from_rational(1, 24, prec), prec) 
   c = mpf_sqrt(b, prec) 
   (ch, sh) = mpf_cosh_sinh(mpf_mul(a, c), prec) 
   D = mpf_div(mpf_sqrt(j, prec), mpf_mul(mpf_mul(sqrt8, b), pi), prec) 
   E = mpf_sub(mpf_mul(a, ch), mpf_div(sh, c, prec), prec) 
   return mpf_mul(D, E)"," 'D = _d(n, j, prec, sq23pi, sqrt8) 
 Computes D = _d(n, j, prec, sq23pi, sqrt8) 
 where j = sqrt(23*pi) and sqrt8 = sqrt(2)**(1/4). 
 This function is used in the computation of 
 sin(pi/8)**2 cos(pi/8)**2 sin(pi/8)**2 cos(pi/8)**2 
 sin(pi/8)**2 cos(pi/8)**2 sin(pi/8)**2 cos(pi/8)**2 
 sin(pi/8)**2 cos(pi/8)**2 sin(pi/8)**2 cos(pi/8)**2 
 sin(pi/8)**2 cos(pi/8)**2 sin(pi/8)**2 cos(pi/8)**2 
 sin(pi/8)**2 cos(pi/8)**2 sin(pi/8)**2 cos(pi/8)**2 
","'Compute the sinh term in the outer sum of the HRR formula. 
 The constants sqrt(2/3*pi) and sqrt(8) must be precomputed.'"
"def t_BOOLCONSTANT(t): 
    t.value = (t.value == 'true') 
   return t"," 'Boolean constant. 
 t_BOOLCONSTANT(t) -> bool 
 t.value = (t.value == ""true"")'",'\btrue\b|\bfalse\b'
"def write_excellon(): 
    filename = string_cam_file.get() 
   file = open(filename, 'wb') 
   units = cad.inches_per_unit 
   file.write('%FSLAX24Y24*%\n') 
   file.write('%MOIN*%\n') 
   file.write('%OFA0B0*%\n') 
   ixs = cad.x[::2] 
   xs = (cad.xmin + (((cad.xmax - cad.xmin) * (ixs + 0.5)) / float(cad.nx))) 
   ixe = cad.x[1::2] 
   xe = (cad.xmin + (((cad.xmax - cad.xmin) * (ixe + 0.5)) / float(cad.nx))) 
   idx = (ixe - ixs) 
   dx = (xe - xs) 
   iys = cad.y[::2] 
   ys = (cad.ymin + (((cad.ymax - cad.ymin) * (iys + 0.5)) / float(cad.ny))) 
   iye = cad.y[1::2] 
   ye = (cad.ymin + (((cad.ymax - cad.ymin) * (iye + 0.5)) / float(cad.ny))) 
   idy = (iye - iys) 
   dy = (ye - ys) 
   mins = where((idx < idy), idx, idy) 
   uniques = unique(mins) 
   apertures = (((cad.xmax - cad.xmin) * uniques) / float(cad.nx)) 
   index = searchsorted(uniques, mins) 
   for i in range(len(uniques)): 
      file.write(('%%ADD%dR,%.4fX%.4f*%%\n' % ((i + 10), apertures[i], apertures[i]))) 
   coords = arange(len(mins)) 
   for i in range(len(uniques)): 
      file.write(('D%d*\n' % (i + 10))) 
      coord = coords[(index == i)] 
      delta = (apertures[i] / 2.0) 
      ixs = (10000 * (xs + delta)).astype(int32) 
      ixe = (10000 * (xe - delta)).astype(int32) 
      iys = (10000 * (ys + delta)).astype(int32) 
      iye = (10000 * (ye - delta)).astype(int32) 
      for j in range(len(coord)): 
         n = coord[j] 
         if (idx[n] == idy[n]): 
            file.write(('X%dY%dD03*\n' % (ixs[n], iys[n]))) 
         elif (idx[n] > idy[n]): 
            file.write(('X%dY%dD02*\n' % (ixs[n], iys[n]))) 
            file.write(('X%dY%dD01*\n' % (ixe[n], iys[n]))) 
         else: 
            file.write(('X%dY%dD02*\n' % (ixs[n], iys[n]))) 
            file.write(('X%dY%dD01*\n' % (ixs[n], iye[n]))) 
   file.write('M02*\n') 
   file.close() 
   string_msg.set(('wrote   %s   (RS-274X)' % filename)) 
   root.update()", 'write an RS-274X file for the current drawing',"'%       Rewind and Stop 
 X#Y#    Move and Drill 
 T#      Tool Selection 
 M30     End of Program 
 M00     End of Program 
 R#X#Y#  Repeat Hole 
 G05, G81        Select Drill Mode 
 G90     Absolute Mode 
 G91     Incremental Mode 
 G92 X#Y#        Set Zero 
 G93 X#Y#        Set Zero 
 M48     Program Header to first ""%"" 
 M72     English-Imperial Mode'"
"def oas(X, assume_centered=False): 
    X = np.asarray(X) 
   if ((len(X.shape) == 2) and (X.shape[1] == 1)): 
      if (not assume_centered): 
         X = (X - X.mean()) 
      return (np.atleast_2d((X ** 2).mean()), 0.0) 
   if (X.ndim == 1): 
      X = np.reshape(X, (1, (-1))) 
      warnings.warn('Only   one   sample   available.   You   may   want   to   reshape   your   data   array') 
      n_samples = 1 
      n_features = X.size 
   else: 
      (n_samples, n_features) = X.shape 
   emp_cov = empirical_covariance(X, assume_centered=assume_centered) 
   mu = (np.trace(emp_cov) / n_features) 
   alpha = np.mean((emp_cov ** 2)) 
   num = (alpha + (mu ** 2)) 
   den = ((n_samples + 1.0) * (alpha - ((mu ** 2) / n_features))) 
   shrinkage = (1.0 if (den == 0) else min((num / den), 1.0)) 
   shrunk_cov = ((1.0 - shrinkage) * emp_cov) 
   shrunk_cov.flat[::(n_features + 1)] += (shrinkage * mu) 
   return (shrunk_cov, shrinkage)"," 'Compute the OAS for a given data set. 
 Parameters 
 X : array 
 Data set to compute OAS on. 
 assume_centered : boolean 
 If true, data is assumed to be centered. 
 Returns 
 cov : array 
 Covariance matrix of the data set. 
 shrinkage : float 
 Shrinkage parameter. 
 References 
 .. [1] http://en.wikipedia.org/wiki/OAS 
 Examples 
 >>> from sklearn.covariance import oas 
 >>> X = [[1, 2], [3, 4]] 
 >>> oas(X) 
 (array([[ 1.0,  0.0], 
 [ 0.0,  1.0]]), 0.0)'","'Estimate covariance with the Oracle Approximating Shrinkage algorithm. 
 Parameters 
 X : array-like, shape (n_samples, n_features) 
 Data from which to compute the covariance estimate. 
 assume_centered : boolean 
 If True, data are not centered before computation. 
 Useful to work with data whose mean is significantly equal to 
 zero but is not exactly zero. 
 If False, data are centered before computation. 
 Returns 
 shrunk_cov : array-like, shape (n_features, n_features) 
 Shrunk covariance. 
 shrinkage : float 
 Coefficient in the convex combination used for the computation 
 of the shrunk estimate. 
 Notes 
 The regularised (shrunk) covariance is: 
 (1 - shrinkage)*cov 
 + shrinkage * mu * np.identity(n_features) 
 where mu = trace(cov) / n_features 
 The formula we used to implement the OAS 
 does not correspond to the one given in the article. It has been taken 
 from the MATLAB program available from the author\'s webpage 
 (http://tbayes.eecs.umich.edu/yilun/covestimation).'"
"def virtualenv_no_global(): 
    site_mod_dir = os.path.dirname(os.path.abspath(site.__file__)) 
   no_global_file = os.path.join(site_mod_dir, 'no-global-site-packages.txt') 
   if (running_under_virtualenv() and os.path.isfile(no_global_file)): 
      return True"," 'Return True if running under a virtualenv and no global site-packages 
 directory exists.'",'Return True if in a venv and no system site packages.'
"def pkt_line(data): 
    if (data is None): 
      return '0000' 
   return (('%04x' % (len(data) + 4)).encode('ascii') + data)"," 'Convert a packet to a string. 
 :param data: The packet to convert to a string. 
 :type data: bytes 
 :return: The packet as a string. 
 :rtype: str'","'Wrap data in a pkt-line. 
 :param data: The data to wrap, as a str or None. 
 :return: The data prefixed with its length in pkt-line format; if data was 
 None, returns the flush-pkt (\'0000\').'"
"def assert_raises_regexp(exception, reg, run, *args, **kwargs): 
    __tracebackhide__ = True 
   try: 
      run(*args, **kwargs) 
      assert False, ('%s   should   have   been   thrown' % exception) 
   except exception: 
      e = sys.exc_info()[1] 
      p = re.compile(reg) 
      assert p.search(str(e)), str(e)"," 'Assert that the given exception is raised with a given regular expression 
 in the message. 
 :param exception: The exception to raise 
 :param reg: The regular expression to match 
 :param run: The function to run 
 :param *args: The arguments to pass to the function 
 :param **kwargs: The keyword arguments to pass to the function 
 :raise AssertionError: If the exception is not raised 
 :raise AssertionError: If the message does not match the regular expression'",'Like assertRaisesRegexp in unittest'
"def network_list(call=None, **kwargs): 
    conn = get_conn() 
   return conn.network_list()", 'Returns a list of all networks.','List private networks'
"@fixture 
 def patch_network_functions(monkeypatch): 
    import inbox.actions.backends 
   for backend in inbox.actions.backends.module_registry.values(): 
      for method_name in backend.__all__: 
         monkeypatch.setattr(((backend.__name__ + '.') + method_name), (lambda *args, **kwargs: None))", 'Patches all network functions to do nothing.',"'Monkeypatch syncback functions that actually talk to Gmail so that the 
 tests can run faster.'"
"def mkdir_p(path): 
    try: 
      os.makedirs(path) 
   except OSError as exc: 
      if ((exc.errno == errno.EEXIST) and os.path.isdir(path)): 
         pass 
      else: 
         raise"," 'Create a directory, if it does not exist.'",'like `mkdir -p`'
"def get_docstring_and_rest(filename): 
    with open(filename, 'rb') as fid: 
      content = fid.read() 
   content = content.replace('\r\n', '\n') 
   try: 
      node = ast.parse(content) 
   except SyntaxError: 
      return (SYNTAX_ERROR_DOCSTRING, content.decode('utf-8')) 
   if (not isinstance(node, ast.Module)): 
      raise TypeError('This   function   only   supports   modules.   You   provided   {0}'.format(node.__class__.__name__)) 
   if (node.body and isinstance(node.body[0], ast.Expr) and isinstance(node.body[0].value, ast.Str)): 
      docstring_node = node.body[0] 
      docstring = docstring_node.value.s 
      if hasattr(docstring, 'decode'): 
         docstring = docstring.decode('utf-8') 
      rest = content.decode('utf-8').split('\n', docstring_node.lineno)[(-1)] 
      return (docstring, rest) 
   else: 
      raise ValueError('Could   not   find   docstring   in   file   ""{0}"".   A   docstring   is   required   by   sphinx-gallery'.format(filename))", 'Return the docstring and the rest of the file.',"'Separate `filename` content between docstring and the rest 
 Strongly inspired from ast.get_docstring. 
 Returns 
 docstring: str 
 docstring of `filename` 
 rest: str 
 `filename` content without the docstring'"
"@pytest.mark.parametrize('constructor,   attrs,   expected', [(False, {}, '<test_utils.Obj>'), (False, {'foo': None}, '<test_utils.Obj   foo=None>'), (False, {'foo': ""b'ar"", 'baz': 2}, '<test_utils.Obj   baz=2   foo=""b\'ar"">'), (True, {}, 'test_utils.Obj()'), (True, {'foo': None}, 'test_utils.Obj(foo=None)'), (True, {'foo': ""te'st"", 'bar': 2}, 'test_utils.Obj(bar=2,   foo=""te\'st"")')]) 
 def test_get_repr(constructor, attrs, expected): 
    assert (utils.get_repr(Obj(), constructor, **attrs) == expected)", 'Test get_repr.','Test get_repr().'
"@treeio_login_required 
 @handle_response_format 
 def ordered_product_delete(request, ordered_product_id, response_format='html'): 
    ordered_product = get_object_or_404(OrderedProduct, pk=ordered_product_id) 
   if ((not request.user.profile.has_permission(ordered_product, mode='w')) and (not request.user.profile.is_admin('treeio.sales'))): 
      return user_denied(request, ""You   don't   have   access   to   this   Sale   Status"", response_format) 
   if request.POST: 
      if ('delete' in request.POST): 
         order_id = ordered_product.order_id 
         if ('trash' in request.POST): 
            ordered_product.trash = True 
            ordered_product.save() 
         else: 
            ordered_product.delete() 
         ordered_product.order.update_total() 
         return HttpResponseRedirect(reverse('sales_order_view', args=[order_id])) 
      elif ('cancel' in request.POST): 
         return HttpResponseRedirect(reverse('sales_ordered_product_view', args=[ordered_product.id])) 
   order = ordered_product.order 
   return render_to_response('sales/ordered_product_delete', {'ordered_product': ordered_product, 'order': order}, context_instance=RequestContext(request), response_format=response_format)"," 'Delete an Ordered Product. 
 This view deletes an Ordered Product from the database. 
 :param request: The current request. 
 :param ordered_product_id: The ID of the Ordered Product to delete. 
 :param response_format: The format of the response. 
 :return: A response in the specified format. 
 :rtype: :class:`django.http.HttpResponse` 
 .. versionchanged:: 0.5 
 Adds `response_format` parameter.'",'OrderedProduct delete'
"def snmp_preprocessor(a_device, oid='.1.3.6.1.2.1.1.1.0'): 
    if (not (a_device.snmp_credentials.snmp_mode == 'snmp3')): 
      raise ValueError('Invalid   SNMP   mode   in   config_detect   {}'.format(a_device.snmp_credentials.snmp_mode)) 
   snmp_device = (a_device.ip_address, a_device.snmp_port) 
   snmp_user = (a_device.snmp_credentials.username, a_device.snmp_credentials.auth_key, a_device.snmp_credentials.encrypt_key) 
   auth_proto = a_device.snmp_credentials.auth_proto 
   encrypt_proto = a_device.snmp_credentials.encrypt_proto 
   return {'snmp_device': snmp_device, 'snmp_user': snmp_user, 'oid': oid, 'auth_proto': auth_proto, 'encrypt_proto': encrypt_proto}", 'Returns a dictionary with SNMP information.',"'Extract snmp parameters from NetworkDevice object 
 Only supports SNMPv3'"
"def _can_do_sum_of_squares(n, k): 
    if (k < 1): 
      return False 
   if (n < 0): 
      return False 
   if (n == 0): 
      return True 
   if (k == 1): 
      return is_square(n) 
   if (k == 2): 
      if (n in (1, 2)): 
         return True 
      if isprime(n): 
         if ((n % 4) == 1): 
            return 1 
         return False 
      else: 
         f = factorint(n) 
         for (p, m) in f.items(): 
            if (((p % 4) == 3) and (m % 2)): 
               return False 
         return True 
   if (k == 3): 
      if (((n // (4 ** multiplicity(4, n))) % 8) == 7): 
         return False 
   return True", 'Return True if n is a sum of squares of integers k times or less.',"'Return True if n can be written as the sum of k squares, 
 False if it can\'t, or 1 if k == 2 and n is prime (in which 
 case it *can* be written as a sum of two squares). A False 
 is returned only if it can\'t be written as k-squares, even 
 if 0s are allowed.'"
"@require_context 
 @require_volume_exists 
 def volume_glance_metadata_create(context, volume_id, key, value, session=None): 
    if (session is None): 
      session = get_session() 
   with session.begin(): 
      rows = session.query(models.VolumeGlanceMetadata).filter_by(volume_id=volume_id).filter_by(key=key).filter_by(deleted=False).all() 
      if (len(rows) > 0): 
         raise exception.GlanceMetadataExists(key=key, volume_id=volume_id) 
      vol_glance_metadata = models.VolumeGlanceMetadata() 
      vol_glance_metadata.volume_id = volume_id 
      vol_glance_metadata.key = key 
      vol_glance_metadata.value = value 
      vol_glance_metadata.save(session=session) 
   return", 'Create glance metadata on volume.',"'Update the Glance metadata for a volume by adding a new key:value pair. 
 This API does not support changing the value of a key once it has been 
 created.'"
"def _tile_perimeter_width(coord, projection): 
    perimeter = _tile_perimeter(coord, projection, False) 
   return (perimeter[8][0] - perimeter[0][0])", 'Return the width of the tile perimeter in pixels.',"'Get the width in projected coordinates of the coordinate tile polygon. 
 Uses _tile_perimeter().'"
"def update_connection_pool(maxsize=1): 
    get_pool().connection_pool_kw.update(maxsize=maxsize)", 'Update the connection pool\'s maxsize.',"'Update the global connection pool manager parameters. 
 maxsize: Number of connections to save that can be reused (default=1). 
 More than 1 is useful in multithreaded situations.'"
"@LocalContext 
 def alphanumeric(raw_bytes, *a, **kw): 
    return encode(raw_bytes, expr=re_alphanumeric, *a, **kw)"," 'Returns the bytes encoded as alphanumeric characters. 
 This function is equivalent to:: 
 encode(raw_bytes, expr=re_alphanumeric, *a, **kw)'","'alphanumeric(raw_bytes) -> str 
 Encode the shellcode ``raw_bytes`` such that it does not 
 contain any bytes except for [A-Za-z0-9]. 
 Accepts the same arguments as :func:`encode`.'"
"def libvlc_media_list_set_media(p_ml, p_md): 
    f = (_Cfunctions.get('libvlc_media_list_set_media', None) or _Cfunction('libvlc_media_list_set_media', ((1,), (1,)), None, None, MediaList, Media)) 
   return f(p_ml, p_md)"," 'Set media in media list. 
 @param p_ml: media list. 
 @param p_md: media to set. 
 @return: 0 on success, -1 on error.'","'Associate media instance with this media list instance. 
 If another media instance was present it will be released. 
 The L{libvlc_media_list_lock} should NOT be held upon entering this function. 
 @param p_ml: a media list instance. 
 @param p_md: media instance to add.'"
"def _GetPdbPath(target_dict, config_name, vars): 
    config = target_dict['configurations'][config_name] 
   msvs = config.setdefault('msvs_settings', {}) 
   linker = msvs.get('VCLinkerTool', {}) 
   pdb_path = linker.get('ProgramDatabaseFile') 
   if pdb_path: 
      return pdb_path 
   variables = target_dict.get('variables', {}) 
   pdb_path = variables.get('msvs_large_pdb_path', None) 
   if pdb_path: 
      return pdb_path 
   pdb_base = target_dict.get('product_name', target_dict['target_name']) 
   pdb_base = ('%s.%s.pdb' % (pdb_base, TARGET_TYPE_EXT[target_dict['type']])) 
   pdb_path = ((vars['PRODUCT_DIR'] + '/') + pdb_base) 
   return pdb_path"," 'Get the path to the pdb file for a given target. 
 :param target_dict: A dict containing the target\'s properties 
 :param config_name: The name of the configuration 
 :param vars: The variables dict 
 :returns: The path to the pdb file'","'Returns the path to the PDB file that will be generated by a given 
 configuration. 
 The lookup proceeds as follows: 
 - Look for an explicit path in the VCLinkerTool configuration block. 
 - Look for an \'msvs_large_pdb_path\' variable. 
 - Use \'<(PRODUCT_DIR)/<(product_name).(exe|dll).pdb\' if \'product_name\' is 
 specified. 
 - Use \'<(PRODUCT_DIR)/<(target_name).(exe|dll).pdb\'. 
 Arguments: 
 target_dict: The target dictionary to be searched. 
 config_name: The name of the configuration of interest. 
 vars: A dictionary of common GYP variables with generator-specific values. 
 Returns: 
 The path of the corresponding PDB file.'"
"@constructor 
 def max(x, axis=None, keepdims=False): 
    try: 
      out = max_and_argmax(x, axis)[0] 
   except Exception: 
      out = CAReduce(scal.maximum, axis)(x) 
   if keepdims: 
      out = makeKeepDims(x, out, axis) 
   return out"," 'Computes the maximum over the specified axis. 
 Parameters 
 x : array_like 
 An array of values. 
 axis : int, optional 
 The axis along which the maximum is computed. 
 If None, the maximum is computed over all axes. 
 keepdims : bool, optional 
 If True, axes that are reduced are left in the result as dimensions 
 with size one. If False, reduced axes are removed from the result. 
 Returns 
 out : ndarray 
 The maximum value. 
 Notes 
 This is the same as `numpy.amax` for non-recursive arrays. 
 Examples 
 >>> x = np.array([1, 2, 3, 4]) 
 >>> max(x) 
 4 
 >>> max(x, axis=1) 
 4 
 >>> max(x, axis=0) 
 4 
 >>> max(x, axis=None) 
 4 
 >>> max(x, axis=(-1)) 
 4 
 >>> max(x, axis=(-2)) 
 4 
 >>> max","'Returns maximum elements obtained by iterating over given axis. 
 When axis is None (the default value), the max is performed 
 over the flattened tensor. 
 Parameters 
 keepdims: bool 
 If this is set to True, the axes which are reduced are left in 
 the result as dimensions with size one. With this option, the result 
 will broadcast correctly against the original tensor. 
 Notes 
 We return an error as numpy when we reduce a dim with a shape of 0.'"
"def staff_member_required(view_func): 
    def _checklogin(request, *args, **kwargs): 
      if (request.user.is_authenticated() and request.user.is_staff): 
         return view_func(request, *args, **kwargs) 
      assert hasattr(request, 'session'), ""The   Django   admin   requires   session   middleware   to   be   installed.   Edit   your   MIDDLEWARE_CLASSES   setting   to   insert   'django.contrib.sessions.middleware.SessionMiddleware'."" 
      if (not request.POST.has_key(LOGIN_FORM_KEY)): 
         if request.POST: 
            message = _('Please   log   in   again,   because   your   session   has   expired.') 
         else: 
            message = '' 
         return _display_login_form(request, message) 
      if (not request.session.test_cookie_worked()): 
         message = _(""Looks   like   your   browser   isn't   configured   to   accept   cookies.   Please   enable   cookies,   reload   this   page,   and   try   again."") 
         return _display_login_form(request, message) 
      username = request.POST.get('username', None) 
      password = request.POST.get('password', None) 
      user = authenticate(username=username, password=password) 
      if (user is None): 
         message = ERROR_MESSAGE 
         if ('@' in username): 
            try: 
               user = User.objects.get(email=username) 
            except User.DoesNotExist: 
               message = _(""Usernames   cannot   contain   the   '@'   character."") 
            else: 
               message = (_(""Your   e-mail   address   is   not   your   username.   Try   '%s'   instead."") % user.username) 
         return _display_login_form(request, message) 
      elif (user.is_active and user.is_staff): 
         login(request, user) 
         user.last_login = datetime.datetime.now() 
         user.save() 
         return http.HttpResponseRedirect(request.path) 
      else: 
         return _display_login_form(request, ERROR_MESSAGE) 
   return _checklogin"," 'Decorator that requires a user to be logged in and a staff member. 
 This decorator checks that the user is logged in and a staff member, and 
 redirects the user to the login page if they are not. 
 This decorator is meant to be used on views that require a user to be 
 logged in and a staff member. 
 This decorator does not check if the user is a staff member. 
 This decorator is used on views that require a user to be logged in and a 
 staff member. 
 This decorator does not check if the user is a staff member. 
 This decorator is used on views that require a user to be logged in and a 
 staff member. 
 This decorator does not check if the user is a staff member.'","'Decorator for views that checks that the user is logged in and is a staff 
 member, displaying the login page if necessary.'"
"def _is_axial_coil(coil): 
    is_ax = (coil['coil_class'] in (FIFF.FWD_COILC_MAG, FIFF.FWD_COILC_AXIAL_GRAD, FIFF.FWD_COILC_AXIAL_GRAD2)) 
   return is_ax", 'Check if a coil is an axial coil.','Determine if the coil is axial.'
"@public 
 def guess_generating_function_rational(v, X=Symbol('x')): 
    q = find_simple_recurrence_vector(v) 
   n = len(q) 
   if (n <= 1): 
      return None 
   p = [sum(((v[(i - k)] * q[k]) for k in range(min((i + 1), n)))) for i in range(len(v))] 
   return (sum(((p[k] * (X ** k)) for k in range(len(p)))) / sum(((q[k] * (X ** k)) for k in range(n))))"," 'Guess a generating function for a vector of rational numbers. 
 The function will return a generating function if one exists. 
 Examples 
 >>> from sympy.polys.polytools import guess_generating_function_rational 
 >>> from sympy import Symbol, Eq, ZZ 
 >>> v = [ZZ(1)/ZZ(2), ZZ(3)/ZZ(2), ZZ(5)/ZZ(2)] 
 >>> guess_generating_function_rational(v) 
 1/(x - 2) 
 >>> v = [ZZ(1)/ZZ(2), ZZ(1)/ZZ(2), ZZ(1)/ZZ(2)] 
 >>> guess_generating_function_rational(v) 
 1/(x - 2) 
 >>> v = [ZZ(1)/ZZ(2), ZZ(1)/ZZ(2), ZZ(1)/ZZ(2), ZZ(1)/ZZ(2)] 
 >>> guess_generating_function_rational(v) 
 1/(x - ","'Tries to ""guess"" a rational generating function for a sequence of rational 
 numbers v. 
 Examples 
 >>> from sympy.concrete.guess import guess_generating_function_rational 
 >>> from sympy import fibonacci 
 >>> l = [fibonacci(k) for k in range(5,15)] 
 >>> guess_generating_function_rational(l) 
 (3*x + 5)/(-x**2 - x + 1) 
 See also 
 See function sympy.series.approximants and mpmath.pade'"
"def generate_strings(total_strings, string_length=20): 
    statements = [] 
   for _ in range(0, total_strings): 
      text = ''.join((random.choice(((string.ascii_letters + string.digits) + '   ')) for _ in range(string_length))) 
      statements.append(text) 
   return statements", 'Generate random strings of length `string_length`','Generate a list of random strings.'
"def parse_requirements(strs): 
    lines = iter(yield_lines(strs)) 
   def scan_list(ITEM, TERMINATOR, line, p, groups, item_name): 
      items = [] 
      while (not TERMINATOR(line, p)): 
         if CONTINUE(line, p): 
            try: 
               line = next(lines) 
               p = 0 
            except StopIteration: 
               raise ValueError('\\   must   not   appear   on   the   last   nonblank   line') 
         match = ITEM(line, p) 
         if (not match): 
            msg = (('Expected   ' + item_name) + '   in') 
            raise ValueError(msg, line, 'at', line[p:]) 
         items.append(match.group(*groups)) 
         p = match.end() 
         match = COMMA(line, p) 
         if match: 
            p = match.end() 
         elif (not TERMINATOR(line, p)): 
            msg = ""Expected   ','   or   end-of-list   in"" 
            raise ValueError(msg, line, 'at', line[p:]) 
      match = TERMINATOR(line, p) 
      if match: 
         p = match.end() 
      return (line, p, items) 
   for line in lines: 
      match = DISTRO(line) 
      if (not match): 
         raise ValueError('Missing   distribution   spec', line) 
      project_name = match.group(1) 
      p = match.end() 
      extras = [] 
      match = OBRACKET(line, p) 
      if match: 
         p = match.end() 
         (line, p, extras) = scan_list(DISTRO, CBRACKET, line, p, (1,), ""'extra'   name"") 
      (line, p, specs) = scan_list(VERSION, LINE_END, line, p, (1, 2), 'version   spec') 
      specs = [(op, safe_version(val)) for (op, val) in specs] 
      (yield Requirement(project_name, specs, extras))"," 'Parse a requirements file and return a list of Requirement objects. 
 Each Requirement object is a tuple with three parts: 
 - a project name (e.g. \'setuptools\') 
 - a version spec (e.g. \'<1.0a1\') 
 - a list of extras (e.g. \'[distribute, six]\' 
 This function is called by :func:`parse_requirements_file` and 
 :func:`parse_requirements_file_from_string`. 
 :param strs: a string containing a requirements file 
 :returns: a list of Requirement objects'","'Yield ``Requirement`` objects for each specification in `strs` 
 `strs` must be a string, or a (possibly-nested) iterable thereof.'"
"def dynamize_value(val): 
    dynamodb_type = get_dynamodb_type(val) 
   if (dynamodb_type == 'N'): 
      val = {dynamodb_type: serialize_num(val)} 
   elif (dynamodb_type == 'S'): 
      val = {dynamodb_type: val} 
   elif (dynamodb_type == 'NS'): 
      val = {dynamodb_type: list(map(serialize_num, val))} 
   elif (dynamodb_type == 'SS'): 
      val = {dynamodb_type: [n for n in val]} 
   elif (dynamodb_type == 'B'): 
      if isinstance(val, bytes): 
         val = Binary(val) 
      val = {dynamodb_type: val.encode()} 
   elif (dynamodb_type == 'BS'): 
      val = {dynamodb_type: [n.encode() for n in val]} 
   return val"," 'Dynamize a value for DynamoDB. 
 DynamoDB supports only a subset of Python types. 
 This function takes a value and returns a dict with the 
 DynamoDB type as the key and the value as the value. 
 :param val: The value to dynamize 
 :type val: object 
 :return: The dynamized value 
 :rtype: dict'","'Take a scalar Python value and return a dict consisting 
 of the Amazon DynamoDB type specification and the value that 
 needs to be sent to Amazon DynamoDB.  If the type of the value 
 is not supported, raise a TypeError'"
"def add_completer(widget, items): 
    completer = QtWidgets.QCompleter(items, widget) 
   completer.setCaseSensitivity(Qt.CaseInsensitive) 
   completer.setCompletionMode(QtWidgets.QCompleter.InlineCompletion) 
   widget.setCompleter(completer)"," 'Adds a completion widget to the given widget. 
 :param widget: The widget to add the completion to. 
 :param items: The items to be used by the completion widget. 
 :type items: list(str)'",'Add simple completion to a widget'
"def _compute_hash_v1(get_deps_dict, hash): 
    uniquedeps = get_deps_dict['uniquedeps'] 
   spec = get_deps_dict['spec'] 
   hash.update(spec.text) 
   for d in uniquedeps: 
      hash.update(roslib.msgs.get_registered(d).text) 
   return hash.hexdigest()", 'Computes the hash of the package for use in the registry.',"'subroutine of compute_md5_v1() 
 @param get_deps_dict: dictionary returned by get_dependencies call 
 @type  get_deps_dict: dict 
 @param hash: hash instance 
 @type  hash: hash instance'"
"def negative_sampling(x, t, W, sampler, sample_size): 
    return NegativeSamplingFunction(sampler, sample_size)(x, t, W)"," 'Negative sampling of a sequence x, using a sampling function sampler 
 and a sample size sample_size. 
 Parameters 
 x : a sequence 
 t : a sequence 
 W : a sequence 
 sampler : a function 
 sample_size : an integer 
 Returns 
 y : a sequence'","'Negative sampling loss function. 
 In natural language processing, especially language modeling, the number of 
 words in a vocabulary can be very large. 
 Therefore, you need to spend a lot of time calculating the gradient of the 
 embedding matrix. 
 By using the negative sampling trick you only need to calculate the 
 gradient for a few sampled negative examples. 
 The objective function is below: 
 .. math:: 
 f(x, p) = \log \sigma(x^\top w_p) + \ 
 k E_{i \sim P(i)}[\log \sigma(- x^\top w_i)], 
 where :math:`\sigma(\cdot)` is a sigmoid function, :math:`w_i` is the 
 weight vector for the word :math:`i`, and :math:`p` is a positive example. 
 It is approximeted with :math:`k` examples :math:`N` sampled from 
 probability :math:`P(i)`, like this: 
 .. math:: 
 f(x, p) \approx \log \sigma(x^\top w_p) + \ 
 \sum_{n \in N} \log \sigma(-x^\top w_n). 
 Each sample of :math:`N` is drawn from the word distribution :math:`P(w)`. 
 This is calculated as :math:`P(w) = \frac{1}{Z} c(w)^\alpha`, where 
 :math:`c(w)` is the unigram count of the word :math:`w`, :math:`\alpha` is 
 a hyper-parameter, and :math:`Z` is the normalization constant. 
 Args: 
 x (~chainer.Variable): Batch of input vectors. 
 t (~chainer.Variable): Vector of groundtruth labels. 
 W (~chainer.Variable): Weight matrix. 
 sampler (function): Sampling function. It takes a shape and returns an 
 integer array of the shape. Each element of this array is a sample 
 from the word distribution. A :class:`~chainer.utils.WalkerAlias` 
 object built with the power distribution of word frequency is 
 recommended. 
 sample_size (int): Number of samples. 
 See: `Distributed Representations of Words and Phrases and their         Compositionality <http://arxiv.org/abs/1310.4546>`_ 
 .. seealso:: :class:`~chainer.links.NegativeSampling`.'"
"@sensitive_post_parameters() 
 @csrf_protect 
 @never_cache 
 def login(request, template_name='registration/login.html', redirect_field_name=REDIRECT_FIELD_NAME, authentication_form=AuthenticationForm, current_app=None, extra_context=None): 
    redirect_to = request.REQUEST.get(redirect_field_name, '') 
   if (request.method == 'POST'): 
      form = authentication_form(data=request.POST) 
      if form.is_valid(): 
         if (not is_safe_url(url=redirect_to, host=request.get_host())): 
            redirect_to = resolve_url(settings.LOGIN_REDIRECT_URL) 
         auth_login(request, form.get_user()) 
         if request.session.test_cookie_worked(): 
            request.session.delete_test_cookie() 
         return HttpResponseRedirect(redirect_to) 
   else: 
      form = authentication_form(request) 
   request.session.set_test_cookie() 
   current_site = get_current_site(request) 
   context = {'form': form, redirect_field_name: redirect_to, 'site': current_site, 'site_name': current_site.name} 
   if (extra_context is not None): 
      context.update(extra_context) 
   return TemplateResponse(request, template_name, context, current_app=current_app)"," 'Logs in the user and returns the user to the specified redirect URL. 
 If the user is not logged in, the user is redirected to the login page. 
 The redirect URL can be overridden using the REDIRECT_FIELD_NAME request 
 variable. 
 If the user is logged in, the user is redirected to the specified redirect URL. 
 If the user is not logged in, the user is redirected to the login page. 
 The redirect URL can be overridden using the REDIRECT_FIELD_NAME request 
 variable. 
 If the user is logged in, the user is redirected to the specified redirect URL. 
 If the user is not logged in, the user is redirected to the login page. 
 The redirect URL can be overridden using the REDIRECT_FIELD_NAME request 
 variable. 
 If the user is logged in, the user is redirected to the specified redirect URL. 
 If the user is not logged in, the user is redirected to the login page. 
 The redirect URL can be overridden using the REDIRECT_FIELD_NAME request 
 variable.",'Displays the login form and handles the login action.'
"def split_title(title, width, title_fs): 
    titles = [] 
   if (not title): 
      return titles 
   size = reverse_text_len(width, (title_fs * 1.1)) 
   title_lines = title.split('\n') 
   for title_line in title_lines: 
      while (len(title_line) > size): 
         title_part = title_line[:size] 
         i = title_part.rfind('   ') 
         if (i == (-1)): 
            i = len(title_part) 
         titles.append(title_part[:i]) 
         title_line = title_line[i:].strip() 
      titles.append(title_line) 
   return titles"," 'Split a title into multiple lines, fitting each line into a given width. 
 Parameters 
 title : str 
 The title to split. 
 width : int 
 The width in characters to fit each line into. 
 title_fs : float 
 The font size to use when calculating the width of each line. 
 Returns 
 titles : list of str 
 The split titles. 
 Raises 
 ValueError 
 If the title cannot be split into multiple lines.'",'Split a string for a specified width and font size'
"def hpsModelSynth(hfreq, hmag, hphase, stocEnv, N, H, fs): 
    yh = SM.sineModelSynth(hfreq, hmag, hphase, N, H, fs) 
   yst = STM.stochasticModelSynth(stocEnv, H, (H * 2)) 
   y = (yh[:min(yh.size, yst.size)] + yst[:min(yh.size, yst.size)]) 
   return (y, yh, yst)"," 'Synthesize a harmonic sinusoid (H) and a stochastic process (ST) 
 using the harmonic model synthesis method. 
 Parameters 
 hfreq : ndarray 
 The harmonic frequencies. 
 hmag : ndarray 
 The harmonic magnitudes. 
 hphase : ndarray 
 The harmonic phases. 
 stocEnv : ndarray 
 The stochastic process envelope. 
 N : int 
 The number of harmonics. 
 H : int 
 The harmonic bandwidth. 
 fs : float 
 The sampling frequency. 
 Returns 
 y : ndarray 
 The synthesized signal. 
 yh : ndarray 
 The harmonic model synthesis result. 
 yst : ndarray 
 The stochastic process synthesis result. 
 Notes 
 This function is a wrapper for the harmonic model synthesis method. 
 The stochastic process synthesis result is not used in the harmonic model 
 synthesis method, but is returned for the convenience of the user. 
 References 
 .. [1] H.","'Synthesis of a sound using the harmonic plus stochastic model 
 hfreq, hmag: harmonic frequencies and amplitudes; stocEnv: stochastic envelope 
 Ns: synthesis FFT size; H: hop size, fs: sampling rate 
 returns y: output sound, yh: harmonic component, yst: stochastic component'"
"def to_nnf(expr, simplify=True): 
    if is_nnf(expr, simplify): 
      return expr 
   return expr.to_nnf(simplify)"," 'Converts a boolean expression to a nnf expression. 
 This function is similar to :func:`to_cnf`, except that it 
 returns a nnf expression instead of a cnf expression. 
 >>> from sympy.logic.booleanalg import to_cnf, to_nnf 
 >>> from sympy.logic.boolalg import nnf, cnf 
 >>> expr = (x & (x | (~ x))) 
 >>> to_cnf(expr) 
 (x & (x | ~x)) 
 >>> to_nnf(expr) 
 (x & (x | ~x)) 
 >>> expr = (x & (x | (~ x))) 
 >>> to_cnf(expr, simplify=False) 
 (x & (x | ~x)) 
 >>> to_nnf(expr, simplify=False) 
 (x & (x | ~x)) 
 >>> expr = (x & (x | (~ x))) 
 >>> to_cnf(expr, simplify=True) 
 (x & (x | ~x)) 
 >>> to_nn","'Converts expr to Negation Normal Form. 
 A logical expression is in Negation Normal Form (NNF) if it 
 contains only And, Or and Not, and Not is applied only to literals. 
 If simpify is True, the result contains no redundant clauses. 
 Examples 
 >>> from sympy.abc import A, B, C, D 
 >>> from sympy.logic.boolalg import Not, Equivalent, to_nnf 
 >>> to_nnf(Not((~A & ~B) | (C & D))) 
 And(Or(A, B), Or(Not(C), Not(D))) 
 >>> to_nnf(Equivalent(A >> B, B >> A)) 
 And(Or(A, And(A, Not(B)), Not(B)), Or(And(B, Not(A)), B, Not(A)))'"
"def fromstring(*args, **kwargs): 
    global ET 
   _bootstrap() 
   return ET.fromstring(*args, **kwargs)", 'Construct a new ElementTree object from a string.',"'Helper func to provide easy access to the (possibly) moving target that is 
 C{ET}.'"
"@pytest.mark.django_db 
 def test_submit_with_suggestion_and_comment(client, request_users, settings): 
    settings.POOTLE_CAPTCHA_ENABLED = False 
   Comment = get_comment_model() 
   unit = Unit.objects.filter(suggestion__state='pending', state=UNTRANSLATED)[0] 
   sugg = Suggestion.objects.filter(unit=unit, state='pending')[0] 
   user = request_users['user'] 
   if (user.username != 'nobody'): 
      client.login(username=user.username, password=request_users['password']) 
   url = ('/xhr/units/%d/' % unit.id) 
   edited_target = ('Edited   %s' % sugg.target_f) 
   comment = 'This   is   a   comment!' 
   response = client.post(url, {'state': False, 'target_f_0': edited_target, 'suggestion': sugg.id, 'comment': comment}, HTTP_X_REQUESTED_WITH='XMLHttpRequest') 
   if check_permission('translate', response.wsgi_request): 
      assert (response.status_code == 200) 
      content = json.loads(response.content) 
      assert (content['newtargets'] == [edited_target]) 
      assert (content['user_score'] == response.wsgi_request.user.public_score) 
      assert (content['checks'] is None) 
      accepted_suggestion = Suggestion.objects.get(id=sugg.id) 
      updated_unit = Unit.objects.get(id=unit.id) 
      assert (accepted_suggestion.state == 'accepted') 
      assert (str(updated_unit.target) == edited_target) 
      assert (Comment.objects.for_model(accepted_suggestion).get().comment == comment) 
   else: 
      assert (response.status_code == 403)", 'Submit a suggestion with comment and comment is added to the suggestion.','Tests translation can be applied after suggestion is accepted.'
"def get_app(services, registry_path=forms.DEFAULT_REGISTRY_PATH, debug=False, config=None): 
    mappings = service_mapping(services, registry_path=registry_path) 
   return webapp2.WSGIApplication(routes=mappings, debug=debug, config=config)"," 'Creates an app using the given services. 
 :param services: a list of services to use. 
 :param registry_path: a path to a registry file containing service mappings. 
 :param debug: if True, will use the debug version of the services. 
 :param config: a config object. 
 :returns: an app object.'","'Returns a WSGI application configured for the given services. 
 Parameters are the same as :func:`service_mapping`, plus: 
 :param debug: 
 WSGI application debug flag: True to enable debug mode. 
 :param config: 
 WSGI application configuration dictionary.'"
"def getMatrixTetragridMatrix(matrixTetragrid, prefix, xmlElement): 
    matrixKey = (prefix + 'matrix') 
   evaluatedDictionary = evaluate.getEvaluatedDictionary([matrixKey], xmlElement) 
   if (len(evaluatedDictionary.keys()) < 1): 
      return matrixTetragrid 
   value = evaluatedDictionary[matrixKey] 
   if ((value == None) or (value == 'None')): 
      print 'Warning,   value   in   getMatrixTetragridMatrix   in   matrix   is   None   for   matrixKey   for   dictionary:' 
      print matrixKey 
      print evaluatedDictionary 
   else: 
      matrixTetragrid = getIdentityMatrixTetragrid(matrixTetragrid) 
      for (rowIndex, row) in enumerate(value): 
         for (elementIndex, element) in enumerate(row): 
            matrixTetragrid[rowIndex][elementIndex] = element 
   euclidean.removeListFromDictionary(xmlElement.attributeDictionary, [matrixKey]) 
   return matrixTetragrid"," 'Get the matrix for the tetragrid. 
 This function gets the matrix for the tetragrid from the XML file. 
 The matrix is the list of values for each element of the tetragrid. 
 The matrix is stored as a dictionary with the key being the matrix key 
 and the value being the list of values. 
 The matrix key is the prefix of the matrix key. 
 For example, if the matrix key is \'matrix\', then the prefix is \'matrix\'. 
 The matrix key is stored in the XML file as an attribute of the 
 \'matrix\' element. 
 The matrix is returned as a list of lists. 
 Parameters 
 matrixTetragrid : list 
 The list of lists for the tetragrid. 
 prefix : string 
 The prefix for the matrix key. 
 xmlElement : xml.etree.ElementTree.Element 
 The XML element for the tetragrid. 
 Returns 
 matrixTetragrid : list 
 The list of lists for the tetragrid. 
 Examples 
 >>> from nipype.interfaces.freesurfer import getMatrixTetragrid",'Get the matrix Tetragrid from the xmlElement matrix value.'
"def get_feature_permission(request, feature, operation=None): 
    network_config = getattr(settings, 'OPENSTACK_NEUTRON_NETWORK', {}) 
   feature_info = FEATURE_MAP.get(feature) 
   if (not feature_info): 
      raise ValueError(_(""The   requested   feature   '%(feature)s'   is   unknown.   Please   make   sure   to   specify   a   feature   defined   in   FEATURE_MAP."")) 
   feature_config = feature_info.get('config') 
   if feature_config: 
      if (not network_config.get(feature_config['name'], feature_config['default'])): 
         return False 
   feature_policies = feature_info.get('policies') 
   if feature_policies: 
      policy_name = feature_policies.get(operation) 
      if (not policy_name): 
         raise ValueError((_(""The   'operation'   parameter   for   get_feature_permission   '%(feature)s'   is   invalid.   It   should   be   one   of   %(allowed)s"") % {'feature': feature, 'allowed': '   '.join(feature_policies.keys())})) 
      role = (('network', policy_name),) 
      if (not policy.check(role, request)): 
         return False 
   feature_extension = feature_info.get('extension') 
   if feature_extension: 
      try: 
         return is_extension_supported(request, feature_extension) 
      except Exception: 
         msg = (_(""Failed   to   check   Neutron   '%s'   extension   is   not   supported"") % feature_extension) 
         LOG.info(msg) 
         return False 
   return True"," 'Checks if the request has the permission to perform the operation. 
 :param request: the request to check 
 :param feature: the feature to check 
 :param operation: the operation to check 
 :return: True if the request has the permission to perform the operation, 
 False otherwise.'","'Check if a feature-specific field can be displayed. 
 This method check a permission for a feature-specific field. 
 Such field is usually provided through Neutron extension. 
 :param request: Request Object 
 :param feature: feature name defined in FEATURE_MAP 
 :param operation (optional): Operation type. The valid value should be 
 defined in FEATURE_MAP[feature][\'policies\'] 
 It must be specified if FEATURE_MAP[feature] has \'policies\'.'"
"def _GetTimezone(timezone_string): 
    if (pytz is None): 
      if timezone_string: 
         raise ValueError('need   pytz   in   order   to   specify   a   timezone') 
      return None 
   if timezone_string: 
      return pytz.timezone(timezone_string) 
   else: 
      return pytz.utc"," 'Returns a pytz timezone object based on the string timezone_string. 
 If timezone_string is None, returns None. 
 If timezone_string is not None, returns a pytz timezone object. 
 If timezone_string is not a valid pytz timezone string, raises a ValueError.'","'Converts a timezone string to a pytz timezone object. 
 Arguments: 
 timezone_string: a string representing a timezone, or None 
 Returns: 
 a pytz timezone object, or None 
 Raises: 
 ValueError: if timezone_string is specified, but pytz module could not be 
 loaded'"
"def cmp_version(a, b): 
    a = a.split('.') 
   b = b.split('.') 
   for (va, vb) in zip(a, b): 
      ret = (int(va) - int(vb)) 
      if ret: 
         return ret 
   return (len(a) - len(b))"," 'Compares two versions. 
 :param a: A version 
 :param b: A version 
 :return: 0 if a == b, 1 if a > b, -1 if a < b 
 :rtype: int'",'Compare two version strings (eg 0.0.1.10 > 0.0.1.9).'
"def ToScatteredId(v): 
    if (v >= _MAX_SCATTERED_COUNTER): 
      raise datastore_errors.BadArgumentError(('counter   value   too   large   (%d)' % v)) 
   return ((_MAX_SEQUENTIAL_ID + 1) + long(ReverseBitsInt64((v << _SCATTER_SHIFT))))"," 'Convert a sequential id to a scattered id. 
 :param v: The sequential id. 
 :type v: int 
 :returns: The scattered id. 
 :rtype: int'","'Map counter value v to the scattered ID space. 
 Translate to scattered ID space, then reverse bits. 
 Args: 
 v: Counter value from which to produce ID. 
 Returns: 
 Integer ID. 
 Raises: 
 datastore_errors.BadArgumentError if counter value exceeds the range of 
 the scattered ID space.'"
"@handle_response_format 
 @treeio_login_required 
 def item_view(request, folderPath, itemPath, response_format='html'): 
    try: 
      item = KnowledgeItem.by_path(folderPath, itemPath) 
   except KnowledgeItem.DoesNotExist: 
      raise Http404 
   if (not item): 
      raise Http404 
   items = Object.filter_permitted(manager=KnowledgeItem.objects, user=request.user.profile, mode='r') 
   if (not request.user.profile.has_permission(item)): 
      return user_denied(request, message=""You   don't   have   access   to   this   Knowledge   Item"") 
   context = _get_default_context(request) 
   context.update({'items': items, 'item': item}) 
   return render_to_response('knowledge/item_view', context, context_instance=RequestContext(request), response_format=response_format)"," 'View a single Knowledge Item. 
 :param request: The HTTP request. 
 :param folderPath: The folder path of the item. 
 :param itemPath: The item path of the item. 
 :param response_format: The format of the response. 
 :returns: A response in the given format. 
 :rtype: :class:`django.http.HttpResponse`'",'Single knowledge item view page'
"def getFirstWord(splitLine): 
    if (len(splitLine) > 0): 
      return splitLine[0] 
   return ''", 'Returns the first word of a string.','Get the first word of a split line.'
"def create_and_check_dir(path): 
    if (not os.path.exists(path)): 
      os.makedirs(path) 
   elif (not os.access(path, os.W_OK)): 
      raise OSError('DATA_DIR   {0}   is   not   writable!'.format(path))", 'Create and check the existence of the given path.','Ensure directory exists and is writable by us'
"def validate_input(trans, error_map, param_values, page_param_map): 
    first = param_values['name1'] 
   second = param_values['name2'] 
   if (first == second): 
      error_map['name1'] = 'The   value   names   should   be   different.'", 'Validate the input parameters.',"'Validates the user input, before execution.'"
"def parse_date(string): 
    return get_i18n().parse_date(string)"," 'Parse a date string into a datetime object. 
 :param string: The date string to parse. 
 :returns: A datetime object or None if the string could not be parsed. 
 :rtype: datetime.datetime or None 
 :raises ValueError: If the string is not a valid date.'",'See :meth:`I18n.parse_date`'
"def test_nonexistent_options_listed_in_order(script, data): 
    result = script.pip('install', '--no-index', ('--find-links=' + data.find_links), 'simplewheel[nonexistent,   nope]', expect_stderr=True) 
   msg = ""      simplewheel   2.0   does   not   provide   the   extra   'nonexistent'\n      simplewheel   2.0   does   not   provide   the   extra   'nope'"" 
   assert (msg in result.stderr)"," 'Test that options that do not exist in the wheel are listed in the 
 error message'",'Warn the user for each extra that doesn\'t exist.'
"def build_dict(): 
    containers = dict([(c, (['all'] + (lxc.Container(c).get_config_item('lxc.group') or []))) for c in lxc.list_containers()]) 
   groups = set(sum([g for g in containers.values()], [])) 
   return dict([(g, {'hosts': [k for (k, v) in containers.items() if (g in v)], 'vars': {'ansible_connection': 'lxc'}}) for g in groups])", 'Builds a dictionary of groups and their hosts.',"'Returns a dictionary keyed to the defined LXC groups. All 
 containers, including the ones not in any group, are included in the 
 ""all"" group.'"
"def decryptAndCheckPubkeyPayload(data, address): 
    try: 
      (status, addressVersion, streamNumber, ripe) = decodeAddress(address) 
      readPosition = 20 
      (embeddedAddressVersion, varintLength) = decodeVarint(data[readPosition:(readPosition + 10)]) 
      readPosition += varintLength 
      (embeddedStreamNumber, varintLength) = decodeVarint(data[readPosition:(readPosition + 10)]) 
      readPosition += varintLength 
      storedData = data[20:readPosition] 
      if (addressVersion != embeddedAddressVersion): 
         logger.info('Pubkey   decryption   was   UNsuccessful   due   to   address   version   mismatch.') 
         return 'failed' 
      if (streamNumber != embeddedStreamNumber): 
         logger.info('Pubkey   decryption   was   UNsuccessful   due   to   stream   number   mismatch.') 
         return 'failed' 
      tag = data[readPosition:(readPosition + 32)] 
      readPosition += 32 
      signedData = data[8:readPosition] 
      encryptedData = data[readPosition:] 
      (toAddress, cryptorObject) = shared.neededPubkeys[tag] 
      if (toAddress != address): 
         logger.critical(('decryptAndCheckPubkeyPayload   failed   due   to   toAddress   mismatch.   This   is   very   peculiar.   toAddress:   %s,   address   %s' % (toAddress, address))) 
         return 'failed' 
      try: 
         decryptedData = cryptorObject.decrypt(encryptedData) 
      except: 
         logger.info('Pubkey   decryption   was   unsuccessful.') 
         return 'failed' 
      readPosition = 0 
      bitfieldBehaviors = decryptedData[readPosition:(readPosition + 4)] 
      readPosition += 4 
      publicSigningKey = ('\x04' + decryptedData[readPosition:(readPosition + 64)]) 
      readPosition += 64 
      publicEncryptionKey = ('\x04' + decryptedData[readPosition:(readPosition + 64)]) 
      readPosition += 64 
      (specifiedNonceTrialsPerByte, specifiedNonceTrialsPerByteLength) = decodeVarint(decryptedData[readPosition:(readPosition + 10)]) 
      readPosition += specifiedNonceTrialsPerByteLength 
      (specifiedPayloadLengthExtraBytes, specifiedPayloadLengthExtraBytesLength) = decodeVarint(decryptedData[readPosition:(readPosition + 10)]) 
      readPosition += specifiedPayloadLengthExtraBytesLength 
      storedData += decryptedData[:readPosition] 
      signedData += decryptedData[:readPosition] 
      (signatureLength, signatureLengthLength) = decodeVarint(decryptedData[readPosition:(readPosition + 10)]) 
      readPosition += signatureLengthLength 
      signature = decryptedData[readPosition:(readPosition + signatureLength)] 
      if highlevelcrypto.verify(signedData, signature, hexlify(publicSigningKey)): 
         logger.info('ECDSA   verify   passed   (within   decryptAndCheckPubkeyPayload)') 
      else: 
         logger.info('ECDSA   verify   failed   (within   decryptAndCheckPubkeyPayload)') 
         return 'failed' 
      sha = hashlib.new('sha512') 
      sha.update((publicSigningKey + publicEncryptionKey)) 
      ripeHasher = hashlib.new('ripemd160') 
      ripeHasher.update(sha.digest()) 
      embeddedRipe = ripeHasher.digest() 
      if (embeddedRipe != ripe): 
         logger.info('Pubkey   decryption   was   UNsuccessful   due   to   RIPE   mismatch.') 
         return 'failed' 
      logger.info(('within   decryptAndCheckPubkeyPayload,   addressVersion:   %s,   streamNumber:   %s   \n                                                            ripe   %s\n                                                            publicSigningKey   in   hex:   %s\n                                                            publicEncryptionKey   in   hex:   %s' % (addressVersion, streamNumber, hexlify(ripe), hexlify(publicSigningKey), hexlify(publicEncryptionKey)))) 
      t = (address, addressVersion, storedData, int(time.time()), 'yes') 
      sqlExecute('INSERT   INTO   pubkeys   VALUES   (?,?,?,?,?)', *t) 
      return 'successful' 
   except varintDecodeError as e: 
      logger.info('Pubkey   decryption   was   UNsuccessful   due   to   a   malformed   varint.') 
      return 'failed' 
   except Exception as e: 
      logger.critical(('Pubkey   decryption   was   UNsuccessful   because   of   an   unhandled   exception!   This   is   definitely   a   bug!   \n%s' % traceback.format_exc())) 
      return 'failed'"," 'Decrypts a pubkey payload and checks the integrity of the result. 
 This function decrypts the payload and checks that the result is valid. 
 It then checks that the result matches the stored data. 
 If the result matches the stored data, it will insert the result into the 
 database. 
 :param data: The data to be decrypted. 
 :param address: The address of the account. 
 :return: \'successful\' or \'failed\' 
 :raises: Exception'","'Version 4 pubkeys are encrypted. This function is run when we already have the 
 address to which we want to try to send a message. The \'data\' may come either 
 off of the wire or we might have had it already in our inventory when we tried 
 to send a msg to this particular address.'"
"def create_mac_string(num, splitter=u':'): 
    mac = hex(num)[2:] 
   if (mac[(-1)] == u'L'): 
      mac = mac[:(-1)] 
   pad = max((12 - len(mac)), 0) 
   mac = ((u'0' * pad) + mac) 
   mac = splitter.join([mac[x:(x + 2)] for x in range(0, 12, 2)]) 
   mac = mac.upper() 
   return mac"," 'Creates a MAC string from a number. 
 :param num: the number to convert 
 :param splitter: the character to use as a delimiter 
 :return: the MAC string'","'Return the mac address interpretation of num, 
 in the form eg \'00:11:22:33:AA:BB\'. 
 :param num: a 48-bit integer (eg from uuid.getnode) 
 :param spliiter: a string to join the hex pairs with'"
"def get_rising_items(omit_sr_ids, count=4): 
    all_rising = rising.get_all_rising() 
   candidate_sr_ids = {sr_id for (link, score, sr_id) in all_rising}.difference(omit_sr_ids) 
   link_fullnames = [link for (link, score, sr_id) in all_rising if (sr_id in candidate_sr_ids)] 
   link_fullnames_to_show = random_sample(link_fullnames, count) 
   rising_links = Link._by_fullname(link_fullnames_to_show, return_dict=False, data=True) 
   rising_items = [ExploreItem(TYPE_RISING, 'ris', Subreddit._byID(l.sr_id), l) for l in rising_links] 
   return rising_items", 'Returns a list of ExploreItems for the rising subreddits.','Get links that are rising right now.'
"def get_datasources(orgname=None, profile='grafana'): 
    if isinstance(profile, string_types): 
      profile = __salt__['config.option'](profile) 
   if orgname: 
      switch_org(orgname, profile) 
   response = requests.get('{0}/api/datasources'.format(profile['grafana_url']), auth=_get_auth(profile), headers=_get_headers(profile), timeout=profile.get('grafana_timeout', 3)) 
   if (response.status_code >= 400): 
      response.raise_for_status() 
   return response.json()"," 'Get all datasources from grafana 
 Returns: 
 dict: Datasources 
 Example: 
 .. code-block:: python 
 get_datasources()'","'List all datasources in an organisation. 
 orgname 
 Name of the organization. 
 profile 
 Configuration profile used to connect to the Grafana instance. 
 Default is \'grafana\'. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' grafana4.get_datasources <orgname>'"
"def _get_storage_path(path, app_id): 
    (_, _, app_id) = app_id.replace(':', '_').rpartition('~') 
   if (path is None): 
      for path in _generate_storage_paths(app_id): 
         try: 
            os.mkdir(path, 448) 
         except OSError as e: 
            if (e.errno == errno.EEXIST): 
               if ((sys.platform == 'win32') or (((os.stat(path).st_mode & 511) == 448) and os.path.isdir(path))): 
                  return path 
               else: 
                  continue 
            raise 
         else: 
            return path 
   elif (not os.path.exists(path)): 
      os.mkdir(path) 
      return path 
   elif (not os.path.isdir(path)): 
      raise IOError(('the   given   storage   path   %r   is   a   file,   a   directory   was   expected' % path)) 
   else: 
      return path"," 'Get a storage path for the given app id and path. 
 The path can be None, which means that a path should be generated. 
 If the path is not None, it must be a valid directory. 
 :param path: The path to use. 
 :param app_id: The app id to use. 
 :return: The storage path.'",'Returns a path to the directory where stub data can be stored.'
"def url_for(endpoint, **values): 
    appctx = _app_ctx_stack.top 
   reqctx = _request_ctx_stack.top 
   if (appctx is None): 
      raise RuntimeError('Attempted   to   generate   a   URL   without   the   application   context   being   pushed.   This   has   to   be   executed   when   application   context   is   available.') 
   if (reqctx is not None): 
      url_adapter = reqctx.url_adapter 
      blueprint_name = request.blueprint 
      if (not reqctx.request._is_old_module): 
         if (endpoint[:1] == '.'): 
            if (blueprint_name is not None): 
               endpoint = (blueprint_name + endpoint) 
            else: 
               endpoint = endpoint[1:] 
      elif ('.' not in endpoint): 
         if (blueprint_name is not None): 
            endpoint = ((blueprint_name + '.') + endpoint) 
      elif endpoint.startswith('.'): 
         endpoint = endpoint[1:] 
      external = values.pop('_external', False) 
   else: 
      url_adapter = appctx.url_adapter 
      if (url_adapter is None): 
         raise RuntimeError('Application   was   not   able   to   create   a   URL   adapter   for   request   independent   URL   generation.   You   might   be   able   to   fix   this   by   setting   the   SERVER_NAME   config   variable.') 
      external = values.pop('_external', True) 
   anchor = values.pop('_anchor', None) 
   method = values.pop('_method', None) 
   scheme = values.pop('_scheme', None) 
   appctx.app.inject_url_defaults(endpoint, values) 
   if (scheme is not None): 
      if (not external): 
         raise ValueError('When   specifying   _scheme,   _external   must   be   True') 
      url_adapter.url_scheme = scheme 
   try: 
      rv = url_adapter.build(endpoint, values, method=method, force_external=external) 
   except BuildError as error: 
      values['_external'] = external 
      values['_anchor'] = anchor 
      values['_method'] = method 
      return appctx.app.handle_url_build_error(error, endpoint, values) 
   if (anchor is not None): 
      rv += ('#' + url_quote(anchor)) 
   return rv"," 'Generate a URL for the given endpoint. 
 :param endpoint: The endpoint to generate a URL for. 
 :param values: A dict of values to be passed to the URL adapter. 
 :param _external: If True, the URL is generated for an external request. 
 :param _anchor: If present, the anchor is appended to the URL. 
 :param _method: If present, the method is appended to the URL. 
 :param _scheme: If present, the scheme is appended to the URL. 
 :returns: The generated URL. 
 :raises BuildError: If the URL adapter cannot build the URL. 
 :raises ValueError: If the scheme is specified when _external is False.'","'Generates a URL to the given endpoint with the method provided. 
 Variable arguments that are unknown to the target endpoint are appended 
 to the generated URL as query arguments.  If the value of a query argument 
 is `None`, the whole pair is skipped.  In case blueprints are active 
 you can shortcut references to the same blueprint by prefixing the 
 local endpoint with a dot (``.``). 
 This will reference the index function local to the current blueprint:: 
 url_for(\'.index\') 
 For more information, head over to the :ref:`Quickstart <url-building>`. 
 To integrate applications, :class:`Flask` has a hook to intercept URL build 
 errors through :attr:`Flask.build_error_handler`.  The `url_for` function 
 results in a :exc:`~werkzeug.routing.BuildError` when the current app does 
 not have a URL for the given endpoint and values.  When it does, the 
 :data:`~flask.current_app` calls its :attr:`~Flask.build_error_handler` if 
 it is not `None`, which can return a string to use as the result of 
 `url_for` (instead of `url_for`\'s default to raise the 
 :exc:`~werkzeug.routing.BuildError` exception) or re-raise the exception. 
 An example:: 
 def external_url_handler(error, endpoint, **values): 
 ""Looks up an external URL when `url_for` cannot build a URL."" 
 # This is an example of hooking the build_error_handler. 
 # Here, lookup_url is some utility function you\'ve built 
 # which looks up the endpoint in some external URL registry. 
 url = lookup_url(endpoint, **values) 
 if url is None: 
 # External lookup did not have a URL. 
 # Re-raise the BuildError, in context of original traceback. 
 exc_type, exc_value, tb = sys.exc_info() 
 if exc_value is error: 
 raise exc_type, exc_value, tb 
 else: 
 raise error 
 # url_for will use this result, instead of raising BuildError. 
 return url 
 app.build_error_handler = external_url_handler 
 Here, `error` is the instance of :exc:`~werkzeug.routing.BuildError`, and 
 `endpoint` and `**values` are the arguments passed into `url_for`.  Note 
 that this is for building URLs outside the current application, and not for 
 handling 404 NotFound errors. 
 .. versionadded:: 0.10 
 The `_scheme` parameter was added. 
 .. versionadded:: 0.9 
 The `_anchor` and `_method` parameters were added. 
 .. versionadded:: 0.9 
 Calls :meth:`Flask.handle_build_error` on 
 :exc:`~werkzeug.routing.BuildError`. 
 :param endpoint: the endpoint of the URL (name of the function) 
 :param values: the variable arguments of the URL rule 
 :param _external: if set to `True`, an absolute URL is generated. Server 
 address can be changed via `SERVER_NAME` configuration variable which 
 defaults to `localhost`. 
 :param _scheme: a string specifying the desired URL scheme. The `_external` 
 parameter must be set to `True` or a `ValueError` is raised. 
 :param _anchor: if provided this is added as anchor to the URL. 
 :param _method: if provided this explicitly specifies an HTTP method.'"
"def test_delayed_epochs(): 
    (raw, events, picks) = _get_data() 
   events = events[:10] 
   picks = np.concatenate([pick_types(raw.info, meg=True, eeg=True)[::22], pick_types(raw.info, meg=False, eeg=False, ecg=True, eog=True)]) 
   picks = np.sort(picks) 
   raw.load_data().pick_channels([raw.ch_names[pick] for pick in picks]) 
   raw.info.normalize_proj() 
   del picks 
   n_epochs = 2 
   raw.info['lowpass'] = 40.0 
   for decim in (1, 3): 
      proj_data = Epochs(raw, events, event_id, tmin, tmax, proj=True, reject=reject, decim=decim) 
      use_tmin = proj_data.tmin 
      proj_data = proj_data.get_data() 
      noproj_data = Epochs(raw, events, event_id, tmin, tmax, proj=False, reject=reject, decim=decim).get_data() 
      assert_equal(proj_data.shape, noproj_data.shape) 
      assert_equal(proj_data.shape[0], n_epochs) 
      for preload in (True, False): 
         for proj in (True, False, 'delayed'): 
            for ii in range(3): 
               print (decim, preload, proj, ii) 
               comp = (proj_data if (proj is True) else noproj_data) 
               if (ii in (0, 1)): 
                  epochs = Epochs(raw, events, event_id, tmin, tmax, proj=proj, reject=reject, preload=preload, decim=decim) 
               else: 
                  fake_events = np.zeros((len(comp), 3), int) 
                  fake_events[:, 0] = np.arange(len(comp)) 
                  fake_events[:, 2] = 1 
                  epochs = EpochsArray(comp, raw.info, tmin=use_tmin, event_id=1, events=fake_events, proj=proj) 
                  epochs.info['sfreq'] /= decim 
                  assert_equal(len(epochs), n_epochs) 
               assert_true((raw.proj is False)) 
               assert_true((epochs.proj is (True if (proj is True) else False))) 
               if (ii == 1): 
                  epochs.load_data() 
               picks_data = pick_types(epochs.info, meg=True, eeg=True) 
               evoked = epochs.average(picks=picks_data) 
               assert_equal(evoked.nave, n_epochs, epochs.drop_log) 
               if (proj is True): 
                  evoked.apply_proj() 
               else: 
                  assert_true((evoked.proj is False)) 
               assert_array_equal(evoked.ch_names, np.array(epochs.ch_names)[picks_data]) 
               assert_allclose(evoked.times, epochs.times) 
               epochs_data = epochs.get_data() 
               assert_allclose(evoked.data, epochs_data.mean(axis=0)[picks_data], rtol=1e-05, atol=1e-20) 
               assert_allclose(epochs_data, comp, rtol=1e-05, atol=1e-20)", 'Test delayed epochs','Test delayed projection on Epochs.'
"def git_status(path): 
    cmd = (git_cmd_base(path) + ['status', '--porcelain']) 
   return run_subprocess(cmd, stderr=None, universal_newlines=True)[0]"," 'Returns the status of the given path. 
 :param path: Path to check 
 :return: Status of the given path 
 :rtype: str'","'Return a string listing all changes to the working tree in a git 
 repository.'"
"def create_instance(c_instance): 
    return GenericScript(c_instance, Live.MidiMap.MapMode.absolute, Live.MidiMap.MapMode.absolute, DEVICE_CONTROLS, TRANSPORT_CONTROLS, VOLUME_CONTROLS, TRACKARM_CONTROLS, BANK_CONTROLS, CONTROLLER_DESCRIPTIONS)"," 'Creates a new instance of the MidiMap class. 
 :param c_instance: the c instance to create the MidiMap with 
 :type c_instance: ctypes.c_void_p 
 :return: the newly created MidiMap instance'",'The generic script can be customised by using parameters (see config.py).'
"def cloud_query_sinfo(cookie, tokens, source_path): 
    url = ''.join([const.PAN_URL, 'rest/2.0/services/cloud_dl?channel=chunlei&clienttype=0&web=1', '&method=query_sinfo&app_id=250528', '&bdstoken=', tokens['bdstoken'], '&source_path=', encoder.encode_uri_component(source_path), '&type=2', '&t=', util.timestamp()]) 
   req = net.urlopen(url, headers={'Cookie': cookie.header_output()}) 
   if req: 
      content = req.data 
      return json.loads(content.decode()) 
   else: 
      return None"," 'Query cloud info by cookie and token. 
 :param cookie: 
 :param tokens: 
 :param source_path: 
 :return: 
 :rtype: dict'",'source_path - BTç§å­çç»å¯¹è·¯å¾.'
"def build_encoder_bi(tparams, options): 
    embedding = tensor.tensor3('embedding', dtype='float32') 
   embeddingr = embedding[::(-1)] 
   x_mask = tensor.matrix('x_mask', dtype='float32') 
   xr_mask = x_mask[::(-1)] 
   proj = get_layer(options['encoder'])[1](tparams, embedding, options, prefix='encoder', mask=x_mask) 
   projr = get_layer(options['encoder'])[1](tparams, embeddingr, options, prefix='encoder_r', mask=xr_mask) 
   ctx = tensor.concatenate([proj[0][(-1)], projr[0][(-1)]], axis=1) 
   return (embedding, x_mask, ctx)", 'Build the encoder for the bidirectional LSTM.',"'build bidirectional encoder, given pre-computed word embeddings'"
"def _minimize_bfgs(fun, x0, args=(), jac=None, callback=None, gtol=1e-05, norm=Inf, eps=_epsilon, maxiter=None, disp=False, return_all=False, **unknown_options): 
    _check_unknown_options(unknown_options) 
   f = fun 
   fprime = jac 
   epsilon = eps 
   retall = return_all 
   x0 = asarray(x0).flatten() 
   if (x0.ndim == 0): 
      x0.shape = (1,) 
   if (maxiter is None): 
      maxiter = (len(x0) * 200) 
   (func_calls, f) = wrap_function(f, args) 
   if (fprime is None): 
      (grad_calls, myfprime) = wrap_function(approx_fprime, (f, epsilon)) 
   else: 
      (grad_calls, myfprime) = wrap_function(fprime, args) 
   gfk = myfprime(x0) 
   k = 0 
   N = len(x0) 
   I = numpy.eye(N, dtype=int) 
   Hk = I 
   old_fval = f(x0) 
   old_old_fval = (old_fval + (np.linalg.norm(gfk) / 2)) 
   xk = x0 
   if retall: 
      allvecs = [x0] 
   sk = [(2 * gtol)] 
   warnflag = 0 
   gnorm = vecnorm(gfk, ord=norm) 
   while ((gnorm > gtol) and (k < maxiter)): 
      pk = (- numpy.dot(Hk, gfk)) 
      try: 
         (alpha_k, fc, gc, old_fval, old_old_fval, gfkp1) = _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval, old_old_fval, amin=1e-100, amax=1e+100) 
      except _LineSearchError: 
         warnflag = 2 
         break 
      xkp1 = (xk + (alpha_k * pk)) 
      if retall: 
         allvecs.append(xkp1) 
      sk = (xkp1 - xk) 
      xk = xkp1 
      if (gfkp1 is None): 
         gfkp1 = myfprime(xkp1) 
      yk = (gfkp1 - gfk) 
      gfk = gfkp1 
      if (callback is not None): 
         callback(xk) 
      k += 1 
      gnorm = vecnorm(gfk, ord=norm) 
      if (gnorm <= gtol): 
         break 
      if (not numpy.isfinite(old_fval)): 
         warnflag = 2 
         break 
      try: 
         rhok = (1.0 / numpy.dot(yk, sk)) 
      except ZeroDivisionError: 
         rhok = 1000.0 
         if disp: 
            print('Divide-by-zero   encountered:   rhok   assumed   large') 
      if isinf(rhok): 
         rhok = 1000.0 
         if disp: 
            print('Divide-by-zero   encountered:   rhok   assumed   large') 
      A1 = (I - ((sk[:, numpy.newaxis] * yk[numpy.newaxis, :]) * rhok)) 
      A2 = (I - ((yk[:, numpy.newaxis] * sk[numpy.newaxis, :]) * rhok)) 
      Hk = (numpy.dot(A1, numpy.dot(Hk, A2)) + ((rhok * sk[:, numpy.newaxis]) * sk[numpy.newaxis, :])) 
   fval = old_fval 
   if np.isnan(fval): 
      warnflag = 2 
   if (warnflag == 2): 
      msg = _status_message['pr_loss'] 
      if disp: 
         print(('Warning:   ' + msg)) 
         print(('                           Current   function   value:   %f' % fval)) 
         print(('                           Iterations:   %d' % k)) 
         print(('                           Function   evaluations:   %d' % func_calls[0])) 
         print(('                           Gradient   evaluations:   %d' % grad_calls[0])) 
   elif (k >= maxiter): 
      warnflag = 1 
      msg = _status_message['maxiter'] 
      if disp: 
         print(('Warning:   ' + msg)) 
         print(('                           Current   function   value:   %f' % fval)) 
         print(('                           Iterations:   %d' % k)) 
         print(('                           Function   evaluations:   %d' % func_calls[0])) 
         print(('                           Gradient   evaluations:   %d' % grad_calls[0])) 
   else: 
      msg = _status_message['success'] 
      if disp: 
         print(msg) 
         print(('                           Current   function   value:   %f' % fval)) 
         print(('                           Iterations:   %d' % k)) 
         print(('                           Function   evaluations:   %d' % func_calls[0])) 
         print(('                           Gradient   evaluations:   %d' % grad_calls[0])) 
   result = OptimizeResult(fun=fval, jac=gfk, hess_inv=Hk, nfev=func_calls[0], njev=grad_calls[0], status=warnflag, success=(warnflag == 0), message=msg, x=xk, nit=k) 
   if retall: 
      result['allvecs'] = allvecs 
   return result"," 'Minimize a function using the BFGS algorithm. 
 The BFGS algorithm is an iterative method for minimizing a function 
 of several variables.  It is an iterative method, meaning that it is 
 called repeatedly until the function value has not changed.  The 
 algorithm is based on the following assumptions: 
 1. The function is twice continuously differentiable. 
 2. The function is strictly convex. 
 3. The Hessian matrix is positive definite. 
 4. The initial point is in the interior of the feasible region. 
 5. The initial point is close enough to the minimum. 
 6. The initial step size is small enough. 
 7. The initial step size is large enough. 
 8. The initial step size is chosen in such a way that the step size 
 decreases in each iteration. 
 The BFGS algorithm is based on the Newton-Raphson method. 
 The BFGS algorithm is a method for approximating the inverse of 
 the Hessian matrix.  The Hessian matrix is a matrix that contains 
 the second derivatives of","'Minimization of scalar function of one or more variables using the 
 BFGS algorithm. 
 Options 
 disp : bool 
 Set to True to print convergence messages. 
 maxiter : int 
 Maximum number of iterations to perform. 
 gtol : float 
 Gradient norm must be less than `gtol` before successful 
 termination. 
 norm : float 
 Order of norm (Inf is max, -Inf is min). 
 eps : float or ndarray 
 If `jac` is approximated, use this value for the step size.'"
"def terminal_action(parent, fn): 
    action = cmd_action(parent, cmds.LaunchTerminal, (lambda : utils.select_directory(fn())), hotkeys.TERMINAL) 
   return action", 'Decorator to add terminal action to a command.','Launch a terminal -> QAction'
"def present(name, DomainName, ElasticsearchClusterConfig=None, EBSOptions=None, AccessPolicies=None, SnapshotOptions=None, AdvancedOptions=None, Tags=None, region=None, key=None, keyid=None, profile=None, ElasticsearchVersion='1.5'): 
    ret = {'name': DomainName, 'result': True, 'comment': '', 'changes': {}} 
   if (ElasticsearchClusterConfig is None): 
      ElasticsearchClusterConfig = {'DedicatedMasterEnabled': False, 'InstanceCount': 1, 'InstanceType': 'm3.medium.elasticsearch', 'ZoneAwarenessEnabled': False} 
   if (EBSOptions is None): 
      EBSOptions = {'EBSEnabled': False} 
   if (SnapshotOptions is None): 
      SnapshotOptions = {'AutomatedSnapshotStartHour': 0} 
   if (AdvancedOptions is None): 
      AdvancedOptions = {'rest.action.multi.allow_explicit_index': 'true'} 
   if (Tags is None): 
      Tags = {} 
   if ((AccessPolicies is not None) and isinstance(AccessPolicies, six.string_types)): 
      try: 
         AccessPolicies = json.loads(AccessPolicies) 
      except ValueError as e: 
         ret['result'] = False 
         ret['comment'] = 'Failed   to   create   domain:   {0}.'.format(e.message) 
         return ret 
   r = __salt__['boto_elasticsearch_domain.exists'](DomainName=DomainName, region=region, key=key, keyid=keyid, profile=profile) 
   if ('error' in r): 
      ret['result'] = False 
      ret['comment'] = 'Failed   to   create   domain:   {0}.'.format(r['error']['message']) 
      return ret 
   if (not r.get('exists')): 
      if __opts__['test']: 
         ret['comment'] = 'Domain   {0}   is   set   to   be   created.'.format(DomainName) 
         ret['result'] = None 
         return ret 
      r = __salt__['boto_elasticsearch_domain.create'](DomainName=DomainName, ElasticsearchClusterConfig=ElasticsearchClusterConfig, EBSOptions=EBSOptions, AccessPolicies=AccessPolicies, SnapshotOptions=SnapshotOptions, AdvancedOptions=AdvancedOptions, ElasticsearchVersion=str(ElasticsearchVersion), region=region, key=key, keyid=keyid, profile=profile) 
      if (not r.get('created')): 
         ret['result'] = False 
         ret['comment'] = 'Failed   to   create   domain:   {0}.'.format(r['error']['message']) 
         return ret 
      _describe = __salt__['boto_elasticsearch_domain.describe'](DomainName, region=region, key=key, keyid=keyid, profile=profile) 
      ret['changes']['old'] = {'domain': None} 
      ret['changes']['new'] = _describe 
      ret['comment'] = 'Domain   {0}   created.'.format(DomainName) 
      return ret 
   ret['comment'] = os.linesep.join([ret['comment'], 'Domain   {0}   is   present.'.format(DomainName)]) 
   ret['changes'] = {} 
   _status = __salt__['boto_elasticsearch_domain.status'](DomainName=DomainName, region=region, key=key, keyid=keyid, profile=profile)['domain'] 
   if (_status.get('ElasticsearchVersion') != str(ElasticsearchVersion)): 
      ret['result'] = False 
      ret['comment'] = 'Failed   to   update   domain:   version   cannot   be   modified   from   {0}   to   {1}.'.format(_status.get('ElasticsearchVersion'), str(ElasticsearchVersion)) 
      return ret 
   _describe = __salt__['boto_elasticsearch_domain.describe'](DomainName=DomainName, region=region, key=key, keyid=keyid, profile=profile)['domain'] 
   _describe['AccessPolicies'] = json.loads(_describe['AccessPolicies']) 
   if (not _describe.get('EBSOptions', {}).get('EBSEnabled')): 
      opts = _describe.get('EBSOptions', {}) 
      opts.pop('VolumeSize', None) 
      opts.pop('VolumeType', None) 
   comm_args = {} 
   need_update = False 
   es_opts = {'ElasticsearchClusterConfig': ElasticsearchClusterConfig, 'EBSOptions': EBSOptions, 'AccessPolicies': AccessPolicies, 'SnapshotOptions': SnapshotOptions, 'AdvancedOptions': AdvancedOptions} 
   for (k, v) in six.iteritems(es_opts): 
      if (not _compare_json(v, _describe[k])): 
         need_update = True 
         comm_args[k] = v 
         ret['changes'].setdefault('new', {})[k] = v 
         ret['changes'].setdefault('old', {})[k] = _describe[k] 
   if need_update: 
      if __opts__['test']: 
         msg = 'Domain   {0}   set   to   be   modified.'.format(DomainName) 
         ret['comment'] = msg 
         ret['result'] = None 
         return ret 
      ret['comment'] = os.linesep.join([ret['comment'], 'Domain   to   be   modified']) 
      r = __salt__['boto_elasticsearch_domain.update'](DomainName=DomainName, region=region, key=key, keyid=keyid, profile=profile, **comm_args) 
      if (not r.get('updated')): 
         ret['result'] = False 
         ret['comment'] = 'Failed   to   update   domain:   {0}.'.format(r['error']) 
         ret['changes'] = {} 
         return ret 
   return ret"," 'Create or update Elasticsearch domain. 
 :param name: Domain name 
 :param DomainName: Domain name 
 :param ElasticsearchClusterConfig: Elasticsearch cluster configuration 
 :param EBSOptions: Elasticsearch EBS options 
 :param AccessPolicies: Elasticsearch access policies 
 :param SnapshotOptions: Elasticsearch snapshot options 
 :param AdvancedOptions: Elasticsearch advanced options 
 :param Tags: Elasticsearch tags 
 :param region: Region 
 :param key: AWS access key 
 :param keyid: AWS access key ID 
 :param profile: AWS profile 
 :param ElasticsearchVersion: Elasticsearch version'","'Ensure domain exists. 
 name 
 The name of the state definition 
 DomainName 
 Name of the domain. 
 ElasticsearchClusterConfig 
 Configuration options for an Elasticsearch domain. Specifies the 
 instance type and number of instances in the domain cluster. 
 InstanceType (string) -- 
 The instance type for an Elasticsearch cluster. 
 InstanceCount (integer) -- 
 The number of instances in the specified domain cluster. 
 DedicatedMasterEnabled (boolean) -- 
 A boolean value to indicate whether a dedicated master node is enabled. 
 See About Dedicated Master Nodes for more information. 
 ZoneAwarenessEnabled (boolean) -- 
 A boolean value to indicate whether zone awareness is enabled. See About 
 Zone Awareness for more information. 
 DedicatedMasterType (string) -- 
 The instance type for a dedicated master node. 
 DedicatedMasterCount (integer) -- 
 Total number of dedicated master nodes, active and on standby, for the 
 cluster. 
 EBSOptions 
 Options to enable, disable and specify the type and size of EBS storage 
 volumes. 
 EBSEnabled (boolean) -- 
 Specifies whether EBS-based storage is enabled. 
 VolumeType (string) -- 
 Specifies the volume type for EBS-based storage. 
 VolumeSize (integer) -- 
 Integer to specify the size of an EBS volume. 
 Iops (integer) -- 
 Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). 
 AccessPolicies 
 IAM access policy 
 SnapshotOptions 
 Option to set time, in UTC format, of the daily automated snapshot. 
 Default value is 0 hours. 
 AutomatedSnapshotStartHour (integer) -- 
 Specifies the time, in UTC format, when the service takes a daily 
 automated snapshot of the specified Elasticsearch domain. Default value 
 is 0 hours. 
 AdvancedOptions 
 Option to allow references to indices in an HTTP request body. Must be 
 false when configuring access to individual sub-resources. By default, 
 the value is true . 
 region 
 Region to connect to. 
 key 
 Secret key to be used. 
 keyid 
 Access key to be used. 
 profile 
 A dict with region, key and keyid, or a pillar key (string) that 
 contains a dict with region, key and keyid. 
 ElasticsearchVersion 
 String of format X.Y to specify version for the Elasticsearch domain eg. 
 ""1.5"" or ""2.3"".'"
"def locate_prefix_by_name(ctx, name): 
    if (name == ROOT_ENV_NAME): 
      return ctx.root_dir 
   for envs_dir in chain((ctx.envs_dirs + (os.getcwd(),))): 
      prefix = join(envs_dir, name) 
      if isdir(prefix): 
         return prefix 
   raise CondaEnvironmentNotFoundError(name)"," 'Looks for a conda environment with the given name. 
 Returns the path to the environment if found, None otherwise.'","'Find the location of a prefix given a conda env name. 
 Args: 
 ctx (Context): the context object 
 name (str): the name of prefix to find 
 Returns: 
 str: the location of the prefix found, or CondaValueError will raise if not found 
 Raises: 
 CondaValueError: when no prefix is found'"
"def split(line): 
    if (not line.strip()): 
      raise exceptions.MpdNoCommand(u'No   command   given') 
   match = WORD_RE.match(line) 
   if (not match): 
      raise exceptions.MpdUnknownError(u'Invalid   word   character') 
   (whitespace, command, remainder) = match.groups() 
   if whitespace: 
      raise exceptions.MpdUnknownError(u'Letter   expected') 
   result = [command] 
   while remainder: 
      match = PARAM_RE.match(remainder) 
      if (not match): 
         msg = _determine_error_message(remainder) 
         raise exceptions.MpdArgError(msg, command=command) 
      (unquoted, quoted, remainder) = match.groups() 
      result.append((unquoted or UNESCAPE_RE.sub(u'\\g<1>', quoted))) 
   return result"," 'Split a line into a list of words. 
 :param line: 
 :return: 
 :rtype: 
 :raises: 
 :example: 
 >>> split(\'play\') 
 \'play\' 
 >>> split(\'play track 01\') 
 \'play\' 
 >>> split(\'play track 01\') 
 \'play\' 
 >>> split(\'play track 01\') 
 \'play\' 
 >>> split(\'play track 01\') 
 \'play\' 
 >>> split(\'play track 01\') 
 \'play\' 
 >>> split(\'play track 01\') 
 \'play\' 
 >>> split(\'play track 01\') 
 \'play\' 
 >>> split(\'play track 01\') 
 \'play\' 
 >>> split(\'play track 01\') 
 \'play\' 
 >>> split(\'play track 01\') 
 \'play\' 
 >>> split(\'play track 01\')","'Splits a line into tokens using same rules as MPD. 
 - Lines may not start with whitespace 
 - Tokens are split by arbitrary amount of spaces or tabs 
 - First token must match `[a-z][a-z0-9_]*` 
 - Remaining tokens can be unquoted or quoted tokens. 
 - Unquoted tokens consist of all printable characters except double quotes, 
 single quotes, spaces and tabs. 
 - Quoted tokens are surrounded by a matching pair of double quotes. 
 - The closing quote must be followed by space, tab or end of line. 
 - Any value is allowed inside a quoted token. Including double quotes, 
 assuming it is correctly escaped. 
 - Backslash inside a quoted token is used to escape the following 
 character. 
 For examples see the tests for this function.'"
"def number_aware_alphabetical_cmp(str1, str2): 
    def flatten_tokens(tokens): 
      l = [] 
      for token in tokens: 
         if isinstance(token, str): 
            for char in token: 
               l.append(char) 
         else: 
            assert isinstance(token, float) 
            l.append(token) 
      return l 
   seq1 = flatten_tokens(tokenize_by_number(str1)) 
   seq2 = flatten_tokens(tokenize_by_number(str2)) 
   l = min(len(seq1), len(seq2)) 
   i = 0 
   while (i < l): 
      if (isinstance(seq1[i], float) and isinstance(seq2[i], string_types)): 
         return (-1) 
      elif (isinstance(seq1[i], string_types) and isinstance(seq2[i], float)): 
         return 1 
      elif (seq1[i] < seq2[i]): 
         return (-1) 
      elif (seq1[i] > seq2[i]): 
         return 1 
      i += 1 
   if (len(seq1) < len(seq2)): 
      return (-1) 
   elif (len(seq1) > len(seq2)): 
      return 1 
   return 0"," 'Compares two strings using number-aware alphabetical sorting. 
 This function uses the same algorithm as the Python standard library 
 \'sorted\' function, but it uses the number-aware alphabetical sorting 
 algorithm described in http://www.python.org/dev/peps/pep-0241/. 
 The algorithm is based on the idea that numbers are ordered in a 
 predictable way. For example, 2000 comes before 2001. 
 This function is used by the \'number_aware_alphabetical_sort\' function 
 to sort strings. 
 >>> number_aware_alphabetical_cmp(""2000"", ""2001"") 
 -1 
 >>> number_aware_alphabetical_cmp(""2001"", ""2000"") 
 1 
 >>> number_aware_alphabetical_cmp(""2001"", ""2000"") 
 0 
 >>> number_aware_alphabetical_cmp(""2001"", ""2000"") 
 0 
 >>> number_aware_alphabet","'cmp function for sorting a list of strings by alphabetical 
 order, but with numbers sorted numerically, i.e. `foo1, 
 foo2, foo10, foo11` instead of `foo1, foo10, foo11, foo2`. 
 Parameters 
 str1 : str 
 WRITEME 
 str2 : str 
 WRITEME 
 Returns 
 WRITEME'"
"def get_all_collections(bus): 
    service_obj = bus_get_object(bus, SS_PATH) 
   service_props_iface = dbus.Interface(service_obj, dbus.PROPERTIES_IFACE) 
   for collection_path in service_props_iface.Get(SERVICE_IFACE, 'Collections', signature='ss'): 
      (yield Collection(bus, collection_path))"," 'Return all collections in the system. 
 :param bus: the DBus connection to use. 
 :type bus: dbus.Interface'",'Returns a generator of all available collections.'
"def kvToSeq(data, strict=False): 
    def err(msg): 
      formatted = ('kvToSeq   warning:   %s:   %r' % (msg, data)) 
      if strict: 
         raise KVFormError(formatted) 
      else: 
         oidutil.log(formatted) 
   lines = data.split('\n') 
   if lines[(-1)]: 
      err('Does   not   end   in   a   newline') 
   else: 
      del lines[(-1)] 
   pairs = [] 
   line_num = 0 
   for line in lines: 
      line_num += 1 
      if (not line.strip()): 
         continue 
      pair = line.split(':', 1) 
      if (len(pair) == 2): 
         (k, v) = pair 
         k_s = k.strip() 
         if (k_s != k): 
            fmt = 'In   line   %d,   ignoring   leading   or   trailing   whitespace   in   key   %r' 
            err((fmt % (line_num, k))) 
         if (not k_s): 
            err(('In   line   %d,   got   empty   key' % (line_num,))) 
         v_s = v.strip() 
         if (v_s != v): 
            fmt = 'In   line   %d,   ignoring   leading   or   trailing   whitespace   in   value   %r' 
            err((fmt % (line_num, v))) 
         pairs.append((k_s.decode('UTF8'), v_s.decode('UTF8'))) 
      else: 
         err(('Line   %d   does   not   contain   a   colon' % line_num)) 
   return pairs"," 'Convert a key=value pair string into a list of (key, value) tuples. 
 This is a very basic implementation of the KVFormError class. 
 :param data: A string containing a key=value pair. 
 :param strict: If true, raise KVFormError if the string does not 
 contain a key=value pair. 
 :returns: A list of (key, value) tuples.'","'After one parse, seqToKV and kvToSeq are inverses, with no warnings:: 
 seq = kvToSeq(s) 
 seqToKV(kvToSeq(seq)) == seq'"
"def detect(source): 
    return (SIGNATURE in source)"," 'Detect if a source contains the signature of a PyCryptodome module. 
 :param source: The source to check. 
 :type source: str 
 :return: True if the source contains the signature, False otherwise. 
 :rtype: bool'",'Detects MyObfuscate.com packer.'
"def test__FileIO___reduce__(): 
    pass", 'Test the __reduce__ method of FileIO','TODO'
"def is_valid_csv(parser, file_name, row_limit): 
    row_count = 0 
   for row in csv.reader(open(file_name)): 
      row_count += 1 
   if (row_limit > row_count): 
      parser.error(""The   'row_count'   of   '{}'   is   >   the   number   of   rows   in   '{}'!"".format(row_limit, file_name)) 
      sys.exit(1)"," 'Checks if the number of rows in a file is greater than the number of rows 
 specified in the config file. 
 :param parser: The parser object. 
 :param file_name: The name of the file. 
 :param row_limit: The row limit. 
 :type row_limit: int 
 :return: None 
 :rtype: None'","'Ensure that the # of rows in the input_file 
 is greater than the row_limit.'"
"def parse_certificate_issuer_id(id): 
    return parse_object_id('certificates/issuers', id)"," 'Parse a certificate issuer ID into a ``CertificateIssuer`` object. 
 :param id: The certificate issuer ID. 
 :type id: str 
 :returns: The certificate issuer object. 
 :rtype: CertificateIssuer'","':param id: The resource collection type. 
 :type id: str 
 :rtype: KeyVaultId'"
"def _parse_qsub_job_id(qsub_out): 
    return int(qsub_out.split()[2])", 'Parse the job id from the qsub output.',"'Parse job id from qsub output string. 
 Assume format: 
 ""Your job <job_id> (""<job_name>"") has been submitted""'"
"def not_found(*args, **kwargs): 
    raise falcon.HTTPNotFound()", 'Return a 404 response.','Redirects request handling to the not found render'
"def strip_esc_sequence(txt): 
    if isinstance(txt, six.string_types): 
      return txt.replace('\x1b', '?') 
   else: 
      return txt"," 'Strip the escape sequence from a string. 
 This is needed because the terminal emulators that we use 
 don\'t support the escape sequence, but they do support the 
 ? character. 
 :param txt: the string to strip the escape sequence from 
 :type txt: string'","'Replace ESC (ASCII 27/Oct 33) to prevent unsafe strings 
 from writing their own terminal manipulation commands'"
"def compute_use_defs(blocks): 
    var_use_map = {} 
   var_def_map = {} 
   for (offset, ir_block) in blocks.items(): 
      var_use_map[offset] = use_set = set() 
      var_def_map[offset] = def_set = set() 
      for stmt in ir_block.body: 
         if isinstance(stmt, ir.Assign): 
            if isinstance(stmt.value, ir.Inst): 
               rhs_set = set((var.name for var in stmt.value.list_vars())) 
            elif isinstance(stmt.value, ir.Var): 
               rhs_set = set([stmt.value.name]) 
            elif isinstance(stmt.value, (ir.Arg, ir.Const, ir.Global, ir.FreeVar)): 
               rhs_set = () 
            else: 
               raise AssertionError('unreachable', type(stmt.value)) 
            if (stmt.target.name not in rhs_set): 
               def_set.add(stmt.target.name) 
         for var in stmt.list_vars(): 
            if (var.name not in def_set): 
               use_set.add(var.name) 
   return _use_defs_result(usemap=var_use_map, defmap=var_def_map)"," 'Compute use and definition sets for a block. 
 Returns a tuple of use and definition sets for the block. 
 If the block is a single statement, then the use and definition 
 sets are the same. 
 Args: 
 blocks (dict): A dictionary mapping IR block numbers to IR blocks. 
 Returns: 
 (use_set, def_set): The use and definition sets for the block.'",'Find variable use/def per block.'
"@sensitive_post_parameters() 
 @never_cache 
 def password_reset_confirm(request, uidb64=None, token=None, template_name='registration/password_reset_confirm.html', token_generator=default_token_generator, set_password_form=SetPasswordForm, post_reset_redirect=None, current_app=None, extra_context=None): 
    UserModel = get_user_model() 
   assert ((uidb64 is not None) and (token is not None)) 
   if (post_reset_redirect is None): 
      post_reset_redirect = reverse('password_reset_complete') 
   else: 
      post_reset_redirect = resolve_url(post_reset_redirect) 
   try: 
      uid = urlsafe_base64_decode(uidb64) 
      user = UserModel._default_manager.get(pk=uid) 
   except (TypeError, ValueError, OverflowError, UserModel.DoesNotExist): 
      user = None 
   if ((user is not None) and token_generator.check_token(user, token)): 
      validlink = True 
      if (request.method == 'POST'): 
         form = set_password_form(user, request.POST) 
         if form.is_valid(): 
            form.save() 
            return HttpResponseRedirect(post_reset_redirect) 
      else: 
         form = set_password_form(None) 
   else: 
      validlink = False 
      form = None 
   context = {'form': form, 'validlink': validlink} 
   if (extra_context is not None): 
      context.update(extra_context) 
   return TemplateResponse(request, template_name, context, current_app=current_app)"," 'Confirm a password reset link. 
 This view will check that the token is valid, and if it is, will 
 display a form to enter a new password for the user. 
 If the token is invalid or expired, it will redirect the user to the 
 password reset page with an error message. 
 If the token is valid but the form is not submitted, it will redirect 
 the user to the password reset page with a form pre-filled with the 
 user\'s information. 
 If the token is valid and the form is submitted, it will redirect the 
 user to the password reset complete page. 
 If the token is invalid or expired, it will redirect the user to the 
 password reset page. 
 :param request: A django request object. 
 :param uidb64: The user ID encoded as a base64 string. 
 :param token: The token generated by the token generator. 
 :param template_name: The name of the template to render. 
 :param token_generator: The token generator used to generate the token. 
 :param set_password_form: The form used to set a password","'View that checks the hash in a password reset link and presents a 
 form for entering a new password.'"
"def _delAccountRights(sidObject, user_right): 
    try: 
      _polHandle = win32security.LsaOpenPolicy(None, win32security.POLICY_ALL_ACCESS) 
      user_rights_list = [user_right] 
      _ret = win32security.LsaRemoveAccountRights(_polHandle, sidObject, False, user_rights_list) 
      return True 
   except Exception as e: 
      log.error('Error   attempting   to   delete   account   right,   exception   was   {0}'.format(e)) 
      return False"," 'Attempts to remove the specified account right from the specified user. 
 @param sidObject: The SID of the user. 
 @param user_right: The account right to be removed. 
 @return: True if the account right was removed, False otherwise.'",'helper function to remove an account right from a user'
"def filter_non_model_columns(data, model): 
    columns = [c.name for c in model.__table__.columns] 
   return dict(((k, v) for (k, v) in six.iteritems(data) if ((k in columns) or isinstance(getattr(model, k, None), associationproxy.AssociationProxy))))"," 'Filter out non-model columns from the data. 
 :param data: 
 :param model: 
 :return: 
 :rtype: 
 :raises: 
 :example: 
 >>> data = {""id"": 1, ""name"": ""John"", ""age"": 25, ""phone"": ""123-456-7890"", ""address"": {""street"": ""123 Main Street"", ""city"": ""Anytown"", ""state"": ""CA"", ""zip"": ""12345""}} 
 >>> model = MyModel 
 >>> data = filter_non_model_columns(data, model) 
 >>> data 
 {""id"": 1, ""name"": ""John"", ""age"": 25}'","'Return the attributes from data which are model columns. 
 Return a new dict with items from data that whose keys are columns in 
 the model or are association proxies of the model.'"
"@gen.coroutine 
 def ShareExisting(client, obj_store, user_id, device_id, request): 
    request['user_id'] = user_id 
   (yield Activity.VerifyActivityId(client, user_id, device_id, request['activity']['activity_id'])) 
   num_photos = 0 
   for ep_dict in request['episodes']: 
      (yield Episode.VerifyEpisodeId(client, user_id, device_id, ep_dict['new_episode_id'])) 
      num_photos += len(ep_dict['photo_ids']) 
   (yield gen.Task(Operation.CreateAndExecute, client, user_id, device_id, 'ShareExistingOperation.Execute', request)) 
   logging.info(('SHARE   EXISTING:   user:   %d,   device:   %d,   viewpoint:   %s,   %d   episodes,   %d   photos' % (user_id, device_id, request['viewpoint_id'], len(request['episodes']), num_photos))) 
   raise gen.Return({})"," 'Share existing photos from the user\'s library to the viewpoint. 
 :param client: the :class:`~.client.Client` object. 
 :param obj_store: the :class:`~.store.ObjectStore` object. 
 :param user_id: the user ID. 
 :param device_id: the device ID. 
 :param request: the :class:`~.request.Request` object. 
 :return: a dictionary of the response.'",'Shares photos from existing episodes with the followers of an existing viewpoint.'
"@log_call 
 def metadef_resource_type_create(context, values): 
    global DATA 
   resource_type_values = copy.deepcopy(values) 
   resource_type_name = resource_type_values['name'] 
   allowed_attrubites = ['name', 'protected'] 
   for resource_type in DATA['metadef_resource_types']: 
      if (resource_type['name'] == resource_type_name): 
         raise exception.Duplicate() 
   incorrect_keys = (set(resource_type_values.keys()) - set(allowed_attrubites)) 
   if incorrect_keys: 
      raise exception.Invalid(('The   keys   %s   are   not   valid' % str(incorrect_keys))) 
   resource_type = _format_resource_type(resource_type_values) 
   DATA['metadef_resource_types'].append(resource_type) 
   return resource_type"," 'Creates a new resource type. 
 :param values: a dictionary containing the resource type\'s name and 
 protected attribute. 
 :return: a dictionary containing the resource type\'s name and 
 protected attribute.'",'Create a metadef resource type'
"def __virtual__(): 
    if (get_configured_provider() is False): 
      return False 
   if (get_dependencies() is False): 
      return False 
   return __virtualname__", 'Checks if the module is available','Check for aliyun configurations'
"def publish_string(source, source_path=None, destination_path=None, reader=None, reader_name='standalone', parser=None, parser_name='restructuredtext', writer=None, writer_name='pseudoxml', settings=None, settings_spec=None, settings_overrides=None, config_section=None, enable_exit_status=False): 
    (output, pub) = publish_programmatically(source_class=io.StringInput, source=source, source_path=source_path, destination_class=io.StringOutput, destination=None, destination_path=destination_path, reader=reader, reader_name=reader_name, parser=parser, parser_name=parser_name, writer=writer, writer_name=writer_name, settings=settings, settings_spec=settings_spec, settings_overrides=settings_overrides, config_section=config_section, enable_exit_status=enable_exit_status) 
   return output"," 'Publish a string to a string. 
 :param source: The source string. 
 :param source_path: The path of the source file. 
 :param destination_path: The path of the destination file. 
 :param reader: The reader to use. 
 :param reader_name: The name of the reader. 
 :param parser: The parser to use. 
 :param parser_name: The name of the parser. 
 :param writer: The writer to use. 
 :param writer_name: The name of the writer. 
 :param settings: The settings to use. 
 :param settings_spec: The settings specification. 
 :param settings_overrides: The settings overrides. 
 :param config_section: The config section. 
 :param enable_exit_status: Whether to enable the exit status. 
 :returns: The output string. 
 :rtype: str'","'Set up & run a `Publisher` for programmatic use with string I/O.  Return 
 the encoded string or Unicode string output. 
 For encoded string output, be sure to set the \'output_encoding\' setting to 
 the desired encoding.  Set it to \'unicode\' for unencoded Unicode string 
 output.  Here\'s one way:: 
 publish_string(..., settings_overrides={\'output_encoding\': \'unicode\'}) 
 Similarly for Unicode string input (`source`):: 
 publish_string(..., settings_overrides={\'input_encoding\': \'unicode\'}) 
 Parameters: see `publish_programmatically`.'"
"def Synchronized(f): 
    @functools.wraps(f) 
   def NewFunction(self, *args, **kw): 
      with self.lock: 
         return f(self, *args, **kw) 
   return NewFunction"," 'Decorator for synchronized methods. 
 Decorated methods will be called with a lock held. 
 The lock is released when the method returns. 
 Example: 
 >>> class Foo(object): 
 ...     def __init__(self, bar): 
 ...         self.bar = bar 
 ...     @Synchronized 
 ...     def do_stuff(self): 
 ...         self.bar = \'bar\' 
 >>> foo = Foo(\'foo\') 
 >>> foo.do_stuff() 
 \'foo\' 
 >>> foo.bar 
 \'bar\' 
 >>> foo.do_stuff() 
 \'foo\' 
 >>> foo.bar 
 \'bar\' 
 >>> foo.do_stuff() 
 \'foo\' 
 >>> foo.bar 
 \'bar\' 
 >>> foo.do_stuff() 
 \'foo\' 
 >>> foo.bar 
 \'bar\' 
 >>> foo.do_stuff() 
 \'foo\' 
 >>> foo.bar 
 \'bar\' 
 >>> foo.do",'Synchronization decorator.'
"def get_id(opts, cache_minion_id=False): 
    if (opts['root_dir'] is None): 
      root_dir = salt.syspaths.ROOT_DIR 
   else: 
      root_dir = opts['root_dir'] 
   config_dir = salt.syspaths.CONFIG_DIR 
   if config_dir.startswith(salt.syspaths.ROOT_DIR): 
      config_dir = config_dir.split(salt.syspaths.ROOT_DIR, 1)[(-1)] 
   id_cache = os.path.join(root_dir, config_dir.lstrip(os.path.sep), 'minion_id') 
   if opts.get('minion_id_caching', True): 
      try: 
         with salt.utils.fopen(id_cache) as idf: 
            name = idf.readline().strip() 
            bname = salt.utils.to_bytes(name) 
            if bname.startswith(codecs.BOM): 
               name = salt.utils.to_str(bname.replace(codecs.BOM, '', 1)) 
         if (name and (name != 'localhost')): 
            log.debug('Using   cached   minion   ID   from   {0}:   {1}'.format(id_cache, name)) 
            return (name, False) 
      except (IOError, OSError): 
         pass 
   if (('__role' in opts) and (opts.get('__role') == 'minion')): 
      log.debug('Guessing   ID.   The   id   can   be   explicitly   set   in   {0}'.format(os.path.join(salt.syspaths.CONFIG_DIR, 'minion'))) 
   newid = salt.utils.network.generate_minion_id() 
   if (('__role' in opts) and (opts.get('__role') == 'minion')): 
      log.debug('Found   minion   id   from   generate_minion_id():   {0}'.format(newid)) 
   if (cache_minion_id and opts.get('minion_id_caching', True)): 
      _cache_id(newid, id_cache) 
   is_ipv4 = salt.utils.network.is_ipv4(newid) 
   return (newid, is_ipv4)"," 'Generate a minion ID. 
 This function will generate a minion ID and store it in the \'id_cache\' file 
 (if \'minion_id_caching\' is enabled). 
 :param opts: Salt configuration options 
 :param cache_minion_id: If True, the minion ID will be cached 
 :return: A tuple with the minion ID and a boolean indicating if the ID is 
 IPv4-compatible or not'","'Guess the id of the minion. 
 If CONFIG_DIR/minion_id exists, use the cached minion ID from that file. 
 If no minion id is configured, use multiple sources to find a FQDN. 
 If no FQDN is found you may get an ip address. 
 Returns two values: the detected ID, and a boolean value noting whether or 
 not an IP address is being used for the ID.'"
"def deg(r): 
    return ((r / pi) * 180)", 'Convert radians to degrees','Return the degree value for the given radians (pi = 180 degrees).'
"def parsePWDResponse(response): 
    match = re.search('""(.*)""', response) 
   if match: 
      return match.groups()[0] 
   else: 
      return None", 'Parse the PWDResponse string',"'Returns the path from a response to a PWD command. 
 Responses typically look like:: 
 257 ""/home/andrew"" is current directory. 
 For this example, I will return C{\'/home/andrew\'}. 
 If I can\'t find the path, I return C{None}.'"
"def cross_entropy_seq(logits, target_seqs, batch_size=1, num_steps=None): 
    loss = tf.nn.seq2seq.sequence_loss_by_example([logits], [tf.reshape(target_seqs, [(-1)])], [tf.ones([(batch_size * num_steps)])]) 
   cost = (tf.reduce_sum(loss) / batch_size) 
   return cost"," 'Compute the cross-entropy loss for a sequence of targets. 
 Parameters 
 logits : tensor 
 Logits for a sequence of targets. 
 target_seqs : tensor 
 A tensor of the same shape as `logits` with the target sequence. 
 batch_size : int 
 The number of samples per batch. 
 num_steps : int, optional 
 The number of steps in the sequence. 
 Returns 
 loss : tensor 
 The cross-entropy loss. 
 Examples 
 >>> logits = tf.constant([[1, 2], [3, 4]]) 
 >>> target_seqs = tf.constant([[1, 2, 3, 4], [5, 6, 7, 8]]) 
 >>> loss = tf.reduce_sum(tf.nn.seq2seq.sequence_loss_by_example([logits], [tf.reshape(target_seqs, [(-1)])], [tf.ones([2])])) 
 >>> print(loss) 
 0.0 
 >>> loss = tf.reduce_sum(tf.","'Returns the expression of cross-entropy of two sequences, implement 
 softmax internally. Normally be used for Fixed Length RNN outputs. 
 Parameters 
 logits : Tensorflow variable 
 2D tensor, ``network.outputs``, [batch_size*n_steps (n_examples), number of output units] 
 target_seqs : Tensorflow variable 
 target : 2D tensor [batch_size, n_steps], if the number of step is dynamic, please use ``cross_entropy_seq_with_mask`` instead. 
 batch_size : a int, default is 1 
 RNN batch_size, number of concurrent processes, divide the loss by batch_size. 
 num_steps : a int 
 sequence length 
 Examples 
 >>> see PTB tutorial for more details 
 >>> input_data = tf.placeholder(tf.int32, [batch_size, num_steps]) 
 >>> targets = tf.placeholder(tf.int32, [batch_size, num_steps]) 
 >>> cost = tf.cost.cross_entropy_seq(network.outputs, targets, batch_size, num_steps)'"
"def assert_crypto_availability(f): 
    @functools.wraps(f) 
   def wrapper(*args, **kwds): 
      if (AES is None): 
         raise CryptoUnavailableError() 
      return f(*args, **kwds) 
   return wrapper", 'Decorator for asserting that the crypto module is available.','Ensure Crypto module is available.'
"def test_color_array(): 
    x = ColorArray(['r', 'g', 'b']) 
   assert_array_equal(x.rgb, np.eye(3)) 
   assert isinstance(x[0], ColorArray) 
   assert isinstance(x[:], ColorArray) 
   assert_array_equal(x.rgba[:], x[:].rgba) 
   assert_array_equal(x.rgba[0], x[0].rgba.squeeze()) 
   assert_array_equal(x.rgba[1:3], x[1:3].rgba) 
   assert_raises(ValueError, x.__getitem__, (0, 1)) 
   x[0] = 0 
   assert_array_equal(x.rgba[0, :], np.zeros(4)) 
   assert_array_equal(x.rgba, x[:].rgba) 
   x[1] = 1 
   assert_array_equal(x[1].rgba, np.ones((1, 4))) 
   x[:] = 0.5 
   assert_array_equal(x.rgba, (0.5 * np.ones((3, 4)))) 
   assert_raises(ValueError, x.__setitem__, (0, 1), 0) 
   x = ColorArray(color_space='hsv', color=[(0, 0, 1), (0, 0, 0.5), (0, 0, 0)]) 
   assert_array_equal(x.rgba[0], [1, 1, 1, 1]) 
   assert_array_equal(x.rgba[1], [0.5, 0.5, 0.5, 1]) 
   assert_array_equal(x.rgba[2], [0, 0, 0, 1]) 
   x = ColorArray(color_space='hsv') 
   assert_array_equal(x.rgba[0], [0, 0, 0, 1])", 'Test the ColorArray class','Basic tests for ColorArray class'
"def create_urllib3_context(ssl_version=None, cert_reqs=None, options=None, ciphers=None): 
    context = SSLContext((ssl_version or ssl.PROTOCOL_SSLv23)) 
   cert_reqs = (ssl.CERT_REQUIRED if (cert_reqs is None) else cert_reqs) 
   if (options is None): 
      options = 0 
      options |= OP_NO_SSLv2 
      options |= OP_NO_SSLv3 
      options |= OP_NO_COMPRESSION 
   context.options |= options 
   if getattr(context, 'supports_set_ciphers', True): 
      context.set_ciphers((ciphers or DEFAULT_CIPHERS)) 
   context.verify_mode = cert_reqs 
   if (getattr(context, 'check_hostname', None) is not None): 
      context.check_hostname = False 
   return context"," 'Create a new URLlib3 context for use with the HTTPS adapter. 
 :param ssl_version: The desired SSL version to use. 
 :param cert_reqs: The desired certificate requirements. 
 :param options: The desired options. 
 :param ciphers: The desired ciphers. 
 :returns: A new SSLContext instance. 
 :rtype: :class:`~urllib3.util.ssl_ 
 :raises: :exc:`~urllib3.exceptions.SSLContextError` if an error occurs.'","'All arguments have the same meaning as ``ssl_wrap_socket``. 
 By default, this function does a lot of the same work that 
 ``ssl.create_default_context`` does on Python 3.4+. It: 
 - Disables SSLv2, SSLv3, and compression 
 - Sets a restricted set of server ciphers 
 If you wish to enable SSLv3, you can do:: 
 from urllib3.util import ssl_ 
 context = ssl_.create_urllib3_context() 
 context.options &= ~ssl_.OP_NO_SSLv3 
 You can do the same to enable compression (substituting ``COMPRESSION`` 
 for ``SSLv3`` in the last line above). 
 :param ssl_version: 
 The desired protocol version to use. This will default to 
 PROTOCOL_SSLv23 which will negotiate the highest protocol that both 
 the server and your installation of OpenSSL support. 
 :param cert_reqs: 
 Whether to require the certificate verification. This defaults to 
 ``ssl.CERT_REQUIRED``. 
 :param options: 
 Specific OpenSSL options. These default to ``ssl.OP_NO_SSLv2``, 
 ``ssl.OP_NO_SSLv3``, ``ssl.OP_NO_COMPRESSION``. 
 :param ciphers: 
 Which cipher suites to allow the server to select. 
 :returns: 
 Constructed SSLContext object with specified options 
 :rtype: SSLContext'"
"@verbose 
 def _merge_info(infos, force_update_to_first=False, verbose=None): 
    for info in infos: 
      info._check_consistency() 
   if (force_update_to_first is True): 
      infos = deepcopy(infos) 
      _force_update_info(infos[0], infos[1:]) 
   info = Info() 
   info['chs'] = [] 
   for this_info in infos: 
      info['chs'].extend(this_info['chs']) 
   info._update_redundant() 
   duplicates = set([ch for ch in info['ch_names'] if (info['ch_names'].count(ch) > 1)]) 
   if (len(duplicates) > 0): 
      msg = ('The   following   channels   are   present   in   more   than   one   input   measurement   info   objects:   %s' % list(duplicates)) 
      raise ValueError(msg) 
   transforms = ['ctf_head_t', 'dev_head_t', 'dev_ctf_t'] 
   for trans_name in transforms: 
      trans = [i[trans_name] for i in infos if i[trans_name]] 
      if (len(trans) == 0): 
         info[trans_name] = None 
      elif (len(trans) == 1): 
         info[trans_name] = trans[0] 
      elif all(((np.all((trans[0]['trans'] == x['trans'])) and (trans[0]['from'] == x['from']) and (trans[0]['to'] == x['to'])) for x in trans[1:])): 
         info[trans_name] = trans[0] 
      else: 
         msg = ('Measurement   infos   provide   mutually   inconsistent   %s' % trans_name) 
         raise ValueError(msg) 
   kit_sys_ids = [i['kit_system_id'] for i in infos if i['kit_system_id']] 
   if (len(kit_sys_ids) == 0): 
      info['kit_system_id'] = None 
   elif (len(set(kit_sys_ids)) == 1): 
      info['kit_system_id'] = kit_sys_ids[0] 
   else: 
      raise ValueError('Trying   to   merge   channels   from   different   KIT   systems') 
   other_fields = ['acq_pars', 'acq_stim', 'bads', 'buffer_size_sec', 'comps', 'custom_ref_applied', 'description', 'dig', 'experimenter', 'file_id', 'highpass', 'hpi_results', 'hpi_meas', 'hpi_subsystem', 'events', 'line_freq', 'lowpass', 'meas_date', 'meas_id', 'proj_id', 'proj_name', 'projs', 'sfreq', 'subject_info', 'sfreq', 'xplotter_layout'] 
   for k in other_fields: 
      info[k] = _merge_dict_values(infos, k) 
   info._check_consistency() 
   return info"," 'Merge multiple measurement infos into a single info object. 
 Parameters 
 infos : list 
 List of measurement infos to merge. 
 force_update_to_first : bool, optional 
 If True, the first info object in the list will be updated with 
 information from the rest of the infos. 
 verbose : int, optional 
 Verbosity level (0-3). 
 Returns 
 info : Info 
 Merged info object. 
 Notes 
 The following fields are updated: 
 - `chs`: the channels are merged into a single list. 
 - `acq_pars`: the acquisition parameters are merged into a single 
 dictionary. 
 - `acq_stim`: the stimulus parameters are merged into a single 
 dictionary. 
 - `bads`: the bad channels are merged into a single list. 
 - `buffer_size_sec`: the buffer size is set to the maximum value 
 across all input infos. 
 - `comps`: the components are merged into a single list. 
 - `custom_ref_applied`: the custom reference applied is merged into","'Merge multiple measurement info dictionaries. 
 - Fields that are present in only one info object will be used in the 
 merged info. 
 - Fields that are present in multiple info objects and are the same 
 will be used in the merged info. 
 - Fields that are present in multiple info objects and are different 
 will result in a None value in the merged info. 
 - Channels will be concatenated. If multiple info objects contain 
 channels with the same name, an exception is raised. 
 Parameters 
 infos | list of instance of Info 
 Info objects to merge into one info object. 
 force_update_to_first : bool 
 If True, force the fields for objects in `info` will be updated 
 to match those in the first item. Use at your own risk, as this 
 may overwrite important metadata. 
 verbose : bool, str, int, or NonIe 
 If not None, override default verbose level (see :func:`mne.verbose` 
 and :ref:`Logging documentation <tut_logging>` for more). 
 Returns 
 info : instance of Info 
 The merged info object.'"
"def _get_block_count_url_string(xblock_type_set): 
    block_url = '' 
   if (len(xblock_type_set) > 0): 
      block_url += '&all_blocks=true&block_counts=' 
      for (index, block_type) in enumerate(xblock_type_set): 
         block_url += block_type 
         if (index < (len(xblock_type_set) - 1)): 
            block_url += ',' 
   return block_url"," 'Returns a string that contains the block count URL parameters 
 for the given xblock_type_set. 
 :param xblock_type_set: A set of xblock types to query for block counts. 
 :type xblock_type_set: set 
 :return: A string containing the block count URL parameters. 
 :rtype: str'","'Build the string from the xBlock type set to append to the Block url for block_count types 
 Arguments: 
 xblock_type_set (set): A set of strings for all the block types 
 Returns: 
 str: The portion to append to the block url'"
"def create_territories(): 
    from frappe.utils.nestedset import get_root_of 
   country = frappe.db.get_default(u'country') 
   root_territory = get_root_of(u'Territory') 
   for name in (country, _(u'Rest   Of   The   World')): 
      if (name and (not frappe.db.exists(u'Territory', name))): 
         frappe.get_doc({u'doctype': u'Territory', u'territory_name': name.replace(u""'"", u''), u'parent_territory': root_territory, u'is_group': u'No'}).insert()", 'Create territories if not exist.',"'create two default territories, one for home country and one named Rest of the World'"
"@task 
 def GitHub_release_text(): 
    shortversion = get_sympy_short_version() 
   htmltable = table() 
   out = 'See   https://github.com/sympy/sympy/wiki/release-notes-for-{shortversion}   for   the   release   notes.\n\n{htmltable}\n\n**Note**:   Do   not   download   the   **Source   code   (zip)**   or   the   **Source   code   (tar.gz)**\nfiles   below.\n' 
   out = out.format(shortversion=shortversion, htmltable=htmltable) 
   print(blue('Here   are   the   release   notes   to   copy   into   the   GitHub   release   Markdown   form:', bold=True)) 
   print() 
   print(out) 
   return out"," 'Generate the release notes for the current version. 
 This is a task that is run automatically by the release 
 process.'",'Generate text to put in the GitHub release Markdown box'
"def build_desired_iface_config(module): 
    module.custom_desired_config = {'addr_family': None, 'auto': True, 'config': {}, 'name': module.params.get('name')} 
   build_addr_method(module) 
   build_address(module) 
   build_vids(module) 
   build_pvid(module) 
   build_speed(module) 
   build_alias_name(module) 
   build_vrr(module) 
   for _attr in ['mtu', 'mstpctl_portnetwork', 'mstpctl_portadminedge', 'mstpctl_bpduguard', 'clagd_enable', 'clagd_priority', 'clagd_peer_ip', 'clagd_sys_mac', 'clagd_args']: 
      build_generic_attr(module, _attr)", 'Builds the desired config for the interface.','take parameters defined and build ifupdown2 compatible hash'
"def _extract_id_token(id_token): 
    if (type(id_token) == bytes): 
      segments = id_token.split('.') 
   else: 
      segments = id_token.split(u'.') 
   if (len(segments) != 3): 
      raise VerifyJwtTokenError('Wrong   number   of   segments   in   token:   {0}'.format(id_token)) 
   return json.loads(_helpers._from_bytes(_helpers._urlsafe_b64decode(segments[1])))"," 'Extract the id token from the given JWT. 
 :param id_token: The JWT to extract the id token from. 
 :type id_token: str or bytes 
 :return: The id token. 
 :rtype: dict'","'Extract the JSON payload from a JWT. 
 Does the extraction w/o checking the signature. 
 Args: 
 id_token: string or bytestring, OAuth 2.0 id_token. 
 Returns: 
 object, The deserialized JSON payload.'"
"def unwatch_log(): 
    log_root = logging.getLogger(None).logger 
   to_replace = [h for h in log_root.handlers if isinstance(h, handlers.WatchedFileHandler)] 
   for handler in to_replace: 
      new_handler = std_logging.FileHandler(handler.baseFilename, mode=handler.mode, encoding=handler.encoding) 
      log_root.removeHandler(handler) 
      log_root.addHandler(new_handler)", 'Removes the WatchedFileHandler and replaces it with a FileHandler.',"'Replace WatchedFileHandler handlers by FileHandler ones. 
 Neutron logging uses WatchedFileHandler handlers but they do not 
 support privileges drop, this method replaces them by FileHandler 
 handlers supporting privileges drop.'"
"def compress(body, compress_level): 
    import zlib 
   (yield '\x1f\x8b') 
   (yield '\x08') 
   (yield '\x00') 
   (yield struct.pack('<L', (int(time.time()) & 4294967295L))) 
   (yield '\x02') 
   (yield '\xff') 
   crc = zlib.crc32('') 
   size = 0 
   zobj = zlib.compressobj(compress_level, zlib.DEFLATED, (- zlib.MAX_WBITS), zlib.DEF_MEM_LEVEL, 0) 
   for line in body: 
      size += len(line) 
      crc = zlib.crc32(line, crc) 
      (yield zobj.compress(line)) 
   (yield zobj.flush()) 
   (yield struct.pack('<L', (crc & 4294967295L))) 
   (yield struct.pack('<L', (size & 4294967295L)))"," 'Compresses the body of the request into a zlib stream. 
 :param body: The body to compress. 
 :type body: bytes 
 :param compress_level: The level of compression to use. 
 :type compress_level: int 
 :returns: A generator of bytes.'",'Compress \'body\' at the given compress_level.'
"def alignment_summary(alignment, index='      ', vertical_threshold=5): 
    answer = [] 
   alignment_len = alignment.get_alignment_length() 
   rec_count = len(alignment) 
   if (rec_count < vertical_threshold): 
      for record in alignment: 
         answer.append(('%s%s   %s' % (index, str_summary(str(record.seq)), record.id))) 
   else: 
      for i in range(min(5, alignment_len)): 
         answer.append(((index + str_summary(alignment[:, i])) + ('   alignment   column   %i' % i))) 
      if (alignment_len > 5): 
         i = (alignment_len - 1) 
         answer.append(((index + str_summary(('|' * rec_count))) + '   ...')) 
         answer.append(((index + str_summary(alignment[:, i])) + ('   alignment   column   %i' % i))) 
   return '\n'.join(answer)"," 'Return a summary of the alignment, suitable for printing. 
 This function is intended to be used for printing alignments. 
 It will summarize the alignment by printing the first five columns 
 (if there are fewer than five columns, it will print the first 
 columns available). 
 Parameters 
 alignment : Alignment 
 The alignment to summarize. 
 index : str 
 The character to use as the column separator. 
 vertical_threshold : int 
 The number of columns to use as the vertical threshold. 
 Returns 
 summary : str 
 A summary of the alignment.'",'Returns a concise summary of an Alignment object as a string.'
"def _check_module_dependencies(is_imbalanced_dataset_installing=False): 
    for (module_name, module_metadata) in REQUIRED_MODULE_METADATA: 
      if (not (is_imbalanced_dataset_installing and (not module_metadata['required_at_installation']))): 
         _import_module_with_version_check(module_name=module_name, minimum_version=module_metadata['min_version'], install_info=module_metadata.get('install_info'))"," 'Checks that all required modules are installed and up-to-date. 
 :param is_imbalanced_dataset_installing: Whether the imbalanced dataset is being installed or not. 
 :type is_imbalanced_dataset_installing: bool 
 :return: None'","'Throw an exception if imbalanced-learn dependencies are not installed. 
 Parameters 
 is_imbalanced_dataset_installing: boolean 
 if True, only error on missing packages that cannot be auto-installed. 
 if False, error on any missing package. 
 Throws 
 ImportError'"
"def setup_platform(hass, config, add_devices, discovery_info=None): 
    import pyowm 
   longitude = config.get(CONF_LONGITUDE, round(hass.config.longitude, 5)) 
   latitude = config.get(CONF_LATITUDE, round(hass.config.latitude, 5)) 
   name = config.get(CONF_NAME) 
   try: 
      owm = pyowm.OWM(config.get(CONF_API_KEY)) 
   except pyowm.exceptions.api_call_error.APICallError: 
      _LOGGER.error('Error   while   connecting   to   OpenWeatherMap') 
      return False 
   data = WeatherData(owm, latitude, longitude) 
   add_devices([OpenWeatherMapWeather(name, data, hass.config.units.temperature_unit)], True)", 'Set up the OpenWeatherMap platform.','Setup the OpenWeatherMap weather platform.'
"def restart(name, jail=None): 
    cmd = '{0}   {1}   onerestart'.format(_cmd(jail), name) 
   return (not __salt__['cmd.retcode'](cmd, python_shell=False))"," 'Restart the jail. 
 name: the name of the jail to restart 
 jail: the jail to restart (default: the jail specified in the minion\'s 
 configuration)'","'Restart the named service 
 .. versionchanged:: 2016.3.4 
 jail: optional jid or jail name 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' service.restart <service name>'"
"@pytest.mark.parametrize('fast_writer', [True, False]) 
 def test_byte_string_output(fast_writer): 
    t = table.Table([['Hello', 'World']], dtype=['S10']) 
   out = StringIO() 
   ascii.write(t, out, fast_writer=fast_writer) 
   assert (out.getvalue().splitlines() == ['col0', 'Hello', 'World'])", 'Test that byte string output works with fast writer',"'Test the fix for #4350 where byte strings were output with a 
 leading `b` on Py3.'"
"def _name_value_to_bson(name, value, check_keys, opts): 
    try: 
      return _ENCODERS[type(value)](name, value, check_keys, opts) 
   except KeyError: 
      pass 
   marker = getattr(value, '_type_marker', None) 
   if (isinstance(marker, int) and (marker in _MARKERS)): 
      func = _MARKERS[marker] 
      _ENCODERS[type(value)] = func 
      return func(name, value, check_keys, opts) 
   for base in _ENCODERS: 
      if isinstance(value, base): 
         func = _ENCODERS[base] 
         _ENCODERS[type(value)] = func 
         return func(name, value, check_keys, opts) 
   raise InvalidDocument(('cannot   convert   value   of   type   %s   to   bson' % type(value)))"," 'Convert a name-value pair to a BSON object. 
 :param name: The name of the key. 
 :param value: The value of the key. 
 :param check_keys: If True, check that the key is valid. 
 :param opts: The options to use when encoding the value. 
 :returns: The encoded BSON object. 
 :raises: InvalidDocument if the key is invalid.'","'Encode a single name, value pair.'"
"def getGeometryOutput(derivation, elementNode): 
    if (derivation == None): 
      derivation = ExtrudeDerivation(elementNode) 
   if (len(euclidean.getConcatenatedList(derivation.target)) == 0): 
      print 'Warning,   in   extrude   there   are   no   paths.' 
      print elementNode.attributes 
      return None 
   return getGeometryOutputByLoops(derivation, derivation.target)"," 'Returns the geometry output of an extrude operation. 
 If the operation has no loops, return None. 
 :param derivation: The derivation. 
 :param elementNode: The element node. 
 :return: The geometry output of the extrude operation. 
 :rtype: list of :py:class:`~babylon.core.geometry.BoundingBox` 
 :raises: :py:exc:`~babylon.core.Exceptions.BabylonException` 
 :raises: :py:exc:`~babylon.core.Exceptions.BabylonException`'",'Get triangle mesh from attribute dictionary.'
"def getRandomRange(a, b, randfunc=None): 
    range_ = ((b - a) - 1) 
   bits = size(range_) 
   value = getRandomInteger(bits, randfunc) 
   while (value > range_): 
      value = getRandomInteger(bits, randfunc) 
   return (a + value)"," 'Returns a random value between a and b, inclusive. 
 Parameters 
 a : int 
 The lower bound of the random value. 
 b : int 
 The upper bound of the random value. 
 randfunc : callable 
 A function to generate random numbers. 
 Returns 
 The random value. 
 Examples 
 >>> from sympy.physics.quantum.random import getRandomRange 
 >>> getRandomRange(0, 10) 
 2 
 >>> getRandomRange(0, 10, randfunc=lambda: int(random())) 
 8'","'getRandomRange(a:int, b:int, randfunc:callable):long 
 Return a random number n so that a <= n < b. 
 If randfunc is omitted, then Random.new().read is used. 
 This function is for internal use only and may be renamed or removed in 
 the future.'"
"def convert_tree(beautiful_soup_tree, makeelement=None): 
    root = _convert_tree(beautiful_soup_tree, makeelement) 
   children = root.getchildren() 
   for child in children: 
      root.remove(child) 
   return children"," 'Convert a BeautifulSoup tree into a list of ElementTree elements. 
 This is a convenience function for converting a BeautifulSoup tree into 
 ElementTree elements. It also removes all children from the root element 
 so that the ElementTree tree is a single-level tree. 
 :param beautiful_soup_tree: A BeautifulSoup tree 
 :type beautiful_soup_tree: BeautifulSoup 
 :param makeelement: A callable to convert a BeautifulSoup element to an 
 ElementTree element. 
 :type makeelement: callable 
 :return: A list of ElementTree elements.'","'Convert a BeautifulSoup tree to a list of Element trees. 
 Returns a list instead of a single root Element to support 
 HTML-like soup with more than one root element. 
 You can pass a different Element factory through the `makeelement` 
 keyword.'"
"def AddUpdateOptions(parser): 
    parser.add_option('--retain_upload_dir', action='store_true', dest='retain_upload_dir', default=False, help='Do   not   delete   temporary   (staging)   directory   used   in   uploading   Java   apps') 
   parser.add_option('--no_symlinks', action='store_true', dest='no_symlinks', default=False, help='Do   not   use   symbolic   links   when   making   the   temporary   (staging)   directory   for   uploading   Java   apps') 
   parser.add_option('--compile_encoding', action='store', dest='compile_encoding', default='UTF-8', help='Set   the   encoding   to   be   used   when   compiling   Java   source   files   (default   ""UTF-8"").') 
   parser.add_option('--disable_jar_jsps', action='store_false', dest='jar_jsps', default=True, help='Do   not   jar   the   classes   generated   from   JSPs.') 
   parser.add_option('--delete_jsps', action='store_true', dest='delete_jsps', default=False, help='Delete   the   JSP   source   files   after   compilation.') 
   parser.add_option('--enable_jar_classes', action='store_true', dest='do_jar_classes', default=False, help='Jar   the   WEB-INF/classes   content.') 
   parser.add_option('--enable_jar_splitting', action='store_true', dest='do_jar_splitting', default=False, help='Split   large   jar   files   (>   32M)   into   smaller   fragments.') 
   parser.add_option('--jar_splitting_excludes', action='store', dest='jar_splitting_exclude_suffixes', default='', help='When   --enable_jar_splitting   is   specified   and   --jar_splitting_excludes   specifies   a   comma-separated   list   of   suffixes,   a   file   in   a   jar   whose   name   ends   with   one   of   the   suffixes   will   not   be   included   in   the   split   jar   fragments.')"," 'Adds options to the command line parser that control the behavior of the 
 uploader.'","'Adds options specific to the \'update\' command on Java apps to \'parser\'. 
 Args: 
 parser: An instance of OptionsParser.'"
"def _determine_toggles(payload, toggles): 
    for (toggle, definition) in six.iteritems(toggles): 
      if (definition['value'] is not None): 
         if (((definition['value'] is True) or (definition['value'] == 'yes')) and (definition['type'] == 'yes_no')): 
            payload[toggle] = 'yes' 
         elif (((definition['value'] is False) or (definition['value'] == 'no')) and (definition['type'] == 'yes_no')): 
            payload[toggle] = 'no' 
         if (((definition['value'] is True) or (definition['value'] == 'yes')) and (definition['type'] == 'true_false')): 
            payload[toggle] = True 
         elif (((definition['value'] is False) or (definition['value'] == 'no')) and (definition['type'] == 'true_false')): 
            payload[toggle] = False 
   return payload"," 'Determine the value of toggles in the payload. 
 Args: 
 payload: 
 toggles: 
 Returns: 
 A payload with toggles set to the correct value.'","'BigIP can\'t make up its mind if it likes yes / no or true or false. 
 Figure out what it likes to hear without confusing the user.'"
"def create_mpl_fig(fig=None, figsize=None): 
    if (fig is None): 
      plt = _import_mpl() 
      fig = plt.figure(figsize=figsize) 
   return fig"," 'Create a matplotlib figure with the specified size. 
 Parameters 
 fig : matplotlib figure 
 figsize : tuple of 2 numbers 
 size of the figure in inches 
 Returns 
 matplotlib figure'","'Helper function for when multiple plot axes are needed. 
 Those axes should be created in the functions they are used in, with 
 ``fig.add_subplot()``. 
 Parameters 
 fig : Matplotlib figure instance, optional 
 If given, this figure is simply returned.  Otherwise a new figure is 
 created. 
 Returns 
 fig : Matplotlib figure instance 
 If `fig` is None, the created figure.  Otherwise the input `fig` is 
 returned. 
 See Also 
 create_mpl_ax'"
"def serialize_revision(node, record, version, index, anon=False): 
    if anon: 
      user = None 
   else: 
      user = {u'name': version.creator.fullname, u'url': version.creator.url} 
   return {u'user': user, u'index': (index + 1), u'date': version.date_created.isoformat(), u'downloads': record.get_download_count(version=index), u'md5': version.metadata.get(u'md5'), u'sha256': version.metadata.get(u'sha256')}", 'Serialize a revision into a dict.',"'Serialize revision for use in revisions table. 
 :param Node node: Root node 
 :param FileRecord record: Root file record 
 :param FileVersion version: The version to serialize 
 :param int index: One-based index of version'"
"def now(parser, token): 
    bits = token.contents.split('""') 
   if (len(bits) != 3): 
      raise TemplateSyntaxError(""'now'   statement   takes   one   argument"") 
   format_string = bits[1] 
   return NowNode(format_string)"," 'Generate a NowNode. 
 .. versionchanged:: 0.9 
 Now nodes are now allowed to have a format string. 
 .. versionchanged:: 0.8 
 Now nodes can now take a format string. 
 .. versionchanged:: 0.7 
 Now nodes can now take a format string. 
 .. versionchanged:: 0.6 
 Now nodes can now take a format string. 
 .. versionchanged:: 0.5 
 Now nodes can now take a format string. 
 .. versionchanged:: 0.4 
 Now nodes can now take a format string. 
 .. versionchanged:: 0.3 
 Now nodes can now take a format string. 
 .. versionchanged:: 0.2 
 Now nodes can now take a format string. 
 .. versionchanged:: 0.1 
 Now nodes can now take a format string. 
 .. versionchanged:: 0.0 
 Now nodes can now take a format string. 
 :param parser: The parser object. 
 :param token: The next token to be parsed. 
 :return: A NowNode. 
 :rtype","'Displays the date, formatted according to the given string. 
 Uses the same format as PHP\'s ``date()`` function; see http://php.net/date 
 for all the possible values. 
 Sample usage:: 
 It is {% now ""jS F Y H:i"" %}'"
"def _validate_list(key, value): 
    for (ind, element) in enumerate(value): 
      if (not (isinstance(element, basestring) or isinstance(element, datetime.date) or isinstance(element, datetime.datetime) or isinstance(element, numbers.Number))): 
         raise ValueError(('All   values   of   a   multi-valued   field   must   be   numbers,   strings,   date   or   datetime   instances,   The   %dth   value   for   field   %s   has   type   %s.' % (ind, key, type(element))))"," 'Validate the values of a list field. 
 :param key: The name of the field. 
 :param value: The values of the field. 
 :raises ValueError: If any of the values are not numbers, strings, dates or 
 datetime instances.'","'Validates a list to be included as document fields. The key is just 
 passed in to make better error messages.'"
"def test_continuous_error(): 
    y = np.linspace(0, 1, 20) 
   cnn = CondensedNearestNeighbour(random_state=RND_SEED) 
   assert_warns(UserWarning, cnn.fit, X, y)", 'Test continuous error',"'Test either if an error is raised when the target are continuous 
 type'"
"def ntohl(bs): 
    return struct.unpack('!I', bs)[0]", 'Convert a 32-bit integer to network byte order (big-endian).','Convert integer in \'n\' from network-byte order to host-byte order.'
"def mbruteforce(func, alphabet, length, method='upto', start=None, threads=None): 
    def bruteforcewrap(func, alphabet, length, method, start, databag): 
      oldloglevel = context.log_level 
      context.log_level = 'critical' 
      res = bruteforce(func, alphabet, length, method=method, start=start, databag=databag) 
      context.log_level = oldloglevel 
      databag['result'] = res 
   if (start == None): 
      start = (1, 1) 
   if (threads == None): 
      try: 
         threads = multiprocessing.cpu_count() 
      except NotImplementedError: 
         threads = 1 
   h = log.waitfor('MBruteforcing') 
   processes = ([None] * threads) 
   shareddata = ([None] * threads) 
   (i2, N2) = start 
   totalchunks = (threads * N2) 
   for i in range(threads): 
      shareddata[i] = multiprocessing.Manager().dict() 
      shareddata[i]['result'] = None 
      shareddata[i]['current_item'] = '' 
      shareddata[i]['items_done'] = 0 
      shareddata[i]['items_total'] = 0 
      chunkid = (((i2 - 1) + (i * N2)) + 1) 
      processes[i] = multiprocessing.Process(target=bruteforcewrap, args=(func, alphabet, length, method, (chunkid, totalchunks), shareddata[i])) 
      processes[i].start() 
   done = False 
   while (not done): 
      current_item_list = ','.join([('""%s""' % x['current_item']) for x in shareddata if (x != None)]) 
      items_done = sum([x['items_done'] for x in shareddata if (x != None)]) 
      items_total = sum([x['items_total'] for x in shareddata if (x != None)]) 
      progress = (((100.0 * items_done) / items_total) if (items_total != 0) else 0.0) 
      h.status(('Trying   %s   --   %0.3f%%' % (current_item_list, progress))) 
      for i in range(threads): 
         if (processes[i] and (processes[i].exitcode != None)): 
            res = shareddata[i]['result'] 
            processes[i].join() 
            processes[i] = None 
            if (res != None): 
               for i in range(threads): 
                  if (processes[i] != None): 
                     processes[i].terminate() 
                     processes[i].join() 
                     processes[i] = None 
               h.success(('Found   key:   ""%s""' % res)) 
               return res 
            if all([(x == None) for x in processes]): 
               done = True 
      time.sleep(0.3) 
   h.failure('No   matches   found')"," 'Bruteforce the given method on the given alphabet. 
 :param func: The function to bruteforce. 
 :param alphabet: The alphabet to bruteforce. 
 :param length: The length of the password. 
 :param method: The method to bruteforce. 
 :param start: The start point for the bruteforce. 
 :param threads: The number of threads to use for the bruteforce. 
 :return: The found password, or None if no password was found.'","'mbruteforce(func, alphabet, length, method = \'upto\', start = None, threads = None) 
 Same functionality as bruteforce(), but multithreaded. 
 Arguments: 
 func, alphabet, length, method, start: same as for bruteforce() 
 threads: Amount of threads to spawn, default is the amount of cores.'"
"def get_dataset_file(dataset, default_dataset, origin): 
    (data_dir, data_file) = os.path.split(dataset) 
   if ((data_dir == '') and (not os.path.isfile(dataset))): 
      new_path = os.path.join(os.path.split(__file__)[0], '..', 'data', dataset) 
      if (os.path.isfile(new_path) or (data_file == default_dataset)): 
         dataset = new_path 
   if ((not os.path.isfile(dataset)) and (data_file == default_dataset)): 
      from six.moves import urllib 
      print(('Downloading   data   from   %s' % origin)) 
      urllib.request.urlretrieve(origin, dataset) 
   return dataset"," 'Get dataset file. 
 If the dataset is not found, download it from the origin. 
 Parameters 
 dataset : str 
 Name of the dataset. 
 default_dataset : str 
 Name of the default dataset. 
 origin : str 
 Origin of the dataset. 
 Returns 
 dataset : str 
 Path of the dataset. 
 Examples 
 >>> get_dataset_file(\'mnist\', \'mnist\', \'http://yann.lecun.com/exdb/mnist/') 
 \'/home/matt/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/data/datasets/mnist.pt\' 
 >>> get_dataset_file(\'mnist\', \'mnist\', \'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\') 
 \'/home/matt/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/data/","'Look for it as if it was a full path, if not, try local file, 
 if not try in the data directory. 
 Download dataset if it is not present'"
"def test_epochs_hash(): 
    (raw, events) = _get_data()[:2] 
   epochs = Epochs(raw, events, event_id, tmin, tmax) 
   assert_raises(RuntimeError, epochs.__hash__) 
   epochs = Epochs(raw, events, event_id, tmin, tmax, preload=True) 
   assert_equal(hash(epochs), hash(epochs)) 
   epochs_2 = Epochs(raw, events, event_id, tmin, tmax, preload=True) 
   assert_equal(hash(epochs), hash(epochs_2)) 
   assert_true((pickle.dumps(epochs) == pickle.dumps(epochs_2))) 
   epochs_2._data[(0, 0, 0)] -= 1 
   assert_not_equal(hash(epochs), hash(epochs_2))", 'Test epochs hash.','Test epoch hashing.'
"def read_plain_int32(file_obj, count): 
    length = (4 * count) 
   data = file_obj.read(length) 
   if (len(data) != length): 
      raise EOFError(u'Expected   {0}   bytes   but   got   {1}   bytes'.format(length, len(data))) 
   res = struct.unpack('<{0}i'.format(count).encode(u'utf-8'), data) 
   return res"," 'Reads an int32 from the given file object and returns it as a list of 4 
 bytes. 
 :param file_obj: The file object to read from. 
 :param count: The number of int32 to read. 
 :return: The int32 as a list of 4 bytes.'",'Read `count` 32-bit ints using the plain encoding.'
"def load_stored_item(cache, path, item): 
    return cache.load_parser(path, (item.change_time - 1))"," 'Loads the stored item. 
 :param cache: The cache. 
 :param path: The path. 
 :param item: The item. 
 :return: The item. 
 :rtype: Item'",'Load `item` stored at `path` in `cache`.'
"def stash_conf_values(): 
    conf = {'bind_host': CONF.bind_host, 'bind_port': CONF.bind_port, 'tcp_keepidle': CONF.cert_file, 'backlog': CONF.backlog, 'key_file': CONF.key_file, 'cert_file': CONF.cert_file} 
   return conf", 'Stash the config values in a dictionary for later use.',"'Make a copy of some of the current global CONF\'s settings. 
 Allows determining if any of these values have changed 
 when the config is reloaded.'"
"def _strxfrm(s): 
    return s", 'Returns the string s unchanged.',"'strxfrm(string) -> string. 
 Returns a string that behaves for cmp locale-aware.'"
"def rjust(s, width): 
    n = (width - len(s)) 
   if (n <= 0): 
      return s 
   return (('   ' * n) + s)"," 'Return a string that is right-justified to the width. 
 >>> rjust(""Hello"", 5) 
 \'Hello    \' 
 >>> rjust(""Hello"", 10) 
 \'Hello    \''","'rjust(s, width) -> string 
 Return a right-justified version of s, in a field of the 
 specified width, padded with spaces as needed.  The string is 
 never truncated.'"
"def WRatio(s1, s2, force_ascii=True): 
    p1 = utils.full_process(s1, force_ascii=force_ascii) 
   p2 = utils.full_process(s2, force_ascii=force_ascii) 
   if (not utils.validate_string(p1)): 
      return 0 
   if (not utils.validate_string(p2)): 
      return 0 
   try_partial = True 
   unbase_scale = 0.95 
   partial_scale = 0.9 
   base = ratio(p1, p2) 
   len_ratio = (float(max(len(p1), len(p2))) / min(len(p1), len(p2))) 
   if (len_ratio < 1.5): 
      try_partial = False 
   if (len_ratio > 8): 
      partial_scale = 0.6 
   if try_partial: 
      partial = (partial_ratio(p1, p2) * partial_scale) 
      ptsor = ((partial_token_sort_ratio(p1, p2, full_process=False) * unbase_scale) * partial_scale) 
      ptser = ((partial_token_set_ratio(p1, p2, full_process=False) * unbase_scale) * partial_scale) 
      return utils.intr(max(base, partial, ptsor, ptser)) 
   else: 
      tsor = (token_sort_ratio(p1, p2, full_process=False) * unbase_scale) 
      tser = (token_set_ratio(p1, p2, full_process=False) * unbase_scale) 
      return utils.intr(max(base, tsor, tser))"," 'Calculate the WRatio of two strings. 
 This ratio is the ratio of the number of characters in the two strings 
 (excluding punctuation). 
 This is the ratio of the number of characters in the two strings (excluding 
 punctuation). 
 The WRatio is the ratio of the number of characters in the two strings (excluding 
 punctuation). 
 :param s1: A string. 
 :type s1: str 
 :param s2: A string. 
 :type s2: str 
 :return: The WRatio of the two strings. 
 :rtype: float'","'Return a measure of the sequences\' similarity between 0 and 100, using different algorithms. 
 **Steps in the order they occur** 
 #. Run full_process from utils on both strings 
 #. Short circuit if this makes either string empty 
 #. Take the ratio of the two processed strings (fuzz.ratio) 
 #. Run checks to compare the length of the strings 
 * If one of the strings is more than 1.5 times as long as the other 
 use partial_ratio comparisons - scale partial results by 0.9 
 (this makes sure only full results can return 100) 
 * If one of the strings is over 8 times as long as the other 
 instead scale by 0.6 
 #. Run the other ratio functions 
 * if using partial ratio functions call partial_ratio, 
 partial_token_sort_ratio and partial_token_set_ratio 
 scale all of these by the ratio based on length 
 * otherwise call token_sort_ratio and token_set_ratio 
 * all token based comparisons are scaled by 0.95 
 (on top of any partial scalars) 
 #. Take the highest value from these results 
 round it and return it as an integer. 
 :param s1: 
 :param s2: 
 :param force_ascii: Allow only ascii characters 
 :type force_ascii: bool 
 :return:'"
"def create_tags(filesystemid, tags, keyid=None, key=None, profile=None, region=None, **kwargs): 
    client = _get_conn(key=key, keyid=keyid, profile=profile, region=region) 
   new_tags = [] 
   for (k, v) in tags.iteritems(): 
      new_tags.append({'Key': k, 'Value': v}) 
   client.create_tags(FileSystemId=filesystemid, Tags=new_tags)", 'Create tags for a file system.',"'Creates or overwrites tags associated with a file system. 
 Each tag is a key-value pair. If a tag key specified in the request 
 already exists on the file system, this operation overwrites 
 its value with the value provided in the request. 
 filesystemid 
 (string) - ID of the file system for whose tags will be modified. 
 tags 
 (dict) - The tags to add to the file system 
 CLI Example: 
 .. code-block:: 
 salt \'my-minion\' boto_efs.create_tags'"
"def has_flag(conf, atom, flag): 
    if (flag in get_flags_from_package_conf(conf, atom)): 
      return True 
   return False"," 'Returns whether the given flag is enabled in the given atom 
 :param conf: The configuration to check 
 :param atom: The atom to check 
 :param flag: The flag to check 
 :return: True if the flag is enabled, False otherwise'","'Verify if the given package or DEPEND atom has the given flag. 
 Warning: This only works if the configuration files tree is in the correct 
 format (the one enforced by enforce_nice_config) 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' portage_config.has_flag license salt Apache-2.0'"
"def read_template(tmplf): 
    if pkg_resources.resource_exists('roscreate', tmplf): 
      f = pkg_resources.resource_stream('roscreate', tmplf) 
      t = f.read() 
   else: 
      r = rospkg.RosPack() 
      with open(os.path.join(r.get_path('roscreate'), 'templates', tmplf)) as f: 
         t = f.read() 
   try: 
      t = t.decode('utf-8') 
   except AttributeError: 
      pass 
   return t", 'Read a template file.',"'Read resource template from egg installation, or fallback on rospkg otherwise. 
 :returns: text of template file'"
"@celery_app.task(base=ArchiverTask, ignore_result=False) 
 @logged('make_copy_request') 
 def make_copy_request(job_pk, url, data): 
    create_app_context() 
   job = ArchiveJob.load(job_pk) 
   (src, dst, user) = job.info() 
   provider = data['source']['provider'] 
   logger.info('Sending   copy   request   for   addon:   {0}   on   node:   {1}'.format(provider, dst._id)) 
   res = requests.post(url, data=json.dumps(data)) 
   if (res.status_code not in (http.OK, http.CREATED, http.ACCEPTED)): 
      raise HTTPError(res.status_code)", 'Send a copy request to the provider.',"'Make the copy request to the WaterBulter API and handle 
 successful and failed responses 
 :param job_pk: primary key of ArchiveJob 
 :param url: URL to send request to 
 :param data: <dict> of setting to send in POST to WaterBulter API 
 :return: None'"
"def obtain_lock_id_to_hog(): 
    for id in board_ids(): 
      if _obtain_lock(id): 
         return id 
   return (-1)"," 'Obtains the lock ID for the HOG module. 
 Returns: 
 The lock ID for the HOG module, or -1 if no lock is available.'","'Finds a free id, locks it and returns integer id, or -1 if none free. 
 * Lock must be freed manually *'"
"def create_instance(options): 
    project = get_project(options) 
   print 'Creating   instance   {project}/{zone}/{instance}'.format(project=project, zone=get_zone(options), instance=options.instance) 
   print '      with   --machine_type={type}   and   --disk_size={disk_size}...'.format(type=options.machine_type, disk_size=options.disk_size) 
   google_dev_dir = os.path.join(os.path.dirname(__file__), '../google/dev') 
   dev_dir = os.path.dirname(__file__) 
   project_dir = os.path.join(dev_dir, '..') 
   install_dir = '{dir}/../install'.format(dir=dev_dir) 
   startup_command = ['/opt/spinnaker/install/install_spinnaker.sh   --dependencies_only', '/opt/spinnaker/install/install_development.sh'] 
   (fd, temp_startup) = tempfile.mkstemp() 
   os.write(fd, ';'.join(startup_command)) 
   os.close(fd) 
   metadata_files = ['startup-script={google_dev_dir}/google_install_loader.py,sh_bootstrap_dev={dev_dir}/bootstrap_dev.sh,sh_install_spinnaker={project_dir}/InstallSpinnaker.sh,sh_install_development={dev_dir}/install_development.sh,startup_command={temp_startup}'.format(google_dev_dir=google_dev_dir, dev_dir=dev_dir, project_dir=project_dir, temp_startup=temp_startup)] 
   metadata = ','.join(['startup_loader_files=sh_install_spinnaker+sh_install_development+sh_bootstrap_dev']) 
   command = ['gcloud', 'compute', 'instances', 'create', options.instance, '--project', get_project(options), '--zone', get_zone(options), '--machine-type', options.machine_type, '--image', 'ubuntu-14-04', '--scopes', 'compute-rw,storage-rw', '--boot-disk-size={size}'.format(size=options.disk_size), '--boot-disk-type={type}'.format(type=options.disk_type), '--metadata', metadata, '--metadata-from-file={files}'.format(files=','.join(metadata_files))] 
   if options.address: 
      command.extend(['--address', options.address]) 
   check_run_quick('   '.join(command), echo=False)", 'Create an instance in the specified zone.','Creates new GCE VM instance for development.'
"def make_or_verify_needed_dirs(config): 
    make_or_verify_core_dir(config.config_dir, constants.CONFIG_DIRS_MODE, os.geteuid(), config.strict_permissions) 
   make_or_verify_core_dir(config.work_dir, constants.CONFIG_DIRS_MODE, os.geteuid(), config.strict_permissions) 
   make_or_verify_core_dir(config.logs_dir, 448, os.geteuid(), config.strict_permissions)", 'Make or verify the directories needed by the config system.',"'Create or verify existance of config, work, or logs directories'"
"def name_validator(value, context): 
    if (not isinstance(value, basestring)): 
      raise Invalid(_('Names   must   be   strings')) 
   if (value in ['new', 'edit', 'search']): 
      raise Invalid(_('That   name   cannot   be   used')) 
   if (len(value) < 2): 
      raise Invalid((_('Must   be   at   least   %s   characters   long') % 2)) 
   if (len(value) > PACKAGE_NAME_MAX_LENGTH): 
      raise Invalid((_('Name   must   be   a   maximum   of   %i   characters   long') % PACKAGE_NAME_MAX_LENGTH)) 
   if (not name_match.match(value)): 
      raise Invalid(_('Must   be   purely   lowercase   alphanumeric   (ascii)   characters   and   these   symbols:   -_')) 
   return value"," 'Validate the name of a package. 
 :param value: The name to validate 
 :type value: string 
 :param context: The context to validate in 
 :type context: string 
 :raise Invalid: If the name is invalid'","'Return the given value if it\'s a valid name, otherwise raise Invalid. 
 If it\'s a valid name, the given value will be returned unmodified. 
 This function applies general validation rules for names of packages, 
 groups, users, etc. 
 Most schemas also have their own custom name validator function to apply 
 custom validation rules after this function, for example a 
 ``package_name_validator()`` to check that no package with the given name 
 already exists. 
 :raises ckan.lib.navl.dictization_functions.Invalid: if ``value`` is not 
 a valid name'"
"def repeat(x, repeats, axis=None): 
    repeats = tensor.as_tensor_variable(repeats) 
   if (repeats.ndim > 1): 
      raise ValueError('The   dimension   of   repeats   should   not   exceed   1.') 
   if ((repeats.ndim == 1) and (not repeats.broadcastable[0])): 
      return RepeatOp(axis=axis)(x, repeats) 
   else: 
      if (repeats.ndim == 1): 
         repeats = repeats[0] 
      if (x.dtype == 'uint64'): 
         raise TypeError(""theano.tensor.repeat   don't   support   dtype   uint64"") 
      if (axis is None): 
         axis = 0 
         x = x.flatten() 
      else: 
         if (axis >= x.ndim): 
            raise ValueError('Axis   should   not   exceed   x.ndim-1.') 
         if (axis < 0): 
            axis = (x.ndim + axis) 
      shape = [x.shape[i] for i in xrange(x.ndim)] 
      shape_ = shape[:] 
      shape_.insert((axis + 1), repeats) 
      shape[axis] = (shape[axis] * repeats) 
      dims_ = list(numpy.arange(x.ndim)) 
      dims_.insert((axis + 1), 'x') 
      z = tensor.alloc(x.dimshuffle(*dims_), *shape_).reshape(shape) 
      return z"," 'Repeat the given tensor ``x`` ``repeats`` times along the given axis. 
 Parameters 
 x : tensor 
 The tensor to repeat. 
 repeats : int 
 The number of times to repeat. 
 axis : int, optional 
 The axis to repeat along. If None, the axis is chosen to be the last 
 axis. 
 Returns 
 tensor 
 The repeated tensor. 
 Examples 
 >>> x = T.matrix(""x"") 
 >>> x = T.tensor3(""x"", [1, 2, 3]) 
 >>> repeat(x, 3) 
 tensor(1, 2, 3) 
 >>> repeat(x, 2) 
 tensor(1, 2, 3) 
 >>> repeat(x, 2, 1) 
 tensor(1, 2, 3) 
 >>> repeat(x, 2, 0) 
 tensor(1, 2, 3) 
 >>> repeat(x, 3, 0) 
 tensor(1, 2, 3)'","'Repeat elements of an array. 
 It returns an array which has the same shape as `x`, except 
 along the given axis. The axis is used to speficy along which 
 axis to repeat values. By default, use the flattened input 
 array, and return a flat output array. 
 The number of repetitions for each element is `repeat`. 
 `repeats` is broadcasted to fit the length of the given `axis`. 
 Parameters 
 x 
 Input data, tensor variable. 
 repeats 
 int, scalar or tensor variable 
 axis : int, optional 
 See Also 
 tensor.tile 
 .. versionadded:: 0.6'"
"@pytest.mark.hasgpu 
 def test_hist(nbin_offset_dim_dtype_inp, backend_pair): 
    ((nbins, offset), dim, dtype, (name, inp_gen)) = nbin_offset_dim_dtype_inp 
   gpuflag = (check_gpu.get_compute_capability(0) >= 3.0) 
   if (gpuflag is False): 
      raise RuntimeError('Device   does   not   have   CUDA   compute   capability   3.0   or   greater') 
   (ng, nc) = backend_pair 
   ng.set_hist_buffers(nbins, offset) 
   nc.set_hist_buffers(nbins, offset) 
   np_inp = inp_gen(dim).astype(dtype) 
   np_hist = ref_hist(np_inp, nbins=nbins, offset=offset) 
   for be in [ng, nc]: 
      be_inp = be.array(np_inp, dtype=dtype) 
      be_hist = be_inp.hist(name) 
      assert tensors_allclose(np_hist, be_hist)", 'Test histogram with a numpy input',"'Compare the nervanagpu and nervanacpu hist implementation to the reference 
 implementation above. 
 Parameterized test case, uses pytest_generate_test to enumerate dim_dtype_inp 
 tuples that drive the test.'"
"def _estimate_rank_meeg_cov(data, info, scalings, tol='auto', return_singular=False): 
    picks_list = _picks_by_type(info) 
   scalings = _handle_default('scalings_cov_rank', scalings) 
   _apply_scaling_cov(data, picks_list, scalings) 
   if (data.shape[1] < data.shape[0]): 
      ValueError(""You've   got   fewer   samples   than   channels,   your   rank   estimate   might   be   inaccurate."") 
   out = estimate_rank(data, tol=tol, norm=False, return_singular=return_singular) 
   rank = (out[0] if isinstance(out, tuple) else out) 
   ch_type = '   +   '.join(list(zip(*picks_list))[0]) 
   logger.info(('estimated   rank   (%s):   %d' % (ch_type, rank))) 
   _undo_scaling_cov(data, picks_list, scalings) 
   return out"," 'Estimate the rank of the covariance matrix for the data. 
 Parameters 
 data : ndarray 
 The data to estimate the rank of. 
 info : dict 
 Information about the data (e.g. channels, sampling rate, etc.) 
 scalings : dict 
 Dictionary of scalings to apply to the data. 
 tol : float, optional 
 Tolerance for the rank estimate. 
 return_singular : bool, optional 
 Return the singular values. 
 Returns 
 out : ndarray 
 The estimated rank. 
 Notes 
 The rank of the covariance matrix is the number of independent components 
 in the data. 
 Examples 
 >>> data = np.random.randn(10, 10) 
 >>> info = dict(ch_names=[""ch1"", ""ch2"", ""ch3"", ""ch4""], 
 ...            ch_types=[""EEG"", ""EEG"", ""EEG"", ""EEG""], 
 ...            sfreq=100.0, 
 ...            sampling_rate=100.0, 
","'Estimate rank of M/EEG covariance data, given the covariance. 
 Parameters 
 data : np.ndarray of float, shape (n_channels, n_channels) 
 The M/EEG covariance. 
 info : Info 
 The measurment info. 
 scalings : dict | \'norm\' | np.ndarray | None 
 The rescaling method to be applied. If dict, it will override the 
 following default dict: 
 dict(mag=1e12, grad=1e11, eeg=1e5) 
 If \'norm\' data will be scaled by channel-wise norms. If array, 
 pre-specified norms will be used. If None, no scaling will be applied. 
 tol : float | str 
 Tolerance. See ``estimate_rank``. 
 return_singular : bool 
 If True, also return the singular values that were used 
 to determine the rank. 
 Returns 
 rank : int 
 Estimated rank of the data. 
 s : array 
 If return_singular is True, the singular values that were 
 thresholded to determine the rank are also returned.'"
"def _force_mutable(x): 
    if getattr(x, 'is_Matrix', False): 
      return x.as_mutable() 
   elif isinstance(x, Basic): 
      return x 
   elif hasattr(x, '__array__'): 
      a = x.__array__() 
      if (len(a.shape) == 0): 
         return sympify(a) 
      return Matrix(x) 
   return x", 'Forces a symbolic expression to be mutable.',"'Return a matrix as a Matrix, otherwise return x.'"
"def _check_cron_env(user, name, value=None): 
    if (value is None): 
      value = '' 
   lst = __salt__['cron.list_tab'](user) 
   for env in lst['env']: 
      if (name == env['name']): 
         if (value != env['value']): 
            return 'update' 
         return 'present' 
   return 'absent'"," 'Check cron environment variable 
 name, value: Name and value of environment variable 
 user: User to check environment variable for 
 Examples: 
 >>> _check_cron_env(\'user1\', \'NAME\') 
 \'absent\' 
 >>> _check_cron_env(\'user1\', \'NAME\', \'NAME=VALUE\') 
 \'present\' 
 >>> _check_cron_env(\'user1\', \'NAME\', \'NAME=VALUE\') 
 \'update\''",'Return the environment changes'
"def test_daophot_indef(): 
    table = ascii.read('t/daophot2.dat', Reader=ascii.Daophot) 
   for colname in table.colnames: 
      mask_value = (colname in ('OTIME', 'MAG', 'MERR', 'XAIRMASS')) 
      assert np.all((table[colname].mask == mask_value))", 'Test that the masking is correct for the indef columns.','Test that INDEF is correctly interpreted as a missing value'
"def delete_vpc_peering_connection(name, conn_id=None, conn_name=None, region=None, key=None, keyid=None, profile=None): 
    log.debug('Called   state   to   delete   VPC   peering   connection') 
   ret = {'name': name, 'result': True, 'changes': {}, 'comment': 'Boto   VPC   peering   state'} 
   if conn_name: 
      vpc_ids = __salt__['boto_vpc.describe_vpc_peering_connection'](conn_name, region=region, key=key, keyid=keyid, profile=profile).get('VPC-Peerings', []) 
   else: 
      vpc_ids = [conn_id] 
   if (not vpc_ids): 
      ret['comment'] = 'No   VPC   connection   found,   nothing   to   be   done.' 
      return ret 
   if __opts__['test']: 
      if vpc_ids: 
         ret['comment'] = 'VPC   peering   connection   would   be   deleted' 
      return ret 
   log.debug('Called   module   to   delete   VPC   peering   connection') 
   result = __salt__['boto_vpc.delete_vpc_peering_connection'](conn_id=conn_id, conn_name=conn_name, region=region, key=key, keyid=keyid, profile=profile) 
   if ('error' in result): 
      ret['comment'] = 'Failed   to   delete   VPC   peering:   {0}'.format(result['error']) 
      ret['result'] = False 
      return ret 
   ret['changes'].update({'old': '', 'new': result['msg']}) 
   return ret", 'Delete VPC peering connection.',"'name 
 Name of the state 
 conn_id 
 ID of the peering connection to delete.  Exlusive with conn_name. 
 conn_name 
 The name of the peering connection to delete.  Exlusive with conn_id. 
 region 
 Region to connect to. 
 key 
 Secret key to be used. 
 keyid 
 Access key to be used. 
 profile 
 A dict with region, key and keyid, or a pillar key (string) that 
 contains a dict with region, key and keyid. 
 .. versionadded:: 2016.11.0 
 Example: 
 .. code-block:: yaml 
 delete a vpc peering connection: 
 boto_vpc.delete_vpc_peering_connection: 
 - region: us-west-2 
 - conn_id: pcx-4613b12e 
 Connection name can be specified (instead of ID). 
 Specifying both conn_name and conn_id will result in an 
 error. 
 .. code-block:: yaml 
 delete a vpc peering connection: 
 boto_vpc.delete_vpc_peering_connection: 
 - conn_name: salt_vpc_peering'"
"def _type_check(arg, msg): 
    if (arg is None): 
      return type(None) 
   if isinstance(arg, basestring): 
      arg = _ForwardRef(arg) 
   if ((isinstance(arg, _TypingBase) and (type(arg).__name__ == u'_ClassVar')) or ((not isinstance(arg, (type, _TypingBase))) and (not callable(arg)))): 
      raise TypeError((msg + (u'   Got   %.100r.' % (arg,)))) 
   if (((type(arg).__name__ in (u'_Union', u'_Optional')) and (not getattr(arg, u'__origin__', None))) or (isinstance(arg, TypingMeta) and (_gorg(arg) in (Generic, _Protocol)))): 
      raise TypeError((u'Plain   %s   is   not   valid   as   type   argument' % arg)) 
   return arg"," 'Checks that the given argument is of the correct type. 
 :param arg: The argument to check. 
 :param msg: A string to be appended to the error message. 
 :raises TypeError: If the argument is not of the correct type. 
 :raises ValueError: If the argument is a plain class or union. 
 :raises TypeError: If the argument is not a class or union.'","'Check that the argument is a type, and return it (internal helper). 
 As a special case, accept None and return type(None) instead. 
 Also, _TypeAlias instances (e.g. Match, Pattern) are acceptable. 
 The msg argument is a human-readable error message, e.g. 
 ""Union[arg, ...]: arg should be a type."" 
 We append the repr() of the actual value (truncated to 100 chars).'"
"def test_multiclass_error(): 
    y = np.linspace(0, 1, 15) 
   iht = InstanceHardnessThreshold(random_state=RND_SEED) 
   assert_warns(UserWarning, iht.fit, X, y) 
   y = np.array(((([0] * 10) + ([1] * 3)) + ([2] * 2))) 
   iht = InstanceHardnessThreshold(random_state=RND_SEED) 
   assert_warns(UserWarning, iht.fit, X, y)", 'Test for multiclass error.',"'Test either if an error is raised when the target are not binary 
 type.'"
"def add_password_arg(cmd, psw, required=False): 
    if (UNRAR_TOOL == ALT_TOOL): 
      return 
   if (psw is not None): 
      cmd.append(('-p' + psw)) 
   else: 
      cmd.append('-p-')", 'Add password arg to command','Append password switch to commandline.'
"def getNewDerivation(elementNode): 
    return SolidDerivation(elementNode)"," 'Returns a SolidDerivation object for the given node. 
 :param elementNode: The node to get the derivation for. 
 :returns: A SolidDerivation object.'",'Get new derivation.'
"def _set_sentinel(): 
    return LockType()", 'Set the sentinel lock type.','Dummy implementation of _thread._set_sentinel().'
"def formset_factory(form, formset=BaseFormSet, extra=1, can_order=False, can_delete=False, max_num=None, validate_max=False, min_num=None, validate_min=False): 
    if (min_num is None): 
      min_num = DEFAULT_MIN_NUM 
   if (max_num is None): 
      max_num = DEFAULT_MAX_NUM 
   absolute_max = (max_num + DEFAULT_MAX_NUM) 
   attrs = {'form': form, 'extra': extra, 'can_order': can_order, 'can_delete': can_delete, 'min_num': min_num, 'max_num': max_num, 'absolute_max': absolute_max, 'validate_min': validate_min, 'validate_max': validate_max} 
   return type((form.__name__ + 'FormSet'), (formset,), attrs)"," 'Create a formset factory. 
 :param form: The form to use for this formset. 
 :param formset: The formset class to use for this formset. 
 :param extra: The number of extra form elements to add to the formset. 
 :param can_order: Whether the forms can be ordered. 
 :param can_delete: Whether the forms can be deleted. 
 :param max_num: The maximum number of forms in the formset. 
 :param validate_max: Whether to validate the maximum number of forms. 
 :param min_num: The minimum number of forms in the formset. 
 :param validate_min: Whether to validate the minimum number of forms. 
 :returns: A formset factory. 
 :rtype: type'",'Return a FormSet for the given form class.'
"def onLoggerAppShutDown(): 
    INFO_MSG('onLoggerAppShutDown()')"," 'Called when the logger app is shut down. 
 This method is called when the logger app is shut down. 
 This method is called when the logger app is shut down. 
 This method is called when the logger app is shut down. 
 This method is called when the logger app is shut down. 
 This method is called when the logger app is shut down. 
 This method is called when the logger app is shut down. 
 This method is called when the logger app is shut down. 
 This method is called when the logger app is shut down. 
 This method is called when the logger app is shut down. 
 This method is called when the logger app is shut down. 
 This method is called when the logger app is shut down. 
 This method is called when the logger app is shut down. 
 This method is called when the logger app is shut down. 
 This method is called when the logger app is shut down. 
 This method is called when the logger app is shut down. 
 This method is called when the logger app is shut down. 
 This method is called when the logger app is shut down. 
 This method is called when","'KBEngine method. 
 è¿ä¸ªloggerè¢«å³é­åçåè°å½æ°'"
"def project_get_networks(context, project_id, associate=True): 
    return IMPL.project_get_networks(context, project_id, associate)", 'Get all networks associated with project.',"'Return the network associated with the project. 
 If associate is true, it will attempt to associate a new 
 network if one is not found, otherwise it returns None.'"
"def new_test_client(cls, **kwargs): 
    client = cls(debug_logging=True) 
   client.login(**kwargs) 
   return client"," 'Return a new TestClient instance with the given credentials. 
 :param cls: The class to use for the client 
 :param kwargs: The kwargs to use to login to the client 
 :returns: A new TestClient instance 
 :rtype: :class:`~.client.TestClient`'","'Make an instance of a client, login, and return it. 
 kwargs are passed through to cls.login().'"
"def relpath_to_site(lang, target_lang): 
    path = _SITES_RELPATH_DB.get((lang, target_lang), None) 
   if (path is None): 
      siteurl = _SITE_DB.get(lang, _MAIN_SITEURL) 
      target_siteurl = _SITE_DB.get(target_lang, _MAIN_SITEURL) 
      path = posixpath.relpath(get_site_path(target_siteurl), get_site_path(siteurl)) 
      _SITES_RELPATH_DB[(lang, target_lang)] = path 
   return path"," 'Return the relative path from the given target site to the given source site. 
 This is a cached value, so this method may be called multiple times for the 
 same source and target sites. 
 :param lang: the source language 
 :param target_lang: the target language 
 :return: the relative path from the target site to the source site'","'Get relative path from siteurl of lang to siteurl of base_lang 
 the output is cached in _SITES_RELPATH_DB'"
"def add_nic(si, vm, network): 
    spec = vim.vm.ConfigSpec() 
   nic_changes = [] 
   nic_spec = vim.vm.device.VirtualDeviceSpec() 
   nic_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add 
   nic_spec.device = vim.vm.device.VirtualE1000() 
   nic_spec.device.deviceInfo = vim.Description() 
   nic_spec.device.deviceInfo.summary = 'vCenter   API   test' 
   nic_spec.device.backing = vim.vm.device.VirtualEthernetCard.NetworkBackingInfo() 
   nic_spec.device.backing.useAutoDetect = False 
   content = si.RetrieveContent() 
   nic_spec.device.backing.network = get_obj(content, [vim.Network], network) 
   nic_spec.device.backing.deviceName = network 
   nic_spec.device.connectable = vim.vm.device.VirtualDevice.ConnectInfo() 
   nic_spec.device.connectable.startConnected = True 
   nic_spec.device.connectable.startConnected = True 
   nic_spec.device.connectable.allowGuestControl = True 
   nic_spec.device.connectable.connected = False 
   nic_spec.device.connectable.status = 'untried' 
   nic_spec.device.wakeOnLanEnabled = True 
   nic_spec.device.addressType = 'assigned' 
   nic_changes.append(nic_spec) 
   spec.deviceChange = nic_changes 
   e = vm.ReconfigVM_Task(spec=spec) 
   print 'NIC   CARD   ADDED'", 'Add a NIC to the VM.',"':param si: Service Instance 
 :param vm: Virtual Machine Object 
 :param network: Virtual Network'"
"def load_tests(loader, tests, pattern): 
    test_dir = os.path.join(os.path.dirname(__file__), TESTS_DIR) 
   return driver.build_tests(test_dir, loader, host=None, intercept=fixture_module.setup_app, fixture_module=fixture_module)"," 'Loads tests from the tests/ directory. 
 :param loader: The unittest loader. 
 :param tests: The tests to run. 
 :param pattern: The pattern to match tests against. 
 :returns: The test suite.'",'Provide a TestSuite to the discovery process.'
"def format_value(val, limit=100, level=10): 
    return _format_value(val, limit, level)"," 'Format a value for display. 
 :param val: A value to format. 
 :param limit: The maximum number of characters to display. 
 :param level: The number of decimal places to display. 
 :returns: A formatted value. 
 :rtype: str'",'Wrapper around _format_value().'
"def default_formats(): 
    return {'html': {'nbconvert_template': 'basic', 'label': 'Notebook', 'icon': 'book'}, 'slides': {'nbconvert_template': 'slides_reveal', 'label': 'Slides', 'icon': 'gift', 'test': (lambda nb, json: ('""slideshow""' in json))}, 'script': {'label': 'Code', 'icon': 'code', 'content_type': 'text/plain;   charset=UTF-8'}}"," 'Return a dict of default formats. 
 This dict contains the keys: 
 - \'nbconvert_template\': The name of the nbconvert template to use for the 
 given format. 
 - \'label\': The label to use for the given format. 
 - \'icon\': The icon to use for the given format. 
 - \'content_type\': The content type to use for the given format. 
 The format \'script\' has a special \'test\' key. If the given notebook is a 
 script, it returns True if the notebook has a \'slideshow\' key, otherwise it 
 returns False. 
 This function is used to provide default values for the formats key in the 
 output of :meth:`ipython.notebook.Notebook.nbconvert`.'","'Return the currently-implemented formats. 
 These are not classes, but maybe should be: would they survive pickling? 
 - exporter: 
 an Exporter subclass. 
 if not defined, and key is in nbconvert.export.exporter_map, it will be added 
 automatically 
 - nbconvert_template: 
 the name of the nbconvert template to add to config.ExporterClass 
 - test: 
 a function(notebook_object, notebook_json) 
 conditionally offer a format based on content if truthy. see 
 `RenderingHandler.filter_exporters` 
 - postprocess: 
 a function(html, resources) 
 perform any modifications to html and resources after nbconvert 
 - content_Type: 
 a string specifying the Content-Type of the response from this format. 
 Defaults to  text/html; charset=UTF-8'"
"def get_latest_flexget_version_number(): 
    try: 
      page = requests.get(u'http://download.flexget.com/latestversion') 
   except requests.RequestException: 
      return 
   ver = page.text.strip() 
   return ver", 'Get the latest FlexGet version number.','Return latest Flexget version from http://download.flexget.com/latestversion'
"def edit_service(protocol=None, service_address=None, scheduler=None): 
    cmd = '{0}   -E   {1}'.format(__detect_os(), _build_cmd(protocol=protocol, service_address=service_address, scheduler=scheduler)) 
   out = __salt__['cmd.run_all'](cmd, python_shell=False) 
   if out['retcode']: 
      ret = out['stderr'].strip() 
   else: 
      ret = True 
   return ret"," 'Edit the configuration of the service. 
 service_address: The address of the service to edit. 
 scheduler: The scheduler to edit. 
 Returns: 
 True if the service was edited successfully, False otherwise.'","'Edit the virtual service. 
 protocol 
 The service protocol(only support tcp, udp and fwmark service). 
 service_address 
 The LVS service address. 
 scheduler 
 Algorithm for allocating TCP connections and UDP datagrams to real servers. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' lvs.edit_service tcp 1.1.1.1:80 rr'"
"@register_vcs_handler('git', 'keywords') 
 def git_versions_from_keywords(keywords, tag_prefix, verbose): 
    if (not keywords): 
      raise NotThisMethod('no   keywords   at   all,   weird') 
   date = keywords.get('date') 
   if (date is not None): 
      date = date.strip().replace('   ', 'T', 1).replace('   ', '', 1) 
   refnames = keywords['refnames'].strip() 
   if refnames.startswith('$Format'): 
      if verbose: 
         print('keywords   are   unexpanded,   not   using') 
      raise NotThisMethod('unexpanded   keywords,   not   a   git-archive   tarball') 
   refs = set([r.strip() for r in refnames.strip('()').split(',')]) 
   TAG = 'tag:   ' 
   tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)]) 
   if (not tags): 
      tags = set([r for r in refs if re.search('\\d', r)]) 
      if verbose: 
         print((""discarding   '%s',   no   digits"" % ','.join((refs - tags)))) 
   if verbose: 
      print(('likely   tags:   %s' % ','.join(sorted(tags)))) 
   for ref in sorted(tags): 
      if ref.startswith(tag_prefix): 
         r = ref[len(tag_prefix):] 
         if verbose: 
            print(('picking   %s' % r)) 
         return {'version': r, 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': None, 'date': date} 
   if verbose: 
      print('no   suitable   tags,   using   unknown   +   full   revision   id') 
   return {'version': '0+unknown', 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': 'no   suitable   tags', 'date': None}"," 'Pick out the version from the keywords. 
 :param keywords: A dict of keywords, or a list of keywords. 
 :param tag_prefix: The prefix to strip from the tag name. 
 :param verbose: Whether to print out debugging info. 
 :returns: A dict of version, full-revisionid, dirty, error, date. 
 :rtype: dict'",'Get version information from git keywords.'
"def value_present(name, datastore, path, config): 
    ret = {'name': name, 'result': False, 'changes': {}, 'comment': ''} 
   existing = __salt__['cisconso.get_data'](datastore, path) 
   if cmp(existing, config): 
      ret['result'] = True 
      ret['comment'] = 'Config   is   already   set' 
   elif (__opts__['test'] is True): 
      ret['result'] = None 
      ret['comment'] = 'Config   will   be   added' 
      diff = _DictDiffer(existing, config) 
      ret['changes']['new'] = diff.added() 
      ret['changes']['removed'] = diff.removed() 
      ret['changes']['changed'] = diff.changed() 
   else: 
      __salt__['cisconso.set_data_value'](datastore, path, config) 
      ret['result'] = True 
      ret['comment'] = 'Successfully   added   config' 
      diff = _DictDiffer(existing, config) 
      ret['changes']['new'] = diff.added() 
      ret['changes']['removed'] = diff.removed() 
      ret['changes']['changed'] = diff.changed() 
   return ret"," 'Check if a config value is present on the device. 
 This function will check if the given config value is present on the 
 device. If the config value is already present, the function will return 
 with the result set to True. If the config value is not present, the 
 function will return with the result set to False. 
 :param name: The name of the config value to check. 
 :param datastore: The name of the data store to use. 
 :param path: The path to the config value to check. 
 :param config: The config value to check against. 
 :return: The result of the check.'","'Ensure a specific value exists at a given path 
 :param name: The name for this rule 
 :type  name: ``str`` 
 :param datastore: The datastore, e.g. running, operational. 
 One of the NETCONF store IETF types 
 :type  datastore: :class:`DatastoreType` (``str`` enum). 
 :param path: The device path to set the value at, 
 a list of element names in order, / seperated 
 :type  path: ``list``, ``str`` OR ``tuple`` 
 :param config: The new value at the given path 
 :type  config: ``dict`` 
 Examples: 
 .. code-block:: yaml 
 enable pap auth: 
 cisconso.config_present: 
 - name: enable_pap_auth 
 - datastore: running 
 - path: devices/device/ex0/config/sys/interfaces/serial/ppp0/authentication 
 - config: 
 authentication: 
 method: pap 
 ""list-name"": foobar'"
"def test_cache_deactivated_get_data(config_stub, tmpdir): 
    config_stub.data = {'storage': {'cache-size': 1024}, 'general': {'private-browsing': True}} 
   disk_cache = cache.DiskCache(str(tmpdir)) 
   url = QUrl('http://www.example.com/') 
   assert (disk_cache.data(url) is None)", 'Test that get_data() returns None when cache is disabled.','Query some data from a deactivated cache.'
"@hook.command('wordusage', 'wordexample', 'usage') 
 def word_usage(text): 
    if (not api_key): 
      return 'This   command   requires   an   API   key   from   wordnik.com.' 
   word = sanitize(text) 
   url = (API_URL + 'word.json/{}/examples'.format(word)) 
   params = {'api_key': api_key, 'limit': 10} 
   json = requests.get(url, params=params).json() 
   if json: 
      out = '\x02{}\x02:   '.format(word) 
      example = random.choice(json['examples']) 
      out += '{}   '.format(example['text']) 
      return '   '.join(out.split()) 
   else: 
      return 'I   could   not   find   any   usage   examples   for   \x02{}\x02.'.format(word)"," 'Get usage examples for a word. 
 Example: 
 .. code-block:: 
 :nowrap: 
 >>> word_usage \'soup\' 
 \'Soup   is   a   broth   or   liquid   food   that   is   made   by   cooking   or   processing   vegetables   or   meat   with   liquid. 
 Soup   is   typically   served   with   bread   on   the   side.\' 
 :nowrap: 
 >>> word_usage \'soup\' 
 \'Soup   is   a   broth   or   liquid   food   that   is   made   by   cooking   or   processing   vegetables   or   meat   with   liquid. 
 Soup   is   typically   served   with   bread   on   the   side.\''",'<word> -- Returns an example sentence showing the usage of <word>.'
"def ui_open(*files): 
    if files: 
      osname = get_os_name() 
      opener = _OPENER_BY_OS.get(osname) 
      if opener: 
         opener(files) 
      else: 
         raise OpenError((u'Open   currently   not   supported   for   ' + osname))"," 'Open the given files. 
 :param files: the files to open 
 :return: None'","'Attempts to open the given files using the preferred desktop viewer or editor. 
 :raises :class:`OpenError`: if there is a problem opening any of the files.'"
"def is_pure_elemwise(graph, inputs): 
    allowed_ops = (tensor.basic.DimShuffle, tensor.basic.Elemwise) 
   owner = graph.owner 
   op = (graph.owner.op if (graph.owner is not None) else None) 
   if ((owner is None) and (graph in inputs)): 
      return True 
   elif ((owner is None) and isinstance(graph, tensor.basic.TensorConstant)): 
      return True 
   elif ((owner is None) and (graph not in inputs)): 
      return False 
   elif ((op is not None) and (not isinstance(op, allowed_ops))): 
      return False 
   else: 
      if isinstance(graph.owner.op, tensor.basic.DimShuffle): 
         shuffled = graph.owner.inputs[0] 
         if (not isinstance(shuffled, tensor.basic.TensorConstant)): 
            return False 
      for inp in graph.owner.inputs: 
         if (not is_pure_elemwise(inp, inputs)): 
            return False 
      return True"," 'Returns True if graph is pure elemwise, False otherwise. 
 Args: 
 graph: The graph to check. 
 inputs: The inputs of the graph. 
 Returns: 
 True if graph is pure elemwise, False otherwise.'","'Checks whether a graph is purely elementwise and containing only 
 inputs from a given list. 
 Parameters 
 graph : TensorVariable object 
 Graph to perform checks against. 
 inputs : list 
 List of acceptable inputs to the graph. 
 Returns 
 elemwise_or_not : bool 
 Returns `True` if 
 a) everything in the graph is an Elemwise or a DimShuffle 
 (DimShuffles are only acceptable to broadcast up constants) 
 and 
 b) all nodes without an owner appear in `inputs` or are 
 constants. 
 Returns `False` otherwise.'"
"def audio_codec(): 
    rebulk = Rebulk().regex_defaults(flags=re.IGNORECASE, abbreviations=[dash]).string_defaults(ignore_case=True) 
   def audio_codec_priority(match1, match2): 
      '\n                        Gives   priority   to   audio_codec\n                        :param   match1:\n                        :type   match1:\n                        :param   match2:\n                        :type   match2:\n                        :return:\n                        :rtype:\n                        ' 
      if ((match1.name == 'audio_codec') and (match2.name in ['audio_profile', 'audio_channels'])): 
         return match2 
      if ((match1.name in ['audio_profile', 'audio_channels']) and (match2.name == 'audio_codec')): 
         return match1 
      return '__default__' 
   rebulk.defaults(name='audio_codec', conflict_solver=audio_codec_priority) 
   rebulk.regex('MP3', 'LAME', 'LAME(?:\\d)+-?(?:\\d)+', value='MP3') 
   rebulk.regex('Dolby', 'DolbyDigital', 'Dolby-Digital', 'DD', value='DolbyDigital') 
   rebulk.regex('DolbyAtmos', 'Dolby-Atmos', 'Atmos', value='DolbyAtmos') 
   rebulk.regex('AAC', value='AAC') 
   rebulk.regex('AC3D?', value='AC3') 
   rebulk.regex('Flac', value='FLAC') 
   rebulk.regex('DTS', value='DTS') 
   rebulk.regex('True-?HD', value='TrueHD') 
   rebulk.defaults(name='audio_profile') 
   rebulk.string('HD', value='HD', tags='DTS') 
   rebulk.regex('HD-?MA', value='HDMA', tags='DTS') 
   rebulk.string('HE', value='HE', tags='AAC') 
   rebulk.string('LC', value='LC', tags='AAC') 
   rebulk.string('HQ', value='HQ', tags='AC3') 
   rebulk.defaults(name='audio_channels') 
   rebulk.regex('(7[\\W_][01](?:ch)?)(?:[^\\d]|$)', value='7.1', children=True) 
   rebulk.regex('(5[\\W_][01](?:ch)?)(?:[^\\d]|$)', value='5.1', children=True) 
   rebulk.regex('(2[\\W_]0(?:ch)?)(?:[^\\d]|$)', value='2.0', children=True) 
   rebulk.string('7ch', '8ch', value='7.1') 
   rebulk.string('5ch', '6ch', value='5.1') 
   rebulk.string('2ch', 'stereo', value='2.0') 
   rebulk.string('1ch', 'mono', value='1.0') 
   rebulk.rules(DtsRule, AacRule, Ac3Rule, AudioValidatorRule, HqConflictRule) 
   return rebulk", 'Returns a Rebulk instance with audio codec rules.',"'Builder for rebulk object. 
 :return: Created Rebulk object 
 :rtype: Rebulk'"
"def get_bucket(conn, bucket_id): 
    bucket = conn.get_bucket(bucket_id) 
   if (not bucket): 
      msg = (_('Could   not   find   bucket   with   ID   %(bucket_id)s') % locals()) 
      LOG.debug(msg) 
      raise exception.NotFound(msg) 
   return bucket", 'Get a bucket by ID.',"'Get a bucket from an s3 connection 
 :param conn: The ``boto.s3.connection.S3Connection`` 
 :param bucket_id: ID of the bucket to fetch 
 :raises ``glance.exception.NotFound`` if bucket is not found.'"
"def range_error_message(error_message, what_to_enter, minimum, maximum): 
    if (error_message is None): 
      error_message = ('Enter   ' + what_to_enter) 
      if ((minimum is not None) and (maximum is not None)): 
         error_message += '   between   %(min)g   and   %(max)g' 
      elif (minimum is not None): 
         error_message += '   greater   than   or   equal   to   %(min)g' 
      elif (maximum is not None): 
         error_message += '   less   than   or   equal   to   %(max)g' 
   if (type(maximum) in [int, long]): 
      maximum -= 1 
   return (translate(error_message) % dict(min=minimum, max=maximum))"," 'Generate a range error message. 
 :param error_message: The error message to translate. 
 :type error_message: str 
 :param what_to_enter: The thing that is being asked for. 
 :type what_to_enter: str 
 :param minimum: The minimum value. 
 :type minimum: int or float 
 :param maximum: The maximum value. 
 :type maximum: int or float 
 :return: The translated error message.'",'build the error message for the number range validators'
"def instance_get_floating_address(context, instance_id): 
    return IMPL.instance_get_floating_address(context, instance_id)", 'Get the floating IP address associated with the instance.','Get the first floating ip address of an instance.'
"def _authenticate_mongo_cr(credentials, sock_info): 
    source = credentials.source 
   username = credentials.username 
   password = credentials.password 
   response = sock_info.command(source, {'getnonce': 1}) 
   nonce = response['nonce'] 
   key = _auth_key(nonce, username, password) 
   query = SON([('authenticate', 1), ('user', username), ('nonce', nonce), ('key', key)]) 
   sock_info.command(source, query)"," 'Authenticate against the mongo server. 
 :param credentials: Credentials to use. 
 :param sock_info: The socket info to use. 
 :return: None'",'Authenticate using MONGODB-CR.'
"def entails(expr, formula_set={}): 
    formula_set = list(formula_set) 
   formula_set.append(Not(expr)) 
   return (not satisfiable(And(*formula_set)))"," 'Return True if the given formula is a logical consequence of the 
 given formula set. 
 >>> entails(Not(P), [P]) 
 True 
 >>> entails(P, [P]) 
 True 
 >>> entails(P, [P, Not(P)]) 
 True 
 >>> entails(P, [P, Not(P), Not(P)]) 
 True 
 >>> entails(P, [P, Not(P), Not(P), Not(P)]) 
 False 
 >>> entails(P, [P, Not(P), Not(P), Not(P), Not(P)]) 
 True 
 >>> entails(P, [P, Not(P), Not(P), Not(P), Not(P), Not(P)]) 
 False 
 >>> entails(P, [P, Not(P), Not(P), Not(P), Not(P), Not(P), Not(P)]) 
 True 
 >>> entails(P, [P, Not(P), Not(P), Not(P), Not(P), Not(P), Not(","'Check whether the given expr_set entail an expr. 
 If formula_set is empty then it returns the validity of expr. 
 Examples 
 >>> from sympy.abc import A, B, C 
 >>> from sympy.logic.inference import entails 
 >>> entails(A, [A >> B, B >> C]) 
 False 
 >>> entails(C, [A >> B, B >> C, A]) 
 True 
 >>> entails(A >> B) 
 False 
 >>> entails(A >> (B >> A)) 
 True 
 References 
 .. [1] http://en.wikipedia.org/wiki/Logical_consequence'"
"def spawn_raw(function, *args, **kwargs): 
    if (not callable(function)): 
      raise TypeError('function   must   be   callable') 
   hub = get_hub() 
   if kwargs: 
      function = _functools_partial(function, *args, **kwargs) 
      g = RawGreenlet(function, hub) 
      hub.loop.run_callback(g.switch) 
   else: 
      g = RawGreenlet(function, hub) 
      hub.loop.run_callback(g.switch, *args) 
   return g"," 'Spawns a greenlet with the given function, args and kwargs. 
 The greenlet will be run in the same thread as the hub. 
 The greenlet will be spawned as a raw greenlet, 
 and the hub\'s loop will be used to run the callback. 
 This is useful for creating greenlets that are not 
 part of the hub\'s event loop. 
 :param function: the function to call when the greenlet is run 
 :param args: the arguments to pass to the function 
 :param kwargs: the keyword arguments to pass to the function 
 :return: a greenlet that will be run in the same thread as the hub 
 :rtype: :class:`Greenlet` 
 :raises TypeError: if function is not callable'","'Create a new :class:`greenlet.greenlet` object and schedule it to 
 run ``function(*args, **kwargs)``. 
 This returns a raw :class:`~greenlet.greenlet` which does not have all the useful 
 methods that :class:`gevent.Greenlet` has. Typically, applications 
 should prefer :func:`~gevent.spawn`, but this method may 
 occasionally be useful as an optimization if there are many 
 greenlets involved. 
 .. versionchanged:: 1.1b1 
 If *function* is not callable, immediately raise a :exc:`TypeError` 
 instead of spawning a greenlet that will raise an uncaught TypeError. 
 .. versionchanged:: 1.1rc2 
 Accept keyword arguments for ``function`` as previously (incorrectly) 
 documented. Note that this may incur an additional expense. 
 .. versionchanged:: 1.1a3 
 Verify that ``function`` is callable, raising a TypeError if not. Previously, 
 the spawned greenlet would have failed the first time it was switched to.'"
"def isproxy(obj): 
    return orig_isinstance(obj, NetProxy)"," 'Return True if obj is an instance of NetProxy. 
 This is a simple wrapper around the isinstance() function that 
 checks for the presence of the NetProxy class.  This is used by 
 the ``is_proxy`` decorator. 
 :param obj: 
 The object to test. 
 :returns: 
 True if obj is an instance of NetProxy, False otherwise.'",'indicates whether the given object is a NetProxy'
"def _to_app_identity_error(error): 
    error_map = {app_identity_service_pb.AppIdentityServiceError.NOT_A_VALID_APP: InternalError, app_identity_service_pb.AppIdentityServiceError.DEADLINE_EXCEEDED: BackendDeadlineExceeded, app_identity_service_pb.AppIdentityServiceError.BLOB_TOO_LARGE: BlobSizeTooLarge, app_identity_service_pb.AppIdentityServiceError.UNKNOWN_ERROR: InternalError, app_identity_service_pb.AppIdentityServiceError.UNKNOWN_SCOPE: InvalidScope, app_identity_service_pb.AppIdentityServiceError.NOT_ALLOWED: NotAllowed, app_identity_service_pb.AppIdentityServiceError.NOT_IMPLEMENTED: OperationNotImplemented} 
   if (error.application_error in error_map): 
      return error_map[error.application_error](error.error_detail) 
   else: 
      return InternalError(('%s:   %s' % (error.application_error, error.error_detail)))"," 'Convert a google.appengine.api.appidentity.AppIdentityServiceError 
 to a Flask-AppIdentityError. 
 :param error: The AppIdentityServiceError object. 
 :return: A Flask-AppIdentityError object.'","'Translate an application error to an external Error, if possible. 
 Args: 
 error: An ApplicationError to translate. 
 Returns: 
 error: app identity API specific error message.'"
"def get_argument_from_call(callfunc_node, position=None, keyword=None): 
    if ((position is None) and (keyword is None)): 
      raise ValueError('Must   specify   at   least   one   of:   position   or   keyword.') 
   try: 
      if ((position is not None) and (not isinstance(callfunc_node.args[position], astroid.Keyword))): 
         return callfunc_node.args[position] 
   except IndexError as error: 
      raise NoSuchArgumentError(error) 
   if keyword: 
      for arg in callfunc_node.args: 
         if (isinstance(arg, astroid.Keyword) and (arg.arg == keyword)): 
            return arg.value 
   raise NoSuchArgumentError"," 'Returns the value of the specified argument in the call. 
 If the argument is a keyword, then the keyword is returned. 
 If the argument is not a keyword, then the value is returned.'","'Returns the specified argument from a function call. 
 :param callfunc_node: Node representing a function call to check. 
 :param int position: position of the argument. 
 :param str keyword: the keyword of the argument. 
 :returns: The node representing the argument, None if the argument is not found. 
 :raises ValueError: if both position and keyword are None. 
 :raises NoSuchArgumentError: if no argument at the provided position or with 
 the provided keyword.'"
"def dynamic_class_import(class_path): 
    try: 
      tmp = class_path.split('.') 
      module_path = '.'.join(tmp[0:(-1)]) 
      package = __import__(module_path) 
      return reduce(getattr, tmp[1:], package) 
   except Exception as e: 
      log.error(LOGMSG_ERR_FAB_ADDON_IMPORT.format(class_path, e))"," 'Load a class from a dynamic path. 
 :param class_path: A class path. 
 :type class_path: str 
 :return: A class object.'","'Will dynamically import a class from a string path 
 :param class_path: string with class path 
 :return: class'"
"def _decomp_quad(P, cond=None, rcond=None, lower=True, check_finite=True): 
    (w, V) = LA.eigh(P, lower=lower, check_finite=check_finite) 
   if (rcond is not None): 
      cond = rcond 
   if (cond in (None, (-1))): 
      t = V.dtype.char.lower() 
      factor = {'f': 1000.0, 'd': 1000000.0} 
      cond = (factor[t] * np.finfo(t).eps) 
   scale = max(np.absolute(w)) 
   w_scaled = (w / scale) 
   maskp = (w_scaled > cond) 
   maskn = (w_scaled < (- cond)) 
   if (np.any(maskp) and np.any(maskn)): 
      warnings.warn('Forming   a   nonconvex   expression   quad_form(x,   indefinite).') 
   M1 = (V[:, maskp] * np.sqrt(w_scaled[maskp])) 
   M2 = (V[:, maskn] * np.sqrt((- w_scaled[maskn]))) 
   return (scale, M1, M2)"," 'Return a tuple (scale, M1, M2) where M1 and M2 are the 2x2 matrices 
 corresponding to the two positive and negative eigenvalues of P. 
 The scale is the maximum of the absolute values of the eigenvalues. 
 The matrices M1 and M2 are normalized to have unit norm. 
 The matrices M1 and M2 are stored in the same order as the eigenvalues. 
 Parameters 
 P : array 
 A Hermitian matrix. 
 cond : float 
 The relative tolerance for the eigenvalues. 
 rcond : float 
 The relative tolerance for the eigenvalues. 
 lower : bool 
 If True, the eigenvalues are sorted in descending order. 
 check_finite : bool 
 If True, the eigenvalues are checked for finite values. 
 Returns 
 scale : float 
 The maximum absolute value of the eigenvalues. 
 M1 : ndarray 
 The matrix corresponding to the positive eigenvalues. 
 M2 : ndarray 
 The matrix corresponding to the negative eigenvalues. 
 Examples 
 >>> from sympy import sqrt, Matrix, eye 
 >>> from sympy.mat","'Compute a matrix decomposition. 
 Compute sgn, scale, M such that P = sgn * scale * dot(M, M.T). 
 The strategy of determination of eigenvalue negligibility follows 
 the pinvh contributions from the scikit-learn project to scipy. 
 Parameters 
 P : matrix or ndarray 
 A real symmetric positive or negative (semi)definite input matrix 
 cond, rcond : float, optional 
 Cutoff for small eigenvalues. 
 Singular values smaller than rcond * largest_eigenvalue 
 are considered negligible. 
 If None or -1, suitable machine precision is used (default). 
 lower : bool, optional 
 Whether the array data is taken from the lower or upper triangle of P. 
 The default is to take it from the lower triangle. 
 check_finite : bool, optional 
 Whether to check that the input matrix contains only finite numbers. 
 The default is True; disabling may give a performance gain 
 but may result in problems (crashes, non-termination) if the inputs 
 contain infinities or NaNs. 
 Returns 
 scale : float 
 induced matrix 2-norm of P 
 M1, M2 : 2d ndarray 
 A rectangular ndarray such that P = scale * (dot(M1, M1.T) - dot(M2, M2.T))'"
"def writeOutput(fileName=''): 
    fileName = fabmetheus_interpret.getFirstTranslatorFileNameUnmodified(fileName) 
   if (fileName != ''): 
      skeinforge_craft.writeChainTextWithNounMessage(fileName, 'unpause')"," 'Write the output file to disk. 
 @param fileName: name of output file 
 @type fileName: str 
 @raise ValueError: if fileName is empty'",'Unpause a gcode linear move file.'
"def _putResultInDeferred(reactor, deferred, f, args, kwargs): 
    try: 
      result = f(*args, **kwargs) 
   except Exception: 
      f = failure.Failure() 
      reactor.callFromThread(deferred.errback, f) 
   else: 
      reactor.callFromThread(deferred.callback, result)"," 'Called by the reactor when a callback is registered. 
 This is the place where you should put the result in the deferred, and 
 call the callback. 
 @param reactor: The reactor to use 
 @param deferred: The deferred to register the callback with 
 @param f: The function to call with the result 
 @param args: The arguments to pass to the function 
 @param kwargs: The keyword arguments to pass to the function'",'Run a function and give results to a Deferred.'
"def fft(x, n=None, axis=(-1), overwrite_x=False): 
    tmp = _asfarray(x) 
   try: 
      work_function = _DTYPE_TO_FFT[tmp.dtype] 
   except KeyError: 
      raise ValueError(('type   %s   is   not   supported' % tmp.dtype)) 
   if (not (istype(tmp, numpy.complex64) or istype(tmp, numpy.complex128))): 
      overwrite_x = 1 
   overwrite_x = (overwrite_x or _datacopied(tmp, x)) 
   if (n is None): 
      n = tmp.shape[axis] 
   elif (n != tmp.shape[axis]): 
      (tmp, copy_made) = _fix_shape(tmp, n, axis) 
      overwrite_x = (overwrite_x or copy_made) 
   if (n < 1): 
      raise ValueError(('Invalid   number   of   FFT   data   points   (%d)   specified.' % n)) 
   if ((axis == (-1)) or (axis == (len(tmp.shape) - 1))): 
      return work_function(tmp, n, 1, 0, overwrite_x) 
   tmp = swapaxes(tmp, axis, (-1)) 
   tmp = work_function(tmp, n, 1, 0, overwrite_x) 
   return swapaxes(tmp, axis, (-1))"," 'Computes the discrete Fourier transform (DFT) of a real or complex 
 array. 
 Parameters 
 x : ndarray 
 The input array. 
 n : int, optional 
 The number of points in the DFT. If not specified, the input array 
 determines the size of the output array. 
 axis : int, optional 
 The axis along which the DFT is computed. 
 overwrite_x : bool, optional 
 If true, the input array is overwritten with the result of the FFT. 
 Returns 
 out : ndarray 
 The discrete Fourier transform of the input array. 
 Raises 
 ValueError 
 If the number of points in the DFT is not a power of 2. 
 Notes 
 The input array must be real or complex. 
 The input array can be real, complex, or complex-symmetric. 
 The input array can be real or complex-valued. 
 Examples 
 >>> x = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) 
","'Return discrete Fourier transform of real or complex sequence. 
 The returned complex array contains ``y(0), y(1),..., y(n-1)`` where 
 ``y(j) = (x * exp(-2*pi*sqrt(-1)*j*np.arange(n)/n)).sum()``. 
 Parameters 
 x : array_like 
 Array to Fourier transform. 
 n : int, optional 
 Length of the Fourier transform.  If ``n < x.shape[axis]``, `x` is 
 truncated.  If ``n > x.shape[axis]``, `x` is zero-padded. The 
 default results in ``n = x.shape[axis]``. 
 axis : int, optional 
 Axis along which the fft\'s are computed; the default is over the 
 last axis (i.e., ``axis=-1``). 
 overwrite_x : bool, optional 
 If True, the contents of `x` can be destroyed; the default is False. 
 Returns 
 z : complex ndarray 
 with the elements:: 
 [y(0),y(1),..,y(n/2),y(1-n/2),...,y(-1)]        if n is even 
 [y(0),y(1),..,y((n-1)/2),y(-(n-1)/2),...,y(-1)]  if n is odd 
 where:: 
 y(j) = sum[k=0..n-1] x[k] * exp(-sqrt(-1)*j*k* 2*pi/n), j = 0..n-1 
 Note that ``y(-j) = y(n-j).conjugate()``. 
 See Also 
 ifft : Inverse FFT 
 rfft : FFT of a real sequence 
 Notes 
 The packing of the result is ""standard"": If ``A = fft(a, n)``, then 
 ``A[0]`` contains the zero-frequency term, ``A[1:n/2]`` contains the 
 positive-frequency terms, and ``A[n/2:]`` contains the negative-frequency 
 terms, in order of decreasingly negative frequency. So for an 8-point 
 transform, the frequencies of the result are [0, 1, 2, 3, -4, -3, -2, -1]. 
 To rearrange the fft output so that the zero-frequency component is 
 centered, like [-4, -3, -2, -1,  0,  1,  2,  3], use `fftshift`. 
 For `n` even, ``A[n/2]`` contains the sum of the positive and 
 negative-frequency terms.  For `n` even and `x` real, ``A[n/2]`` will 
 always be real. 
 Both single and double precision routines are implemented.  Half precision 
 inputs will be converted to single precision.  Non floating-point inputs 
 will be converted to double precision.  Long-double precision inputs are 
 not supported. 
 This function is most efficient when `n` is a power of two, and least 
 efficient when `n` is prime. 
 If the data type of `x` is real, a ""real FFT"" algorithm is automatically 
 used, which roughly halves the computation time.  To increase efficiency 
 a little further, use `rfft`, which does the same calculation, but only 
 outputs half of the symmetrical spectrum.  If the data is both real and 
 symmetrical, the `dct` can again double the efficiency, by generating 
 half of the spectrum from half of the signal. 
 Examples 
 >>> from scipy.fftpack import fft, ifft 
 >>> x = np.arange(5) 
 >>> np.allclose(fft(ifft(x)), x, atol=1e-15)  # within numerical accuracy. 
 True'"
"def setup(): 
    os.makedirs(os.path.join(HOME_TEST_DIR, 'ipython'))", 'Setup the test environment',"'Setup testenvironment for the module: 
 - Adds dummy home dir tree'"
"def make_logged_in_client(username='test', password='test', is_superuser=True, recreate=False, groupname=None): 
    try: 
      user = User.objects.get(username=username) 
      if recreate: 
         user.delete() 
         raise User.DoesNotExist 
   except User.DoesNotExist: 
      user = User.objects.create_user(username, (username + '@localhost'), password) 
      user.is_superuser = is_superuser 
      user.save() 
   else: 
      if (user.is_superuser != is_superuser): 
         user.is_superuser = is_superuser 
         user.save() 
   if (groupname is not None): 
      (group, created) = Group.objects.get_or_create(name=groupname) 
      if (not user.groups.filter(name=group.name).exists()): 
         user.groups.add(group) 
         user.save() 
   c = Client() 
   ret = c.login(username=username, password=password) 
   assert ret, (""Login   failed   (user   '%s')."" % username) 
   return c"," 'Make a logged in client for testing. 
 :param username: the username 
 :param password: the password 
 :param is_superuser: whether the user is a superuser 
 :param recreate: whether to recreate the user if it already exists 
 :param groupname: the name of the group to add to the user 
 :returns: a logged in client'","'Create a client with a user already logged in. 
 Sometimes we recreate the user, because some tests like to 
 mess with is_active and such.'"
"def test_invalid_sigma_clip(): 
    data = np.ones((5, 5)) 
   data[(2, 2)] = 1000 
   data[(3, 4)] = np.nan 
   data[(1, 1)] = np.inf 
   result = sigma_clip(data) 
   assert result.mask[(2, 2)] 
   assert result.mask[(3, 4)] 
   assert result.mask[(1, 1)]", 'Test invalid sigma clip.','Test sigma_clip of data containing invalid values.'
"def create_gzip_cache(pelican): 
    for (dirpath, _, filenames) in os.walk(pelican.settings['OUTPUT_PATH']): 
      for name in filenames: 
         if should_compress(name): 
            filepath = os.path.join(dirpath, name) 
            create_gzip_file(filepath, should_overwrite(pelican.settings))"," 'Create gzip files for all compressed files in the OUTPUT_PATH. 
 This is only needed if the cache is not enabled, or if the cache is enabled 
 and the settings.COMPRESS_OUTPUT is set to False.'","'Create a gzip cache file for every file that a webserver would 
 reasonably want to cache (e.g., text type files). 
 :param pelican: The Pelican instance'"
"@removals.remove(message='Use   keystoneclient.session.request   instead.', version='1.7.0', removal_version='2.0.0') 
 def request(*args, **kwargs): 
    return client_session.request(*args, **kwargs)"," 'Requests a resource from the service. 
 :param args: 
 :param kwargs: 
 :returns: 
 :raises: 
 :rtype: 
 :seealso: keystoneclient.session.request'","'Make a request. 
 This function is deprecated as of the 1.7.0 release in favor of 
 :func:`keystoneclient.session.request` and may be removed in the 
 2.0.0 release.'"
"def unique_substrings(l, min_size=1): 
    return [unique_substring(s, [x for x in l if (x is not s)], min_size) for s in l]"," 'Return a list of unique substrings of length >= min_size 
 :param l: 
 :param min_size: 
 :return: 
 :rtype: 
 :raises: 
 :version: 
 :author: 
 :date: 
 :copyright: 
 :license: 
 :requires: 
 :depends: 
 :todo: 
 :notes: 
 :bugs: 
 :wiki: 
 :examples: 
 :references: 
 :related: 
 :deprecated: 
 :trackers: 
 :binaries: 
 :sphinx: 
 :github: 
 :doc: 
 :source: 
 :contributors: 
 :issue_tracker: 
 :bug_tracker: 
 :mailing_list: 
 :wiki_page: 
 :license: 
 :copyright: 
 :credits: 
 :maintainer: 
 :maintainer_email: 
 :maintainer_url: 
 :history: 
 :revision: ","'.. todo:: 
 WRITEME'"
"def event_source_mapping_present(name, EventSourceArn, FunctionName, StartingPosition, Enabled=True, BatchSize=100, region=None, key=None, keyid=None, profile=None): 
    ret = {'name': None, 'result': True, 'comment': '', 'changes': {}} 
   r = __salt__['boto_lambda.event_source_mapping_exists'](EventSourceArn=EventSourceArn, FunctionName=FunctionName, region=region, key=key, keyid=keyid, profile=profile) 
   if ('error' in r): 
      ret['result'] = False 
      ret['comment'] = 'Failed   to   create   event   source   mapping:   {0}.'.format(r['error']['message']) 
      return ret 
   if (not r.get('exists')): 
      if __opts__['test']: 
         ret['comment'] = 'Event   source   mapping   {0}   is   set   to   be   created.'.format(FunctionName) 
         ret['result'] = None 
         return ret 
      r = __salt__['boto_lambda.create_event_source_mapping'](EventSourceArn=EventSourceArn, FunctionName=FunctionName, StartingPosition=StartingPosition, Enabled=Enabled, BatchSize=BatchSize, region=region, key=key, keyid=keyid, profile=profile) 
      if (not r.get('created')): 
         ret['result'] = False 
         ret['comment'] = 'Failed   to   create   event   source   mapping:   {0}.'.format(r['error']['message']) 
         return ret 
      _describe = __salt__['boto_lambda.describe_event_source_mapping'](EventSourceArn=EventSourceArn, FunctionName=FunctionName, region=region, key=key, keyid=keyid, profile=profile) 
      ret['name'] = _describe['event_source_mapping']['UUID'] 
      ret['changes']['old'] = {'event_source_mapping': None} 
      ret['changes']['new'] = _describe 
      ret['comment'] = 'Event   source   mapping   {0}   created.'.format(ret['name']) 
      return ret 
   ret['comment'] = os.linesep.join([ret['comment'], 'Event   source   mapping   is   present.']) 
   ret['changes'] = {} 
   _describe = __salt__['boto_lambda.describe_event_source_mapping'](EventSourceArn=EventSourceArn, FunctionName=FunctionName, region=region, key=key, keyid=keyid, profile=profile)['event_source_mapping'] 
   need_update = False 
   options = {'BatchSize': 'BatchSize'} 
   for (val, var) in six.iteritems(options): 
      if (_describe[val] != locals()[var]): 
         need_update = True 
         ret['changes'].setdefault('new', {})[var] = locals()[var] 
         ret['changes'].setdefault('old', {})[var] = _describe[val] 
   function_arn = _get_function_arn(FunctionName, region=region, key=key, keyid=keyid, profile=profile) 
   if (_describe['FunctionArn'] != function_arn): 
      need_update = True 
      ret['changes'].setdefault('new', {})['FunctionArn'] = function_arn 
      ret['changes'].setdefault('old', {})['FunctionArn'] = _describe['FunctionArn'] 
   if need_update: 
      ret['comment'] = os.linesep.join([ret['comment'], 'Event   source   mapping   to   be   modified']) 
      if __opts__['test']: 
         msg = 'Event   source   mapping   {0}   set   to   be   modified.'.format(_describe['UUID']) 
         ret['comment'] = msg 
         ret['result'] = None 
         return ret 
      _r = __salt__['boto_lambda.update_event_source_mapping'](UUID=_describe['UUID'], FunctionName=FunctionName, Enabled=Enabled, BatchSize=BatchSize, region=region, key=key, keyid=keyid, profile=profile) 
      if (not _r.get('updated')): 
         ret['result'] = False 
         ret['comment'] = 'Failed   to   update   mapping:   {0}.'.format(_r['error']['message']) 
         ret['changes'] = {} 
   return ret"," 'Checks if the event source mapping exists and if not, creates it. 
 If the event source mapping already exists, it checks if the 
 parameters are the same and if they are, no action is taken. 
 If the parameters are different, it updates the event source mapping. 
 event_source_mapping_present(name, EventSourceArn, FunctionName, StartingPosition, Enabled=True, BatchSize=100, region=None, key=None, keyid=None, profile=None)'","'Ensure event source mapping exists. 
 name 
 The name of the state definition. 
 EventSourceArn 
 The Amazon Resource Name (ARN) of the Amazon Kinesis or the Amazon 
 DynamoDB stream that is the event source. 
 FunctionName 
 The Lambda function to invoke when AWS Lambda detects an event on the 
 stream. 
 You can specify an unqualified function name (for example, ""Thumbnail"") 
 or you can specify Amazon Resource Name (ARN) of the function (for 
 example, ""arn:aws:lambda:us-west-2:account-id:function:ThumbNail""). AWS 
 Lambda also allows you to specify only the account ID qualifier (for 
 example, ""account-id:Thumbnail""). Note that the length constraint 
 applies only to the ARN. If you specify only the function name, it is 
 limited to 64 character in length. 
 StartingPosition 
 The position in the stream where AWS Lambda should start reading. 
 (TRIM_HORIZON | LATEST) 
 Enabled 
 Indicates whether AWS Lambda should begin polling the event source. By 
 default, Enabled is true. 
 BatchSize 
 The largest number of records that AWS Lambda will retrieve from your 
 event source at the time of invoking your function. Your function 
 receives an event with all the retrieved records. The default is 100 
 records. 
 region 
 Region to connect to. 
 key 
 Secret key to be used. 
 keyid 
 Access key to be used. 
 profile 
 A dict with region, key and keyid, or a pillar key (string) that 
 contains a dict with region, key and keyid.'"
"def getCraftedTextFromText(gcodeText, exportRepository=None): 
    if gcodec.isProcedureDoneOrFileIsEmpty(gcodeText, 'export'): 
      return gcodeText 
   if (exportRepository == None): 
      exportRepository = settings.getReadRepository(ExportRepository()) 
   if (not exportRepository.activateExport.value): 
      return gcodeText 
   return ExportSkein().getCraftedGcode(exportRepository, gcodeText)"," 'Returns the crafted G-code text from the given text. 
 :param gcodeText: G-code text to be exported. 
 :param exportRepository: Export repository to use for exporting. 
 :return: Crafted G-code text.'",'Export a gcode linear move text.'
"def test_gnb_pfit_wrong_nb_features(): 
    clf = GaussianNB() 
   clf.fit(X, y) 
   assert_raises(ValueError, clf.partial_fit, np.hstack((X, X)), y)"," 'Testing that partial_fit is not supported for GaussianNB with wrong number 
 of features.'","'Test whether an error is raised when the number of feature changes 
 between two partial fit'"
"def _handle_key(key): 
    code = int(key[(key.index(u'k') + 1):]) 
   value = chr(code) 
   if ((code >= 65) and (code <= 90)): 
      if (u'shift+' in key): 
         key = key.replace(u'shift+', u'') 
      else: 
         value = value.lower() 
   elif ((code >= 48) and (code <= 57)): 
      if (u'shift+' in key): 
         value = u')!@#$%^&*('[int(value)] 
         key = key.replace(u'shift+', u'') 
   elif ((code >= 112) and (code <= 123)): 
      value = (u'f%s' % (code - 111)) 
   elif ((code >= 96) and (code <= 105)): 
      value = (u'%s' % (code - 96)) 
   elif ((code in _SHIFT_LUT) and (u'shift+' in key)): 
      key = key.replace(u'shift+', u'') 
      value = _SHIFT_LUT[code] 
   elif (code in _LUT): 
      value = _LUT[code] 
   key = (key[:key.index(u'k')] + value) 
   return key"," 'Handle a keypress. 
 :param key: the key to handle 
 :return: the key as a string'",'Handle key codes'
"@mock_ec2 
 def test_igw_desribe(): 
    conn = boto.connect_vpc(u'the_key', u'the_secret') 
   igw = conn.create_internet_gateway() 
   igw_by_search = conn.get_all_internet_gateways([igw.id])[0] 
   igw.id.should.equal(igw_by_search.id)", 'Test that the create_internet_gateway() method returns the correct ID','internet gateway fetch by id'
"def rpc(cmd=None, dest=None, format='xml', *args, **kwargs): 
    conn = __proxy__['junos.conn']() 
   ret = dict() 
   ret['out'] = True 
   op = dict() 
   if ('__pub_arg' in kwargs): 
      if isinstance(kwargs['__pub_arg'][(-1)], dict): 
         op.update(kwargs['__pub_arg'][(-1)]) 
   else: 
      op.update(kwargs) 
   if ((dest is None) and (format != 'xml')): 
      log.warning('Format   ignored   as   it   is   only   used   for   output   which   is   dumped   in   the   file.') 
   write_response = '' 
   try: 
      if (cmd in ['get-config', 'get_config']): 
         filter_reply = None 
         if ('filter' in op): 
            filter_reply = etree.XML(op['filter']) 
         xml_reply = getattr(conn.rpc, cmd.replace('-', '_'))(filter_reply, options=op) 
         ret['message'] = jxmlease.parse(etree.tostring(xml_reply)) 
         write_response = etree.tostring(xml_reply) 
         if ((dest is not None) and (format != 'xml')): 
            op.update({'format': format}) 
            rpc_reply = getattr(conn.rpc, cmd.replace('-', '_'))(filter_reply, options=op) 
            if (format == 'json'): 
               write_response = json.dumps(rpc_reply, indent=1) 
            else: 
               write_response = rpc_reply.text 
      else: 
         xml_reply = getattr(conn.rpc, cmd.replace('-', '_'))(**op) 
         ret['message'] = jxmlease.parse(etree.tostring(xml_reply)) 
         write_response = etree.tostring(xml_reply) 
         if ((dest is not None) and (format != 'xml')): 
            rpc_reply = getattr(conn.rpc, cmd.replace('-', '_'))({'format': format}, **op) 
            if (format == 'json'): 
               write_response = json.dumps(rpc_reply, indent=1) 
            else: 
               write_response = rpc_reply.text 
   except Exception as exception: 
      ret['message'] = 'Execution   failed   due   to   ""{0}""'.format(exception) 
      ret['out'] = False 
   if (dest is not None): 
      with fopen(dest, 'w') as fp: 
         fp.write(write_response) 
   return ret"," 'This function is used to send the RPC commands to the Junos device. 
 It returns the response as a dictionary and the response is also dumped 
 in the file. 
 :param cmd: RPC command 
 :param dest: Destination file 
 :param format: Format of the response 
 :param args: Arguments to the RPC command 
 :param kwargs: Keyword arguments to the RPC command 
 :return: Response as a dictionary'","'This function executes the rpc provided as arguments on the junos device. 
 The returned data can be stored in a file whose destination can be 
 specified with \'dest\' keyword in the arguments. 
 Usage: 
 .. code-block:: bash 
 salt \'device\' junos.rpc \'get_config\' \'text\' filter=\'<configuration><system/></configuration>\' 
 salt \'device\' junos.rpc \'get-interface-information\' \'/home/user/interface.log\' interface_name=\'lo0\' terse=True 
 Options: 
 * cmd: the rpc to be executed 
 * dest: destination file where the rpc ouput is dumped 
 * format: the format in which the rpc reply must be stored in file specified in the dest (used only when dest is specified) 
 * args: other arguments as taken by rpc call of PyEZ 
 * kwargs: keyworded arguments taken by rpc call of PyEZ'"
"def read(results_file): 
    results = {} 
   if (not os.path.exists(results_file)): 
      raise IOError('Results   file   does   not   exist.') 
   with open(results_file) as handle: 
      lines = handle.readlines() 
   (results, num_params) = _parse_baseml.parse_basics(lines, results) 
   results = _parse_baseml.parse_parameters(lines, results, num_params) 
   if (results.get('version') is None): 
      raise ValueError('Invalid   results   file') 
   return results"," 'Reads a results file and returns the results dictionary. 
 Parameters 
 results_file : str 
 The path to the results file to read. 
 Returns 
 results : dict 
 A dictionary of the results. 
 Raises 
 IOError 
 If the results file does not exist.'",'Parse a BASEML results file.'
"def export_stats(request, project, subproject): 
    subprj = get_subproject(request, project, subproject) 
   data = [trans.get_stats() for trans in subprj.translation_set.all()] 
   return export_response(request, ('stats-%s-%s.csv' % (subprj.project.slug, subprj.slug)), ('name', 'code', 'total', 'translated', 'translated_percent', 'total_words', 'translated_words', 'failing', 'failing_percent', 'fuzzy', 'fuzzy_percent', 'url_translate', 'url', 'last_change', 'last_author'), data)", 'Export stats for the subproject.','Exports stats in JSON format.'
"@bp.route('/') 
 def nodes(): 
    nodes = Node.query.order_by(Node.updated.desc()).all() 
   return render_template('node/nodes.html', nodes=nodes)", 'Show a list of all nodes.','Nodes pages.'
"@register.filter(is_safe=True) 
 @stringfilter 
 def striptags(value): 
    return strip_tags(value)"," 'Strip tags from a string. 
 This is the same as Django\'s ``strip_tags()`` method, except that it 
 works in the context of a template. 
 This filter is intended for use in the context of a template, and will 
 raise a TemplateSyntaxError if used in any other context. 
 This filter is not intended for use in the context of a context processor. 
 See the documentation for ``{{striptags}}`` for more information. 
 :param value: The string to strip tags from. 
 :return: The string without tags. 
 :rtype: str'",'Strips all [X]HTML tags.'
"def delete_image(gce, name, module): 
    try: 
      gce.ex_delete_image(name) 
      return True 
   except ResourceNotFoundError: 
      return False 
   except GoogleBaseError as e: 
      module.fail_json(msg=str(e), changed=False)", 'Delete the image from the GCE Compute Engine.','Delete a specific image resource by name.'
"def _write_proc_history(fid, info): 
    if ('proc_history' not in info): 
      return 
   if (len(info['proc_history']) > 0): 
      start_block(fid, FIFF.FIFFB_PROCESSING_HISTORY) 
      for record in info['proc_history']: 
         start_block(fid, FIFF.FIFFB_PROCESSING_RECORD) 
         for (key, id_, writer) in zip(_proc_keys, _proc_ids, _proc_writers): 
            if (key in record): 
               writer(fid, id_, record[key]) 
         _write_maxfilter_record(fid, record['max_info']) 
         if ('smartshield' in record): 
            for ss in record['smartshield']: 
               start_block(fid, FIFF.FIFFB_SMARTSHIELD) 
               end_block(fid, FIFF.FIFFB_SMARTSHIELD) 
         end_block(fid, FIFF.FIFFB_PROCESSING_RECORD) 
      end_block(fid, FIFF.FIFFB_PROCESSING_HISTORY)"," 'Write the processing history to the file. 
 Parameters 
 fid : open file descriptor 
 info : dict 
 The info dictionary from the FIFF file. 
 Returns 
 None.'",'Write processing history to file.'
"def test_finder_installs_pre_releases(data): 
    req = InstallRequirement.from_line('bar', None) 
   finder = PackageFinder([], [data.index_url('pre')], allow_all_prereleases=True, session=PipSession()) 
   link = finder.find_requirement(req, False) 
   assert link.url.endswith('bar-2.0b1.tar.gz'), link.url 
   links = ['https://foo/bar-1.0.tar.gz', 'https://foo/bar-2.0b1.tar.gz'] 
   finder = PackageFinder(links, [], allow_all_prereleases=True, session=PipSession()) 
   with patch.object(finder, '_get_pages', (lambda x, y: [])): 
      link = finder.find_requirement(req, False) 
      assert (link.url == 'https://foo/bar-2.0b1.tar.gz') 
   links.reverse() 
   finder = PackageFinder(links, [], allow_all_prereleases=True, session=PipSession()) 
   with patch.object(finder, '_get_pages', (lambda x, y: [])): 
      link = finder.find_requirement(req, False) 
      assert (link.url == 'https://foo/bar-2.0b1.tar.gz')", 'find_requirement() finds pre-releases by default','Test PackageFinder finds pre-releases if asked to.'
"def get_config(): 
    profiles = {} 
   curr = None 
   cmd = ['netsh', 'advfirewall', 'show', 'allprofiles'] 
   for line in __salt__['cmd.run'](cmd, python_shell=False).splitlines(): 
      if (not curr): 
         tmp = re.search('(.*)   Profile   Settings:', line) 
         if tmp: 
            curr = tmp.group(1) 
      elif line.startswith('State'): 
         profiles[curr] = (line.split()[1] == 'ON') 
         curr = None 
   return profiles"," 'Get firewall profile settings. 
 Returns a dictionary of all profiles with their state as key and the 
 value being True or False. 
 This function is based on the following article: 
 http://www.petri.co.il/windows-7-firewall-advanced-rules.htm'","'Get the status of all the firewall profiles 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' firewall.get_config'"
"def _fastq_solexa_convert_fastq_solexa(in_handle, out_handle, alphabet=None): 
    mapping = ''.join((([chr(0) for ascii in range(0, 59)] + [chr(ascii) for ascii in range(59, 127)]) + [chr(0) for ascii in range(127, 256)])) 
   assert (len(mapping) == 256) 
   return _fastq_generic(in_handle, out_handle, mapping)", 'Convert fastq to fastq with the Solexa/Illumina mapping.',"'Fast Solexa FASTQ to Solexa FASTQ conversion (PRIVATE). 
 Useful for removing line wrapping and the redundant second identifier 
 on the plus lines. Will check also check the quality string is valid. 
 Avoids creating SeqRecord and Seq objects in order to speed up this 
 conversion.'"
"def _comp_match(item, filter_, scope='collection'): 
    filter_length = len(filter_) 
   if (scope == 'collection'): 
      tag = item.collection.get_meta('tag') 
   else: 
      for component in item.components(): 
         if (component.name in ('VTODO', 'VEVENT', 'VJOURNAL')): 
            tag = component.name 
            break 
      else: 
         return False 
   if (filter_length == 0): 
      return (filter_.get('name') == tag) 
   else: 
      if (filter_length == 1): 
         if (filter_[0].tag == _tag('C', 'is-not-defined')): 
            return (filter_.get('name') != tag) 
      if (filter_[0].tag == _tag('C', 'time-range')): 
         if (not _time_range_match(item.item, filter_[0], tag)): 
            return False 
         filter_ = filter_[1:] 
      return all(((_prop_match(item, child) if (child.tag == _tag('C', 'prop-filter')) else _comp_match(item, child, scope='component')) for child in filter_))"," 'Compare a filter with a component. 
 :param item: the component to compare 
 :param filter_: the filter to compare with 
 :param scope: the scope to compare with 
 :return: True if the filter matches, False otherwise'","'Check whether the ``item`` matches the comp ``filter_``. 
 If ``scope`` is ``""collection""``, the filter is applied on the 
 item\'s collection. Otherwise, it\'s applied on the item. 
 See rfc4791-9.7.1.'"
"def register_translation(src_image, target_image, upsample_factor=1, space='real'): 
    if (src_image.shape != target_image.shape): 
      raise ValueError('Error:   images   must   be   same   size   for   register_translation') 
   if ((src_image.ndim != 2) and (upsample_factor > 1)): 
      raise NotImplementedError('Error:   register_translation   only   supports   subpixel   registration   for   2D   images') 
   if (space.lower() == 'fourier'): 
      src_freq = src_image 
      target_freq = target_image 
   elif (space.lower() == 'real'): 
      src_image = np.array(src_image, dtype=np.complex128, copy=False) 
      target_image = np.array(target_image, dtype=np.complex128, copy=False) 
      src_freq = np.fft.fftn(src_image) 
      target_freq = np.fft.fftn(target_image) 
   else: 
      raise ValueError('Error:   register_translation   only   knows   the   ""real""   and   ""fourier""   values   for   the   ``space``   argument.') 
   shape = src_freq.shape 
   image_product = (src_freq * target_freq.conj()) 
   cross_correlation = np.fft.ifftn(image_product) 
   maxima = np.unravel_index(np.argmax(np.abs(cross_correlation)), cross_correlation.shape) 
   midpoints = np.array([np.fix((axis_size / 2)) for axis_size in shape]) 
   shifts = np.array(maxima, dtype=np.float64) 
   shifts[(shifts > midpoints)] -= np.array(shape)[(shifts > midpoints)] 
   if (upsample_factor == 1): 
      src_amp = (np.sum((np.abs(src_freq) ** 2)) / src_freq.size) 
      target_amp = (np.sum((np.abs(target_freq) ** 2)) / target_freq.size) 
      CCmax = cross_correlation.max() 
   else: 
      shifts = (np.round((shifts * upsample_factor)) / upsample_factor) 
      upsampled_region_size = np.ceil((upsample_factor * 1.5)) 
      dftshift = np.fix((upsampled_region_size / 2.0)) 
      upsample_factor = np.array(upsample_factor, dtype=np.float64) 
      normalization = (src_freq.size * (upsample_factor ** 2)) 
      sample_region_offset = (dftshift - (shifts * upsample_factor)) 
      cross_correlation = _upsampled_dft(image_product.conj(), upsampled_region_size, upsample_factor, sample_region_offset).conj() 
      cross_correlation /= normalization 
      maxima = np.array(np.unravel_index(np.argmax(np.abs(cross_correlation)), cross_correlation.shape), dtype=np.float64) 
      maxima -= dftshift 
      shifts = (shifts + (maxima / upsample_factor)) 
      CCmax = cross_correlation.max() 
      src_amp = _upsampled_dft((src_freq * src_freq.conj()), 1, upsample_factor)[(0, 0)] 
      src_amp /= normalization 
      target_amp = _upsampled_dft((target_freq * target_freq.conj()), 1, upsample_factor)[(0, 0)] 
      target_amp /= normalization 
   for dim in range(src_freq.ndim): 
      if (shape[dim] == 1): 
         shifts[dim] = 0 
   return (shifts, _compute_error(CCmax, src_amp, target_amp), _compute_phasediff(CCmax))"," 'Registers the images `src_image` and `target_image` using cross-correlation. 
 Parameters 
 src_image : ndarray 
 The source image. 
 target_image : ndarray 
 The target image. 
 upsample_factor : int 
 The upsampling factor. 
 Returns 
 shifts : ndarray 
 The shift vector. 
 error : ndarray 
 The error vector. 
 phasediff : ndarray 
 The phase difference vector. 
 Notes 
 This function requires that `src_image` and `target_image` have the same shape. 
 It also requires that `src_image` and `target_image` are real-valued. 
 If `upsample_factor` is not 1, the images are upsampled before the cross-correlation 
 is computed. 
 References 
 .. [1] http://en.wikipedia.org/wiki/Cross-correlation'","'Efficient subpixel image translation registration by cross-correlation. 
 This code gives the same precision as the FFT upsampled cross-correlation 
 in a fraction of the computation time and with reduced memory requirements. 
 It obtains an initial estimate of the cross-correlation peak by an FFT and 
 then refines the shift estimation by upsampling the DFT only in a small 
 neighborhood of that estimate by means of a matrix-multiply DFT. 
 Parameters 
 src_image : ndarray 
 Reference image. 
 target_image : ndarray 
 Image to register.  Must be same dimensionality as ``src_image``. 
 upsample_factor : int, optional 
 Upsampling factor. Images will be registered to within 
 ``1 / upsample_factor`` of a pixel. For example 
 ``upsample_factor == 20`` means the images will be registered 
 within 1/20th of a pixel.  Default is 1 (no upsampling) 
 space : string, one of ""real"" or ""fourier"", optional 
 Defines how the algorithm interprets input data.  ""real"" means data 
 will be FFT\'d to compute the correlation, while ""fourier"" data will 
 bypass FFT of input data.  Case insensitive. 
 Returns 
 shifts : ndarray 
 Shift vector (in pixels) required to register ``target_image`` with 
 ``src_image``.  Axis ordering is consistent with numpy (e.g. Z, Y, X) 
 error : float 
 Translation invariant normalized RMS error between ``src_image`` and 
 ``target_image``. 
 phasediff : float 
 Global phase difference between the two images (should be 
 zero if images are non-negative). 
 References 
 .. [1] Manuel Guizar-Sicairos, Samuel T. Thurman, and James R. Fienup, 
 ""Efficient subpixel image registration algorithms,"" 
 Optics Letters 33, 156-158 (2008). DOI:10.1364/OL.33.000156 
 .. [2] James R. Fienup, ""Invariant error metrics for image reconstruction"" 
 Optics Letters 36, 8352-8357 (1997). DOI:10.1364/AO.36.008352'"
"@task() 
 @timeit 
 def maybe_award_badge(badge_template, year, user): 
    badge = get_or_create_badge(badge_template, year) 
   if badge.is_awarded_to(user): 
      return 
   from kitsune.questions.models import Answer 
   qs = Answer.objects.filter(creator=user, created__gte=date(year, 1, 1), created__lt=date((year + 1), 1, 1)) 
   if (qs.count() >= 30): 
      badge.award_to(user) 
      return True"," 'Award badge to user if they have answered at least 30 questions. 
 :param badge_template: Badge template to use 
 :type badge_template: BadgeTemplate 
 :param year: Year to check for 
 :type year: int 
 :param user: User to check for 
 :type user: User 
 :return: True if badge was awarded, False if not'",'Award the specific badge to the user if they\'ve earned it.'
"def GetRegisteredExe(exeAlias): 
    return win32api.RegQueryValue(GetRootKey(), ((GetAppPathsKey() + '\\') + exeAlias))"," 'Get the path to an application by its alias. 
 :param exeAlias: The alias of the application 
 :returns: The path to the application, or None if the alias is not registered 
 :rtype: str'",'Get a registered .exe'
"def json_decode(x): 
    return json.loads(x, object_hook=json_hook)"," 'Decode a JSON string into a Python object. 
 This function is used to convert a JSON string into a Python object. 
 It uses the json.loads function, but the object_hook argument is 
 set to the json_hook function, which will convert any Python objects 
 into a JSON string before they are passed to the json.loads function.'",'JSON decoder that does ugly first-level datetime handling'
"def xy_color(color): 
    rgb_cie_converter = rgb_cie.Converter() 
   hex_color_re = re.compile('(?<!\\w)([a-f0-9]){2}([a-f0-9]){2}([a-f0-9]){2}\\b', re.IGNORECASE) 
   if (color in CSS_LITERALS): 
      color = CSS_LITERALS[color] 
   color = color.lstrip('#') 
   if (not hex_color_re.match(color)): 
      print 'Invalid   color.   Please   use   a   6-digit   hex   color.' 
      sys.exit() 
   return rgb_cie_converter.hexToCIE1931(color)"," 'Convert a hex color to a CIE XYZ color. 
 Parameters 
 color : str 
 A hex color in the format #RRGGBB 
 Returns 
 rgb : list 
 A list of 3 floats in the range [0, 1] 
 Examples 
 >>> rgb = xy_color(""#FF0000"") 
 >>> rgb 
 [1.0, 0.0, 0.0]'",'Validate and convert hex color to XY space.'
"def zero_value_config_set(kodi_setting, all_settings): 
    try: 
      if (int(kodi_setting) == 0): 
         return 'remove_this_line' 
   except: 
      pass 
   return kodi_setting"," 'Returns the appropriate action to take on a setting when it has 
 a zero value. 
 :param kodi_setting: The Kodi setting to check. 
 :param all_settings: All of the settings in the config file. 
 :return: The action to take on the setting if it is zero. 
 :rtype: str'","'If the value of the kodi setting is zero, then remove the 
 entry from the config.txt. This should be used where zero is 
 the default (pi natural) value.'"
"@public 
 def sring(exprs, *symbols, **options): 
    single = False 
   if (not is_sequence(exprs)): 
      (exprs, single) = ([exprs], True) 
   exprs = list(map(sympify, exprs)) 
   opt = build_options(symbols, options) 
   (reps, opt) = _parallel_dict_from_expr(exprs, opt) 
   if (opt.domain is None): 
      coeffs = sum([list(rep.values()) for rep in reps], []) 
      (opt.domain, _) = construct_domain(coeffs, opt=opt) 
   _ring = PolyRing(opt.gens, opt.domain, opt.order) 
   polys = list(map(_ring.from_dict, reps)) 
   if single: 
      return (_ring, polys[0]) 
   else: 
      return (_ring, polys)"," 'Return a poly ring and a list of polynomials from exprs. 
 The optional symbols are used to define the ring. 
 Examples 
 >>> from sympy import sring 
 >>> from sympy.abc import x 
 >>> sring([x**2 + x + 1, x**2 + 1], x) 
 (PolyRing(x), [x**2 + x + 1, x**2 + 1]) 
 >>> sring([x**2 + x + 1, x**2 + 1], x, domain=\'ZZ\') 
 (PolyRing(x, ZZ), [x**2 + x + 1, x**2 + 1]) 
 >>> sring([x**2 + x + 1, x**2 + 1], x, domain=\'ZZ\', order=\'lexicographic\') 
 (PolyRing(x, ZZ, lexicographic), [x**2 + x + 1, x**2 + 1]) 
 >>> sring([x**2 + x + 1, x**2 + 1],","'Construct a ring deriving generators and domain from options and input expressions. 
 Parameters 
 exprs : :class:`Expr` or sequence of :class:`Expr` (sympifiable) 
 symbols : sequence of :class:`Symbol`/:class:`Expr` 
 options : keyword arguments understood by :class:`Options` 
 Examples 
 >>> from sympy.core import symbols 
 >>> from sympy.polys.rings import sring 
 >>> from sympy.polys.domains import ZZ 
 >>> from sympy.polys.orderings import lex 
 >>> x, y, z = symbols(""x,y,z"") 
 >>> R, f = sring(x + 2*y + 3*z) 
 >>> R 
 Polynomial ring in x, y, z over ZZ with lex order 
 >>> f 
 x + 2*y + 3*z 
 >>> type(_) 
 <class \'sympy.polys.rings.PolyElement\'>'"
"def getInsetLoopsFromLoops(loops, radius): 
    insetLoops = [] 
   for loop in loops: 
      insetLoops += getInsetLoopsFromLoop(loop, radius) 
   return insetLoops"," 'Return a list of inset loops from a list of loops. 
 :param loops: A list of loops. 
 :param radius: The radius of the inset loops. 
 :return: A list of inset loops.'","'Get the inset loops, which might overlap.'"
"def setup_logging(): 
    logger = logging.getLogger() 
   logger.setLevel(LOG_LEVEL) 
   fmt = logging.Formatter('[%(asctime)s]   %(levelname)s:   %(message)s') 
   handler = logging.StreamHandler() 
   handler.setFormatter(fmt) 
   logger.addHandler(handler)", 'Setup logging','Setup logging for the script'
"def guess_net_inet_tcp_sendbuf_max(): 
    return (16 * MB)"," 'Guess the TCP send buffer size. 
 This is based on the fact that the default TCP send buffer size is 16MB. 
 This is used to determine the size of the memory to be allocated for 
 the send buffer. 
 :return: The TCP send buffer size in bytes.'","'Maximum size for TCP send buffers 
 See guess_kern_ipc_maxsockbuf().'"
"def convert_opt(key, val): 
    if (key == 'env'): 
      val = env_to_str(val) 
   elif (val is None): 
      val = '' 
   else: 
      val = str(val) 
   return val", 'Convert a key/value pair from the command line to a string.','get opt'
"def _generate_zip_package(target, sources, sources_dir): 
    zip = zipfile.ZipFile(target, 'w', zipfile.ZIP_DEFLATED) 
   manifest = _archive_package_sources(zip.write, sources, sources_dir) 
   zip.writestr(_PACKAGE_MANIFEST, ('\n'.join(manifest) + '\n')) 
   zip.close() 
   return None"," 'Generate a zip package containing the sources of the given package. 
 :param target: Path to the generated zip file. 
 :param sources: List of source files to include. 
 :param sources_dir: Path to the directory containing the sources. 
 :return: None'",'Generate a zip archive containing all of the source files.'
"def getNewRepository(): 
    return PostscriptRepository()", 'Returns a new PostscriptRepository object.','Get the repository constructor.'
"def url_decode(s, charset='utf-8', decode_keys=False, include_empty=True, errors='replace', separator='&', cls=None): 
    if (cls is None): 
      cls = MultiDict 
   if (isinstance(s, text_type) and (not isinstance(separator, text_type))): 
      separator = separator.decode((charset or 'ascii')) 
   elif (isinstance(s, bytes) and (not isinstance(separator, bytes))): 
      separator = separator.encode((charset or 'ascii')) 
   return cls(_url_decode_impl(s.split(separator), charset, decode_keys, include_empty, errors))"," 'Decode a URL-encoded string into a dictionary. 
 :param s: the URL-encoded string to decode 
 :param charset: the encoding to use for the decoding, defaults to \'utf-8\' 
 :param decode_keys: if True, decode keys 
 :param include_empty: if True, include empty keys in the result 
 :param errors: the errors to use for the decoding, defaults to \'replace\' 
 :param separator: the separator to use for splitting the string into 
 substrings, defaults to \'&\' 
 :param cls: the class to use for the returned dictionary, defaults to 
 :class:`MultiDict <django.utils.http.MultiDict>` 
 :return: the decoded dictionary 
 :rtype: dict'","'Parse a querystring and return it as :class:`MultiDict`.  There is a 
 difference in key decoding on different Python versions.  On Python 3 
 keys will always be fully decoded whereas on Python 2, keys will 
 remain bytestrings if they fit into ASCII.  On 2.x keys can be forced 
 to be unicode by setting `decode_keys` to `True`. 
 If the charset is set to `None` no unicode decoding will happen and 
 raw bytes will be returned. 
 Per default a missing value for a key will default to an empty key.  If 
 you don\'t want that behavior you can set `include_empty` to `False`. 
 Per default encoding errors are ignored.  If you want a different behavior 
 you can set `errors` to ``\'replace\'`` or ``\'strict\'``.  In strict mode a 
 `HTTPUnicodeError` is raised. 
 .. versionchanged:: 0.5 
 In previous versions "";"" and ""&"" could be used for url decoding. 
 This changed in 0.5 where only ""&"" is supported.  If you want to 
 use "";"" instead a different `separator` can be provided. 
 The `cls` parameter was added. 
 :param s: a string with the query string to decode. 
 :param charset: the charset of the query string.  If set to `None` 
 no unicode decoding will take place. 
 :param decode_keys: Used on Python 2.x to control whether keys should 
 be forced to be unicode objects.  If set to `True` 
 then keys will be unicode in all cases. Otherwise, 
 they remain `str` if they fit into ASCII. 
 :param include_empty: Set to `False` if you don\'t want empty values to 
 appear in the dict. 
 :param errors: the decoding error behavior. 
 :param separator: the pair separator to be used, defaults to ``&`` 
 :param cls: an optional dict class to use.  If this is not specified 
 or `None` the default :class:`MultiDict` is used.'"
"def _tagAttr(key, fullpath): 
    attrs = {} 
   _escapedKey = escape4xml(key) 
   if (fullpath in TAGS_TO_MODIFY): 
      (tagName, useTitle) = TAGS_TO_MODIFY[fullpath] 
      if useTitle: 
         attrs['key'] = _escapedKey 
   elif (not isinstance(key, unicode)): 
      if isinstance(key, str): 
         tagName = unicode(key, 'ascii', 'ignore') 
      else: 
         strType = str(type(key)).replace(""<type   '"", '').replace(""'>"", '') 
         attrs['keytype'] = strType 
         tagName = unicode(key) 
   else: 
      tagName = key 
   if isinstance(key, int): 
      attrs['keytype'] = 'int' 
   origTagName = tagName 
   tagName = tagName.lower().replace('   ', '-') 
   tagName = str(tagName).translate(_allchars, _keepchars) 
   if (origTagName != tagName): 
      if ('key' not in attrs): 
         attrs['key'] = _escapedKey 
   if ((not tagName) or tagName[0].isdigit() or (tagName[0] == '-')): 
      tagName = 'item' 
      _utils_logger.error(('invalid   tag:   %s   [%s]' % (_escapedKey, fullpath))) 
      attrs['key'] = _escapedKey 
   return (tagName, u'   '.join([(u'%s=""%s""' % i) for i in attrs.items()]))"," 'Helper function to return the tag name and attributes for a given 
 key and fullpath. 
 :param key: The key to use for the tag 
 :param fullpath: The full path to the tag 
 :return: A tuple of the tag name and the attributes for the tag 
 :rtype: tuple'","'Return a tuple with a tag name and a (possibly empty) attribute, 
 applying the conversions specified in TAGS_TO_MODIFY and checking 
 that the tag is safe for a XML document.'"
"def load_boston(return_X_y=False): 
    module_path = dirname(__file__) 
   fdescr_name = join(module_path, 'descr', 'boston_house_prices.rst') 
   with open(fdescr_name) as f: 
      descr_text = f.read() 
   data_file_name = join(module_path, 'data', 'boston_house_prices.csv') 
   with open(data_file_name) as f: 
      data_file = csv.reader(f) 
      temp = next(data_file) 
      n_samples = int(temp[0]) 
      n_features = int(temp[1]) 
      data = np.empty((n_samples, n_features)) 
      target = np.empty((n_samples,)) 
      temp = next(data_file) 
      feature_names = np.array(temp) 
      for (i, d) in enumerate(data_file): 
         data[i] = np.asarray(d[:(-1)], dtype=np.float64) 
         target[i] = np.asarray(d[(-1)], dtype=np.float64) 
   if return_X_y: 
      return (data, target) 
   return Bunch(data=data, target=target, feature_names=feature_names[:(-1)], DESCR=descr_text)"," 'Loads the boston dataset. 
 Returns 
 X, y : array, array 
 X, y are the training and test features and target. 
 DESCR : string 
 The description of the dataset. 
 Returns 
 X, y : array, array 
 X, y are the training and test features and target. 
 DESCR : string 
 The description of the dataset. 
 .. note:: 
 This dataset is available at http://archive.ics.uci.edu/ml/datasets/Housing+Prices+Data+Set 
 Examples 
 >>> from sklearn.datasets import load_boston 
 >>> X, y = load_boston() 
 >>> X.shape 
 (485, 13) 
 >>> X.dtype 
 dtype(float64) 
 >>> y.shape 
 (485,) 
 >>> y.dtype 
 dtype(float64)'","'Load and return the boston house-prices dataset (regression). 
 Samples total                 506 
 Dimensionality                 13 
 Features           real, positive 
 Targets             real 5. - 50. 
 Parameters 
 return_X_y : boolean, default=False. 
 If True, returns ``(data, target)`` instead of a Bunch object. 
 See below for more information about the `data` and `target` object. 
 .. versionadded:: 0.18 
 Returns 
 data : Bunch 
 Dictionary-like object, the interesting attributes are: 
 \'data\', the data to learn, \'target\', the regression targets, 
 and \'DESCR\', the full description of the dataset. 
 (data, target) : tuple if ``return_X_y`` is True 
 .. versionadded:: 0.18 
 Examples 
 >>> from sklearn.datasets import load_boston 
 >>> boston = load_boston() 
 >>> print(boston.data.shape) 
 (506, 13)'"
"def disinherit(name, objectType, copy_inherited_acl=True): 
    ret = {'name': name, 'result': True, 'changes': {}, 'comment': ''} 
   tRet = __salt__['win_dacl.check_inheritance'](name, objectType) 
   if tRet['result']: 
      if tRet['Inheritance']: 
         if __opts__['test']: 
            ret['result'] = None 
            ret['changes']['Inheritance'] = 'Disabled' 
            ret['comment'] = 'Inheritance   is   set   to   be   disabled.' 
            ret['changes']['Inherited   ACLs'] = ('Are   set   to   be   kept' if copy_inherited_acl else 'Are   set   to   be   removed') 
            return ret 
         eRet = __salt__['win_dacl.disable_inheritance'](name, objectType, copy_inherited_acl) 
         ret['result'] = eRet['result'] 
         if eRet['result']: 
            ret['changes'] = dict(ret['changes'], **eRet['changes']) 
         else: 
            ret['comment'] = '   '.join([ret['comment'], eRet['comment']]) 
      elif __opts__['test']: 
         ret['result'] = None 
         ret['comment'] = 'Inheritance   is   disabled.' 
   else: 
      ret['result'] = False 
      ret['comment'] = tRet['comment'] 
   return ret"," 'Disables inheritance on a given object. 
 name: name of the object 
 objectType: type of the object 
 copy_inherited_acl: copy inherited ACLs 
 Returns: 
 {name: name of the object, result: True if the function executed successfully, 
 changes: dictionary of changes made, comment: comment of the execution} 
 Example: 
 disinherit(\'my_object\', \'objectType\') 
 Disables inheritance on \'my_object\' 
 Disables inheritance on \'my_object\' 
 Disables inheritance on \'my_object\' and keeps inherited ACLs 
 Disables inheritance on \'my_object\' and removes inherited ACLs'",'Ensure an object is not inheriting ACLs from its parent'
"def get_load(jid): 
    query = 'SELECT   load   FROM   salt.jids   WHERE   jid   =   ?;' 
   ret = {} 
   try: 
      data = __salt__['cassandra_cql.cql_query_with_prepare'](query, 'get_load', [jid]) 
      if data: 
         load = data[0].get('load') 
         if load: 
            ret = json.loads(load) 
   except CommandExecutionError: 
      log.critical('Could   not   get   load   from   jids   table.') 
      raise 
   except Exception as e: 
      log.critical('Unexpected   error   while   getting   load   from   jids:   {0}'.format(str(e))) 
      raise 
   return ret"," 'Get the load from the jids table. 
 Returns: 
 load: 
 Dictionary of load information.'",'Return the load data that marks a specified jid'
"def colnum2name(n): 
    assert (n > 0) 
   s = '' 
   while n: 
      (n, m) = divmod((n - 1), 26) 
      s = (chr((m + ord('A'))) + s) 
   return s"," 'Convert column number to column name. 
 :param n: column number 
 :return: column name'","'Translate a column number to name (e.g. 1->\'A\', etc.).'"
"def task_reserved(request): 
    reserved_requests.add(request)"," 'Logs a request to the reserved_requests list. 
 This is used to track requests that should not be logged.'",'Updates global state when a task has been reserved.'
"def pretty_css(container, name, raw): 
    sheet = container.parse_css(raw) 
   return serialize(sheet, u'text/css')"," 'Serialize a CSS file to the given container. 
 :param container: The container to serialize to. 
 :param name: The name of the CSS file. 
 :param raw: The raw contents of the CSS file. 
 :return: The serialized CSS file.'",'Pretty print the CSS represented as a string in raw'
"@csrf_exempt 
 @require_POST 
 def cspreport(request): 
    report = ('blocked-uri', 'violated-directive', 'original-policy') 
   if (not waffle.sample_is_active('csp-store-reports')): 
      return HttpResponse() 
   try: 
      v = json.loads(request.body)['csp-report'] 
      meta = request.META.copy() 
      meta['PATH_INFO'] = v.get('document-uri', meta['PATH_INFO']) 
      v = [(k, v[k]) for k in report if (k in v)] 
      log_cef('CSPViolation', 5, meta, signature='CSPREPORT', msg='A   client   reported   a   CSP   violation', cs6=v, cs6Label='ContentPolicy') 
   except (KeyError, ValueError) as e: 
      log.debug(('Exception   in   CSP   report:   %s' % e), exc_info=True) 
      return HttpResponseBadRequest() 
   return HttpResponse()"," 'Store a CSP violation report in the database. 
 This is used to store the CSP violation reports sent by the client 
 to the server.'",'Accept CSP reports and log them.'
"def extract_bool(name, value): 
    if (str(value).lower() not in ('true', 'false')): 
      raise ValueError((_('Unrecognized   value   ""%(value)s""   for   ""%(name)s"",   acceptable   values   are:   true,   false.') % {'value': value, 'name': name})) 
   return strutils.bool_from_string(value, strict=True)"," 'Convert a string value into a boolean. 
 :param name: The name of the parameter. 
 :param value: The value of the parameter. 
 :raises ValueError: if the value is not a recognized boolean. 
 :returns: The boolean value.'","'Convert any true/false string to its corresponding boolean value. 
 Value is case insensitive.'"
"def format_version(module, attr, call=False): 
    try: 
      if call: 
         version = getattr(module, attr)() 
      else: 
         version = getattr(module, attr) 
   except Exception as e: 
      print e 
      version = 'Version   could   not   be   acquired!' 
   if (not isinstance(version, str)): 
      version = list2string(version) 
   return version"," 'Returns the version of the module. 
 If the module is not installed, returns \'Version   could   not   be   acquired!\'. 
 If the module is installed, returns the version string. 
 If the module is installed and call=True, returns the version string as a 
 function.'",'Format the version.'
"def require_open(func): 
    def wrapper(self, *args, **kwargs): 
      if self.closed: 
         raise IOError(errno.EBADF, 'I/O   operation   on   closed   file') 
      return func(self, *args, **kwargs) 
   return wrapper", 'Decorator that requires that the file object be open.',"'Decorator that ensures that the file instance isn\'t closed when the 
 function is run.'"
"def evaluate(hps, logdir, traindir, subset='valid', return_val=False): 
    hps.batch_size = 100 
   with tf.Graph().as_default(): 
      with tf.device('/cpu:0'): 
         with tf.variable_scope('model') as var_scope: 
            eval_model = RealNVP(hps) 
            summary_writer = tf.summary.FileWriter(logdir) 
            var_scope.reuse_variables() 
         saver = tf.train.Saver() 
         sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) 
         tf.train.start_queue_runners(sess) 
         previous_global_step = 0 
         with sess.as_default(): 
            while True: 
               ckpt_state = tf.train.get_checkpoint_state(traindir) 
               if (not (ckpt_state and ckpt_state.model_checkpoint_path)): 
                  print ('No   model   to   eval   yet   at   %s' % traindir) 
                  time.sleep(30) 
                  continue 
               print ('Loading   file   %s' % ckpt_state.model_checkpoint_path) 
               saver.restore(sess, ckpt_state.model_checkpoint_path) 
               current_step = tf.train.global_step(sess, eval_model.step) 
               if (current_step == previous_global_step): 
                  print 'Waiting   for   the   checkpoint   to   be   updated.' 
                  time.sleep(30) 
                  continue 
               previous_global_step = current_step 
               print 'Evaluating...' 
               bit_per_dim = eval_model.eval_epoch(hps) 
               print ('Epoch:   %d,   %s   ->   %.3f   bits/dim' % (current_step, subset, bit_per_dim)) 
               print 'Writing   summary...' 
               summary = tf.Summary() 
               summary.value.extend([tf.Summary.Value(tag='bit_per_dim', simple_value=bit_per_dim)]) 
               summary_writer.add_summary(summary, current_step) 
               if return_val: 
                  return (current_step, bit_per_dim)"," 'Evaluate the model on a dataset. 
 :param hps: Hyperparameters 
 :param logdir: Path to the log directory 
 :param traindir: Path to the training directory 
 :param subset: Evaluate on the training, validation, or test set 
 :param return_val: Return the global step and the bit per dimension'",'Evaluation.'
"def comment(parser, token): 
    parser.skip_past('endcomment') 
   return CommentNode()"," 'Parses a comment. 
 :param parser: The parser to parse with. 
 :param token: The token to parse. 
 :returns: A :class:`CommentNode` object.'",'Ignores everything between ``{% comment %}`` and ``{% endcomment %}``.'
"@dec.skip_win32 
 def test_find_cmd_ls(): 
    path = find_cmd('ls') 
   nt.assert_true(path.endswith('ls'))", 'Test find_cmd() with ls','Make sure we can find the full path to ls.'
"def get_area(y, x): 
    return trapz(y=y, x=x)"," 'Returns the area under the curve y = f(x) between x and x+dx. 
 Parameters 
 y : array_like 
 The function values. 
 x : array_like 
 The independent variable. 
 Returns 
 area : float 
 The area under the curve. 
 Examples 
 >>> from sympy import Function, symbols, cos, trapz 
 >>> x, y = symbols(\'x, y\') 
 >>> f = Function(\'f\') 
 >>> trapz(f(x), x) 
 1/2 
 >>> trapz(f(x), x) 
 1/2 
 >>> trapz(f(x), x) 
 1/2 
 >>> trapz(f(x), x) 
 1/2 
 >>> trapz(f(x), x) 
 1/2 
 >>> trapz(f(x), x) 
 1/2 
 >>> trapz(f(x), x) 
 1/2 
 >>> trapz(f(x), x) 
 1/2",'Get the area under the curve'
"def py_encode_basestring_ascii(s, _PY3=PY3): 
    if _PY3: 
      if isinstance(s, binary_type): 
         s = s.decode('utf-8') 
      if (type(s) is not text_type): 
         s = text_type(s) 
   else: 
      if (isinstance(s, str) and (HAS_UTF8.search(s) is not None)): 
         s = s.decode('utf-8') 
      if (type(s) not in string_types): 
         s = text_type(s) 
   def replace(match): 
      s = match.group(0) 
      try: 
         return ESCAPE_DCT[s] 
      except KeyError: 
         n = ord(s) 
         if (n < 65536): 
            return ('\\u%04x' % (n,)) 
         else: 
            n -= 65536 
            s1 = (55296 | ((n >> 10) & 1023)) 
            s2 = (56320 | (n & 1023)) 
            return ('\\u%04x\\u%04x' % (s1, s2)) 
   return (('""' + str(ESCAPE_ASCII.sub(replace, s))) + '""')", 'encode a string as ASCII in Python 2.x','Return an ASCII-only JSON representation of a Python string'
"def set_reboot_required_witnessed(): 
    return __salt__['reg.set_value'](hive='HKLM', key=MINION_VOLATILE_KEY, volatile=True, vname=REBOOT_REQUIRED_NAME, vdata=1, vtype='REG_DWORD')", 'Set the reboot required flag on the minion\'s registry.',"'.. versionadded:: 2016.11.0 
 This function is used to remember that 
 an event indicating that a reboot is required was witnessed. 
 This function relies on the salt-minion\'s ability to create the following 
 volatile registry key in the *HKLM* hive: 
 *SYSTEM\\CurrentControlSet\\Services\\salt-minion\\Volatile-Data* 
 Because this registry key is volatile, it will not persist 
 beyond the current boot session. 
 Also, in the scope of this key, the name *\'Reboot required\'* will be 
 assigned the value of *1*. 
 (For the time being, this this function is being used 
 whenever an install completes with exit code 3010 and 
 this usage can be extended where appropriate in the future.) 
 :return: A boolean indicating whether or not the salt minion was 
 able to perform the necessary registry operations. 
 :rtype: bool 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' system.set_reboot_required_witnessed'"
"def preprocess_for_eval(image, height, width, central_fraction=0.875, scope=None): 
    with tf.name_scope(scope, 'eval_image', [image, height, width]): 
      if (image.dtype != tf.float32): 
         image = tf.image.convert_image_dtype(image, dtype=tf.float32) 
      if central_fraction: 
         image = tf.image.central_crop(image, central_fraction=central_fraction) 
      if (height and width): 
         image = tf.expand_dims(image, 0) 
         image = tf.image.resize_bilinear(image, [height, width], align_corners=False) 
         image = tf.squeeze(image, [0]) 
      image = tf.sub(image, 0.5) 
      image = tf.mul(image, 2.0) 
      return image"," 'Preprocess an image for evaluation. 
 :param image: An image tensor. 
 :param height: The height of the image. 
 :param width: The width of the image. 
 :param central_fraction: The fraction of the image to crop from the center. 
 :param scope: The name scope for the operation. 
 :return: The preprocessed image. 
 :rtype: tf.Tensor'","'Prepare one image for evaluation. 
 If height and width are specified it would output an image with that size by 
 applying resize_bilinear. 
 If central_fraction is specified it would cropt the central fraction of the 
 input image. 
 Args: 
 image: 3-D Tensor of image. If dtype is tf.float32 then the range should be 
 [0, 1], otherwise it would converted to tf.float32 assuming that the range 
 is [0, MAX], where MAX is largest positive representable number for 
 int(8/16/32) data type (see `tf.image.convert_image_dtype` for details) 
 height: integer 
 width: integer 
 central_fraction: Optional Float, fraction of the image to crop. 
 scope: Optional scope for name_scope. 
 Returns: 
 3-D float Tensor of prepared image.'"
"def user_registered_cb(user, **kwargs): 
    siteconfig = SiteConfiguration.objects.get_current() 
   if siteconfig.get(u'mail_send_new_user_mail'): 
      mail_new_user(user)"," 'This is called when a new user is created. 
 :param user: the new user 
 :type user: User'","'Send e-mail when a user is registered. 
 Listens for new user registrations and sends a new user registration 
 e-mail to administrators, if this type of notification is enabled (through 
 ``mail_send_new_user_mail`` site configuration).'"
"def _is_astropy_source(path=None): 
    if (path is None): 
      path = os.path.join(os.path.dirname(__file__), os.pardir) 
   elif os.path.isfile(path): 
      path = os.path.dirname(path) 
   source_dir = os.path.abspath(path) 
   return os.path.exists(os.path.join(source_dir, '.astropy-root'))", 'Returns True if the source is a path to an astropy source directory.',"'Returns whether the source for this module is directly in an astropy 
 source distribution or checkout.'"
"def data_to_tfrecord(images, labels, filename): 
    print ('Converting   data   into   %s   ...' % filename) 
   cwd = os.getcwd() 
   writer = tf.python_io.TFRecordWriter(filename) 
   for (index, img) in enumerate(images): 
      img_raw = img.tobytes() 
      label = int(labels[index]) 
      example = tf.train.Example(features=tf.train.Features(feature={'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label])), 'img_raw': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw]))})) 
      writer.write(example.SerializeToString()) 
   writer.close()"," 'Converts the images and labels into tfrecord format. 
 Args: 
 images: 2-D numpy array of images 
 labels: 1-D numpy array of labels 
 filename: path of the output file'",'Save data into TFRecord'
"def save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL): 
    new_fd = False 
   if isinstance(f, str): 
      new_fd = True 
      f = open(f, 'wb') 
   try: 
      return _save(obj, f, pickle_module, pickle_protocol) 
   finally: 
      if new_fd: 
         f.close()"," 'Saves an object to the given file. 
 The file is opened in write mode. 
 :param obj: The object to save. 
 :param f: The file to save the object to. 
 :param pickle_module: The pickle module to use. 
 :param pickle_protocol: The pickle protocol to use. 
 :return: The file descriptor for the file. 
 :rtype: file'","'Saves an object to a disk file. 
 Args: 
 obj: saved object 
 f: a file-like object (has to implement fileno that returns a file descriptor) 
 or a string containing a file name 
 pickle_module: module used for pickling metadata and objects 
 pickle_protocol: can be specified to override the default protocol'"
"def pearson_score(list1, list2): 
    size = len(list1) 
   sum1 = sum(list1) 
   sum2 = sum(list2) 
   sum_sq1 = sum([pow(l, 2) for l in list1]) 
   sum_sq2 = sum([pow(l, 2) for l in list2]) 
   prod_sum = sum([(list1[i] * list2[i]) for i in range(size)]) 
   num = (prod_sum - ((sum1 * sum2) / float(size))) 
   den = sqrt(((sum_sq1 - (pow(sum1, 2.0) / size)) * (sum_sq2 - (pow(sum2, 2.0) / size)))) 
   return (num / den)"," 'Computes Pearson\'s correlation coefficient between two lists. 
 Parameters 
 list1 : list 
 The first list. 
 list2 : list 
 The second list. 
 Returns 
 float 
 The Pearson\'s correlation coefficient between list1 and list2. 
 Examples 
 >>> pearson_score([1, 2, 3, 4], [1, 2, 3, 4]) 
 1.0 
 >>> pearson_score([1, 2, 3, 4], [1, 2, 3, 4]) 
 1.0 
 >>> pearson_score([1, 2, 3, 4], [1, 2, 3, 4]) 
 1.0 
 >>> pearson_score([1, 2, 3, 4], [1, 2, 3, 4]) 
 1.0 
 >>> pearson_score([1, 2, 3, 4], [1, 2, 3, 4]) 
 1.",'Compute the Pearson\' score between 2 lists of vectors.'
"def AskString(prompt, default='', id=261, ok=None, cancel=None): 
    _initialize() 
   _interact() 
   d = GetNewDialog(id, (-1)) 
   if (not d): 
      print ""EasyDialogs:   Can't   get   DLOG   resource   with   id   ="", id, '   (missing   resource   file?)' 
      return 
   h = d.GetDialogItemAsControl(3) 
   SetDialogItemText(h, lf2cr(prompt)) 
   h = d.GetDialogItemAsControl(4) 
   SetDialogItemText(h, lf2cr(default)) 
   d.SelectDialogItemText(4, 0, 999) 
   if (ok is not None): 
      h = d.GetDialogItemAsControl(1) 
      h.SetControlTitle(ok) 
   if (cancel is not None): 
      h = d.GetDialogItemAsControl(2) 
      h.SetControlTitle(cancel) 
   d.SetDialogDefaultItem(1) 
   d.SetDialogCancelItem(2) 
   d.AutoSizeDialog() 
   d.GetDialogWindow().ShowWindow() 
   while 1: 
      n = ModalDialog(None) 
      if (n == 1): 
         h = d.GetDialogItemAsControl(4) 
         return cr2lf(GetDialogItemText(h)) 
      if (n == 2): 
         return None"," 'Asks the user a string input. 
 Prompt is the text that is displayed to the user. 
 Default is the default value that is displayed if the user does not 
 enter anything. 
 Ok is the text that is displayed to the user if the user enters a valid 
 value. 
 Cancel is the text that is displayed to the user if the user enters an 
 invalid value. 
 Returns the user\'s input.'","'Display a PROMPT string and a text entry field with a DEFAULT string. 
 Return the contents of the text entry field when the user clicks the 
 OK button or presses Return. 
 Return None when the user clicks the Cancel button. 
 If omitted, DEFAULT is empty. 
 The PROMPT and DEFAULT strings, as well as the return value, 
 can be at most 255 characters long.'"
"def detect(stream): 
    for fmt in formats.available: 
      try: 
         if fmt.detect(stream): 
            return (fmt, stream) 
      except AttributeError: 
         pass 
   return (None, stream)"," 'Detects the format of a stream. 
 Parameters 
 stream : file-like object 
 The stream to detect. 
 Returns 
 (format, stream) 
 The format and the stream if the format was detected, otherwise (None, stream).'","'Return (format, stream) of given stream.'"
"@flake8ext 
 def validate_log_translations(logical_line, physical_line, filename): 
    if ('neutron/tests' in filename): 
      return 
   if pep8.noqa(physical_line): 
      return 
   msg = 'N320:   Log   messages   require   translation   hints!' 
   if log_translation_hint.match(logical_line): 
      (yield (0, msg))", 'Checks that log messages have translation hints.','N320 - Log messages require translation.'
"def file_join(nzo, workdir, workdir_complete, delete, joinables): 
    newfiles = [] 
   bufsize = ((24 * 1024) * 1024) 
   joinable_sets = {} 
   joinable_set = None 
   for joinable in joinables: 
      (head, tail) = os.path.splitext(joinable) 
      if (tail == '.ts'): 
         head = match_ts(joinable)[1] 
      if (head not in joinable_sets): 
         joinable_sets[head] = [] 
      joinable_sets[head].append(joinable) 
   logging.debug('joinable_sets:   %s', joinable_sets) 
   try: 
      for joinable_set in joinable_sets: 
         current = joinable_sets[joinable_set] 
         joinable_sets[joinable_set].sort() 
         if os.path.exists(joinable_set): 
            logging.debug('file_join():   Skipping   %s,   (probably)   joined   by   par2', joinable_set) 
            if delete: 
               clean_up_joinables(current) 
            continue 
         size = len(current) 
         if (size < 2): 
            continue 
         filename = joinable_set 
         if workdir_complete: 
            filename = filename.replace(workdir, workdir_complete) 
         logging.debug('file_join():   Assembling   %s', filename) 
         joined_file = open(filename, 'ab') 
         n = get_seq_number(current[0]) 
         seq_error = (n > 1) 
         for joinable in current: 
            if (get_seq_number(joinable) != n): 
               seq_error = True 
            perc = ((100.0 / size) * n) 
            logging.debug('Processing   %s', joinable) 
            nzo.set_action_line(T('Joining'), ('%.0f%%' % perc)) 
            f = open(joinable, 'rb') 
            shutil.copyfileobj(f, joined_file, bufsize) 
            f.close() 
            if delete: 
               logging.debug('Deleting   %s', joinable) 
               os.remove(joinable) 
            n += 1 
         clean_up_joinables(current) 
         joined_file.flush() 
         joined_file.close() 
         newfiles.append(filename) 
         if seq_error: 
            msg = T('Incomplete   sequence   of   joinable   files') 
            nzo.fail_msg = (T('File   join   of   %s   failed') % unicoder(joinable_set)) 
            nzo.set_unpack_info('Filejoin', (T('[%s]   Error   ""%s""   while   joining   files') % (unicoder(joinable_set), msg))) 
            logging.error(T('Error   ""%s""   while   running   file_join   on   %s'), msg, nzo.final_name) 
         else: 
            msg = (T('[%s]   Joined   %s   files') % (unicoder(joinable_set), size)) 
            nzo.set_unpack_info('Filejoin', msg, set=joinable_set) 
   except: 
      msg = sys.exc_info()[1] 
      nzo.fail_msg = (T('File   join   of   %s   failed') % msg) 
      nzo.set_unpack_info('Filejoin', (T('[%s]   Error   ""%s""   while   joining   files') % (unicoder(joinable_set), msg))) 
      logging.error(T('Error   ""%s""   while   running   file_join   on   %s'), msg, nzo.final_name) 
      return (True, []) 
   return (False, newfiles)"," 'Join a set of files into a single file. 
 :param nzo: the NZO object 
 :param workdir: the working directory 
 :param workdir_complete: whether the working directory should be complete 
 :param delete: whether to delete the joinable files after joining 
 :param joinables: the joinables to join'","'Join and joinable files in \'workdir\' to \'workdir_complete\' and 
 when successful, delete originals'"
"def _ValidateClientId(client_id): 
    if (not isinstance(client_id, basestring)): 
      raise InvalidChannelClientIdError(('""%s""   is   not   a   string.' % client_id)) 
   if isinstance(client_id, unicode): 
      client_id = client_id.encode('utf-8') 
   if (len(client_id) > MAXIMUM_CLIENT_ID_LENGTH): 
      msg = ('Client   id   length   %d   is   greater   than   max   length   %d' % (len(client_id), MAXIMUM_CLIENT_ID_LENGTH)) 
      raise InvalidChannelClientIdError(msg) 
   return client_id", 'Validates the client id.',"'Validates a client id. 
 Args: 
 client_id: The client id provided by the application. 
 Returns: 
 If the client id is of type str, returns the original client id. 
 If the client id is of type unicode, returns the id encoded to utf-8. 
 Raises: 
 InvalidChannelClientIdError: if client id is not an instance of str or 
 unicode, or if the (utf-8 encoded) string is longer than 64 characters.'"
"def isvector(X): 
    warnings.warn('isvector   has   been   moved   to   matplotlib.mlab   --   please   import   it   from   there', DeprecationWarning) 
   import matplotlib.mlab as mlab 
   return mlab.isvector(x, y, xi, extrap=extrap)", 'Return True if X is a vector.',"'This function has been moved to matplotlib.mlab -- please import 
 it from there'"
"@register_canonicalize 
 @register_specialize 
 @gof.local_optimizer([T.int_div, T.true_div]) 
 def local_zero_div(node): 
    if (isinstance(node.op, T.Elemwise) and isinstance(node.op.scalar_op, (theano.scalar.IntDiv, theano.scalar.TrueDiv))): 
      if (local_mul_canonizer.get_constant(node.inputs[0]) == 0): 
         ret = broadcast_like(0, node.outputs[0], node.fgraph) 
         ret.tag.values_eq_approx = values_eq_approx_remove_nan 
         return [ret]"," 'Local zero division optimization. 
 If the left side of the division is zero, then the right side is 
 also zero. 
 Theano can handle this case, but this optimization makes the 
 code more efficient. 
 Theano can handle this case, but this optimization makes the 
 code more efficient. 
 Parameters 
 node : Node 
 The node to be optimized. 
 Returns 
 A list of nodes.'",'0 / x -> 0'
"def test_nested(a, b, c): 
    def one(): 
      return a 
   def two(): 
      return b 
   def three(): 
      return c 
   def new_closure(a, b): 
      def sum(): 
         return (a + b) 
      return sum 
   (yield one) 
   (yield two) 
   (yield three) 
   (yield new_closure(a, c))", 'Test that nested generators work',"'>>> obj = test_nested(1, 2, 3) 
 >>> [i() for i in obj] 
 [1, 2, 3, 4]'"
"def make_fna(sff_fp, output_fp, use_sfftools=False, no_trim=False): 
    if use_sfftools: 
      _fail_on_gzipped_sff(sff_fp) 
      check_sffinfo() 
      if no_trim: 
         _check_call(['sffinfo', '-notrim', '-s', sff_fp], stdout=open(output_fp, 'w')) 
      else: 
         _check_call(['sffinfo', '-s', sff_fp], stdout=open(output_fp, 'w')) 
   else: 
      try: 
         format_binary_sff_as_fna(qiime_open(sff_fp, 'rb'), open(output_fp, 'w')) 
      except: 
         raise IOError(('Could   not   parse   SFF   %s' % sff_fp))"," 'Convert SFF to FNA. 
 This function is a wrapper around the format_binary_sff_as_fna function. 
 :param sff_fp: Path to SFF file. 
 :param output_fp: Path to FNA file. 
 :param use_sfftools: Whether to use sfftools or not. 
 :param no_trim: Whether to ignore the -notrim option. 
 :return: None'",'Makes fna file from sff file.'
"def worker_e_step(input_queue, result_queue): 
    logger.debug('worker   process   entering   E-step   loop') 
   while True: 
      logger.debug('getting   a   new   job') 
      (chunk_no, chunk, worker_lda) = input_queue.get() 
      logger.debug('processing   chunk   #%i   of   %i   documents', chunk_no, len(chunk)) 
      worker_lda.state.reset() 
      worker_lda.do_estep(chunk) 
      del chunk 
      logger.debug('processed   chunk,   queuing   the   result') 
      result_queue.put(worker_lda.state) 
      del worker_lda 
      logger.debug('result   put')", 'Worker process E-step loop',"'Perform E-step for each (chunk_no, chunk, model) 3-tuple from the 
 input queue, placing the resulting state into the result queue.'"
"def _sl_fit(estimator, X, y): 
    from sklearn.base import clone 
   estimators_ = list() 
   for ii in range(X.shape[(-1)]): 
      est = clone(estimator) 
      est.fit(X[..., ii], y) 
      estimators_.append(est) 
   return estimators_"," 'Fit a set of estimators to the same data. 
 Parameters 
 estimator : estimator 
 The estimator to fit. 
 X : array-like 
 The input data. 
 y : array-like 
 The target values. 
 Returns 
 estimators : list of estimators 
 A list of estimators. 
 Examples 
 >>> from sklearn.svm import SVC 
 >>> from sklearn.ensemble import RandomForestClassifier 
 >>> from sklearn.ensemble import AdaBoostClassifier 
 >>> from sklearn.ensemble import BaggingClassifier 
 >>> from sklearn.ensemble import GradientBoostingClassifier 
 >>> from sklearn.ensemble import ExtraTreesClassifier 
 >>> estimators = [_sl_fit(SVC(), X, y), 
 ...             _sl_fit(RandomForestClassifier(), X, y), 
 ...             _sl_fit(AdaBoostClassifier(), X, y), 
 ...             _sl_fit(BaggingClassifier(), X, y), 
 ...             _sl_fit(GradientBoostingClass","'Aux. function to fit _SearchLight in parallel. 
 Fit a clone estimator to each slice of data. 
 Parameters 
 base_estimator : object 
 The base estimator to iteratively fit on a subset of the dataset. 
 X : array, shape (n_samples, nd_features, n_estimators) 
 The target data. The feature dimension can be multidimensional e.g. 
 X.shape = (n_samples, n_features_1, n_features_2, n_estimators) 
 y : array, shape (n_sample, ) 
 The target values. 
 Returns 
 estimators_ : list of estimators 
 The fitted estimators.'"
"def perm2tensor(t, g, canon_bp=False): 
    if (not isinstance(t, TensExpr)): 
      return t 
   new_tids = get_tids(t).perm2tensor(g, canon_bp) 
   coeff = get_coeff(t) 
   if (g[(-1)] != (len(g) - 1)): 
      coeff = (- coeff) 
   res = TensMul.from_TIDS(coeff, new_tids, is_canon_bp=canon_bp) 
   return res"," 'Convert a tensor expression to tensor indices. 
 Parameters 
 t : TensExpr 
 tensor expression 
 g : list of int 
 tensor indices 
 canon_bp : bool 
 whether to use canonical basis 
 Returns 
 TensExpr 
 tensor expression'","'Returns the tensor corresponding to the permutation ``g`` 
 For further details, see the method in ``TIDS`` with the same name.'"
"def get_build_results(build): 
    r_url = get_results_raw_url(build) 
   if (not r_url): 
      return 
   return convert_json_to_df(r_url)"," 'Get the build results for a build. 
 :param build: The build to get results for 
 :return: A dataframe of the build results'",'Returns a df with the results of the VBENCH job associated with the travis build'
"def get_hash(f): 
    import hashlib 
   m = hashlib.md5() 
   m.update(f) 
   return m.hexdigest()"," 'Generates a hash for a file. 
 :param f: The file to hash. 
 :return: The hash.'",'Gets hexadmecimal md5 hash of a string'
"@requires_sklearn_0_15 
 def test_SearchLight(): 
    from sklearn.linear_model import Ridge, LogisticRegression 
   from sklearn.pipeline import make_pipeline 
   from sklearn.metrics import roc_auc_score, get_scorer, make_scorer 
   (X, y) = make_data() 
   (n_epochs, _, n_time) = X.shape 
   assert_raises(ValueError, _SearchLight, 'foo') 
   sl = _SearchLight(Ridge()) 
   sl = _SearchLight(LogisticRegression()) 
   assert_equal(sl.__repr__()[:14], '<_SearchLight(') 
   sl.fit(X, y) 
   assert_equal(sl.__repr__()[(-28):], ',   fitted   with   10   estimators>') 
   assert_raises(ValueError, sl.fit, X[1:], y) 
   assert_raises(ValueError, sl.fit, X[:, :, 0], y) 
   assert_raises(ValueError, sl.predict, X[:, :, :2]) 
   y_pred = sl.predict(X) 
   assert_true((y_pred.dtype == int)) 
   assert_array_equal(y_pred.shape, [n_epochs, n_time]) 
   y_proba = sl.predict_proba(X) 
   assert_true((y_proba.dtype == float)) 
   assert_array_equal(y_proba.shape, [n_epochs, n_time, 2]) 
   score = sl.score(X, y) 
   assert_array_equal(score.shape, [n_time]) 
   assert_true((np.sum(np.abs(score)) != 0)) 
   assert_true((score.dtype == float)) 
   sl = _SearchLight(LogisticRegression()) 
   assert_equal(sl.scoring, None) 
   for (err, scoring) in [(ValueError, 'foo'), (TypeError, 999)]: 
      sl = _SearchLight(LogisticRegression(), scoring=scoring) 
      sl.fit(X, y) 
      assert_raises(err, sl.score, X, y) 
   sl = _SearchLight(LogisticRegression(), scoring='roc_auc') 
   y = (np.arange(len(X)) % 3) 
   sl.fit(X, y) 
   assert_raises(ValueError, sl.score, X, y) 
   y = ((np.arange(len(X)) % 2) + 1) 
   sl.fit(X, y) 
   score = sl.score(X, y) 
   assert_array_equal(score, [roc_auc_score((y - 1), (_y_pred - 1)) for _y_pred in sl.decision_function(X).T]) 
   y = (np.arange(len(X)) % 2) 
   for (method, scoring) in [('predict_proba', 'roc_auc'), ('predict', roc_auc_score)]: 
      sl1 = _SearchLight(LogisticRegression(), scoring=scoring) 
      sl1.fit(X, y) 
      np.random.seed(0) 
      X = np.random.randn(*X.shape) 
      score_sl = sl1.score(X, y) 
      assert_array_equal(score_sl.shape, [n_time]) 
      assert_true((score_sl.dtype == float)) 
      if isinstance(scoring, str): 
         scoring = get_scorer(scoring) 
      else: 
         scoring = make_scorer(scoring) 
      score_manual = [scoring(est, x, y) for (est, x) in zip(sl1.estimators_, X.transpose(2, 0, 1))] 
      assert_array_equal(score_manual, score_sl) 
   sl = _SearchLight(LogisticRegression(), n_jobs=2) 
   sl.fit(X, y) 
   sl.predict(X) 
   sl.score(X, y) 
   sl.fit(X[..., [0]], y) 
   sl.predict(X[..., [0]]) 
   class _LogRegTransformer(LogisticRegression, ): 
      def transform(self, X): 
         return super(_LogRegTransformer, self).predict_proba(X)[..., 1] 
   pipe = make_pipeline(_SearchLight(_LogRegTransformer()), LogisticRegression()) 
   pipe.fit(X, y) 
   pipe.predict(X) 
   X = np.random.rand(10, 3, 4, 2) 
   y = (np.arange(10) % 2) 
   y_preds = list() 
   for n_jobs in [1, 2]: 
      pipe = _SearchLight(make_pipeline(Vectorizer(), LogisticRegression()), n_jobs=n_jobs) 
      y_preds.append(pipe.fit(X, y).predict(X)) 
      features_shape = pipe.estimators_[0].steps[0][1].features_shape_ 
      assert_array_equal(features_shape, [3, 4]) 
   assert_array_equal(y_preds[0], y_preds[1])", 'Test the SearchLight class','Test _SearchLight'
"def _parse_output(output, template): 
    ret = {} 
   index = 0 
   if (not (output and template)): 
      return ret 
   if ('translate' in template): 
      ret = _translate_output(output) 
   else: 
      output_list = output.strip().replace('\n', '').split('   ') 
      if (sum(template.values()) != len(output_list)): 
         raise ipmiexcept.IPMIException(_('ipmitool   output   length   mismatch')) 
      for item in template.items(): 
         index_end = (index + item[1]) 
         update_value = output_list[index:index_end] 
         ret[item[0]] = update_value 
         index = index_end 
   return ret", 'Parses output and return a dictionary',"'Parse the return value of IPMI command into dict 
 :param output: output of the execution of IPMI command 
 :param template: a dict that contains the expected items of 
 IPMI command and its length.'"
"def timestamp(x): 
    if (x.tzinfo is None): 
      x = x.replace(tzinfo=utc) 
   if hasattr(x, 'timestamp'): 
      return x.timestamp() 
   else: 
      return (x - datetime(1970, 1, 1, tzinfo=utc)).total_seconds()"," 'Returns the timestamp of the given datetime object. 
 If the object does not have a tzinfo attribute, it will be converted to UTC 
 before the timestamp is returned.'",'Get a timestamp from a date in python 3 and python 2'
"def get_latest_repository_metadata(app, decoded_repository_id, downloadable=False): 
    sa_session = app.model.context.current 
   repository = sa_session.query(app.model.Repository).get(decoded_repository_id) 
   repo = hg_util.get_repo_for_repository(app, repository=repository, repo_path=None, create=False) 
   if downloadable: 
      changeset_revision = get_latest_downloadable_changeset_revision(app, repository, repo) 
   else: 
      changeset_revision = get_latest_changeset_revision(app, repository, repo) 
   return get_repository_metadata_by_changeset_revision(app, app.security.encode_id(repository.id), changeset_revision)"," 'Get the latest repository metadata for a repository. 
 This will return the latest repository metadata for the given repository. 
 If the repository does not exist, or the changeset_revision is not the 
 latest, the latest changeset revision will be returned. 
 :param app: The current application. 
 :param decoded_repository_id: The repository ID, decoded. 
 :param downloadable: Whether or not the repository metadata should be 
 downloadable. 
 :returns: The latest repository metadata.'",'Get last metadata defined for a specified repository from the database.'
"def test_string(): 
    schema = vol.Schema(cv.string) 
   with pytest.raises(vol.MultipleInvalid): 
      schema(None) 
   for value in (True, 1, 'hello'): 
      schema(value)", 'Test that a string schema can be validated.','Test string validation.'
"@gen.engine 
 def ListRecursively(store, pattern, callback): 
    results = (yield gen.Task(ListAllKeys, store, prefix=PrefixFromPattern(pattern))) 
   callback(results)"," 'List recursively all keys in a store matching the given pattern. 
 :param store: Store to list from 
 :param pattern: Pattern to match against keys 
 :param callback: Callback to invoke for each key matching the pattern'",'Recursively list all files matching \'pattern\'. This does not return directories.'
"def _extract_doc_comment(content, line, column, markers): 
    if (markers[1] == ''): 
      return _extract_doc_comment_simple(content, line, column, markers) 
   elif (markers[1] == markers[2]): 
      return _extract_doc_comment_continuous(content, line, column, markers) 
   else: 
      return _extract_doc_comment_standard(content, line, column, markers)"," 'Extract a doc-comment from the given content. 
 This function extracts doc-comments from the given content. 
 :param content: The content to be analyzed. 
 :type content: str 
 :param line: The line number of the content. 
 :type line: int 
 :param column: The column number of the content. 
 :type column: int 
 :param markers: A list of markers to be used for extracting the doc-comment. 
 :type markers: list(str) 
 :return: A tuple containing the start and end of the doc-comment. 
 :rtype: tuple(int, int)'","'Delegates depending on the given markers to the right extraction method. 
 :param content: Presplitted lines of the source-code-string. 
 :param line:    Line where the documentation comment starts (behind the 
 start marker). Zero-based. 
 :param column:  Column where the documentation comment starts (behind the 
 start marker). Zero-based. 
 :param markers: The documentation identifying markers. 
 :return:        If the comment matched layout a triple with end-of-comment 
 line, column and the extracted documentation. If not 
 matched, returns None.'"
"def Time2Internaldate(date_time): 
    if isinstance(date_time, (int, float)): 
      tt = time.localtime(date_time) 
   elif isinstance(date_time, (tuple, time.struct_time)): 
      tt = date_time 
   elif (isinstance(date_time, str) and ((date_time[0], date_time[(-1)]) == ('""', '""'))): 
      return date_time 
   else: 
      raise ValueError('date_time   not   of   a   known   type') 
   dt = time.strftime('%d-%b-%Y   %H:%M:%S', tt) 
   if (dt[0] == '0'): 
      dt = ('   ' + dt[1:]) 
   if (time.daylight and tt[(-1)]): 
      zone = (- time.altzone) 
   else: 
      zone = (- time.timezone) 
   return ((('""' + dt) + ('   %+03d%02d' % divmod((zone // 60), 60))) + '""')"," 'Convert time to internal date string. 
 :param date_time: time to convert 
 :type date_time: int, float, str, tuple, time.struct_time 
 :return: converted date string 
 :rtype: str'","'Convert \'date_time\' to IMAP4 INTERNALDATE representation. 
 Return string in form: \'""DD-Mmm-YYYY HH:MM:SS +HHMM""\''"
"def parseSdr(s): 
    assert isinstance(s, basestring) 
   sdr = [int(c) for c in s if (c in ('0', '1'))] 
   if (len(sdr) != len(s)): 
      raise ValueError(""The   provided   string   %s   is   malformed.   The   string   should   have   only   0's   and   1's."") 
   return sdr"," 'Parses the string into a list of bits. 
 Parameters 
 s : string 
 The string to parse. 
 Returns 
 bits : list 
 A list of bits. 
 Raises 
 ValueError 
 If the string is malformed.'",'Parses a string containing only 0\'s and 1\'s and return a Python list object.'
"def _relpath(path, start='.'): 
    if (not path): 
      raise ValueError('no   path   specified') 
   startList = os.path.abspath(start).split(os.path.sep) 
   pathList = os.path.abspath(path).split(os.path.sep) 
   i = len(os.path.commonprefix([startList, pathList])) 
   relList = ((['..'] * (len(startList) - i)) + pathList[i:]) 
   if (not relList): 
      return path 
   return os.path.join(*relList)", 'Returns a relative path from start to path.',"'This code is based on os.path.relpath in the Python 2.6 distribution, 
 included here for compatibility with Python 2.5'"
"def packages(pkg_list, update=False): 
    pkg_list = [pkg for pkg in pkg_list if (not is_installed(pkg))] 
   if pkg_list: 
      install(pkg_list, update)"," 'Install packages in a list. 
 If update is True, then the packages will be updated if they are already 
 installed.'","'Require several Arch Linux packages to be installed. 
 Example:: 
 from fabtools import require 
 require.arch.packages([ 
 \'foo\', 
 \'bar\', 
 \'baz\','"
"def construct_mirror_name(volume): 
    return ('mirror_' + six.text_type(volume.id))", 'Constructs the mirror name for a volume.','Constructs MirrorView name for volume.'
"def floating_ip_create(kwargs, call=None): 
    if (call != 'function'): 
      raise SaltCloudSystemExit('The   floating_ip_create   action   must   be   called   with   -f   or   --function') 
   if ('pool' not in kwargs): 
      log.error('pool   is   required') 
      return False 
   conn = get_conn() 
   return conn.floating_ip_create(kwargs['pool'])", 'Create a floating IP address in the specified pool.',"'Allocate a floating IP 
 .. versionadded:: 2016.3.0'"
"def top_contributors_l10n(start=None, end=None, locale=None, product=None, count=10, page=1): 
    query = RevisionMetricsMappingType.search().facet('creator_id', filtered=True, size=BIG_NUMBER) 
   if (locale is None): 
      query = query.filter((~ F(locale=settings.WIKI_DEFAULT_LANGUAGE))) 
   query = _apply_filters(query, start, end, locale, product) 
   return _get_creator_counts(query, count, page)"," 'Returns a list of top contributors by locale. 
 :param start: the start date of the query, defaults to None 
 :param end: the end date of the query, defaults to None 
 :param locale: the locale to filter by, defaults to None 
 :param product: the product to filter by, defaults to None 
 :param count: the number of results to return, defaults to 10 
 :param page: the page number to return, defaults to 1 
 :returns: a list of dictionaries containing the following fields: 
 * ""id"": the id of the contributor 
 * ""name"": the name of the contributor 
 * ""creator_id"": the id of the creator 
 * ""creator_name"": the name of the creator 
 * ""total"": the total number of revisions made by the contributor 
 * ""contribs"": the number of revisions made by the contributor 
 * ""revisions"": the number of revisions made by the contributor 
 * ""contribs_per_revision"": the number of revisions made by the contributor 
 * ""contrib",'Get the top l10n contributors for the KB.'
"def assert_array_max_ulp(a, b, maxulp=1, dtype=None): 
    numpy.testing.assert_array_max_ulp(cupy.asnumpy(a), cupy.asnumpy(b), maxulp=maxulp, dtype=dtype)"," 'Assert that the maximum relative error between two arrays is less than maxulp. 
 Parameters 
 a : array 
 Array to compare with b. 
 b : array 
 Array to compare with a. 
 maxulp : float 
 Maximum relative error. 
 dtype : dtype, optional 
 Data type of the arrays. 
 Examples 
 >>> assert_array_max_ulp(a, b, maxulp=1e-3) 
 >>> assert_array_max_ulp(a, b, maxulp=1e-3, dtype=np.float64) 
 >>> assert_array_max_ulp(a, b, maxulp=1e-3, dtype=np.float32)'","'Check that all items of arrays differ in at most N Units in the Last Place. 
 Args: 
 a(numpy.ndarray or cupy.ndarray): The actual object to check. 
 b(numpy.ndarray or cupy.ndarray): The desired, expected object. 
 maxulp(int): The maximum number of units in the last place 
 that elements of ``a`` and ``b`` can differ. 
 dtype(numpy.dtype): Data-type to convert ``a`` and ``b`` to if given. 
 .. seealso:: :func:`numpy.testing.assert_array_max_ulp`'"
"@login_required 
 @ensure_csrf_cookie 
 def assets_handler(request, course_key_string=None, asset_key_string=None): 
    course_key = CourseKey.from_string(course_key_string) 
   if (not has_course_author_access(request.user, course_key)): 
      raise PermissionDenied() 
   response_format = (request.GET.get('format') or request.POST.get('format') or 'html') 
   if ((response_format == 'json') or ('application/json' in request.META.get('HTTP_ACCEPT', 'application/json'))): 
      if (request.method == 'GET'): 
         return _assets_json(request, course_key) 
      else: 
         asset_key = (AssetKey.from_string(asset_key_string) if asset_key_string else None) 
         return _update_asset(request, course_key, asset_key) 
   elif (request.method == 'GET'): 
      return _asset_index(request, course_key) 
   else: 
      return HttpResponseNotFound()"," 'Handles requests for assets. 
 If the request is for an asset, the asset is returned. 
 If the request is for the asset index, the asset index is returned. 
 If the request is for an asset\'s JSON representation, the asset\'s JSON 
 representation is returned. 
 If the request is for an asset\'s JSON representation, the asset\'s JSON 
 representation is returned. 
 If the request is for an asset\'s JSON representation, the asset\'s JSON 
 representation is returned. 
 :param request: The current HTTP request. 
 :param course_key_string: The string representation of the course key. 
 :param asset_key_string: The string representation of the asset key. 
 :return: The response to send to the client. 
 :rtype: HttpResponse'","'The restful handler for assets. 
 It allows retrieval of all the assets (as an HTML page), as well as uploading new assets, 
 deleting assets, and changing the ""locked"" state of an asset. 
 GET 
 html: return an html page which will show all course assets. Note that only the asset container 
 is returned and that the actual assets are filled in with a client-side request. 
 json: returns a page of assets. The following parameters are supported: 
 page: the desired page of results (defaults to 0) 
 page_size: the number of items per page (defaults to 50) 
 sort: the asset field to sort by (defaults to ""date_added"") 
 direction: the sort direction (defaults to ""descending"") 
 POST 
 json: create (or update?) an asset. The only updating that can be done is changing the lock state. 
 PUT 
 json: update the locked state of an asset 
 DELETE 
 json: delete an asset'"
"def savgol_coeffs(window_length, polyorder, deriv=0, delta=1.0, pos=None, use='conv'): 
    if (polyorder >= window_length): 
      raise ValueError('polyorder   must   be   less   than   window_length.') 
   (halflen, rem) = divmod(window_length, 2) 
   if (rem == 0): 
      raise ValueError('window_length   must   be   odd.') 
   if (pos is None): 
      pos = halflen 
   if (not (0 <= pos < window_length)): 
      raise ValueError('pos   must   be   nonnegative   and   less   than   window_length.') 
   if (use not in ['conv', 'dot']): 
      raise ValueError(""`use`   must   be   'conv'   or   'dot'"") 
   x = np.arange((- pos), (window_length - pos), dtype=float) 
   if (use == 'conv'): 
      x = x[::(-1)] 
   order = np.arange((polyorder + 1)).reshape((-1), 1) 
   A = (x ** order) 
   y = np.zeros((polyorder + 1)) 
   y[deriv] = (factorial(deriv) / (delta ** deriv)) 
   (coeffs, _, _, _) = lstsq(A, y) 
   return coeffs"," 'Savitzky-Golay filter coefficients 
 Parameters 
 window_length : int 
 The length of the sliding window. 
 polyorder : int 
 The order of the polynomial. 
 deriv : int 
 The order of the derivative. 
 delta : float 
 The width of the Gaussian smoothing kernel. 
 pos : int 
 The position of the central point of the smoothing kernel. 
 use : \'conv\' or \'dot\' 
 The method of convolution: \'conv\' or \'dot\'. 
 Returns 
 coeffs : ndarray 
 The coefficients of the polynomial. 
 Examples 
 >>> from scipy.signal import savgol_coeffs 
 >>> savgol_coeffs(21, 11) 
 array([ 2.50000000e-01,  6.28318530e-01,  1.15200000e-01,  1.64062500e-01,  2.1","'Compute the coefficients for a 1-d Savitzky-Golay FIR filter. 
 Parameters 
 window_length : int 
 The length of the filter window (i.e. the number of coefficients). 
 `window_length` must be an odd positive integer. 
 polyorder : int 
 The order of the polynomial used to fit the samples. 
 `polyorder` must be less than `window_length`. 
 deriv : int, optional 
 The order of the derivative to compute.  This must be a 
 nonnegative integer.  The default is 0, which means to filter 
 the data without differentiating. 
 delta : float, optional 
 The spacing of the samples to which the filter will be applied. 
 This is only used if deriv > 0. 
 pos : int or None, optional 
 If pos is not None, it specifies evaluation position within the 
 window.  The default is the middle of the window. 
 use : str, optional 
 Either \'conv\' or \'dot\'.  This argument chooses the order of the 
 coefficients.  The default is \'conv\', which means that the 
 coefficients are ordered to be used in a convolution.  With 
 use=\'dot\', the order is reversed, so the filter is applied by 
 dotting the coefficients with the data set. 
 Returns 
 coeffs : 1-d ndarray 
 The filter coefficients. 
 References 
 A. Savitzky, M. J. E. Golay, Smoothing and Differentiation of Data by 
 Simplified Least Squares Procedures. Analytical Chemistry, 1964, 36 (8), 
 pp 1627-1639. 
 See Also 
 savgol_filter 
 Notes 
 .. versionadded:: 0.14.0 
 Examples 
 >>> from scipy.signal import savgol_coeffs 
 >>> savgol_coeffs(5, 2) 
 array([-0.08571429,  0.34285714,  0.48571429,  0.34285714, -0.08571429]) 
 >>> savgol_coeffs(5, 2, deriv=1) 
 array([  2.00000000e-01,   1.00000000e-01,   2.00607895e-16, 
 -1.00000000e-01,  -2.00000000e-01]) 
 Note that use=\'dot\' simply reverses the coefficients. 
 >>> savgol_coeffs(5, 2, pos=3) 
 array([ 0.25714286,  0.37142857,  0.34285714,  0.17142857, -0.14285714]) 
 >>> savgol_coeffs(5, 2, pos=3, use=\'dot\') 
 array([-0.14285714,  0.17142857,  0.34285714,  0.37142857,  0.25714286]) 
 `x` contains data from the parabola x = t**2, sampled at 
 t = -1, 0, 1, 2, 3.  `c` holds the coefficients that will compute the 
 derivative at the last position.  When dotted with `x` the result should 
 be 6. 
 >>> x = np.array([1, 0, 1, 4, 9]) 
 >>> c = savgol_coeffs(5, 2, pos=4, deriv=1, use=\'dot\') 
 >>> c.dot(x) 
 6.0000000000000018'"
"def clamav(registry, xml_parent, data): 
    clamav = XML.SubElement(xml_parent, 'org.jenkinsci.plugins.clamav.ClamAvRecorder') 
   clamav.set('plugin', 'clamav') 
   mappings = [('includes', 'includes', ''), ('excludes', 'excludes', '')] 
   helpers.convert_mapping_to_xml(clamav, data, mappings, fail_required=True)"," 'Create a ClamAV plugin. 
 @param registry: Jenkins Registry 
 @param xml_parent: parent XML element 
 @param data: plugin data 
 @return: ClamAV plugin XML element'","'yaml: clamav 
 Check files with ClamAV, an open source antivirus engine. 
 Requires the Jenkins :jenkins-wiki:`ClamAV Plugin <ClamAV+Plugin>`. 
 :arg str includes: Comma seperated list of files that should be scanned. 
 Must be set for ClamAV to check for artifacts. (default \'\') 
 :arg str excludes: Comma seperated list of files that should be ignored 
 (default \'\') 
 Full Example: 
 .. literalinclude:: /../../tests/publishers/fixtures/clamav-full.yaml 
 :language: yaml 
 Minimal Example: 
 .. literalinclude:: /../../tests/publishers/fixtures/clamav-minimal.yaml 
 :language: yaml'"
"def __virtual__(): 
    if salt.utils.which('nft'): 
      return 'nftables' 
   return (False, 'The   nftables   execution   module   failed   to   load:   nftables   is   not   installed.')", 'Check if nftables is available','Only load the module if nftables is installed'
"def tree_support(master, subsampled_tree): 
    master_tipnames = set(master.getTipNames()) 
   subsampled_tree_trimmed = copy.deepcopy(subsampled_tree) 
   def delete_test(node): 
      if (not node.isTip()): 
         return False 
      else: 
         return (node.Name not in master_tipnames) 
   subsampled_tree_trimmed.removeDeleted(delete_test) 
   subsampled_tree_trimmed.prune() 
   subsampled_tree_nodes_names = [] 
   for node in subsampled_tree_trimmed.iterNontips(include_self=True): 
      subsampled_tree_nodes_names.append(node.getTipNames()) 
   subsampled_tree_nodes_names = map(set, subsampled_tree_nodes_names) 
   for master_node in master.iterNontips(include_self=True): 
      if (set(master_node.getTipNames()) in subsampled_tree_nodes_names): 
         try: 
            master_node.bootstrap_support += 1 
         except AttributeError: 
            master_node.bootstrap_support = 1"," 'Calculate bootstrap support for each node in a subsampled tree 
 :param master: A PhyloBayes object 
 :param subsampled_tree: A subsampled tree 
 :return: A dictionary with the bootstrap support for each node in the 
 subsampled tree'","'compares master tree to subsampled_tree, modifies master in place 
 this calculates bootstrap support of each nontip node in the master tree 
 a given master_tree_node is supported if there exists a node in subsampled 
 tree where sub_tree_node.tips == master_tree_node.tips (by name) 
 each subsampled tree is first modified to remove tips and branches leading 
 to them if the tip isn\'t in the master tree 
 not specific to bootstrap, does node support for trees generated in any 
 manner (e.g.: jackknifing) 
 master is modified to have node.bootstrap_support incremented by 1 if 
 subsampled tree has support for that node'"
"def filter_factory(global_conf, **local_conf): 
    conf = global_conf.copy() 
   conf.update(local_conf) 
   register_swift_info('formpost') 
   return (lambda app: FormPost(app, conf))", 'Decorator to filter incoming requests using the FormPost class.','Returns the WSGI filter for use with paste.deploy.'
"def dlcs_api_request(path, params='', user='', passwd='', throttle=True): 
    if throttle: 
      Waiter() 
   if params: 
      url = ('%s/%s?%s' % (DLCS_API, path, urllib.urlencode(dict0(params)))) 
   else: 
      url = ('%s/%s' % (DLCS_API, path)) 
   if DEBUG: 
      print >>sys.stderr, ('dlcs_api_request:   %s' % url) 
   try: 
      return http_auth_request(url, DLCS_API_HOST, user, passwd, USER_AGENT) 
   except DefaultErrorHandler as e: 
      print >>sys.stderr, ('%s' % e)"," 'Send an API request to the DLCS. 
 :param path: 
 :param params: 
 :param user: 
 :param passwd: 
 :param throttle: 
 :return: 
 :rtype: 
 :raises: 
 :version: 
 :since: 0.0.0'","'Retrieve/query a path within the del.icio.us API. 
 This implements a minimum interval between calls to avoid 
 throttling. [#]_ Use param \'throttle\' to turn this behaviour off. 
 @todo: back off on 503\'s (HTTPError, URLError? @todo: testing). 
 Returned XML does not always correspond with given del.icio.us examples 
 @todo: (cf. help/api/... and post\'s attributes) 
 .. [#] http://del.icio.us/help/api/'"
"def conserve_mpmath_dps(func): 
    import functools 
   import mpmath 
   def func_wrapper(*args, **kwargs): 
      dps = mpmath.mp.dps 
      try: 
         return func(*args, **kwargs) 
      finally: 
         mpmath.mp.dps = dps 
   func_wrapper = functools.update_wrapper(func_wrapper, func) 
   return func_wrapper"," 'Decorator to conserve mpmath dps after function calls. 
 This is necessary because mpmath dps are not restored 
 after function calls. 
 Examples 
 >>> from sympy import Symbol, simplify, sin, cos, tan, exp, log, pi 
 >>> from sympy.abc import x 
 >>> func = sin 
 >>> @conserve_mpmath_dps 
 ... def f(x): 
 ...     return func(x) 
 >>> f(x) 
 sin(x) 
 >>> f(x) 
 sin(x) 
 >>> f(x) 
 sin(x) 
 >>> f(x) 
 sin(x) 
 >>> f(x) 
 sin(x) 
 >>> f(x) 
 sin(x) 
 >>> f(x) 
 sin(x) 
 >>> f(x) 
 sin(x) 
 >>> f(x) 
 sin(x) 
 >>> f(x) 
 sin(x) 
 >>> f(x) 
 sin(x) 
 >>> f(x) ","'After the function finishes, resets the value of mpmath.mp.dps to 
 the value it had before the function was run.'"
"def pathMatchPatterns(path, repos): 
    for repo in repos: 
      if fnmatch(path, repo): 
         return True 
   return False"," 'Check if the given path matches any of the patterns in the given repos. 
 :param path: path to check 
 :param repos: list of repo patterns 
 :return: True if the path matches any of the patterns, False otherwise'","'Check existence of given path against list of path patterns 
 The pattern definition is the as fnmatch.fnmatch.'"
"def minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None): 
    x0 = np.asarray(x0) 
   if (x0.dtype.kind in np.typecodes['AllInteger']): 
      x0 = np.asarray(x0, dtype=float) 
   if (not isinstance(args, tuple)): 
      args = (args,) 
   if (method is None): 
      if constraints: 
         method = 'SLSQP' 
      elif (bounds is not None): 
         method = 'L-BFGS-B' 
      else: 
         method = 'BFGS' 
   if callable(method): 
      meth = '_custom' 
   else: 
      meth = method.lower() 
   if (options is None): 
      options = {} 
   if ((meth in ['nelder-mead', 'powell', 'cobyla']) and bool(jac)): 
      warn(('Method   %s   does   not   use   gradient   information   (jac).' % method), RuntimeWarning) 
   if ((meth not in ('newton-cg', 'dogleg', 'trust-ncg', '_custom')) and (hess is not None)): 
      warn(('Method   %s   does   not   use   Hessian   information   (hess).' % method), RuntimeWarning) 
   if ((meth not in ('newton-cg', 'dogleg', 'trust-ncg', '_custom')) and (hessp is not None)): 
      warn(('Method   %s   does   not   use   Hessian-vector   product   information   (hessp).' % method), RuntimeWarning) 
   if ((meth in ['nelder-mead', 'powell', 'cg', 'bfgs', 'newton-cg', 'dogleg', 'trust-ncg']) and ((bounds is not None) or np.any(constraints))): 
      warn(('Method   %s   cannot   handle   constraints   nor   bounds.' % method), RuntimeWarning) 
   if ((meth in ['l-bfgs-b', 'tnc']) and np.any(constraints)): 
      warn(('Method   %s   cannot   handle   constraints.' % method), RuntimeWarning) 
   if ((meth == 'cobyla') and (bounds is not None)): 
      warn(('Method   %s   cannot   handle   bounds.' % method), RuntimeWarning) 
   if ((meth in ['cobyla']) and (callback is not None)): 
      warn(('Method   %s   does   not   support   callback.' % method), RuntimeWarning) 
   if ((meth in ['l-bfgs-b', 'tnc', 'cobyla', 'slsqp']) and options.get('return_all', False)): 
      warn(('Method   %s   does   not   support   the   return_all   option.' % method), RuntimeWarning) 
   if (not callable(jac)): 
      if bool(jac): 
         fun = MemoizeJac(fun) 
         jac = fun.derivative 
      else: 
         jac = None 
   if (tol is not None): 
      options = dict(options) 
      if (meth == 'nelder-mead'): 
         options.setdefault('xatol', tol) 
         options.setdefault('fatol', tol) 
      if (meth in ['newton-cg', 'powell', 'tnc']): 
         options.setdefault('xtol', tol) 
      if (meth in ['powell', 'l-bfgs-b', 'tnc', 'slsqp']): 
         options.setdefault('ftol', tol) 
      if (meth in ['bfgs', 'cg', 'l-bfgs-b', 'tnc', 'dogleg', 'trust-ncg']): 
         options.setdefault('gtol', tol) 
      if (meth in ['cobyla', '_custom']): 
         options.setdefault('tol', tol) 
   if (meth == '_custom'): 
      return method(fun, x0, args=args, jac=jac, hess=hess, hessp=hessp, bounds=bounds, constraints=constraints, callback=callback, **options) 
   elif (meth == 'nelder-mead'): 
      return _minimize_neldermead(fun, x0, args, callback, **options) 
   elif (meth == 'powell'): 
      return _minimize_powell(fun, x0, args, callback, **options) 
   elif (meth == 'cg'): 
      return _minimize_cg(fun, x0, args, jac, callback, **options) 
   elif (meth == 'bfgs'): 
      return _minimize_bfgs(fun, x0, args, jac, callback, **options) 
   elif (meth == 'newton-cg'): 
      return _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback, **options) 
   elif (meth == 'l-bfgs-b'): 
      return _minimize_lbfgsb(fun, x0, args, jac, bounds, callback=callback, **options) 
   elif (meth == 'tnc'): 
      return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback, **options) 
   elif (meth == 'cobyla'): 
      return _minimize_cobyla(fun, x0, args, constraints, **options) 
   elif (meth == 'slsqp'): 
      return _minimize_slsqp(fun, x0, args, jac, bounds, constraints, callback=callback, **options) 
   elif (meth == 'dogleg'): 
      return _minimize_dogleg(fun, x0, args, jac, hess, callback=callback, **options) 
   elif (meth == 'trust-ncg'): 
      return _minimize_trust_ncg(fun, x0, args, jac, hess, hessp, callback=callback, **options) 
   else: 
      raise ValueError(('Unknown   solver   %s' % method))"," 'Minimize a function over a given domain. 
 This function calls the appropriate minimization method for the given 
 method name. 
 Parameters 
 fun : callable 
 The objective function. 
 x0 : array-like 
 Initial point. 
 args : tuple of array-like 
 Additional arguments for the objective function. 
 method : string, optional 
 The method to use for minimization. 
 jac : callable, optional 
 The Jacobian of the objective function. 
 hess : callable, optional 
 The Hessian of the objective function. 
 hessp : callable, optional 
 The Hessian-vector product of the objective function. 
 bounds : array-like, optional 
 The bounds of the search space. 
 constraints : array-like, optional 
 Constraints on the search space. 
 tol : float, optional 
 The tolerance for the convergence test. 
 callback : callable, optional 
 A callback function to be called at various points during the 
 minimization. 
 options : dict, optional 
 A dictionary of options for the minimization method","'Minimization of scalar function of one or more variables. 
 In general, the optimization problems are of the form:: 
 minimize f(x) subject to 
 g_i(x) >= 0,  i = 1,...,m 
 h_j(x)  = 0,  j = 1,...,p 
 where x is a vector of one or more variables. 
 ``g_i(x)`` are the inequality constraints. 
 ``h_j(x)`` are the equality constrains. 
 Optionally, the lower and upper bounds for each element in x can also be 
 specified using the `bounds` argument. 
 Parameters 
 fun : callable 
 Objective function. 
 x0 : ndarray 
 Initial guess. 
 args : tuple, optional 
 Extra arguments passed to the objective function and its 
 derivatives (Jacobian, Hessian). 
 method : str or callable, optional 
 Type of solver.  Should be one of 
 - \'Nelder-Mead\' :ref:`(see here) <optimize.minimize-neldermead>` 
 - \'Powell\'      :ref:`(see here) <optimize.minimize-powell>` 
 - \'CG\'          :ref:`(see here) <optimize.minimize-cg>` 
 - \'BFGS\'        :ref:`(see here) <optimize.minimize-bfgs>` 
 - \'Newton-CG\'   :ref:`(see here) <optimize.minimize-newtoncg>` 
 - \'L-BFGS-B\'    :ref:`(see here) <optimize.minimize-lbfgsb>` 
 - \'TNC\'         :ref:`(see here) <optimize.minimize-tnc>` 
 - \'COBYLA\'      :ref:`(see here) <optimize.minimize-cobyla>` 
 - \'SLSQP\'       :ref:`(see here) <optimize.minimize-slsqp>` 
 - \'dogleg\'      :ref:`(see here) <optimize.minimize-dogleg>` 
 - \'trust-ncg\'   :ref:`(see here) <optimize.minimize-trustncg>` 
 - custom - a callable object (added in version 0.14.0), 
 see below for description. 
 If not given, chosen to be one of ``BFGS``, ``L-BFGS-B``, ``SLSQP``, 
 depending if the problem has constraints or bounds. 
 jac : bool or callable, optional 
 Jacobian (gradient) of objective function. Only for CG, BFGS, 
 Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg. 
 If `jac` is a Boolean and is True, `fun` is assumed to return the 
 gradient along with the objective function. If False, the 
 gradient will be estimated numerically. 
 `jac` can also be a callable returning the gradient of the 
 objective. In this case, it must accept the same arguments as `fun`. 
 hess, hessp : callable, optional 
 Hessian (matrix of second-order derivatives) of objective function or 
 Hessian of objective function times an arbitrary vector p.  Only for 
 Newton-CG, dogleg, trust-ncg. 
 Only one of `hessp` or `hess` needs to be given.  If `hess` is 
 provided, then `hessp` will be ignored.  If neither `hess` nor 
 `hessp` is provided, then the Hessian product will be approximated 
 using finite differences on `jac`. `hessp` must compute the Hessian 
 times an arbitrary vector. 
 bounds : sequence, optional 
 Bounds for variables (only for L-BFGS-B, TNC and SLSQP). 
 ``(min, max)`` pairs for each element in ``x``, defining 
 the bounds on that parameter. Use None for one of ``min`` or 
 ``max`` when there is no bound in that direction. 
 constraints : dict or sequence of dict, optional 
 Constraints definition (only for COBYLA and SLSQP). 
 Each constraint is defined in a dictionary with fields: 
 type : str 
 Constraint type: \'eq\' for equality, \'ineq\' for inequality. 
 fun : callable 
 The function defining the constraint. 
 jac : callable, optional 
 The Jacobian of `fun` (only for SLSQP). 
 args : sequence, optional 
 Extra arguments to be passed to the function and Jacobian. 
 Equality constraint means that the constraint function result is to 
 be zero whereas inequality means that it is to be non-negative. 
 Note that COBYLA only supports inequality constraints. 
 tol : float, optional 
 Tolerance for termination. For detailed control, use solver-specific 
 options. 
 options : dict, optional 
 A dictionary of solver options. All methods accept the following 
 generic options: 
 maxiter : int 
 Maximum number of iterations to perform. 
 disp : bool 
 Set to True to print convergence messages. 
 For method-specific options, see :func:`show_options()`. 
 callback : callable, optional 
 Called after each iteration, as ``callback(xk)``, where ``xk`` is the 
 current parameter vector. 
 Returns 
 res : OptimizeResult 
 The optimization result represented as a ``OptimizeResult`` object. 
 Important attributes are: ``x`` the solution array, ``success`` a 
 Boolean flag indicating if the optimizer exited successfully and 
 ``message`` which describes the cause of the termination. See 
 `OptimizeResult` for a description of other attributes. 
 See also 
 minimize_scalar : Interface to minimization algorithms for scalar 
 univariate functions 
 show_options : Additional options accepted by the solvers 
 Notes 
 This section describes the available solvers that can be selected by the 
 \'method\' parameter. The default method is *BFGS*. 
 **Unconstrained minimization** 
 Method :ref:`Nelder-Mead <optimize.minimize-neldermead>` uses the 
 Simplex algorithm [1]_, [2]_. This algorithm is robust in many 
 applications. However, if numerical computation of derivative can be 
 trusted, other algorithms using the first and/or second derivatives 
 information might be preferred for their better performance in 
 general. 
 Method :ref:`Powell <optimize.minimize-powell>` is a modification 
 of Powell\'s method [3]_, [4]_ which is a conjugate direction 
 method. It performs sequential one-dimensional minimizations along 
 each vector of the directions set (`direc` field in `options` and 
 `info`), which is updated at each iteration of the main 
 minimization loop. The function need not be differentiable, and no 
 derivatives are taken. 
 Method :ref:`CG <optimize.minimize-cg>` uses a nonlinear conjugate 
 gradient algorithm by Polak and Ribiere, a variant of the 
 Fletcher-Reeves method described in [5]_ pp.  120-122. Only the 
 first derivatives are used. 
 Method :ref:`BFGS <optimize.minimize-bfgs>` uses the quasi-Newton 
 method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS) [5]_ 
 pp. 136. It uses the first derivatives only. BFGS has proven good 
 performance even for non-smooth optimizations. This method also 
 returns an approximation of the Hessian inverse, stored as 
 `hess_inv` in the OptimizeResult object. 
 Method :ref:`Newton-CG <optimize.minimize-newtoncg>` uses a 
 Newton-CG algorithm [5]_ pp. 168 (also known as the truncated 
 Newton method). It uses a CG method to the compute the search 
 direction. See also *TNC* method for a box-constrained 
 minimization with a similar algorithm. 
 Method :ref:`dogleg <optimize.minimize-dogleg>` uses the dog-leg 
 trust-region algorithm [5]_ for unconstrained minimization. This 
 algorithm requires the gradient and Hessian; furthermore the 
 Hessian is required to be positive definite. 
 Method :ref:`trust-ncg <optimize.minimize-trustncg>` uses the 
 Newton conjugate gradient trust-region algorithm [5]_ for 
 unconstrained minimization. This algorithm requires the gradient 
 and either the Hessian or a function that computes the product of 
 the Hessian with a given vector. 
 **Constrained minimization** 
 Method :ref:`L-BFGS-B <optimize.minimize-lbfgsb>` uses the L-BFGS-B 
 algorithm [6]_, [7]_ for bound constrained minimization. 
 Method :ref:`TNC <optimize.minimize-tnc>` uses a truncated Newton 
 algorithm [5]_, [8]_ to minimize a function with variables subject 
 to bounds. This algorithm uses gradient information; it is also 
 called Newton Conjugate-Gradient. It differs from the *Newton-CG* 
 method described above as it wraps a C implementation and allows 
 each variable to be given upper and lower bounds. 
 Method :ref:`COBYLA <optimize.minimize-cobyla>` uses the 
 Constrained Optimization BY Linear Approximation (COBYLA) method 
 [9]_, [10]_, [11]_. The algorithm is based on linear 
 approximations to the objective function and each constraint. The 
 method wraps a FORTRAN implementation of the algorithm. The 
 constraints functions \'fun\' may return either a single number 
 or an array or list of numbers. 
 Method :ref:`SLSQP <optimize.minimize-slsqp>` uses Sequential 
 Least SQuares Programming to minimize a function of several 
 variables with any combination of bounds, equality and inequality 
 constraints. The method wraps the SLSQP Optimization subroutine 
 originally implemented by Dieter Kraft [12]_. Note that the 
 wrapper handles infinite values in bounds by converting them into 
 large floating values. 
 **Custom minimizers** 
 It may be useful to pass a custom minimization method, for example 
 when using a frontend to this method such as `scipy.optimize.basinhopping` 
 or a different library.  You can simply pass a callable as the ``method`` 
 parameter. 
 The callable is called as ``method(fun, x0, args, **kwargs, **options)`` 
 where ``kwargs`` corresponds to any other parameters passed to `minimize` 
 (such as `callback`, `hess`, etc.), except the `options` dict, which has 
 its contents also passed as `method` parameters pair by pair.  Also, if 
 `jac` has been passed as a bool type, `jac` and `fun` are mangled so that 
 `fun` returns just the function values and `jac` is converted to a function 
 returning the Jacobian.  The method shall return an ``OptimizeResult`` 
 object. 
 The provided `method` callable must be able to accept (and possibly ignore) 
 arbitrary parameters; the set of parameters accepted by `minimize` may 
 expand in future versions and then these parameters will be passed to 
 the method.  You can find an example in the scipy.optimize tutorial. 
 .. versionadded:: 0.11.0 
 References 
 .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function 
 Minimization. The Computer Journal 7: 308-13. 
 .. [2] Wright M H. 1996. Direct search methods: Once scorned, now 
 respectable, in Numerical Analysis 1995: Proceedings of the 1995 
 Dundee Biennial Conference in Numerical Analysis (Eds. D F 
 Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK. 
 191-208. 
 .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of 
 a function of several variables without calculating derivatives. The 
 Computer Journal 7: 155-162. 
 .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery. 
 Numerical Recipes (any edition), Cambridge University Press. 
 .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization. 
 Springer New York. 
 .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory 
 Algorithm for Bound Constrained Optimization. SIAM Journal on 
 Scientific and Statistical Computing 16 (5): 1190-1208. 
 .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm 
 778: L-BFGS-B, FORTRAN routines for large scale bound constrained 
 optimization. ACM Transactions on Mathematical Software 23 (4): 
 550-560. 
 .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method. 
 1984. SIAM Journal of Numerical Analysis 21: 770-778. 
 .. [9] Powell, M J D. A direct search optimization method that models 
 the objective and constraint functions by linear interpolation. 
 1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez 
 and J-P Hennart, Kluwer Academic (Dordrecht), 51-67. 
 .. [10] Powell M J D. Direct search algorithms for optimization 
 calculations. 1998. Acta Numerica 7: 287-336. 
 .. [11] Powell M J D. A view of algorithms for optimization without 
 derivatives. 2007.Cambridge University Technical Report DAMTP 
 2007/NA03 
 .. [12] Kraft, D. A software package for sequential quadratic 
 programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace 
 Center -- Institute for Flight Mechanics, Koln, Germany. 
 Examples 
 Let us consider the problem of minimizing the Rosenbrock function. This 
 function (and its respective derivatives) is implemented in `rosen` 
 (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`. 
 >>> from scipy.optimize import minimize, rosen, rosen_der 
 A simple application of the *Nelder-Mead* method is: 
 >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2] 
 >>> res = minimize(rosen, x0, method=\'Nelder-Mead\', tol=1e-6) 
 >>> res.x 
 array([ 1.,  1.,  1.,  1.,  1.]) 
 Now using the *BFGS* algorithm, using the first derivative and a few 
 options: 
 >>> res = minimize(rosen, x0, method=\'BFGS\', jac=rosen_der, 
 ...                options={\'gtol\': 1e-6, \'disp\': True}) 
 Optimization terminated successfully. 
 Current function value: 0.000000 
 Iterations: 26 
 Function evaluations: 31 
 Gradient evaluations: 31 
 >>> res.x 
 array([ 1.,  1.,  1.,  1.,  1.]) 
 >>> print(res.message) 
 Optimization terminated successfully. 
 >>> res.hess_inv 
 array([[ 0.00749589,  0.01255155,  0.02396251,  0.04750988,  0.09495377],  # may vary 
 [ 0.01255155,  0.02510441,  0.04794055,  0.09502834,  0.18996269], 
 [ 0.02396251,  0.04794055,  0.09631614,  0.19092151,  0.38165151], 
 [ 0.04750988,  0.09502834,  0.19092151,  0.38341252,  0.7664427 ], 
 [ 0.09495377,  0.18996269,  0.38165151,  0.7664427,   1.53713523]]) 
 Next, consider a minimization problem with several constraints (namely 
 Example 16.4 from [5]_). The objective function is: 
 >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2 
 There are three constraints defined as: 
 >>> cons = ({\'type\': \'ineq\', \'fun\': lambda x:  x[0] - 2 * x[1] + 2}, 
 ...         {\'type\': \'ineq\', \'fun\': lambda x: -x[0] - 2 * x[1] + 6}, 
 ...         {\'type\': \'ineq\', \'fun\': lambda x: -x[0] + 2 * x[1] + 2}) 
 And variables must be positive, hence the following bounds: 
 >>> bnds = ((0, None), (0, None)) 
 The optimization problem is solved using the SLSQP method as: 
 >>> res = minimize(fun, (2, 0), method=\'SLSQP\', bounds=bnds, 
 ...                constraints=cons) 
 It should converge to the theoretical solution (1.4 ,1.7).'"
"@domain_constructor(loss_target=(-2)) 
 def distractor(): 
    x = hp.uniform('x', (-15), 15) 
   f1 = old_div(1.0, (1.0 + scope.exp((- x)))) 
   f2 = (2 * scope.exp((- ((x + 10) ** 2)))) 
   return {'loss': ((- f1) - f2), 'status': base.STATUS_OK}"," 'A simple example of a loss function. 
 This loss function is the negative log-likelihood of a Bernoulli 
 distribution, which is a discrete distribution. 
 Parameters 
 x : float 
 A single parameter of the Bernoulli distribution. 
 Returns 
 loss : float 
 The negative log-likelihood of the Bernoulli distribution. 
 status : int 
 A status code indicating whether the loss function was successfully 
 constructed. 
 Notes 
 This function is not recommended for use in production code. 
 It is provided as an example of a loss function. 
 See Also 
 loss_target : A convenience function that automatically constructs 
 a loss function for a given target value. 
 Examples 
 >>> from theano import function, gradient, logsumexp, log, rng, tensor, 
 ... import numpy as np 
 >>> rng.seed(1234) 
 >>> x = rng.normal(size=100) 
 >>> y = rng.bernoulli(x) 
 >>> f = function([x], loss_target(y)) 
 >>> y_true =","'This is a nasty function: it has a max in a spike near -10, and a long 
 asymptote that is easy to find, but guides hill-climbing approaches away 
 from the true max. 
 The second peak is at x=-10. 
 The prior mean is 0.'"
"def is_inside_except(node): 
    current = node 
   while (current and (not isinstance(current.parent, astroid.ExceptHandler))): 
      current = current.parent 
   return (current and (current is current.parent.name))", 'Returns true if the node is inside an except handler.','Returns true if node is inside the name of an except handler.'
"def dump(obj, fp, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, cls=None, indent=None, separators=None, encoding='utf-8', default=None, use_decimal=True, **kw): 
    if ((not skipkeys) and ensure_ascii and check_circular and allow_nan and (cls is None) and (indent is None) and (separators is None) and (encoding == 'utf-8') and (default is None) and (not kw)): 
      iterable = _default_encoder.iterencode(obj) 
   else: 
      if (cls is None): 
         cls = JSONEncoder 
      iterable = cls(skipkeys=skipkeys, ensure_ascii=ensure_ascii, check_circular=check_circular, allow_nan=allow_nan, indent=indent, separators=separators, encoding=encoding, default=default, use_decimal=use_decimal, **kw).iterencode(obj) 
   for chunk in iterable: 
      fp.write(chunk)"," 'Dump the JSON representation of an object to a file. 
 This is the same as :meth:`~json.dump` except that the output is written 
 to a file instead of being returned as a string. 
 :param obj: An object to be dumped. 
 :param fp: A file-like object to dump to. 
 :param skipkeys: If ``True``, the :data:`~json.JSONEncoder.use_default` 
 flag is set to ``False``. 
 :param ensure_ascii: If ``True``, the :data:`~json.JSONEncoder.ensure_ascii` 
 flag is set to ``True``. 
 :param check_circular: If ``True``, the :data:`~json.JSONEncoder.check_circular` 
 flag is set to ``True``. 
 :param allow_nan: If ``True``, the :data:`~json.JSONEncoder.allow_nan` 
 flag is set to ``True``. 
 :param indent: The amount of whitespace to use when indenting the JSON. 
 :param separators","'Serialize ``obj`` as a JSON formatted stream to ``fp`` (a 
 ``.write()``-supporting file-like object). 
 If ``skipkeys`` is true then ``dict`` keys that are not basic types 
 (``str``, ``unicode``, ``int``, ``long``, ``float``, ``bool``, ``None``) 
 will be skipped instead of raising a ``TypeError``. 
 If ``ensure_ascii`` is false, then the some chunks written to ``fp`` 
 may be ``unicode`` instances, subject to normal Python ``str`` to 
 ``unicode`` coercion rules. Unless ``fp.write()`` explicitly 
 understands ``unicode`` (as in ``codecs.getwriter()``) this is likely 
 to cause an error. 
 If ``check_circular`` is false, then the circular reference check 
 for container types will be skipped and a circular reference will 
 result in an ``OverflowError`` (or worse). 
 If ``allow_nan`` is false, then it will be a ``ValueError`` to 
 serialize out of range ``float`` values (``nan``, ``inf``, ``-inf``) 
 in strict compliance of the JSON specification, instead of using the 
 JavaScript equivalents (``NaN``, ``Infinity``, ``-Infinity``). 
 If *indent* is a string, then JSON array elements and object members 
 will be pretty-printed with a newline followed by that string repeated 
 for each level of nesting. ``None`` (the default) selects the most compact 
 representation without any newlines. For backwards compatibility with 
 versions of simplejson earlier than 2.1.0, an integer is also accepted 
 and is converted to a string with that many spaces. 
 If ``separators`` is an ``(item_separator, dict_separator)`` tuple 
 then it will be used instead of the default ``(\', \', \': \')`` separators. 
 ``(\',\', \':\')`` is the most compact JSON representation. 
 ``encoding`` is the character encoding for str instances, default is UTF-8. 
 ``default(obj)`` is a function that should return a serializable version 
 of obj or raise TypeError. The default simply raises TypeError. 
 If *use_decimal* is true (default: ``True``) then decimal.Decimal 
 will be natively serialized to JSON with full precision. 
 To use a custom ``JSONEncoder`` subclass (e.g. one that overrides the 
 ``.default()`` method to serialize additional types), specify it with 
 the ``cls`` kwarg.'"
"def require(source_module, target_module, all_macros=False, assignments={}, prefix=''): 
    seen_names = set() 
   if prefix: 
      prefix += '.' 
   for d in (_hy_macros, _hy_reader): 
      for (name, macro) in d[source_module].items(): 
         seen_names.add(name) 
         if all_macros: 
            d[target_module][(prefix + name)] = macro 
         elif (name in assignments): 
            d[target_module][(prefix + assignments[name])] = macro 
   if (not all_macros): 
      unseen = frozenset(assignments.keys()).difference(seen_names) 
      if unseen: 
         raise ImportError(('cannot   require   names:   ' + repr(list(unseen))))"," 'Require the names in ``assignments`` to be defined in ``target_module``. 
 This is used to require that a module define macros, or to require 
 that a module define a macro with a given name. 
 ``prefix`` is used to distinguish between macros defined in the 
 source and target modules. 
 If ``all_macros`` is ``True``, then all macros defined in the source 
 module are defined in the target module, even if they are not in 
 ``assignments``. 
 If ``all_macros`` is ``False``, then only the macros in ``assignments`` 
 are defined in the target module. 
 If ``all_macros`` is ``False``, then ``assignments`` must contain 
 only the names of macros that are defined in the source module. 
 If ``all_macros`` is ``False``, then ``assignments`` must contain 
 only the names of macros that are defined in the source module.'","'Load macros from `source_module` in the namespace of 
 `target_module`. `assignments` maps old names to new names, but is 
 ignored if `all_macros` is true. If `prefix` is nonempty, it is 
 prepended to the name of each imported macro. (This means you get 
 macros named things like ""mymacromodule.mymacro"", which looks like 
 an attribute of a module, although it\'s actually just a symbol 
 with a period in its name.) 
 This function is called from the `require` special form in the compiler.'"
"def libvlc_media_list_player_get_state(p_mlp): 
    f = (_Cfunctions.get('libvlc_media_list_player_get_state', None) or _Cfunction('libvlc_media_list_player_get_state', ((1,),), None, State, MediaListPlayer)) 
   return f(p_mlp)"," 'Get the current state of the media list player. 
 @param p_mlp: the media list player. 
 @return: the current state of the media list player. 
 @version: LibVLC 2.1.0 or later.'","'Get current libvlc_state of media list player. 
 @param p_mlp: media list player instance. 
 @return: libvlc_state_t for media list player.'"
"def get_c_init(r, name, sub): 
    pre = ('\n            py_%(name)s   =   Py_None;\n            {Py_XINCREF(py_%(name)s);}\n            ' % locals()) 
   return (pre + r.type.c_init(name, sub))", 'Get the C-init function for a type.','Wrapper around c_init that initializes py_name to Py_None.'
"def exp(x): 
    np = import_module('numpy') 
   if isinstance(x, (int, float)): 
      return interval(np.exp(x), np.exp(x)) 
   elif isinstance(x, interval): 
      return interval(np.exp(x.start), np.exp(x.end), is_valid=x.is_valid) 
   else: 
      raise NotImplementedError"," 'Returns the interval representing the exponential function. 
 Parameters 
 x : interval or number 
 The number to which the exponential function should be applied. 
 Returns 
 interval 
 The interval representing the exponential function. 
 Examples 
 >>> from sympy import exp 
 >>> from sympy.abc import x 
 >>> from sympy.abc import i 
 >>> exp(x) 
 interval(1, e**x) 
 >>> exp(i*pi) 
 interval(e**(-pi), e**pi) 
 >>> exp(x) - exp(y) 
 (-x + y)'",'evaluates the exponential of an interval'
"def _getinfos_http_client_authorization(spec): 
    infos = {} 
   fullinfos = {} 
   data = spec.get('fullvalue', spec['value']).split(None, 1) 
   if data[1:]: 
      if (data[0].lower() == 'basic'): 
         try: 
            (infos['username'], infos['password']) = ''.join(data[1].strip()).decode('base64').decode('latin-1').split(':', 1) 
            for field in ['username', 'password']: 
               if (len(infos[field]) > utils.MAXVALLEN): 
                  fullinfos[field] = infos[field] 
                  infos[field] = infos[field][:utils.MAXVALLEN] 
         except Exception: 
            pass 
      elif (data[0].lower() == 'digest'): 
         try: 
            infos = dict(((value.split('=', 1) if ('=' in value) else [value, None]) for value in _split_digest_auth(data[1].strip()))) 
            for (key, value) in infos.items(): 
               if (value.startswith('""') and value.endswith('""')): 
                  infos[key] = value[1:(-1)] 
         except Exception: 
            pass 
   res = {} 
   if infos: 
      res['infos'] = infos 
   if fullinfos: 
      res['fullinfos'] = fullinfos 
   return res"," 'Returns the infos of the HTTP client authorization. 
 :param spec: the spec to be parsed 
 :return: the infos of the HTTP client authorization'","'Extract (for now) the usernames and passwords from Basic 
 authorization headers'"
"def setup(base_path, root_module_name='autotest'): 
    if (root_module_name in sys.modules): 
      return 
   _create_module_and_parents(root_module_name) 
   imp.load_package(root_module_name, base_path) 
   sys.path.insert(0, os.path.join(base_path, 'site-packages'))", 'Loads the autotest module and its dependencies.',"'Setup a library namespace, with the appropriate top root module name. 
 Perform all the necessary setup so that all the packages at 
 \'base_path\' can be imported via ""import root_module_name.package"". 
 :param base_path: Base path for the module. 
 :param root_module_name: Top level name for the module.'"
"def get_messages(request): 
    return getattr(request, '_messages', [])"," 'Returns a list of messages for the given request. 
 If the request has no messages, returns an empty list.'","'Returns the message storage on the request if it exists, otherwise returns 
 an empty list.'"
"def configure_paramiko_logging(): 
    l = logging.getLogger('paramiko') 
   l.setLevel(logging.DEBUG) 
   static.create_sc_config_dirs() 
   lh = logging.handlers.RotatingFileHandler(static.SSH_DEBUG_FILE, maxBytes=1048576, backupCount=2) 
   lh.setLevel(logging.DEBUG) 
   format = (('PID:   %s   ' % str(static.PID)) + '%(levelname)-.3s   [%(asctime)s.%(msecs)03d]   thr=%(_threadid)-3d   %(name)s:   %(message)s') 
   date_format = '%Y%m%d-%H:%M:%S' 
   lh.setFormatter(logging.Formatter(format, date_format)) 
   l.addHandler(lh)"," 'Configure the paramiko logging. 
 This is called in the init function to configure the logging for paramiko. 
 It is called once.'",'Configure ssh to log to a file for debug'
"def fixed_ip_get_by_host(context, host): 
    return IMPL.fixed_ip_get_by_host(context, host)", 'Get fixed IP address for host.','Get fixed IPs by compute host.'
"def check_non_negative(X, whom): 
    X = (X.data if sp.issparse(X) else X) 
   if (X < 0).any(): 
      raise ValueError(('Negative   values   in   data   passed   to   %s' % whom))"," 'Check that data passed to a function is non-negative. 
 Parameters 
 X : array-like 
 Data to be passed to the function. 
 whom : string 
 Name of the function that is being passed the data. 
 Returns 
 X : array-like 
 Data passed to the function, with negative values removed. 
 Raises 
 ValueError 
 If negative values are present in `X`.'","'Check if there is any negative value in an array. 
 Parameters 
 X : array-like or sparse matrix 
 Input data. 
 whom : string 
 Who passed X to this function.'"
"def main(): 
    initLogging(verbose=True) 
   initExperimentPrng() 
   @staticmethod 
   def _mockCreate(*args, **kwargs): 
      kwargs.pop('implementation', None) 
      return CLAClassifierDiff(*args, **kwargs) 
   CLAClassifierFactory.create = _mockCreate 
   runExperiment(sys.argv[1:])", 'Runs the experiment.','Run according to options in sys.argv and diff classifiers.'
"@pytest.mark.cmd 
 @pytest.mark.django_db 
 def test_refresh_scores_reset_user(capfd): 
    call_command('refresh_scores', '--reset', '--user=system') 
   (out, err) = capfd.readouterr() 
   assert ('Scores   for   specified   users   were   reset   to   0.' in out)", 'Test that the command refresh_scores --reset --user=system works.','Set scores to zero for given users.'
"def download_cover(log, title=None, authors=None, identifiers={}, timeout=30): 
    rq = Queue() 
   abort = Event() 
   run_download(log, rq, abort, title=title, authors=authors, identifiers=identifiers, timeout=timeout, get_best_cover=True) 
   results = [] 
   while True: 
      try: 
         results.append(rq.get_nowait()) 
      except Empty: 
         break 
   cp = msprefs[u'cover_priorities'] 
   def keygen(result): 
      (plugin, width, height, fmt, data) = result 
      return (cp.get(plugin.name, 1), (1 / (width * height))) 
   results.sort(key=keygen) 
   return (results[0] if results else None)"," 'Download cover image for a book. 
 :param log: log object 
 :param title: title of the book 
 :param authors: authors of the book 
 :param identifiers: identifiers of the book 
 :param timeout: timeout for download 
 :return: cover image or None'","'Synchronous cover download. Returns the ""best"" cover as per user 
 prefs/cover resolution. 
 Returned cover is a tuple: (plugin, width, height, fmt, data) 
 Returns None if no cover is found.'"
"def _map_to_list(game_map): 
    list_map = game_map.split('\n') 
   return [(character.decode('UTF-8') if isinstance(character, basestring) else character) for character in list_map]", 'Convert a game map into a list of characters.',"'Splits multi line map string into list of rows, treats for UTF-8 encoding. 
 Args: 
 game_map (str): An ASCII map 
 Returns: 
 list (list): The map split into rows'"
"def _indent(elem, level=0): 
    i = ('\n' + (level * '      ')) 
   if len(elem): 
      if ((not elem.text) or (not elem.text.strip())): 
         elem.text = (i + '      ') 
      for e in elem: 
         _indent(e, (level + 1)) 
         if ((not e.tail) or (not e.tail.strip())): 
            e.tail = (i + '      ') 
      if ((not e.tail) or (not e.tail.strip())): 
         e.tail = i 
   elif (level and ((not elem.tail) or (not elem.tail.strip()))): 
      elem.tail = i", 'Indent the element.',"'Add line breaks and indentation to ElementTree in-place. 
 Sources: 
 - http://effbot.org/zone/element-lib.htm#prettyprint 
 - http://infix.se/2007/02/06/gentlemen-indent-your-xml'"
"def setvariable(cursor, mysqlvar, value): 
    query = ('SET   GLOBAL   %s   =   ' % mysql_quote_identifier(mysqlvar, 'vars')) 
   try: 
      cursor.execute((query + '%s'), (value,)) 
      cursor.fetchall() 
      result = True 
   except Exception: 
      e = get_exception() 
      result = str(e) 
   return result"," 'Set a global variable. 
 @param cursor: cursor object 
 @param mysqlvar: name of the global variable 
 @param value: value of the global variable'","'Set a global mysql variable to a given value 
 The DB driver will handle quoting of the given value based on its 
 type, thus numeric strings like \'3.0\' or \'8\' are illegal, they 
 should be passed as numeric literals.'"
"def group_membership(): 
    s3db.hrm_configure_pr_group_membership() 
   table = db.pr_group_membership 
   gtable = db.pr_group 
   htable = s3db.hrm_human_resource 
   s3.filter = ((((gtable.system == False) & (gtable.group_type == 3)) & (htable.type == 1)) & (htable.person_id == table.person_id)) 
   def prep(r): 
      if (r.method in ('create', 'create.popup', 'update', 'update.popup')): 
         person_id = get_vars.get('~.person_id', None) 
         if person_id: 
            field = table.person_id 
            field.default = person_id 
            field.readable = field.writable = False 
      return True 
   s3.prep = prep 
   output = s3_rest_controller('pr', 'group_membership', csv_stylesheet=('hrm', 'group_membership.xsl'), csv_template='group_membership') 
   return output", 'List all groups a person is a member of',"'Membership controller 
 - uses the group_membership table from PR'"
"def interface_is_portchannel(interface, module): 
    intf_type = get_interface_type(interface) 
   if (intf_type == 'ethernet'): 
      command = ('show   interface   ' + interface) 
      body = execute_show_command(command, module) 
      try: 
         interface_table = body[0]['TABLE_interface']['ROW_interface'] 
      except (KeyError, AttributeError, IndexError): 
         interface_table = None 
      if interface_table: 
         state = interface_table.get('eth_bundle') 
         if state: 
            return True 
         else: 
            return False 
   return False"," 'Return True if the interface is a port channel, False otherwise.'","'Checks to see if an interface is part of portchannel bundle 
 Args: 
 interface (str): full name of interface, i.e. Ethernet1/1 
 Returns: 
 True/False based on if interface is a member of a portchannel bundle'"
"def wc_reducer(word, counts): 
    (yield (word, sum(counts)))"," 'A reducer that reduces words to their sum of counts. 
 :param word: The current word. 
 :type word: str 
 :param counts: The current counts. 
 :type counts: dict 
 :return: The reduced word. 
 :rtype: str'",'sum up the counts for a word'
"def serviceCommand(methodName, cmdClass=ServiceWrapperCommand): 
    def wrapper(obj, journal, *args, **kwargs): 
      return journal.executeCommand(cmdClass(methodName, args, kwargs)) 
   return wrapper"," 'Decorator to wrap service methods with a journal. 
 :param methodName: The method to wrap. 
 :param cmdClass: The class to wrap the method with.'","'Wrap methods into commands for a journalled service. 
 The resulting callable will have signature identical to wrapped 
 function, except that it expects journal as first argument, and 
 returns a Deferred.'"
"def _native_package_type(): 
    distribution_name = CURRENT_DISTRIBUTION.name.lower() 
   for (package_type, distribution_names) in PACKAGE_TYPE_MAP.items(): 
      if (distribution_name.lower() in distribution_names): 
         return package_type 
   else: 
      raise ValueError('Unknown   distribution.', distribution_name)"," 'Returns the package type for the current distribution. 
 The package type is one of the constants defined in PACKAGE_TYPE_MAP. 
 :return: The package type for the current distribution. 
 :rtype: str'",':return: The ``bytes`` name of the native package format for this platform.'
"@functools.lru_cache(maxsize=None) 
 def get_callable(lookup_view): 
    if callable(lookup_view): 
      return lookup_view 
   if (not isinstance(lookup_view, str)): 
      raise ViewDoesNotExist((""'%s'   is   not   a   callable   or   a   dot-notation   path"" % lookup_view)) 
   (mod_name, func_name) = get_mod_func(lookup_view) 
   if (not func_name): 
      raise ImportError((""Could   not   import   '%s'.   The   path   must   be   fully   qualified."" % lookup_view)) 
   try: 
      mod = import_module(mod_name) 
   except ImportError: 
      (parentmod, submod) = get_mod_func(mod_name) 
      if (submod and (not module_has_submodule(import_module(parentmod), submod))): 
         raise ViewDoesNotExist((""Could   not   import   '%s'.   Parent   module   %s   does   not   exist."" % (lookup_view, mod_name))) 
      else: 
         raise 
   else: 
      try: 
         view_func = getattr(mod, func_name) 
      except AttributeError: 
         raise ViewDoesNotExist((""Could   not   import   '%s'.   View   does   not   exist   in   module   %s."" % (lookup_view, mod_name))) 
      else: 
         if (not callable(view_func)): 
            raise ViewDoesNotExist((""Could   not   import   '%s.%s'.   View   is   not   callable."" % (mod_name, func_name))) 
         return view_func"," 'Returns the view function for the given lookup_view. 
 This function will look for the view function in the current module, 
 in the given module, and in the parent module. 
 :param lookup_view: The lookup_view to find the view function for. 
 :type lookup_view: str or callable 
 :return: The view function. 
 :rtype: callable'","'Return a callable corresponding to lookup_view. 
 * If lookup_view is already a callable, return it. 
 * If lookup_view is a string import path that can be resolved to a callable, 
 import that callable and return it, otherwise raise an exception 
 (ImportError or ViewDoesNotExist).'"
"def process_validation(validation, is_compatibility=False, file_hash=None): 
    validation = fix_addons_linter_output(validation) 
   if is_compatibility: 
      mangle_compatibility_messages(validation) 
   validation.setdefault('ending_tier', 0) 
   if ((not validation['ending_tier']) and validation['messages']): 
      validation['ending_tier'] = max((msg.get('tier', (-1)) for msg in validation['messages'])) 
   if file_hash: 
      ValidationComparator(validation).annotate_results(file_hash) 
   limit_validation_results(validation) 
   htmlify_validation(validation) 
   return validation", 'Process the validation results into a format suitable for display.',"'Process validation results into the format expected by the web 
 frontend, including transforming certain fields into HTML,  mangling 
 compatibility messages, and limiting the number of messages displayed.'"
"@login_required 
 @enforce_shopping_cart_enabled 
 def show_cart(request): 
    cart = Order.get_cart_for_user(request.user) 
   (is_any_course_expired, expired_cart_items, expired_cart_item_names, valid_cart_item_tuples) = verify_for_closed_enrollment(request.user, cart) 
   site_name = configuration_helpers.get_value('SITE_NAME', settings.SITE_NAME) 
   if is_any_course_expired: 
      for expired_item in expired_cart_items: 
         Order.remove_cart_item_from_order(expired_item, request.user) 
      cart.update_order_type() 
   callback_url = request.build_absolute_uri(reverse('shoppingcart.views.postpay_callback')) 
   form_html = render_purchase_form_html(cart, callback_url=callback_url) 
   context = {'order': cart, 'shoppingcart_items': valid_cart_item_tuples, 'amount': cart.total_cost, 'is_course_enrollment_closed': is_any_course_expired, 'expired_course_names': expired_cart_item_names, 'site_name': site_name, 'form_html': form_html, 'currency_symbol': settings.PAID_COURSE_REGISTRATION_CURRENCY[1], 'currency': settings.PAID_COURSE_REGISTRATION_CURRENCY[0], 'enable_bulk_purchase': configuration_helpers.get_value('ENABLE_SHOPPING_CART_BULK_PURCHASE', True)} 
   return render_to_response('shoppingcart/shopping_cart.html', context)"," 'Show the shopping cart page. 
 The user can purchase a course, or add items to the cart. 
 If the user is logged in, the cart will be populated with the user\'s 
 items. 
 If the user is not logged in, the cart will be empty. 
 The user can also purchase a course, or add items to the cart, if the 
 user is not logged in. 
 The user can also purchase a course, or add items to the cart, if the 
 user is not logged in, but is logged in as a different user. 
 The user can also purchase a course, or add items to the cart, if the 
 user is not logged in, but is logged in as a different user, but is 
 using the same OpenID provider. 
 The user can also purchase a course, or add items to the cart, if the 
 user is not logged in, but is logged in as a different user, but is 
 using the same OpenID provider, and is using the same OpenID provider 
 as the logged in user. 
 The user can also purchase a course, or add items to the cart, if the ",'This view shows cart items.'
"def libvlc_video_set_deinterlace(p_mi, psz_mode): 
    f = (_Cfunctions.get('libvlc_video_set_deinterlace', None) or _Cfunction('libvlc_video_set_deinterlace', ((1,), (1,)), None, None, MediaPlayer, ctypes.c_char_p)) 
   return f(p_mi, psz_mode)"," 'Set video deinterlacing mode. 
 @param p_mi: the Media Player. 
 @param psz_mode: the video deinterlacing mode, see L{libvlc_video_deinterlace_mode}. 
 @return the previous value of the mode.'","'Enable or disable deinterlace filter. 
 @param p_mi: libvlc media player. 
 @param psz_mode: type of deinterlace filter, NULL to disable.'"
"def PostVimMessage(message, warning=True, truncate=False): 
    echo_command = (u'echom' if warning else u'echo') 
   vim.command(u'redraw') 
   if warning: 
      vim.command(u'echohl   WarningMsg') 
   message = ToUnicode(message) 
   if truncate: 
      vim_width = GetIntValue(u'&columns') 
      message = message.replace(u'\n', u'   ') 
      if (len(message) > vim_width): 
         message = (message[:(vim_width - 4)] + u'...') 
      old_ruler = GetIntValue(u'&ruler') 
      old_showcmd = GetIntValue(u'&showcmd') 
      vim.command(u'set   noruler   noshowcmd') 
      vim.command(u""{0}   '{1}'"".format(echo_command, EscapeForVim(message))) 
      SetVariableValue(u'&ruler', old_ruler) 
      SetVariableValue(u'&showcmd', old_showcmd) 
   else: 
      for line in message.split(u'\n'): 
         vim.command(u""{0}   '{1}'"".format(echo_command, EscapeForVim(line))) 
   if warning: 
      vim.command(u'echohl   None')"," 'Posts a message to Vim. 
 :param message: The message to post to Vim. 
 :type message: str 
 :param warning: If True, Vim will be in warning mode. 
 :type warning: bool 
 :param truncate: If True, the message will be truncated if it exceeds the 
 current width of the screen. 
 :type truncate: bool 
 :return: None'","'Display a message on the Vim status line. By default, the message is 
 highlighted and logged to Vim command-line history (see :h history). 
 Unset the |warning| parameter to disable this behavior. Set the |truncate| 
 parameter to avoid hit-enter prompts (see :h hit-enter) when the message is 
 longer than the window width.'"
"def getRadiusByPrefix(prefix, sideLength, xmlElement): 
    radius = getFloatByPrefixSide((prefix + 'radius'), sideLength, xmlElement) 
   radius += (0.5 * getFloatByPrefixSide((prefix + 'diameter'), sideLength, xmlElement)) 
   return (radius + (0.5 * getFloatByPrefixSide((prefix + 'size'), sideLength, xmlElement)))"," 'Returns the radius of the circle with the given prefix. 
 The radius is the distance from the center of the circle to the 
 edge of the circle. 
 The radius is the distance from the center of the circle to the 
 edge of the circle. 
 :param prefix: The prefix of the radius. 
 :param sideLength: The side length of the element. 
 :param xmlElement: The xml element. 
 :return: The radius. 
 :rtype: float'",'Get radius by prefix.'
"def addListsToRepository(fileNameHelp, getProfileDirectory, repository): 
    repository.displayEntities = [] 
   repository.executeTitle = None 
   repository.fileNameHelp = fileNameHelp 
   repository.fileNameInput = None 
   repository.lowerName = fileNameHelp.split('.')[(-2)] 
   repository.baseName = (repository.lowerName + '.csv') 
   repository.baseNameSynonym = None 
   repository.capitalizedName = getEachWordCapitalized(repository.lowerName) 
   repository.getProfileDirectory = getProfileDirectory 
   repository.openLocalHelpPage = HelpPage().getOpenFromDocumentationSubName(repository.fileNameHelp) 
   repository.openWikiManualHelpPage = None 
   repository.preferences = [] 
   repository.repositoryDialog = None 
   repository.saveListenerTable = {} 
   repository.title = (repository.capitalizedName + '   Settings') 
   repository.menuEntities = [] 
   repository.saveCloseTitle = 'Save   and   Close' 
   repository.windowPosition = WindowPosition().getFromValue(repository, '0+0') 
   for setting in repository.preferences: 
      setting.repository = repository"," 'Adds the lists to the repository. 
 This is the first step in creating a repository. 
 :param fileNameHelp: The name of the help page to use for the repository. 
 :type fileNameHelp: str 
 :param getProfileDirectory: The function to use to get the profile directory. 
 :type getProfileDirectory: function 
 :param repository: The repository to add the lists to. 
 :type repository: Repository'",'Add the value to the lists.'
"def update(context, id, name, description, is_public=None): 
    if (id is None): 
      msg = _('id   cannot   be   None') 
      raise exception.InvalidGroupType(reason=msg) 
   elevated = (context if context.is_admin else context.elevated()) 
   try: 
      type_updated = db.group_type_update(elevated, id, dict(name=name, description=description, is_public=is_public)) 
   except db_exc.DBError: 
      LOG.exception(_LE('DB   error:')) 
      raise exception.GroupTypeUpdateFailed(id=id) 
   return type_updated"," 'Update a group type. 
 :param context: The security context to use. 
 :param id: The group type id to update. 
 :param name: The name of the group type. 
 :param description: The description of the group type. 
 :param is_public: Whether the group type is public. 
 :returns: The updated group type.'",'Update group type by id.'
"def call_command(name, *args, **options): 
    try: 
      app_name = get_commands()[name] 
      if isinstance(app_name, BaseCommand): 
         klass = app_name 
      else: 
         klass = load_command_class(app_name, name) 
   except KeyError: 
      raise CommandError(('Unknown   command:   %r' % name)) 
   defaults = dict([(o.dest, o.default) for o in klass.option_list if (o.default is not NO_DEFAULT)]) 
   defaults.update(options) 
   return klass.execute(*args, **defaults)", 'Execute a command.',"'Calls the given command, with the given options and args/kwargs. 
 This is the primary API you should use for calling specific commands. 
 Some examples: 
 call_command(\'syncdb\') 
 call_command(\'shell\', plain=True) 
 call_command(\'sqlall\', \'myapp\')'"
"@csrf_protect 
 @permission_required('comments.can_moderate') 
 def delete(request, comment_id, next=None): 
    comment = get_object_or_404(comments.get_model(), pk=comment_id, site__pk=settings.SITE_ID) 
   if (request.method == 'POST'): 
      perform_delete(request, comment) 
      return next_redirect(request, next, delete_done, c=comment.pk) 
   else: 
      return render_to_response('comments/delete.html', {'comment': comment, 'next': next}, template.RequestContext(request))", 'Delete a comment.',"'Deletes a comment. Confirmation on GET, action on POST. Requires the ""can 
 moderate comments"" permission. 
 Templates: `comments/delete.html`, 
 Context: 
 comment 
 the flagged `comments.comment` object'"
"def _filter_doc_html(request, doc, doc_html, rendering_params): 
    if rendering_params['summary']: 
      return doc.get_summary_html() 
   if (not (rendering_params['section'] or rendering_params['raw'] or rendering_params['edit_links'] or rendering_params['include'])): 
      return doc_html 
   tool = kuma.wiki.content.parse(doc_html) 
   if rendering_params['raw']: 
      tool.injectSectionIDs() 
      tool.filterEditorSafety() 
   if rendering_params['section']: 
      tool.extractSection(rendering_params['section']) 
   if ((rendering_params['edit_links'] or (not rendering_params['raw'])) and request.user.is_authenticated() and doc.allows_revision_by(request.user)): 
      tool.injectSectionEditingLinks(doc.slug, doc.locale) 
   doc_html = tool.serialize() 
   if rendering_params['include']: 
      doc_html = kuma.wiki.content.filter_out_noinclude(doc_html) 
   return doc_html"," 'Filter out sections that are not included in the doc. 
 :param request: 
 :param doc: 
 :param doc_html: 
 :param rendering_params: 
 :return: 
 :rtype: str'",'Apply needed filtering/annotating operations to a Document\'s HTML.'
"def create_minibatch_x(minibatches, minibatch_markers, epoch_axis): 
    if epoch_axis: 
      x = np.zeros((minibatches,)) 
      last_e = 0 
      for (e_idx, e) in enumerate(minibatch_markers): 
         e_minibatches = (e - last_e) 
         x[last_e:e] = (e_idx + (np.arange(float(e_minibatches)) / e_minibatches)) 
         last_e = e 
   else: 
      x = np.arange(minibatches) 
   return x"," 'Create a minibatch x array. 
 :param minibatches: the number of minibatches 
 :param minibatch_markers: the minibatch markers 
 :param epoch_axis: whether to use epoch as axis 
 :return: x'","'Helper function to build x axis for data captured per minibatch. 
 Arguments: 
 minibatches (int): how many total minibatches 
 minibatch_markers (int array): cumulative number of minibatches complete at a given epoch 
 epoch_axis (bool): whether to render epoch or minibatch as the integer step in the x axis'"
"def CDLMORNINGDOJISTAR(barDs, count, penetration=(-4e+37)): 
    return call_talib_with_ohlc(barDs, count, talib.CDLMORNINGDOJISTAR, penetration)"," 'CDLMORNINGDOJISTAR(barDs, count, penetration=(-4e+37)) 
 Returns the CDLMORNINGDOJISTAR indicator for the specified barDs, 
 count, and penetration. 
 barDs is a BarData object 
 count is the number of bars to use for the calculation 
 penetration is the penetration value 
 See also 
 CDLMORNINGDOJI 
 CDLMORNINGDOJIUP 
 CDLMORNINGDOJIDOWN 
 CDLMORNINGDOJIINTR 
 CDLMORNINGDOJIREVERSE 
 CDLMORNINGDOJIHIGH 
 CDLMORNINGDOJILOW 
 CDLMORNINGDOJIOPEN 
 CDLMORNINGDOJICLOSE 
 CDLMORNINGDOJIHIGHPRICE 
 CDLMORNINGDOJILOWPRICE 
 CDLMORNINGDOJIOPENPRICE 
 CDLMORNINGDOJICLOSE",'Morning Doji Star'
"def gaussian_convolution(h, Xi, x): 
    return ((1.0 / np.sqrt((4 * np.pi))) * np.exp(((- ((Xi - x) ** 2)) / ((h ** 2) * 4.0))))"," 'Gaussian convolution of a function with a Gaussian kernel. 
 Parameters 
 h : scalar 
 width of the Gaussian kernel. 
 Xi : 1D array 
 coordinates of the Gaussian kernel. 
 x : 1D array 
 coordinates of the function. 
 Returns 
 Y : 1D array 
 value of the function at the coordinates of the Gaussian kernel. 
 Notes 
 This function is defined in [1]_. 
 Examples 
 >>> from sympy.physics.wave import gaussian_convolution 
 >>> from sympy import Symbol, sin, sqrt 
 >>> h = Symbol(\'h\') 
 >>> Xi = Symbol(\'x\') 
 >>> x = Symbol(\'x\') 
 >>> gaussian_convolution(h, Xi, x) 
 (1/(sqrt(4*pi)*h))*exp(-((x-Xi)**2)/(4*h**2))'",'Calculates the Gaussian Convolution Kernel'
"def load_reg(): 
    reg_dir = _reg_dir() 
   regfile = os.path.join(reg_dir, 'register') 
   try: 
      with salt.utils.fopen(regfile, 'r') as fh_: 
         return msgpack.load(fh_) 
   except: 
      log.error('Could   not   write   to   msgpack   file   {0}'.format(__opts__['outdir'])) 
      raise", 'Load the registry file.','Load the register from msgpack files'
"def _map_plays_to_roles(graph, dirs, git_dir, key, type_1, type_2): 
    Node = namedtuple('Node', ['name', 'type']) 
   for d in dirs: 
      d = pathlib2.Path(git_dir, d) 
      for item in d.iterdir(): 
         if item.match('*.yml'): 
            yaml_file = _open_yaml_file(item) 
            if (yaml_file is not None): 
               for play in yaml_file: 
                  if (key in play): 
                     for role in play[key]: 
                        name = _get_role_name(role) 
                        node_1 = Node(item.stem, type_1) 
                        node_2 = Node(name, type_2) 
                        graph.add_edge(node_2, node_1)"," 'Adds edges between roles and their corresponding plays. 
 :param graph: A graph object. 
 :param dirs: The directories to scan for yaml files. 
 :param git_dir: The directory containing the git repository. 
 :param key: The key in the play yaml file that contains the role names. 
 :param type_1: The type of the node that represents the plays. 
 :param type_2: The type of the node that represents the roles.'","'Maps plays to the roles they use. 
 Input: 
 graph: A networkx digraph that is used to map Ansible dependencies. 
 dirs: A list of relative paths to directories in which Ansible playbooks reside. 
 git_dir: A path to the top-most directory in the local git repository tool is to be run in. 
 key: The key in a playbook yaml file in dirs that maps to relevant playbook data. In this case, key is 
 ""roles"", because the roles used by a playbook is of interest. 
 type_1: Given edges A-B, the type of node A. 
 type_2: Given edges A-B, the type of node B. 
 Since this function maps plays to the roles they use, both type_1 is a type of playbook and type_2 is ""role"".'"
"def _installHandlerUsingSetWakeup(fd): 
    if (fd == (-1)): 
      signal.signal(signal.SIGCHLD, signal.SIG_DFL) 
   else: 
      signal.signal(signal.SIGCHLD, _Handler(None)) 
      siginterrupt(signal.SIGCHLD, False) 
   return set_wakeup_fd(fd)"," 'Install a handler for SIGCHLD using set_wakeup_fd(). 
 This is used when the child process is not started by fork(). 
 It is not possible to use set_wakeup_fd() to install a handler for SIGCHLD 
 when the child process is started by fork(), because the signal handler 
 is already installed by the fork() call. 
 :param fd: The file descriptor to use for SIGCHLD. 
 :returns: The file descriptor to use for SIGCHLD. 
 :rtype: int'","'Install a signal handler which will write a byte to C{fd} when 
 I{SIGCHLD} is received. 
 This is implemented by installing an instance of L{_Handler} wrapped 
 around C{None}, setting the I{SIGCHLD} handler as not allowed to 
 interrupt system calls, and using L{signal.set_wakeup_fd} to do the 
 actual writing. 
 @param fd: The file descriptor to which to write when I{SIGCHLD} is 
 received. 
 @type fd: C{int}'"
"def get_user_model(): 
    try: 
      model = get_model(AUTH_USER_APP_LABEL, AUTH_USER_MODEL_NAME) 
   except LookupError: 
      raise ImproperlyConfigured((""AUTH_USER_MODEL   refers   to   model   '%s'   that   has   not   been   installed"" % settings.AUTH_USER_MODEL)) 
   core_fields = set([f.name for f in User._meta.fields]) 
   model_fields = set([f.name for f in model._meta.fields]) 
   new_fields = model_fields.difference(core_fields) 
   model._meta.has_additional_fields = (len(new_fields) > 0) 
   model._meta.additional_fields = new_fields 
   return model", 'Return the User model.',"'Return the User model. Doesn\'t require the app cache to be fully 
 initialised. 
 This used to live in compat to support both Django 1.4\'s fixed User model 
 and custom user models introduced thereafter. 
 Support for Django 1.4 has since been dropped in Oscar, but our 
 get_user_model remains because code relies on us annotating the _meta class 
 with the additional fields, and other code might rely on it as well.'"
"def handler_url(block, handler_name, suffix='', query='', thirdparty=False): 
    view_name = 'xblock_handler' 
   if handler_name: 
      func = getattr(block.__class__, handler_name, None) 
      if (not func): 
         raise ValueError('{!r}   is   not   a   function   name'.format(handler_name)) 
   if thirdparty: 
      view_name = 'xblock_handler_noauth' 
   url = reverse(view_name, kwargs={'course_id': unicode(block.location.course_key), 'usage_id': quote_slashes(unicode(block.scope_ids.usage_id).encode('utf-8')), 'handler': handler_name, 'suffix': suffix}) 
   if (not suffix): 
      url = url.rstrip('/') 
   if query: 
      url += ('?' + query) 
   if thirdparty: 
      scheme = ('https' if (settings.HTTPS == 'on') else 'http') 
      url = '{scheme}://{host}{path}'.format(scheme=scheme, host=settings.SITE_NAME, path=url) 
   return url"," 'Returns a URL for the given block and handler function. 
 If thirdparty is True, then the URL is for the handler function 
 that does not require authentication.'","'This method matches the signature for `xblock.runtime:Runtime.handler_url()` 
 See :method:`xblock.runtime:Runtime.handler_url`'"
"def model_query(context, model, *args, **kwargs): 
    session = (kwargs.get('session') or get_session()) 
   read_deleted = (kwargs.get('read_deleted') or context.read_deleted) 
   project_only = kwargs.get('project_only', False) 
   def issubclassof_nova_base(obj): 
      return (isinstance(obj, type) and issubclass(obj, models.NovaBase)) 
   base_model = model 
   if (not issubclassof_nova_base(base_model)): 
      base_model = kwargs.get('base_model', None) 
      if (not issubclassof_nova_base(base_model)): 
         raise Exception(_('model   or   base_model   parameter   should   be   subclass   of   NovaBase')) 
   query = session.query(model, *args) 
   default_deleted_value = base_model.__mapper__.c.deleted.default.arg 
   if (read_deleted == 'no'): 
      query = query.filter((base_model.deleted == default_deleted_value)) 
   elif (read_deleted == 'yes'): 
      pass 
   elif (read_deleted == 'only'): 
      query = query.filter((base_model.deleted != default_deleted_value)) 
   else: 
      raise Exception((_(""Unrecognized   read_deleted   value   '%s'"") % read_deleted)) 
   if (nova.context.is_user_context(context) and project_only): 
      if (project_only == 'allow_none'): 
         query = query.filter(or_((base_model.project_id == context.project_id), (base_model.project_id == None))) 
      else: 
         query = query.filter_by(project_id=context.project_id) 
   return query", 'Returns a query object for the given model.',"'Query helper that accounts for context\'s `read_deleted` field. 
 :param context: context to query under 
 :param session: if present, the session to use 
 :param read_deleted: if present, overrides context\'s read_deleted field. 
 :param project_only: if present and context is user-type, then restrict 
 query to match the context\'s project_id. If set to \'allow_none\', 
 restriction includes project_id = None. 
 :param base_model: Where model_query is passed a ""model"" parameter which is 
 not a subclass of NovaBase, we should pass an extra base_model 
 parameter that is a subclass of NovaBase and corresponds to the 
 model parameter.'"
"def attr_sparse_matrix(G, edge_attr=None, node_attr=None, normalized=False, rc_order=None, dtype=None): 
    try: 
      import numpy as np 
      from scipy import sparse 
   except ImportError: 
      raise ImportError('attr_sparse_matrix()   requires   scipy:   http://scipy.org/   ') 
   edge_value = _edge_value(G, edge_attr) 
   node_value = _node_value(G, node_attr) 
   if (rc_order is None): 
      ordering = list(set([node_value(n) for n in G])) 
   else: 
      ordering = rc_order 
   N = len(ordering) 
   undirected = (not G.is_directed()) 
   index = dict(zip(ordering, range(N))) 
   M = sparse.lil_matrix((N, N), dtype=dtype) 
   seen = set([]) 
   for (u, nbrdict) in G.adjacency(): 
      for v in nbrdict: 
         (i, j) = (index[node_value(u)], index[node_value(v)]) 
         if (v not in seen): 
            M[(i, j)] += edge_value(u, v) 
            if undirected: 
               M[(j, i)] = M[(i, j)] 
      if undirected: 
         seen.add(u) 
   if normalized: 
      norms = np.asarray(M.sum(axis=1)).ravel() 
      for (i, norm) in enumerate(norms): 
         M[i, :] /= norm 
   if (rc_order is None): 
      return (M, ordering) 
   else: 
      return M"," 'Returns a sparse matrix representation of the graph, where the rows 
 and columns correspond to nodes and the nonzero entries correspond to 
 edges. 
 Parameters 
 G : NetworkX graph 
 The input graph. 
 edge_attr : function, optional 
 The function that takes two nodes and returns the edge value. 
 node_attr : function, optional 
 The function that takes a node and returns the node value. 
 normalized : bool, optional 
 If True, the row norms are normalized to 1. 
 rc_order : list of nodes, optional 
 The ordering of the nodes. 
 dtype : data-type, optional 
 The data-type of the matrix. 
 Returns 
 M : sparse matrix 
 A sparse matrix representing the graph. 
 ordering : list of nodes 
 The ordering of the nodes. 
 Examples 
 >>> G = nx.path_graph(4) 
 >>> attr_sparse_matrix(G) 
 (array([[0, 0, 0, 0], 
 [0, 0, 0, 0], 
 [0","'Returns a SciPy sparse matrix using attributes from G. 
 If only `G` is passed in, then the adjacency matrix is constructed. 
 Let A be a discrete set of values for the node attribute `node_attr`. Then 
 the elements of A represent the rows and columns of the constructed matrix. 
 Now, iterate through every edge e=(u,v) in `G` and consider the value 
 of the edge attribute `edge_attr`.  If ua and va are the values of the 
 node attribute `node_attr` for u and v, respectively, then the value of 
 the edge attribute is added to the matrix element at (ua, va). 
 Parameters 
 G : graph 
 The NetworkX graph used to construct the NumPy matrix. 
 edge_attr : str, optional 
 Each element of the matrix represents a running total of the 
 specified edge attribute for edges whose node attributes correspond 
 to the rows/cols of the matirx. The attribute must be present for 
 all edges in the graph. If no attribute is specified, then we 
 just count the number of edges whose node attributes correspond 
 to the matrix element. 
 node_attr : str, optional 
 Each row and column in the matrix represents a particular value 
 of the node attribute.  The attribute must be present for all nodes 
 in the graph. Note, the values of this attribute should be reliably 
 hashable. So, float values are not recommended. If no attribute is 
 specified, then the rows and columns will be the nodes of the graph. 
 normalized : bool, optional 
 If True, then each row is normalized by the summation of its values. 
 rc_order : list, optional 
 A list of the node attribute values. This list specifies the ordering 
 of rows and columns of the array. If no ordering is provided, then 
 the ordering will be random (and also, a return value). 
 Other Parameters 
 dtype : NumPy data-type, optional 
 A valid NumPy dtype used to initialize the array. Keep in mind certain 
 dtypes can yield unexpected results if the array is to be normalized. 
 The parameter is passed to numpy.zeros(). If unspecified, the NumPy 
 default is used. 
 Returns 
 M : SciPy sparse matrix 
 The attribute matrix. 
 ordering : list 
 If `rc_order` was specified, then only the matrix is returned. 
 However, if `rc_order` was None, then the ordering used to construct 
 the matrix is returned as well. 
 Examples 
 Construct an adjacency matrix: 
 >>> G = nx.Graph() 
 >>> G.add_edge(0,1,thickness=1,weight=3) 
 >>> G.add_edge(0,2,thickness=2) 
 >>> G.add_edge(1,2,thickness=3) 
 >>> M = nx.attr_sparse_matrix(G, rc_order=[0,1,2]) 
 >>> M.todense() 
 matrix([[ 0.,  1.,  1.], 
 [ 1.,  0.,  1.], 
 [ 1.,  1.,  0.]]) 
 Alternatively, we can obtain the matrix describing edge thickness. 
 >>> M = nx.attr_sparse_matrix(G, edge_attr=\'thickness\', rc_order=[0,1,2]) 
 >>> M.todense() 
 matrix([[ 0.,  1.,  2.], 
 [ 1.,  0.,  3.], 
 [ 2.,  3.,  0.]]) 
 We can also color the nodes and ask for the probability distribution over 
 all edges (u,v) describing: 
 Pr(v has color Y | u has color X) 
 >>> G.node[0][\'color\'] = \'red\' 
 >>> G.node[1][\'color\'] = \'red\' 
 >>> G.node[2][\'color\'] = \'blue\' 
 >>> rc = [\'red\', \'blue\'] 
 >>> M = nx.attr_sparse_matrix(G, node_attr=\'color\',                                   normalized=True, rc_order=rc) 
 >>> M.todense() 
 matrix([[ 0.33333333,  0.66666667], 
 [ 1.        ,  0.        ]]) 
 For example, the above tells us that for all edges (u,v): 
 Pr( v is red  | u is red)  = 1/3 
 Pr( v is blue | u is red)  = 2/3 
 Pr( v is red  | u is blue) = 1 
 Pr( v is blue | u is blue) = 0 
 Finally, we can obtain the total weights listed by the node colors. 
 >>> M = nx.attr_sparse_matrix(G, edge_attr=\'weight\',                                  node_attr=\'color\', rc_order=rc) 
 >>> M.todense() 
 matrix([[ 3.,  2.], 
 [ 2.,  0.]]) 
 Thus, the total weight over all edges (u,v) with u and v having colors: 
 (red, red)   is 3   # the sole contribution is from edge (0,1) 
 (red, blue)  is 2   # contributions from edges (0,2) and (1,2) 
 (blue, red)  is 2   # same as (red, blue) since graph is undirected 
 (blue, blue) is 0   # there are no edges with blue endpoints'"
"@protocol.commands.add(u'command_list_ok_begin', list_command=False) 
 def command_list_ok_begin(context): 
    context.dispatcher.command_list_receiving = True 
   context.dispatcher.command_list_ok = True 
   context.dispatcher.command_list = []"," 'Begin receiving command list. 
 This is a placeholder for the beginning of a command list. 
 This command can be used to begin receiving command list from 
 a client. 
 .. versionadded:: 0.5'",'See :meth:`command_list_begin()`.'
"def _ace_to_text(ace, objectType): 
    dc = daclConstants() 
   objectType = dc.getObjectTypeBit(objectType) 
   try: 
      userSid = win32security.LookupAccountSid('', ace[2]) 
      if userSid[1]: 
         userSid = '{1}\\{0}'.format(userSid[0], userSid[1]) 
      else: 
         userSid = '{0}'.format(userSid[0]) 
   except Exception: 
      userSid = win32security.ConvertSidToStringSid(ace[2]) 
   tPerm = ace[1] 
   tAceType = ace[0][0] 
   tProps = ace[0][1] 
   tInherited = '' 
   for x in dc.validAceTypes: 
      if (dc.validAceTypes[x]['BITS'] == tAceType): 
         tAceType = dc.validAceTypes[x]['TEXT'] 
         break 
   for x in dc.rights[objectType]: 
      if (dc.rights[objectType][x]['BITS'] == tPerm): 
         tPerm = dc.rights[objectType][x]['TEXT'] 
         break 
   if ((tProps & win32security.INHERITED_ACE) == win32security.INHERITED_ACE): 
      tInherited = '[Inherited]' 
      tProps = (tProps ^ win32security.INHERITED_ACE) 
   for x in dc.validPropagations[objectType]: 
      if (dc.validPropagations[objectType][x]['BITS'] == tProps): 
         tProps = dc.validPropagations[objectType][x]['TEXT'] 
         break 
   return '{0}   {1}   {2}   on   {3}   {4}'.format(userSid, tAceType, tPerm, tProps, tInherited)"," 'Convert an ACE into a string that can be used in a text format. 
 :param ace: ACE to convert 
 :type ace: win32security.ACE 
 :param objectType: Object type the ACE is for 
 :type objectType: int 
 :return: A string representing the ACE 
 :rtype: str'",'helper function to convert an ace to a textual representation'
"def _build_offset(offset, kwargs, default): 
    if (offset is None): 
      if (not kwargs): 
         return default 
      else: 
         return _td_check(datetime.timedelta(**kwargs)) 
   elif kwargs: 
      raise ValueError('Cannot   pass   kwargs   and   an   offset') 
   elif isinstance(offset, datetime.timedelta): 
      return _td_check(offset) 
   else: 
      raise TypeError(""Must   pass   'hours'   and/or   'minutes'   as   keywords"")"," 'Builds an offset based on the offset keyword arguments. 
 :param offset: The offset to add to the datetime. 
 :type offset: datetime.timedelta or int 
 :param kwargs: Keyword arguments to pass to the datetime.timedelta constructor. 
 :type kwargs: dict 
 :param default: The default value to use if no offset is specified. 
 :type default: datetime.timedelta 
 :return: A datetime.timedelta object.'",'Builds the offset argument for event rules.'
"def dump(object_, file_, parameters=None, use_cpickle=False, protocol=DEFAULT_PROTOCOL, **kwargs): 
    if use_cpickle: 
      pickler = cPickle.Pickler 
   else: 
      pickler = _PicklerWithWarning 
   with closing(tarfile.TarFile(fileobj=file_, mode='w')) as tar_file: 
      external_objects = {} 
      def _save_parameters(f): 
         renamer = _Renamer() 
         named_parameters = {renamer(p): p for p in parameters} 
         numpy.savez(f, **{n: p.get_value() for (n, p) in named_parameters.items()}) 
         for (name, p) in named_parameters.items(): 
            array_ = p.container.storage[0] 
            external_objects[id(array_)] = _mangle_parameter_name(p, name) 
      if parameters: 
         _taradd(_save_parameters, tar_file, '_parameters') 
      if (object_ is not None): 
         save_object = _SaveObject(pickler, object_, external_objects, protocol, **kwargs) 
         _taradd(save_object, tar_file, '_pkl')"," 'Dump a NumPy object to a tar file. 
 Parameters 
 object_ : object 
 The object to be dumped. 
 file_ : file-like object 
 The file to dump to. 
 parameters : dict 
 The parameters to be saved. 
 use_cpickle : bool 
 Whether to use cPickle to dump the object. 
 protocol : int 
 The protocol to use when using cPickle. 
 **kwargs 
 Keyword arguments to pass to the ``pickler`` object. 
 Examples 
 >>> import numpy as np 
 >>> from scipy.io import dump 
 >>> x = np.arange(10) 
 >>> dump(x, open(\'x.tar\', \'w\')) 
 >>> x = np.load(\'x.tar\') 
 >>> x == np.arange(10) 
 True 
 See Also 
 scipy.io.load 
 scipy.io.savemat 
 scipy.io.savetxt 
 scipy.io.savemat 
 scipy.io.","'Pickles an object, optionally saving its parameters separately. 
 Parameters 
 object_ : object 
 The object to pickle. If None, only the parameters passed to the 
 `parameters` argument will be saved. 
 file_ : file 
 The destination for saving. 
 parameters : list, optional 
 Shared variables whose internal numpy arrays should be saved 
 separately in the `_parameters` field of the tar file. 
 pickle_object : bool 
 If False, `object_` will not be serialized, only its parameters. 
 This flag can be used when `object_` is not serializable, but one 
 still want to save its parameters. Default: True 
 use_cpickle : bool 
 Use cPickle instead of pickle. Setting it to true will disable the 
 warning message if you try to pickle objects from the main module, 
 so be sure that there is no warning before turning this flag 
 on. Default: False. 
 protocol : int, optional 
 The pickling protocol to use. Unlike Python\'s built-in pickle, the 
 default is set to `2` instead of 0 for Python 2. The Python 3 
 default (level 3) is maintained. 
 \*\*kwargs 
 Keyword arguments to be passed to `pickle.Pickler`.'"
"def has_required_keys(xblock): 
    for key in ('discussion_id', 'discussion_category', 'discussion_target'): 
      if (getattr(xblock, key, None) is None): 
         log.debug(""Required   key   '%s'   not   in   discussion   %s,   leaving   out   of   category   map"", key, xblock.location) 
         return False 
   return True", 'Ensure that a discussion xblock has all required keys.',"'Returns True iff xblock has the proper attributes for generating metadata 
 with get_discussion_id_map_entry()'"
"def filter_labels(train, label, classes=None): 
    if isinstance(train, theano.tensor.sharedvar.SharedVariable): 
      train = train.get_value(borrow=True) 
   if isinstance(label, theano.tensor.sharedvar.SharedVariable): 
      label = label.get_value(borrow=True) 
   if (not (isinstance(train, numpy.ndarray) or scipy.sparse.issparse(train))): 
      raise TypeError('train   must   be   a   numpy   array,   a   scipy   sparse   matrix,   or   a   theano   shared   array') 
   if (classes is not None): 
      label = label[:, classes] 
   if scipy.sparse.issparse(train): 
      idx = label.sum(axis=1).nonzero()[0] 
      return (train[idx], label[idx]) 
   condition = label.any(axis=1) 
   return tuple((var.compress(condition, axis=0) for var in (train, label)))"," 'Filter out labels that are not in the classes. 
 Parameters 
 train : ndarray or shared variable 
 train data 
 label : ndarray or shared variable 
 label data 
 classes : array-like 
 classes to filter 
 Returns 
 train : ndarray or shared variable 
 train data 
 label : ndarray or shared variable 
 label data 
 Examples 
 >>> X, y = theano.tensor.shared(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]), \ 
 ...                           dtype=\'int32\'), \ 
 ...                           theano.tensor.shared(np.array([0, 1, 2]), \ 
 ...                           dtype=\'int32\') 
 >>> filter_labels(X, y, [0, 1]) 
 (array([[1, 0, 0], 
 [0, 1, 0]]), array([[0, 1, 0], 
 [0, 0, 1]]))'","'Filter examples of train for which we have labels 
 Parameters 
 train : WRITEME 
 label : WRITEME 
 classes : WRITEME 
 Returns 
 WRITEME'"
"def set_desktop_icons(visible_list, ignore_duplicate=True): 
    frappe.db.sql(u'delete   from   `tabDesktop   Icon`   where   standard=0') 
   frappe.db.sql(u'update   `tabDesktop   Icon`   set   blocked=0,   hidden=1') 
   for module_name in visible_list: 
      name = frappe.db.get_value(u'Desktop   Icon', {u'module_name': module_name}) 
      if name: 
         frappe.db.set_value(u'Desktop   Icon', name, u'hidden', 0) 
      elif frappe.db.exists(u'DocType', module_name): 
         try: 
            add_user_icon(module_name, standard=1) 
         except frappe.UniqueValidationError as e: 
            if (not ignore_duplicate): 
               raise e 
            else: 
               visible_list.remove(module_name) 
               if frappe.message_log: 
                  frappe.message_log.pop() 
   set_order(visible_list) 
   clear_desktop_icons_cache()"," 'Set desktop icons for modules. 
 @param visible_list: list of modules to set as desktop icons 
 @param ignore_duplicate: ignore duplicate modules 
 @return: None'","'Resets all lists and makes only the given one standard, 
 if the desktop icon does not exist and the name is a DocType, then will create 
 an icon for the doctype'"
"def prime(nth): 
    n = as_int(nth) 
   if (n < 1): 
      raise ValueError('nth   must   be   a   positive   integer;   prime(1)   ==   2') 
   if (n <= len(sieve._list)): 
      return sieve[n] 
   from sympy.functions.special.error_functions import li 
   from sympy.functions.elementary.exponential import log 
   a = 2 
   b = int((n * (log(n) + log(log(n))))) 
   while (a < b): 
      mid = ((a + b) >> 1) 
      if (li(mid) > n): 
         b = mid 
      else: 
         a = (mid + 1) 
   n_primes = primepi((a - 1)) 
   while (n_primes < n): 
      if isprime(a): 
         n_primes += 1 
      a += 1 
   return (a - 1)"," 'Returns the prime number that is the nth prime. 
 Examples 
 >>> from sympy.ntheory import prime 
 >>> prime(1) 
 2 
 >>> prime(2) 
 3 
 >>> prime(100) 
 97 
 >>> prime(1000) 
 969'","'Return the nth prime, with the primes indexed as prime(1) = 2, 
 prime(2) = 3, etc.... The nth prime is approximately n*log(n). 
 Logarithmic integral of x is a pretty nice approximation for number of 
 primes <= x, i.e. 
 li(x) ~ pi(x) 
 In fact, for the numbers we are concerned about( x<1e11 ), 
 li(x) - pi(x) < 50000 
 Also, 
 li(x) > pi(x) can be safely assumed for the numbers which 
 can be evaluated by this function. 
 Here, we find the least integer m such that li(m) > n using binary search. 
 Now pi(m-1) < li(m-1) <= n, 
 We find pi(m - 1) using primepi function. 
 Starting from m, we have to find n - pi(m-1) more primes. 
 For the inputs this implementation can handle, we will have to test 
 primality for at max about 10**5 numbers, to get our answer. 
 References 
 - https://en.wikipedia.org/wiki/Prime_number_theorem#Table_of_.CF.80.28x.29.2C_x_.2F_log_x.2C_and_li.28x.29 
 - https://en.wikipedia.org/wiki/Prime_number_theorem#Approximations_for_the_nth_prime_number 
 - https://en.wikipedia.org/wiki/Skewes%27_number 
 Examples 
 >>> from sympy import prime 
 >>> prime(10) 
 29 
 >>> prime(1) 
 2 
 >>> prime(100000) 
 1299709 
 See Also 
 sympy.ntheory.primetest.isprime : Test if n is prime 
 primerange : Generate all primes in a given range 
 primepi : Return the number of primes less than or equal to n'"
"@register.function 
 @jinja2.contextfunction 
 def favorites_widget(context, addon, condensed=False): 
    c = dict(context.items()) 
   request = c['request'] 
   if request.user.is_authenticated(): 
      is_favorite = (addon.id in request.user.favorite_addons) 
      faved_class = ('faved' if is_favorite else '') 
      unfaved_text = ('' if condensed else _('Add   to   favorites')) 
      faved_text = (_('Favorite') if condensed else _('Remove   from   favorites')) 
      add_url = reverse('collections.alter', args=[request.user.username, 'favorites', 'add']) 
      remove_url = reverse('collections.alter', args=[request.user.username, 'favorites', 'remove']) 
      c.update(locals()) 
      t = get_env().get_template('bandwagon/favorites_widget.html').render(c) 
      return jinja2.Markup(t)"," 'Render the favorites widget. 
 This widget is used in the addon detail page and in the addons list. 
 :param context: the context to render the template in 
 :param addon: the addon to display 
 :param condensed: whether to show the favorites button or not 
 :return: the rendered template'",'Displays \'Add to Favorites\' widget.'
"def names_from_file(filename): 
    with io.open(filename, 'rt', encoding='utf8') as names_file: 
      for name in filter_koan_names(names_file): 
         (yield name) 
   return"," 'Reads a file containing names of koans and yields them. 
 :param filename: A filename containing the names of koans. 
 :returns: A generator of names of koans. 
 :rtype: generator[unicode]'","'Opens the given ``filename`` and yields the fully-qualified names 
 of TestCases found inside (one per line).'"
"def sanitize_separators(value): 
    if settings.USE_L10N: 
      decimal_separator = get_format('DECIMAL_SEPARATOR') 
      if isinstance(value, six.string_types): 
         parts = [] 
         if (decimal_separator in value): 
            (value, decimals) = value.split(decimal_separator, 1) 
            parts.append(decimals) 
         if settings.USE_THOUSAND_SEPARATOR: 
            parts.append(value.replace(get_format('THOUSAND_SEPARATOR'), '')) 
         else: 
            parts.append(value) 
         value = '.'.join(reversed(parts)) 
   return value"," 'Sanitize a value to remove any separators that are not the decimal separator. 
 This is used to prevent a value such as 1,000.00 from being interpreted as 1000.00 
 when it\'s being displayed as a number.'","'Sanitizes a value according to the current decimal and 
 thousand separator setting. Used with form field input.'"
"def get_argnames(func): 
    if six.PY2: 
      if isinstance(func, functools.partial): 
         spec = inspect.getargspec(func.func) 
      elif inspect.isroutine(func): 
         spec = inspect.getargspec(func) 
      else: 
         spec = inspect.getargspec(func.__call__) 
      args = [arg for arg in spec.args if (arg != 'self')] 
   else: 
      sig = inspect.signature(func) 
      args = [param.name for param in sig.parameters.values() if (param.kind not in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD))] 
   return args"," 'Return the names of the arguments of a function or method. 
 :param func: function or method to get the argument names of 
 :return: the names of the arguments'","'Introspecs the arguments of a callable. 
 Args: 
 func: The callable to introspect 
 Returns: 
 A list of argument names, excluding *arg and **kwargs 
 arguments.'"
"def get_indices(client): 
    try: 
      indices = list(client.indices.get_settings(index='_all', params={'expand_wildcards': 'open,closed'})) 
      version_number = get_version(client) 
      logger.debug('Detected   Elasticsearch   version   {0}'.format('.'.join(map(str, version_number)))) 
      if ((version_number >= (2, 4, 2)) and (version_number < (5, 0, 0))): 
         logger.debug('Using   Elasticsearch   >=   2.4.2   <   5.0.0') 
         if client.indices.exists(index='.security'): 
            logger.debug('Found   the   "".security""   index.      Adding   to   list   of   all   indices') 
            if (not ('.security' in indices)): 
               indices.append('.security') 
      logger.debug('All   indices:   {0}'.format(indices)) 
      return indices 
   except Exception as e: 
      raise FailedExecution('Failed   to   get   indices.   Error:   {0}'.format(e))"," 'Get all indices from the Elasticsearch cluster 
 :param client: Elasticsearch client 
 :type client: Elasticsearch 
 :return: list of indices'","'Get the current list of indices from the cluster. 
 :arg client: An :class:`elasticsearch.Elasticsearch` client object 
 :rtype: list'"
"def test_message_hiding(qtbot, view): 
    with qtbot.waitSignal(view._clear_timer.timeout): 
      view.show_message(usertypes.MessageLevel.info, 'test') 
   assert (not view._messages)", 'Test that the message is hidden when the timer expires.','Messages should be hidden after the timer times out.'
"def update_dmg_makefile(): 
    print 'Updating   fancy-dmg   Makefile...' 
   url = 'https://raw.githubusercontent.com/remko/fancy-dmg/master/Makefile' 
   target_path = os.path.join('scripts', 'dev', 'Makefile-dmg') 
   urllib.request.urlretrieve(url, target_path) 
   urllib.request.urlcleanup()", 'Update the fancy-dmg Makefile to include the latest changes.',"'Update fancy-dmg Makefile. 
 See https://el-tramo.be/blog/fancy-dmg/'"
"def _initialize_headers(headers): 
    return ({} if (headers is None) else dict(headers))", 'Initialize headers to a dict if needed.',"'Creates a copy of the headers. 
 Args: 
 headers: dict, request headers to copy. 
 Returns: 
 dict, the copied headers or a new dictionary if the headers 
 were None.'"
"@contextmanager 
 def secret_model(): 
    orig_model = None 
   try: 
      orig_model = loading.cache.app_models['tests']['secret'] 
      del loading.cache.app_models['tests']['secret'] 
   except KeyError: 
      pass 
   try: 
      attrs = {'name': EncryptedCharField('Name', max_length=Secret._meta.get_field('name').max_length), 'text': EncryptedTextField('Text'), '__module__': 'django_extensions.tests.models', 'Meta': type('Meta', (object,), {'managed': False, 'db_table': Secret._meta.db_table})} 
      (yield type('Secret', (models.Model,), attrs)) 
   except: 
      raise 
   finally: 
      try: 
         loading.cache.app_models['tests']['secret'] = orig_model 
      except KeyError: 
         pass", 'Returns a model with encrypted fields.',"'A context manager that yields a Secret model defined at runtime. 
 All EncryptedField init logic occurs at model class definition time, not at 
 object instantiation time. This means that in order to test different keys 
 and modes, we must generate a new class definition at runtime, after 
 establishing the correct KeyCzar settings. This context manager handles 
 that process. 
 See http://dynamic-models.readthedocs.org/en/latest/ and 
 https://docs.djangoproject.com/en/dev/topics/db/models/ 
 #differences-between-proxy-inheritance-and-unmanaged-models'"
"def lsmod(): 
    ret = [] 
   for line in __salt__['cmd.run']('kldstat').splitlines(): 
      comps = line.split() 
      if (not (len(comps) > 2)): 
         continue 
      if (comps[0] == 'Id'): 
         continue 
      if (comps[4] == 'kernel'): 
         continue 
      ret.append({'module': comps[4][:(-3)], 'size': comps[3], 'depcount': comps[1]}) 
   return ret"," 'Return a list of loaded modules. 
 Example: 
 .. code-block:: bash 
 salt://mods/lsmod 
 .. code-block:: json 
 [ 
    { 
        \'module\': \'foo\', 
        \'size\': 123, 
        \'depcount\': 1 
    }, 
    { 
        \'module\': \'bar\', 
        \'size\': 456, 
        \'depcount\': 2 
    } 
 ]'","'Return a dict containing information about currently loaded modules 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' kmod.lsmod'"
"def mark_as_cover(container, name): 
    if (name not in container.mime_map): 
      raise ValueError((u'Cannot   mark   %s   as   cover   as   it   does   not   exist' % name)) 
   mt = container.mime_map[name] 
   if (not is_raster_image(mt)): 
      raise ValueError((u'Cannot   mark   %s   as   the   cover   image   as   it   is   not   a   raster   image' % name)) 
   if (container.book_type == u'azw3'): 
      mark_as_cover_azw3(container, name) 
   else: 
      mark_as_cover_epub(container, name)"," 'Marks the given image as the cover image. 
 :param container: The container to mark the cover image for. 
 :param name: The name of the image to mark as the cover. 
 :type name: str 
 :raise ValueError: If the given image does not exist or is not a raster image.'",'Mark the specified image as the cover image.'
"def after_VBD_create(vbd_ref, vbd_rec): 
    vbd_rec['currently_attached'] = False 
   vbd_rec['device'] = '' 
   vm_ref = vbd_rec['VM'] 
   vm_rec = _db_content['VM'][vm_ref] 
   vm_rec['VBDs'].append(vbd_ref) 
   vdi_ref = vbd_rec['VDI'] 
   vdi_rec = _db_content['VDI'][vdi_ref] 
   vdi_rec['VBDs'].append(vbd_ref) 
   vm_name_label = _db_content['VM'][vm_ref]['name_label'] 
   vbd_rec['vm_name_label'] = vm_name_label"," 'Called after a VBD is created. 
 Updates the VM and VDI records.'","'Create read-only fields and backref from VM and VDI to VBD when VBD 
 is created.'"
"def is_unit(xblock, parent_xblock=None): 
    if (xblock.category == 'vertical'): 
      if (parent_xblock is None): 
         parent_xblock = get_parent_xblock(xblock) 
      parent_category = (parent_xblock.category if parent_xblock else None) 
      return (parent_category == 'sequential') 
   return False", 'Check if xblock is a unit block.',"'Returns true if the specified xblock is a vertical that is treated as a unit. 
 A unit is a vertical that is a direct child of a sequential (aka a subsection).'"
"def bus_routes(): 
    return wechat.response_news(app.config['BUS_ROUTES_NEWS'])", 'Return the bus routes news.',''
"def identify_format(origin, data_class_required, path, fileobj, args, kwargs): 
    valid_formats = [] 
   for (data_format, data_class) in _identifiers: 
      if _is_best_match(data_class_required, data_class, _identifiers): 
         if _identifiers[(data_format, data_class)](origin, path, fileobj, *args, **kwargs): 
            valid_formats.append(data_format) 
   return valid_formats"," 'Return a list of valid formats for the given origin, data class and path 
 :param origin: the origin of the file 
 :param data_class_required: the data class required by the origin 
 :param path: the path of the file 
 :param fileobj: the file object 
 :param args: the arguments 
 :param kwargs: the keyword arguments'","'Loop through identifiers to see which formats match. 
 Parameters 
 origin : str 
 A string ``""read`` or ``""write""`` identifying whether the file is to be 
 opened for reading or writing. 
 data_class_required : object 
 The specified class for the result of `read` or the class that is to be 
 written. 
 path : str, other path object or None 
 The path to the file or None. 
 fileobj : File object or None. 
 An open file object to read the file\'s contents, or ``None`` if the 
 file could not be opened. 
 args : sequence 
 Positional arguments for the `read` or `write` function. Note that 
 these must be provided as sequence. 
 kwargs : dict-like 
 Keyword arguments for the `read` or `write` function. Note that this 
 parameter must be `dict`-like. 
 Returns 
 valid_formats : list 
 List of matching formats.'"
"def get_messages_from_page(name): 
    return _get_messages_from_page_or_report(u'Page', name)"," 'Returns a list of messages from a page. 
 :param name: name of the page 
 :type name: str 
 :return: list of messages from the page 
 :rtype: list'",'Returns all translatable strings from a :class:`frappe.core.doctype.Page`'
"def render(template, **kwargs): 
    try: 
      return _env.get_template(template).render(**kwargs) 
   except jinja2.exceptions.UndefinedError: 
      log.misc.exception(('UndefinedError   while   rendering   ' + template)) 
      err_path = os.path.join('html', 'undef_error.html') 
      err_template = utils.read_file(err_path) 
      tb = traceback.format_exc() 
      return err_template.format(pagename=template, traceback=tb)"," 'Render a template with the given parameters. 
 :param template: the template to render 
 :param kwargs: parameters to pass to the template'",'Render the given template and pass the given arguments to it.'
"def assert_no_element_by_id(context, _id, wait_time=MAX_WAIT_FOR_UNEXPECTED_ELEMENT): 
    _assert_no_element_by(context, By.ID, _id, wait_time)"," 'Asserts that no element with the given ID is present in the context. 
 :param context: A browser context. 
 :param str _id: The ID of the element to be asserted. 
 :param wait_time: The time to wait for the element to be removed. 
 :raise AssertionError: If the element with the given ID is present in the context. 
 :raise TimeoutError: If the element with the given ID is not removed in the given time.'","'Assert that no element is found. Use a wait in case the element currently exists 
 on the page, and we want to wait for it to disappear before doing the assert. 
 Finds the element using an id.'"
"def extra_padding_y_keep_ratio(original_size, padding): 
    return _resize(original_size, 1, padding=padding, keep_aspect_ratio=True)"," 'Returns the new image size with extra padding. 
 :param original_size: The original image size. 
 :param padding: The padding size. 
 :return: The new image size. 
 :rtype: tuple'","'Reduce the height of `original_size` by `padding`, maintaining the aspect 
 ratio.'"
"def GenerateEnvironmentFiles(toplevel_build_dir, generator_flags, system_includes, open_out): 
    archs = ('x86', 'x64') 
   if generator_flags.get('ninja_use_custom_environment_files', 0): 
      cl_paths = {} 
      for arch in archs: 
         cl_paths[arch] = 'cl.exe' 
      return cl_paths 
   vs = GetVSVersion(generator_flags) 
   cl_paths = {} 
   for arch in archs: 
      args = vs.SetupScript(arch) 
      args.extend(('&&', 'set')) 
      popen = subprocess.Popen(args, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) 
      (variables, _) = popen.communicate() 
      env = _ExtractImportantEnvironment(variables) 
      if system_includes: 
         system_includes = (system_includes | OrderedSet(env.get('INCLUDE', '').split(';'))) 
         env['INCLUDE'] = ';'.join(system_includes) 
      env_block = _FormatAsEnvironmentBlock(env) 
      f = open_out(os.path.join(toplevel_build_dir, ('environment.' + arch)), 'wb') 
      f.write(env_block) 
      f.close() 
      args = vs.SetupScript(arch) 
      args.extend(('&&', 'for', '%i', 'in', '(cl.exe)', 'do', '@echo', 'LOC:%~$PATH:i')) 
      popen = subprocess.Popen(args, shell=True, stdout=subprocess.PIPE) 
      (output, _) = popen.communicate() 
      cl_paths[arch] = _ExtractCLPath(output) 
   return cl_paths"," 'Generate environment files for the given generator flags. 
 :param toplevel_build_dir: The directory to write the environment files to. 
 :param generator_flags: The generator flags. 
 :param system_includes: A list of system include directories to include. 
 :param open_out: The file to write the environment files to.'","'It\'s not sufficient to have the absolute path to the compiler, linker, 
 etc. on Windows, as those tools rely on .dlls being in the PATH. We also 
 need to support both x86 and x64 compilers within the same build (to support 
 msvs_target_platform hackery). Different architectures require a different 
 compiler binary, and different supporting environment variables (INCLUDE, 
 LIB, LIBPATH). So, we extract the environment here, wrap all invocations 
 of compiler tools (cl, link, lib, rc, midl, etc.) via win_tool.py which 
 sets up the environment, and then we do not prefix the compiler with 
 an absolute path, instead preferring something like ""cl.exe"" in the rule 
 which will then run whichever the environment setup has put in the path. 
 When the following procedure to generate environment files does not 
 meet your requirement (e.g. for custom toolchains), you can pass 
 ""-G ninja_use_custom_environment_files"" to the gyp to suppress file 
 generation and use custom environment files prepared by yourself.'"
"def find_dest_path_comp_key(files, src_path=None): 
    src = files['src'] 
   dest = files['dest'] 
   src_type = src['type'] 
   dest_type = dest['type'] 
   if (src_path is None): 
      src_path = src['path'] 
   sep_table = {'s3': '/', 'local': os.sep} 
   if files['dir_op']: 
      rel_path = src_path[len(src['path']):] 
   else: 
      rel_path = src_path.split(sep_table[src_type])[(-1)] 
   compare_key = rel_path.replace(sep_table[src_type], '/') 
   if files['use_src_name']: 
      dest_path = dest['path'] 
      dest_path += rel_path.replace(sep_table[src_type], sep_table[dest_type]) 
   else: 
      dest_path = dest['path'] 
   return (dest_path, compare_key)"," 'Return destination path and compare key. 
 :param files: 
 :param src_path: 
 :return: 
 :rtype: 
 :raises: 
 :example: 
 >>> from fabric.operations import find_dest_path_comp_key'","'This is a helper function that determines the destination path and compare 
 key given parameters received from the ``FileFormat`` class.'"
"def get_long_description(): 
    return 'Mangle   plugins   modify   requests   on   the   fly.\n            '", 'Get the long description',':return: The description for the plugin type.'
"@flake8ext 
 def use_jsonutils(logical_line, filename): 
    msg = 'N321:   jsonutils.%(fun)s   must   be   used   instead   of   json.%(fun)s' 
   json_check_skipped_patterns = ['neutron/plugins/ml2/drivers/openvswitch/agent/xenapi/etc/xapi.d/plugins/netwrap'] 
   for pattern in json_check_skipped_patterns: 
      if (pattern in filename): 
         return 
   if ('json.' in logical_line): 
      json_funcs = ['dumps(', 'dump(', 'loads(', 'load('] 
      for f in json_funcs: 
         pos = logical_line.find(('json.%s' % f)) 
         if (pos != (-1)): 
            (yield (pos, (msg % {'fun': f[:(-1)]})))"," 'JSON utils should be used instead of json. 
 This check skips files in the neutron/plugins/ml2/drivers/openvswitch/agent/xenapi/etc/xapi.d/plugins/netwrap 
 directory.'",'N321 - Use jsonutils instead of json.'
"def get_all_vlanids(): 
    LOG.debug(_('get_all_vlanids()   called')) 
   session = db.get_session() 
   try: 
      vlanids = session.query(network_models_v2.VlanID).all() 
      return vlanids 
   except exc.NoResultFound: 
      return []", 'Get all vlanids.','Gets all the vlanids'
"def get_config(config_file, repo, ctx, dir): 
    config_file = basic_util.strip_path(config_file) 
   for changeset in reversed_upper_bounded_changelog(repo, ctx): 
      changeset_ctx = repo.changectx(changeset) 
      for ctx_file in changeset_ctx.files(): 
         ctx_file_name = basic_util.strip_path(ctx_file) 
         if (ctx_file_name == config_file): 
            return get_named_tmpfile_from_ctx(changeset_ctx, ctx_file, dir) 
   return None"," 'Returns the config file from the given changeset. 
 :param config_file: path to the config file 
 :param repo: git repository 
 :param ctx: context to use 
 :param dir: directory to use 
 :return: path to the config file or None'",'Return the latest version of config_filename from the repository manifest.'
"@masterserviceHandle 
 def serverStop(): 
    log.msg('stop') 
   if GlobalObject().stophandler: 
      GlobalObject().stophandler() 
   reactor.callLater(0.5, reactor.stop) 
   return True", 'stop the master server','ä¾masterè°ç¨çæ¥å£ï¼å³é­æå¡å¨'
"def consume_length_prefix(rlp, start): 
    b0 = safe_ord(rlp[start]) 
   if (b0 < 128): 
      return (str, 1, start) 
   elif (b0 < (128 + 56)): 
      return (str, (b0 - 128), (start + 1)) 
   elif (b0 < 192): 
      ll = (((b0 - 128) - 56) + 1) 
      l = big_endian_to_int(rlp[(start + 1):((start + 1) + ll)]) 
      return (str, l, ((start + 1) + ll)) 
   elif (b0 < (192 + 56)): 
      return (list, (b0 - 192), (start + 1)) 
   else: 
      ll = (((b0 - 192) - 56) + 1) 
      l = big_endian_to_int(rlp[(start + 1):((start + 1) + ll)]) 
      return (list, l, ((start + 1) + ll))"," 'Consume a length prefix and return the length and the start of the payload. 
 :param rlp: the RLP encoded data 
 :param start: the start of the payload 
 :return: the length of the payload and the start of the payload 
 :rtype: tuple'","'Read a length prefix from an RLP string. 
 :param rlp: the rlp string to read from 
 :param start: the position at which to start reading 
 :returns: a tuple ``(type, length, end)``, where ``type`` is either ``str`` 
 or ``list`` depending on the type of the following payload, 
 ``length`` is the length of the payload in bytes, and ``end`` is 
 the position of the first payload byte in the rlp string'"
"def addAssemblyCage(derivation, negatives, positives): 
    addCageGroove(derivation, negatives, positives) 
   for pegCenterX in derivation.pegCenterXs: 
      addPositivePeg(derivation, positives, pegCenterX, (- derivation.pegY)) 
      addPositivePeg(derivation, positives, pegCenterX, derivation.pegY) 
   translate.translateNegativesPositives(negatives, positives, Vector3(0.0, (- derivation.halfSeparationWidth))) 
   femaleNegatives = [] 
   femalePositives = [] 
   addCageGroove(derivation, femaleNegatives, femalePositives) 
   for pegCenterX in derivation.pegCenterXs: 
      addNegativePeg(derivation, femaleNegatives, pegCenterX, (- derivation.pegY)) 
      addNegativePeg(derivation, femaleNegatives, pegCenterX, derivation.pegY) 
   translate.translateNegativesPositives(femaleNegatives, femalePositives, Vector3(0.0, derivation.halfSeparationWidth)) 
   negatives += femaleNegatives 
   positives += femalePositives"," 'Adds a cage to an assembly. 
 Parameters 
 derivation : AssemblyDerivation 
 The derivation of the assembly. 
 negatives : list of Point 
 The negative pegs of the assembly. 
 positives : list of Point 
 The positive pegs of the assembly. 
 Returns 
 negatives : list of Point 
 The negative pegs of the assembly after adding the cage. 
 positives : list of Point 
 The positive pegs of the assembly after adding the cage. 
 Examples 
 >>> from assembly_cage import AssemblyCage, addAssemblyCage 
 >>> from geometry_msgs.msg import Point 
 >>> from geometry_msgs.msg import Vector3 
 >>> from geometry_msgs.msg import Pose 
 >>> from geometry_msgs.msg import Quaternion 
 >>> from geometry_msgs.msg import PoseStamped 
 >>> from geometry_msgs.msg import Wrench 
 >>> from std_msgs.msg import Float64 
 >>> from rviz_msgs.msg import AssemblyCage 
 >>>",'Add assembly linear bearing cage.'
"@treeio_login_required 
 @handle_response_format 
 def order_invoice_view(request, order_id, response_format='html'): 
    order = get_object_or_404(SaleOrder, pk=order_id) 
   if ((not request.user.profile.has_permission(order)) and (not request.user.profile.is_admin('treeio.sales'))): 
      return user_denied(request, message=""You   don't   have   access   to   this   Sale"") 
   ordered_products = order.orderedproduct_set.filter(trash=False) 
   try: 
      conf = ModuleSetting.get_for_module('treeio.finance', 'my_company')[0] 
      my_company = Contact.objects.get(pk=long(conf.value)) 
   except: 
      my_company = None 
   return render_to_response('sales/order_invoice_view', {'order': order, 'ordered_products': ordered_products, 'my_company': my_company}, context_instance=RequestContext(request), response_format=response_format)"," 'View the invoice for a sale order. 
 :param request: The request object. 
 :param order_id: The id of the order to view. 
 :param response_format: The response format to use. 
 :return: A response in the given format.'",'Order view as Invoice'
"def getNewRepository(): 
    return ExportRepository()", 'Returns a new repository object for the current export.','Get the repository constructor.'
"def json_underscore(body, charset='utf-8', **kwargs): 
    return _underscore_dict(json(body, charset=charset))", 'Return a dictionary with all keys in the form of \'_\' + the original key.',"'Converts JSON formatted date to native Python objects. 
 The keys in any JSON dict are transformed from camelcase to underscore separated words.'"
"def git_hook(): 
    (_, files_modified, _) = run('git   diff-index   --cached   --name-only   HEAD') 
   options = parse_options() 
   setup_logger(options) 
   candidates = list(map(str, files_modified)) 
   if candidates: 
      process_paths(options, candidates=candidates)", 'Runs git diff-index --cached --name-only HEAD','Run pylama after git commit.'
"def create_mgr(descr, item_shape=None): 
    if (item_shape is None): 
      item_shape = (N,) 
   offset = 0 
   mgr_items = [] 
   block_placements = OrderedDict() 
   for d in descr.split(';'): 
      d = d.strip() 
      if (not len(d)): 
         continue 
      (names, blockstr) = d.partition(':')[::2] 
      blockstr = blockstr.strip() 
      names = names.strip().split(',') 
      mgr_items.extend(names) 
      placement = list((np.arange(len(names)) + offset)) 
      try: 
         block_placements[blockstr].extend(placement) 
      except KeyError: 
         block_placements[blockstr] = placement 
      offset += len(names) 
   mgr_items = Index(mgr_items) 
   blocks = [] 
   num_offset = 0 
   for (blockstr, placement) in block_placements.items(): 
      typestr = blockstr.split('-')[0] 
      blocks.append(create_block(typestr, placement, item_shape=item_shape, num_offset=num_offset)) 
      num_offset += len(placement) 
   return BlockManager(sorted(blocks, key=(lambda b: b.mgr_locs[0])), ([mgr_items] + [np.arange(n) for n in item_shape]))"," 'Create a BlockManager for a given description string. 
 Parameters 
 descr : str 
 The description string to be parsed. 
 item_shape : tuple 
 The shape of the items in the BlockManager. 
 Returns 
 BlockManager 
 The BlockManager.'","'Construct BlockManager from string description. 
 String description syntax looks similar to np.matrix initializer.  It looks 
 like this:: 
 a,b,c: f8; d,e,f: i8 
 Rules are rather simple: 
 * see list of supported datatypes in `create_block` method 
 * components are semicolon-separated 
 * each component is `NAME,NAME,NAME: DTYPE_ID` 
 * whitespace around colons & semicolons are removed 
 * components with same DTYPE_ID are combined into single block 
 * to force multiple blocks with same dtype, use \'-SUFFIX\':: 
 \'a:f8-1; b:f8-2; c:f8-foobar\''"
"def role_delete(role_id=None, name=None, profile=None, **connection_args): 
    kstone = auth(profile, **connection_args) 
   if name: 
      for role in kstone.roles.list(): 
         if (role.name == name): 
            role_id = role.id 
            break 
   if (not role_id): 
      return {'Error': 'Unable   to   resolve   role   id'} 
   role = kstone.roles.get(role_id) 
   kstone.roles.delete(role) 
   ret = 'Role   ID   {0}   deleted'.format(role_id) 
   if name: 
      ret += '   ({0})'.format(name) 
   return ret"," 'Delete a role. 
 .. versionadded:: 2.0.0 
 :param role_id: The role ID to delete 
 :type role_id: str 
 :param name: The name of the role to delete 
 :type name: str 
 :param profile: The profile to use to authenticate 
 :type profile: :class:`~knox.profile.Profile` 
 :param connection_args: Connection arguments 
 :type connection_args: dict 
 :returns: A dictionary with the status of the operation'","'Delete a role (keystone role-delete) 
 CLI Examples: 
 .. code-block:: bash 
 salt \'*\' keystone.role_delete c965f79c4f864eaaa9c3b41904e67082 
 salt \'*\' keystone.role_delete role_id=c965f79c4f864eaaa9c3b41904e67082 
 salt \'*\' keystone.role_delete name=admin'"
"def _needs_eeg_average_ref_proj(info): 
    eeg_sel = pick_types(info, meg=False, eeg=True, ref_meg=False, exclude='bads') 
   return ((len(eeg_sel) > 0) and (not info['custom_ref_applied']) and (not _has_eeg_average_ref_proj(info['projs'])))", 'Check if the EEG average reference projection needs to be applied.',"'Determine if the EEG needs an averge EEG reference. 
 This returns True if no custom reference has been applied and no average 
 reference projection is present in the list of projections.'"
"@frappe.whitelist() 
 def add_tag(tag, dt, dn, color=None): 
    DocTags(dt).add(dn, tag) 
   return tag"," 'Adds a tag to the document. 
 :param tag: Tag name 
 :param dt: Document name 
 :param dn: Document number 
 :param color: Color of the tag (defaults to blue)'","'adds a new tag to a record, and creates the Tag master'"
"def CreateBudget(client): 
    budget_service = client.GetService('BudgetService', version='v201609') 
   budget = {'name': ('Interplanetary   Cruise   App   Budget   #%s' % uuid.uuid4()), 'amount': {'microAmount': '50000000'}, 'deliveryMethod': 'STANDARD', 'isExplicitlyShared': False} 
   budget_operations = [{'operator': 'ADD', 'operand': budget}] 
   budget_id = budget_service.mutate(budget_operations)['value'][0]['budgetId'] 
   return budget_id"," 'Creates a budget for the application. 
 :param client: The Google Adwords API client. 
 :returns: The budget ID.'","'Creates a budget and returns its budgetId. 
 Args: 
 client: An AdWordsClient instance. 
 Returns: 
 An int budgetId for the created Budget.'"
"def flatten(class_dict): 
    for (cls, fields_list) in class_dict.items(): 
      for fields in fields_list: 
         (yield (cls, fields))"," 'Flatten a dictionary of classes to a list of tuples. 
 Each tuple contains the class name and the list of fields 
 for that class.'","'Flatten a dict from cls -> [fields, ...] and yields values of the form (cls, fields) 
 for each entry in the dictionary value.'"
"def _nova_to_osvif_network(network): 
    netobj = objects.network.Network(id=network['id'], bridge_interface=network.get_meta('bridge_interface'), subnets=_nova_to_osvif_subnets(network['subnets'])) 
   if (network['bridge'] is not None): 
      netobj.bridge = network['bridge'] 
   if (network['label'] is not None): 
      netobj.label = network['label'] 
   if (network.get_meta('mtu') is not None): 
      netobj.mtu = network.get_meta('mtu') 
   if (network.get_meta('multi_host') is not None): 
      netobj.multi_host = network.get_meta('multi_host') 
   if (network.get_meta('should_create_bridge') is not None): 
      netobj.should_provide_bridge = network.get_meta('should_create_bridge') 
   if (network.get_meta('should_create_vlan') is not None): 
      netobj.should_provide_vlan = network.get_meta('should_create_vlan') 
      if (network.get_meta('vlan') is None): 
         raise exception.NovaException((_('Missing   vlan   number   in   %s') % network)) 
      netobj.vlan = network.get_meta('vlan') 
   return netobj", 'Convert a nova network to os-vif network.',"'Convert Nova network object into os_vif object 
 :param network: nova.network.model.Network instance 
 :returns: os_vif.objects.network.Network instance'"
"def norm_constraint(tensor_var, max_norm, norm_axes=None, epsilon=1e-07): 
    ndim = tensor_var.ndim 
   if (norm_axes is not None): 
      sum_over = tuple(norm_axes) 
   elif (ndim == 2): 
      sum_over = (0,) 
   elif (ndim in [3, 4, 5]): 
      sum_over = tuple(range(1, ndim)) 
   else: 
      raise ValueError('Unsupported   tensor   dimensionality   {}.Must   specify   `norm_axes`'.format(ndim)) 
   dtype = np.dtype(theano.config.floatX).type 
   norms = T.sqrt(T.sum(T.sqr(tensor_var), axis=sum_over, keepdims=True)) 
   target_norms = T.clip(norms, 0, dtype(max_norm)) 
   constrained_output = (tensor_var * (target_norms / (dtype(epsilon) + norms))) 
   return constrained_output"," 'Constrain the norm of a tensor to a maximum value. 
 Parameters 
 tensor_var : tensor 
 The tensor to constrain. 
 max_norm : scalar 
 The maximum norm. 
 norm_axes : tuple of ints 
 The axes to constrain. 
 epsilon : scalar 
 The epsilon used for numerical stability. 
 Returns 
 constrained_output : tensor 
 The constrained output.'","'Max weight norm constraints and gradient clipping 
 This takes a TensorVariable and rescales it so that incoming weight 
 norms are below a specified constraint value. Vectors violating the 
 constraint are rescaled so that they are within the allowed range. 
 Parameters 
 tensor_var : TensorVariable 
 Theano expression for update, gradient, or other quantity. 
 max_norm : scalar 
 This value sets the maximum allowed value of any norm in 
 `tensor_var`. 
 norm_axes : sequence (list or tuple) 
 The axes over which to compute the norm.  This overrides the 
 default norm axes defined for the number of dimensions 
 in `tensor_var`. When this is not specified and `tensor_var` is a 
 matrix (2D), this is set to `(0,)`. If `tensor_var` is a 3D, 4D or 
 5D tensor, it is set to a tuple listing all axes but axis 0. The 
 former default is useful for working with dense layers, the latter 
 is useful for 1D, 2D and 3D convolutional layers. 
 (Optional) 
 epsilon : scalar, optional 
 Value used to prevent numerical instability when dividing by 
 very small or zero norms. 
 Returns 
 TensorVariable 
 Input `tensor_var` with rescaling applied to weight vectors 
 that violate the specified constraints. 
 Examples 
 >>> param = theano.shared( 
 ...     np.random.randn(100, 200).astype(theano.config.floatX)) 
 >>> update = param + 100 
 >>> update = norm_constraint(update, 10) 
 >>> func = theano.function([], [], updates=[(param, update)]) 
 >>> # Apply constrained update 
 >>> _ = func() 
 >>> from lasagne.utils import compute_norms 
 >>> norms = compute_norms(param.get_value()) 
 >>> np.isclose(np.max(norms), 10) 
 True 
 Notes 
 When `norm_axes` is not specified, the axes over which the norm is 
 computed depend on the dimensionality of the input variable. If it is 
 2D, it is assumed to come from a dense layer, and the norm is computed 
 over axis 0. If it is 3D, 4D or 5D, it is assumed to come from a 
 convolutional layer and the norm is computed over all trailing axes 
 beyond axis 0. For other uses, you should explicitly specify the axes 
 over which to compute the norm using `norm_axes`.'"
"def delete_disk(kwargs=None, call=None): 
    if (call != 'function'): 
      raise SaltCloudSystemExit('The   delete_disk   function   must   be   called   with   -f   or   --function.') 
   if ((not kwargs) or ('disk_name' not in kwargs)): 
      log.error('A   disk_name   must   be   specified   when   deleting   a   disk.') 
      return False 
   conn = get_conn() 
   disk = conn.ex_get_volume(kwargs.get('disk_name')) 
   __utils__['cloud.fire_event']('event', 'delete   disk', 'salt/cloud/disk/deleting', args={'name': disk.name, 'location': disk.extra['zone'].name, 'size': disk.size}, sock_dir=__opts__['sock_dir'], transport=__opts__['transport']) 
   try: 
      result = conn.destroy_volume(disk) 
   except ResourceInUseError as exc: 
      log.error('Disk   {0}   is   in   use   and   must   be   detached   before   deleting.\nThe   following   exception   was   thrown   by   libcloud:\n{1}'.format(disk.name, exc), exc_info_on_loglevel=logging.DEBUG) 
      return False 
   __utils__['cloud.fire_event']('event', 'deleted   disk', 'salt/cloud/disk/deleted', args={'name': disk.name, 'location': disk.extra['zone'].name, 'size': disk.size}, sock_dir=__opts__['sock_dir'], transport=__opts__['transport']) 
   return result", 'Delete a disk.',"'Permanently delete a persistent disk. 
 CLI Example: 
 .. code-block:: bash 
 salt-cloud -f delete_disk gce disk_name=pd'"
"def _create_wx_app(): 
    wxapp = wx.GetApp() 
   if (wxapp is None): 
      wxapp = wx.PySimpleApp() 
      wxapp.SetExitOnFrameDelete(True) 
      _create_wx_app.theWxApp = wxapp", 'Creates a wx.App object if one does not already exist.','Creates a wx.PySimpleApp instance if a wx.App has not been created.'
"def externals_finder(dirname, filename): 
    found = False 
   f = open(filename, 'rt') 
   for line in iter(f.readline, ''): 
      parts = line.split() 
      if (len(parts) == 2): 
         (kind, length) = parts 
         data = f.read(int(length)) 
         if ((kind == 'K') and (data == 'svn:externals')): 
            found = True 
         elif ((kind == 'V') and found): 
            f.close() 
            break 
   else: 
      f.close() 
      return 
   for line in data.splitlines(): 
      parts = line.split() 
      if parts: 
         (yield joinpath(dirname, parts[0]))"," 'Returns the external files from a Subversion repository. 
 :param dirname: the directory where the repository is located. 
 :param filename: the name of the file to read. 
 :returns: a generator of paths to the external files. 
 :rtype: generator[str]'",'Find any \'svn:externals\' directories'
"def quiet_close(closable): 
    try: 
      closable.close() 
   except Exception: 
      logger.debug(u'Exception   while   closing', exc_info=True)", 'Close closable without printing any warnings or exceptions.',"'Quietly closes a closable object without throwing an exception. 
 :param closable: Object with a ``close`` method.'"
"def add_bbox_regression_targets(roidb): 
    assert (len(roidb) > 0) 
   assert ('max_classes' in roidb[0]), 'Did   you   call   prepare_roidb   first?' 
   num_images = len(roidb) 
   num_classes = roidb[0]['gt_overlaps'].shape[1] 
   for im_i in xrange(num_images): 
      rois = roidb[im_i]['boxes'] 
      max_overlaps = roidb[im_i]['max_overlaps'] 
      max_classes = roidb[im_i]['max_classes'] 
      roidb[im_i]['bbox_targets'] = _compute_targets(rois, max_overlaps, max_classes) 
   if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED: 
      means = np.tile(np.array(cfg.TRAIN.BBOX_NORMALIZE_MEANS), (num_classes, 1)) 
      stds = np.tile(np.array(cfg.TRAIN.BBOX_NORMALIZE_STDS), (num_classes, 1)) 
   else: 
      class_counts = (np.zeros((num_classes, 1)) + cfg.EPS) 
      sums = np.zeros((num_classes, 4)) 
      squared_sums = np.zeros((num_classes, 4)) 
      for im_i in xrange(num_images): 
         targets = roidb[im_i]['bbox_targets'] 
         for cls in xrange(1, num_classes): 
            cls_inds = np.where((targets[:, 0] == cls))[0] 
            if (cls_inds.size > 0): 
               class_counts[cls] += cls_inds.size 
               sums[cls, :] += targets[cls_inds, 1:].sum(axis=0) 
               squared_sums[cls, :] += (targets[cls_inds, 1:] ** 2).sum(axis=0) 
      means = (sums / class_counts) 
      stds = np.sqrt(((squared_sums / class_counts) - (means ** 2))) 
   print 'bbox   target   means:' 
   print means 
   print means[1:, :].mean(axis=0) 
   print 'bbox   target   stdevs:' 
   print stds 
   print stds[1:, :].mean(axis=0) 
   if cfg.TRAIN.BBOX_NORMALIZE_TARGETS: 
      print 'Normalizing   targets' 
      for im_i in xrange(num_images): 
         targets = roidb[im_i]['bbox_targets'] 
         for cls in xrange(1, num_classes): 
            cls_inds = np.where((targets[:, 0] == cls))[0] 
            roidb[im_i]['bbox_targets'][cls_inds, 1:] -= means[cls, :] 
            roidb[im_i]['bbox_targets'][cls_inds, 1:] /= stds[cls, :] 
   else: 
      print 'NOT   normalizing   targets' 
   return (means.ravel(), stds.ravel())"," 'Compute bbox regression targets from ground truths. 
 These are used to compute the regression loss. 
 Returns: 
 means: the mean of each class 
 stdev: the standard deviation of each class 
 Notes: 
 The regression loss is computed as: 
 loss = (sum((targets - means) ** 2) / sum(stdev ** 2)) 
 where targets is the regression target for each class. 
 The regression targets are computed by summing the targets for each 
 ground truth bounding box. 
 This function also computes the means and stdevs of the regression 
 targets. 
 This function is called by the trainer. 
 It is assumed that the `prepare_roidb` function has been called 
 previously.'",'Add information needed to train bounding-box regressors.'
"def is_installed(pkg_name): 
    with settings(warn_only=True): 
      res = run(('pkg_info   -e   %s' % pkg_name)) 
      return (res.succeeded is True)", 'Check if package is installed.','Check if a package is installed.'
"def _cg(A, b, x0=None, tol=1e-10, maxiter=1000): 
    n = b.size 
   assert (A.n == n) 
   assert (A.m == n) 
   b_norm = np.linalg.norm(b) 
   kvec = A.diag 
   kvec = np.where((kvec > 1e-06), kvec, 1e-06) 
   if (x0 is None): 
      x = np.zeros(n) 
   else: 
      x = x0 
   r = (b - A.dot(x)) 
   w = (r / kvec) 
   p = np.zeros(n) 
   beta = 0.0 
   rho = np.dot(r, w) 
   k = 0 
   while ((np.sqrt(abs(rho)) > (tol * b_norm)) and (k < maxiter)): 
      p = (w + (beta * p)) 
      z = A.dot(p) 
      alpha = (rho / np.dot(p, z)) 
      r = (r - (alpha * z)) 
      w = (r / kvec) 
      rhoold = rho 
      rho = np.dot(r, w) 
      x = (x + (alpha * p)) 
      beta = (rho / rhoold) 
      k += 1 
   err = np.linalg.norm((A.dot(x) - b)) 
   return (x, err)"," 'Conjugate gradient method. 
 Parameters 
 A : sparse matrix 
 b : vector 
 x0 : vector, initial guess 
 tol : tolerance 
 maxiter : maximum number of iterations 
 Returns 
 x, err : vector, solution and error'","'Use Preconditioned Conjugate Gradient iteration to solve A x = b 
 A simple Jacobi (diagonal) preconditionner is used. 
 Parameters 
 A: _Sparse_Matrix_coo 
 *A* must have been compressed before by compress_csc or 
 compress_csr method. 
 b: array 
 Right hand side of the linear system. 
 Returns 
 x: array. 
 The converged solution. 
 err: float 
 The absolute error np.linalg.norm(A.dot(x) - b) 
 Other parameters 
 x0: array. 
 Starting guess for the solution. 
 tol: float. 
 Tolerance to achieve. The algorithm terminates when the relative 
 residual is below tol. 
 maxiter: integer. 
 Maximum number of iterations. Iteration will stop 
 after maxiter steps even if the specified tolerance has not 
 been achieved.'"
"def contracted_edge(G, edge, self_loops=True): 
    if (not G.has_edge(*edge)): 
      raise ValueError('Edge   {0}   does   not   exist   in   graph   G;   cannot   contract   it'.format(edge)) 
   return contracted_nodes(G, self_loops=self_loops, *edge)"," 'Contracts a single edge. 
 Parameters 
 G : A Graph 
 The graph to contract the edge in. 
 edge : A tuple 
 The tuple of nodes that make up the edge. 
 self_loops : Boolean 
 Whether to allow self loops. 
 Returns 
 A tuple 
 The tuple of nodes that make up the contracted edge.'","'Returns the graph that results from contracting the specified edge. 
 Edge contraction identifies the two endpoints of the edge as a single node 
 incident to any edge that was incident to the original two nodes. A graph 
 that results from edge contraction is called a *minor* of the original 
 graph. 
 Parameters 
 G : NetworkX graph 
 The graph whose edge will be contracted. 
 edge : tuple 
 Must be a pair of nodes in `G`. 
 self_loops : Boolean 
 If this is True, any edges (including `edge`) joining the 
 endpoints of `edge` in `G` become self-loops on the new node in the 
 returned graph. 
 Returns 
 Networkx graph 
 A new graph object of the same type as `G` (leaving `G` unmodified) 
 with endpoints of `edge` identified in a single node. The right node 
 of `edge` will be merged into the left one, so only the left one will 
 appear in the returned graph. 
 Raises 
 ValueError 
 If `edge` is not an edge in `G`. 
 Examples 
 Attempting to contract two nonadjacent nodes yields an error:: 
 >>> import networkx as nx 
 >>> G = nx.cycle_graph(4) 
 >>> nx.contracted_edge(G, (1, 3)) 
 Traceback (most recent call last): 
 ValueError: Edge (1, 3) does not exist in graph G; cannot contract it 
 Contracting two adjacent nodes in the cycle graph on *n* nodes yields the 
 cycle graph on *n - 1* nodes:: 
 >>> import networkx as nx 
 >>> C5 = nx.cycle_graph(5) 
 >>> C4 = nx.cycle_graph(4) 
 >>> M = nx.contracted_edge(C5, (0, 1), self_loops=False) 
 >>> nx.is_isomorphic(M, C4) 
 True 
 See also 
 contracted_nodes 
 quotient_graph'"
"def test_wheel_compiles_pyc(script, data): 
    script.pip('install', '--compile', 'simple.dist==0.1', '--no-index', ('--find-links=' + data.find_links)) 
   exists = [os.path.exists((script.site_packages_path / 'simpledist/__init__.pyc'))] 
   exists += glob.glob((script.site_packages_path / 'simpledist/__pycache__/__init__*.pyc')) 
   assert any(exists)", 'Test that a wheel installs the __init__.pyc file','Test installing from wheel with --compile on'
"def base_vectors(n): 
    n = (n / np.sqrt(np.square(n).sum(axis=(-1)))) 
   if (abs(n[0]) == 1): 
      l = np.r_[(n[2], 0, (- n[0]))] 
   else: 
      l = np.r_[(0, n[2], (- n[1]))] 
   l = (l / np.sqrt(np.square(l).sum(axis=(-1)))) 
   m = np.cross(n, l) 
   return (n, l, m)"," 'Returns the base vectors for the n-dimensional space. 
 The vectors are returned in the order (n, l, m). 
 Parameters 
 n : array 
 The n-dimensional space. 
 Returns 
 n : array 
 The n-dimensional space. 
 l : array 
 The base vector. 
 m : array 
 The cross product of the base vectors. 
 Examples 
 >>> from sympy.physics.vector import base_vectors 
 >>> base_vectors(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])) 
 (array([1, 0, 0]), array([0, 1, 0]), array([0, 0, 1]))'","'Returns 3 orthognal base vectors, the first one colinear to n.'"
"def DateFromTicks(ticks): 
    return Date(*time.localtime(ticks)[:3])", 'Returns a date from a ticks value',"'Construct an object holding a date value from the given ticks value 
 (number of seconds since the epoch). 
 This function is part of the `DBAPI 2.0 specification 
 <http://www.python.org/dev/peps/pep-0249/>`_. 
 :rtype: :class:`datetime.date`'"
"def instance_tag_delete_all(context, instance_uuid): 
    return IMPL.instance_tag_delete_all(context, instance_uuid)", 'Delete all tags for an instance.','Delete all tags from the instance.'
"def invalidate_star_import_cache(path): 
    try: 
      parser_cache_item = parser_cache[path] 
   except KeyError: 
      pass 
   else: 
      _invalidate_star_import_cache_module(parser_cache_item.parser.module)", 'Invalidate the import cache for a module at path.','On success returns True.'
"def coverage(fn): 
    fp = TraceFuncCoverage(fn) 
   def new_fn(*args, **kw): 
      return fp(*args, **kw) 
   new_fn.__doc__ = fn.__doc__ 
   new_fn.__name__ = fn.__name__ 
   new_fn.__dict__ = fn.__dict__ 
   new_fn.__module__ = fn.__module__ 
   return new_fn"," 'A decorator that replaces the function with a trace-based version. 
 The decorated function is called with the same arguments and returns the 
 same value. 
 The decorated function is called before the original function. 
 The decorated function is called after the original function. 
 The decorated function is called after the original function has been 
 called. 
 The decorated function is called after the original function has been 
 called. 
 The decorated function is called after the original function has been 
 called. 
 The decorated function is called after the original function has been 
 called. 
 The decorated function is called after the original function has been 
 called. 
 The decorated function is called after the original function has been 
 called. 
 The decorated function is called after the original function has been 
 called. 
 The decorated function is called after the original function has been 
 called. 
 The decorated function is called after the original function has been 
 called. 
 The decorated function is called after the original function has been 
 called. 
 The decorated function is called after the original function has been","'Mark `fn` for line coverage analysis. 
 Results will be printed to sys.stdout on program termination. 
 Usage:: 
 def fn(...): 
 fn = coverage(fn) 
 If you are using Python 2.4, you should be able to use the decorator 
 syntax:: 
 @coverage 
 def fn(...):'"
"def _downgrade_sqlite(t_images, t_image_members, t_image_properties): 
    sql_commands = ['CREATE   TABLE   images_backup   (\n                                 id   INTEGER   NOT   NULL,\n                                 name   VARCHAR(255),\n                                 size   INTEGER,\n                                 status   VARCHAR(30)   NOT   NULL,\n                                 is_public   BOOLEAN   NOT   NULL,\n                                 location   TEXT,\n                                 created_at   DATETIME   NOT   NULL,\n                                 updated_at   DATETIME,\n                                 deleted_at   DATETIME,\n                                 deleted   BOOLEAN   NOT   NULL,\n                                 disk_format   VARCHAR(20),\n                                 container_format   VARCHAR(20),\n                                 checksum   VARCHAR(32),\n                                 owner   VARCHAR(255),\n                                 min_disk   INTEGER   NOT   NULL,\n                                 min_ram   INTEGER   NOT   NULL,\n                                 PRIMARY   KEY   (id),\n                                 CHECK   (is_public   IN   (0,   1)),\n                                 CHECK   (deleted   IN   (0,   1))\n                        );', 'INSERT   INTO   images_backup\n                                 SELECT   *   FROM   images;', 'CREATE   TABLE   image_members_backup   (\n                                    id   INTEGER   NOT   NULL,\n                                    image_id   INTEGER   NOT   NULL,\n                                    member   VARCHAR(255)   NOT   NULL,\n                                    can_share   BOOLEAN   NOT   NULL,\n                                    created_at   DATETIME   NOT   NULL,\n                                    updated_at   DATETIME,\n                                    deleted_at   DATETIME,\n                                    deleted   BOOLEAN   NOT   NULL,\n                                    PRIMARY   KEY   (id),\n                                    UNIQUE   (image_id,   member),\n                                    CHECK   (can_share   IN   (0,   1)),\n                                    CHECK   (deleted   IN   (0,   1)),\n                                    FOREIGN   KEY(image_id)   REFERENCES   images   (id)\n                        );', 'INSERT   INTO   image_members_backup\n                                    SELECT   *   FROM   image_members;', 'CREATE   TABLE   image_properties_backup   (\n                                    id   INTEGER   NOT   NULL,\n                                    image_id   INTEGER      NOT   NULL,\n                                    name   VARCHAR(255)   NOT   NULL,\n                                    value   TEXT,\n                                    created_at   DATETIME   NOT   NULL,\n                                    updated_at   DATETIME,\n                                    deleted_at   DATETIME,\n                                    deleted   BOOLEAN   NOT   NULL,\n                                    PRIMARY   KEY   (id),\n                                    CHECK   (deleted   IN   (0,   1)),\n                                    UNIQUE   (image_id,   name),\n                                    FOREIGN   KEY(image_id)   REFERENCES   images   (id)\n                        );', 'INSERT   INTO   image_properties_backup\n                                    SELECT   *   FROM   image_properties;'] 
   for command in sql_commands: 
      meta.bind.execute(command) 
   _sqlite_table_swap(t_image_members, t_image_properties, t_images)"," 'Swap the image table with the image_members and image_properties table 
 :param t_images: Table of images 
 :param t_image_members: Table of image members 
 :param t_image_properties: Table of image properties'",'Downgrade 012 -> 011 with special SQLite-compatible logic.'
"def test_system_dynamic(): 
    print 'TODO'", 'Test dynamic system',"'http://msdn.microsoft.com/en-us/library/system.dynamic(VS.100).aspx 
 Only a sanity check here.  Exhaustive testing of \'dynamic\' will exist 
 elsewhere.  Piggyback off of CSharp/VB\'s work.'"
"def naturaldate(date, include_seconds=False): 
    if (not date): 
      return u'' 
   right_now = now() 
   today = datetime(right_now.year, right_now.month, right_now.day, tzinfo=right_now.tzinfo) 
   delta = (right_now - date) 
   delta_midnight = (today - date) 
   days = delta.days 
   hours = (delta.seconds // 3600) 
   minutes = (delta.seconds // 60) 
   seconds = delta.seconds 
   if (days < 0): 
      return _(u'just   now') 
   if (days == 0): 
      if (hours == 0): 
         if (minutes > 0): 
            return ungettext(_(u'{minutes}   minute   ago'), _(u'{minutes}   minutes   ago'), minutes).format(minutes=minutes) 
         else: 
            if (include_seconds and seconds): 
               return ungettext(_(u'{seconds}   second   ago'), _(u'{seconds}   seconds   ago'), seconds).format(seconds=seconds) 
            return _(u'just   now') 
      else: 
         return ungettext(_(u'{hours}   hour   ago'), _(u'{hours}   hours   ago'), hours).format(hours=hours) 
   if (delta_midnight.days == 0): 
      return _(u'yesterday   at   {time}').format(time=date.strftime(u'%H:%M')) 
   count = 0 
   for (chunk, pluralizefun) in OLDER_CHUNKS: 
      if (days >= chunk): 
         count = int(round(((delta_midnight.days + 1) / chunk), 0)) 
         fmt = pluralizefun(count) 
         return fmt.format(num=count)"," 'Converts a date to a natural string (i.e. ""yesterday"", ""3   days   ago"", ""12   hours   ago"", etc.) 
 :param date: A datetime object 
 :param include_seconds: Whether to include seconds in the output 
 :returns: A string representing the date in a natural way 
 :rtype: str'",'Convert datetime into a human natural date string.'
"def SetupSharedModules(module_dict): 
    output_dict = {} 
   for (module_name, module) in module_dict.iteritems(): 
      if (module is None): 
         continue 
      if IsEncodingsModule(module_name): 
         output_dict[module_name] = module 
         continue 
      shared_prefix = ModuleNameHasPrefix(module_name, SHARED_MODULE_PREFIXES) 
      banned_prefix = ModuleNameHasPrefix(module_name, NOT_SHARED_MODULE_PREFIXES) 
      if (shared_prefix and (not banned_prefix)): 
         output_dict[module_name] = module 
   return output_dict"," 'Returns a dictionary of shared modules. 
 :param module_dict: Dictionary of modules to be shared. 
 :returns: Dictionary of shared modules. 
 :rtype: dict'","'Creates a module dictionary for the hardened part of the process. 
 Module dictionary will contain modules that should be shared between the 
 hardened and unhardened parts of the process. 
 Args: 
 module_dict: Module dictionary from which existing modules should be 
 pulled (usually sys.modules). 
 Returns: 
 A new module dictionary.'"
"@task 
 def clean(): 
    path('build').rmtree() 
   path('dist').rmtree()", 'Clean the build and dist directories','Cleans build directories'
"def test_ada_fit_invalid_ratio(): 
    ratio = (1.0 / 10000.0) 
   ada = ADASYN(ratio=ratio, random_state=RND_SEED) 
   assert_raises(RuntimeError, ada.fit, X, Y)", 'Test that the fit method raises an exception when the ratio is invalid.',"'Test either if an error is raised when the balancing ratio to fit is 
 smaller than the one of the data'"
"def test_custom_model_subclass(): 
    @custom_model 
   def model_a(x, a=1): 
      return (x * a) 
   class model_b(model_a, ): 
      @classmethod 
      def evaluate(cls, x, a): 
         return (- super(model_b, cls).evaluate(x, a)) 
   b = model_b() 
   assert (b.param_names == (u'a',)) 
   assert (b.a == 1) 
   assert (b(1) == (-1)) 
   sig = signature(model_b.__init__) 
   assert (list(sig.parameters.keys()) == [u'self', u'a', u'kwargs']) 
   sig = signature(model_b.__call__) 
   assert (list(sig.parameters.keys()) == [u'self', u'x', u'model_set_axis'])", 'Test custom model subclass.','Test that custom models can be subclassed.'
"def countNonPrintableChars(string): 
    counter = 0 
   for i in range(len(string)): 
      if ((ord(string[i]) <= 31) or (ord(string[i]) > 127)): 
         counter += 1 
   return counter"," 'Counts the number of non-printable characters in the string. 
 :param string: String to count 
 :type string: str 
 :return: Number of non-printable characters 
 :rtype: int'","'Simple method to return the non printable characters found in an string 
 @param string: A string 
 @return: Number of non printable characters in the string'"
"def get_scores(video): 
    if isinstance(video, Episode): 
      return episode_scores 
   elif isinstance(video, Movie): 
      return movie_scores 
   raise ValueError('video   must   be   an   instance   of   Episode   or   Movie')"," 'Get the score for the video. 
 :param video: A Movie or Episode object 
 :return: The score for the video 
 :rtype: int'","'Get the scores dict for the given `video`. 
 This will return either :data:`episode_scores` or :data:`movie_scores` based on the type of the `video`. 
 :param video: the video to compute the score against. 
 :type video: :class:`~subliminal.video.Video` 
 :return: the scores dict. 
 :rtype: dict'"
"def kill_pid(pid, signal=15): 
    try: 
      psutil.Process(pid).send_signal(signal) 
      return True 
   except psutil.NoSuchProcess: 
      return False"," 'Kill a process by sending a signal. 
 :param pid: PID of the process to kill 
 :param signal: Signal to send to the process 
 :returns: True if the process was killed, False otherwise'","'Kill a process by PID. 
 .. code-block:: bash 
 salt \'minion\' ps.kill_pid pid [signal=signal_number] 
 pid 
 PID of process to kill. 
 signal 
 Signal to send to the process. See manpage entry for kill 
 for possible values. Default: 15 (SIGTERM). 
 **Example:** 
 Send SIGKILL to process with PID 2000: 
 .. code-block:: bash 
 salt \'minion\' ps.kill_pid 2000 signal=9'"
"def escape(text): 
    text = text.replace('\\', '\\\\') 
   text = text.replace('""""""', '""""\\""') 
   text = text.replace('   \n', '   \\n\\\n') 
   return text"," 'Escape strings for use in Python code. 
 This function is used to escape strings for use in Python code. 
 It replaces all instances of \', \', \', \n, and \r with their 
 respective backslashes. 
 Parameters 
 text : string 
 The string to escape. 
 Returns 
 string 
 The escaped string.'",'Return `text` in triple-double-quoted Python string form.'
"def int_to_str(value, length=2): 
    try: 
      int(value) 
   except: 
      raise ValueError('expected   an   integer   value') 
   content = str(value) 
   while (len(content) < length): 
      content = ('0' + content) 
   return content", 'Convert an integer value to a string of length length.',"'Converts integer to string eg 3 to ""03""'"
"def validate_payload(payload, api_model, check_required=True): 
    if check_required: 
      for key in api_model: 
         if (api_model[key].required and (key not in payload)): 
            raise ValidationError(field=key, message=""Required   field   '{}'   missing"".format(key)) 
   for key in payload: 
      field = api_model[key] 
      if isinstance(field, fields.List): 
         field = field.container 
         data = payload[key] 
      elif isinstance(field, fields.Nested): 
         if payload[key]: 
            validate_payload(payload[key], field.model) 
      else: 
         data = [payload[key]] 
      if (isinstance(field, CustomField) and hasattr(field, 'validate')): 
         field.payload = payload 
         for i in data: 
            if (not field.validate(i)): 
               raise ValidationError(field=key, message=(field.validation_error % (""'%s'"" % key)))"," 'Validate the payload against the model. 
 :param payload: The payload to validate. 
 :param api_model: The model to validate against. 
 :param check_required: Whether required fields are checked. 
 :raises ValidationError: If validation fails.'","'Validate payload against an api_model. Aborts in case of failure 
 - This function is for custom fields as they can\'t be validated by 
 flask restplus automatically. 
 - This is to be called at the start of a post or put method'"
"def serialize_item(collection, item): 
    if ((item.name is None) or (item.name == '')): 
      raise exceptions.RuntimeError('name   unset   for   item!') 
   if (collection.collection_type() in ['mgmtclass']): 
      filename = ('/var/lib/cobbler/collections/%ses/%s' % (collection.collection_type(), item.name)) 
   else: 
      filename = ('/var/lib/cobbler/collections/%ss/%s' % (collection.collection_type(), item.name)) 
   _dict = item.to_dict() 
   if capi.CobblerAPI().settings().serializer_pretty_json: 
      sort_keys = True 
      indent = 4 
   else: 
      sort_keys = False 
      indent = None 
   filename += '.json' 
   _dict = item.to_dict() 
   fd = open(filename, 'w+') 
   data = simplejson.dumps(_dict, encoding='utf-8', sort_keys=sort_keys, indent=indent) 
   fd.write(data) 
   fd.close()", 'Serialize an item to a file.',"'Save a collection item to file system 
 @param Collection collection collection 
 @param Item item collection item'"
"def get_raising_file_and_line(tb=None): 
    if (not tb): 
      tb = sys.exc_info()[2] 
   (filename, lineno, _context, _line) = traceback.extract_tb(tb)[(-1)] 
   return (filename, lineno)", 'Returns the file and line number of the exception that was raised.',"'Return the file and line number of the statement that raised the tb. 
 Returns: (filename, lineno) tuple'"
"def _minimize_cg(fun, x0, args=(), jac=None, callback=None, gtol=1e-05, norm=Inf, eps=_epsilon, maxiter=None, disp=False, return_all=False, **unknown_options): 
    _check_unknown_options(unknown_options) 
   f = fun 
   fprime = jac 
   epsilon = eps 
   retall = return_all 
   x0 = asarray(x0).flatten() 
   if (maxiter is None): 
      maxiter = (len(x0) * 200) 
   (func_calls, f) = wrap_function(f, args) 
   if (fprime is None): 
      (grad_calls, myfprime) = wrap_function(approx_fprime, (f, epsilon)) 
   else: 
      (grad_calls, myfprime) = wrap_function(fprime, args) 
   gfk = myfprime(x0) 
   k = 0 
   xk = x0 
   old_fval = f(xk) 
   old_old_fval = (old_fval + (np.linalg.norm(gfk) / 2)) 
   if retall: 
      allvecs = [xk] 
   warnflag = 0 
   pk = (- gfk) 
   gnorm = vecnorm(gfk, ord=norm) 
   while ((gnorm > gtol) and (k < maxiter)): 
      deltak = numpy.dot(gfk, gfk) 
      try: 
         (alpha_k, fc, gc, old_fval, old_old_fval, gfkp1) = _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval, old_old_fval, c2=0.4, amin=1e-100, amax=1e+100) 
      except _LineSearchError: 
         warnflag = 2 
         break 
      xk = (xk + (alpha_k * pk)) 
      if retall: 
         allvecs.append(xk) 
      if (gfkp1 is None): 
         gfkp1 = myfprime(xk) 
      yk = (gfkp1 - gfk) 
      beta_k = max(0, (numpy.dot(yk, gfkp1) / deltak)) 
      pk = ((- gfkp1) + (beta_k * pk)) 
      gfk = gfkp1 
      gnorm = vecnorm(gfk, ord=norm) 
      if (callback is not None): 
         callback(xk) 
      k += 1 
   fval = old_fval 
   if (warnflag == 2): 
      msg = _status_message['pr_loss'] 
      if disp: 
         print(('Warning:   ' + msg)) 
         print(('                           Current   function   value:   %f' % fval)) 
         print(('                           Iterations:   %d' % k)) 
         print(('                           Function   evaluations:   %d' % func_calls[0])) 
         print(('                           Gradient   evaluations:   %d' % grad_calls[0])) 
   elif (k >= maxiter): 
      warnflag = 1 
      msg = _status_message['maxiter'] 
      if disp: 
         print(('Warning:   ' + msg)) 
         print(('                           Current   function   value:   %f' % fval)) 
         print(('                           Iterations:   %d' % k)) 
         print(('                           Function   evaluations:   %d' % func_calls[0])) 
         print(('                           Gradient   evaluations:   %d' % grad_calls[0])) 
   else: 
      msg = _status_message['success'] 
      if disp: 
         print(msg) 
         print(('                           Current   function   value:   %f' % fval)) 
         print(('                           Iterations:   %d' % k)) 
         print(('                           Function   evaluations:   %d' % func_calls[0])) 
         print(('                           Gradient   evaluations:   %d' % grad_calls[0])) 
   result = OptimizeResult(fun=fval, jac=gfk, nfev=func_calls[0], njev=grad_calls[0], status=warnflag, success=(warnflag == 0), message=msg, x=xk, nit=k) 
   if retall: 
      result['allvecs'] = allvecs 
   return result"," 'Minimize a function using the conjugate gradient method. 
 Parameters 
 fun : callable 
 Function to minimize. 
 x0 : ndarray 
 Initial guess. 
 args : tuple, optional 
 Additional arguments to pass to the function. 
 jac : callable, optional 
 The function to evaluate the Jacobian of `fun`. 
 callback : callable, optional 
 A function to call after each iteration. 
 gtol : float, optional 
 The tolerance for the function value. 
 norm : float, optional 
 The norm to use for the Jacobian. 
 eps : float, optional 
 The tolerance for the line search. 
 maxiter : int, optional 
 The maximum number of iterations. 
 disp : bool, optional 
 If True, print status messages. 
 return_all : bool, optional 
 If True, return all vectors in `x`. 
 **unknown_options : dict 
 Any additional keyword arguments will be passed to the underlying 
 minimize function. 
 Returns 
 OptimizeResult 
 A dictionary with the following keys: 
 ``fun","'Minimization of scalar function of one or more variables using the 
 conjugate gradient algorithm. 
 Options 
 disp : bool 
 Set to True to print convergence messages. 
 maxiter : int 
 Maximum number of iterations to perform. 
 gtol : float 
 Gradient norm must be less than `gtol` before successful 
 termination. 
 norm : float 
 Order of norm (Inf is max, -Inf is min). 
 eps : float or ndarray 
 If `jac` is approximated, use this value for the step size.'"
"def get_recurring(**filter_data): 
    return rpc_utils.prepare_rows_as_nested_dicts(models.RecurringRun.query_objects(filter_data), ('job', 'owner'))"," 'Returns a list of recurring runs. 
 :param filter_data: Dictionary of filter data. 
 :type filter_data: dict 
 :return: List of recurring runs.'","'Return recurring jobs. 
 :param filter_data: Filters out which recurring jobs to get. 
 :return: Sequence of recurring jobs.'"
"def absent(name): 
    ret = {'name': name, 'changes': {}, 'result': True, 'comment': ''} 
   grp_info = __salt__['group.info'](name) 
   if grp_info: 
      if __opts__['test']: 
         ret['result'] = None 
         ret['comment'] = 'Group   {0}   is   set   for   removal'.format(name) 
         return ret 
      ret['result'] = __salt__['group.delete'](name) 
      if ret['result']: 
         ret['changes'] = {name: ''} 
         ret['comment'] = 'Removed   group   {0}'.format(name) 
         return ret 
      else: 
         ret['comment'] = 'Failed   to   remove   group   {0}'.format(name) 
         return ret 
   else: 
      ret['comment'] = 'Group   not   present' 
      return ret"," 'Remove a group. 
 name: the name of the group to remove 
 Example: 
 salt-call group.absent testgroup'","'Ensure that the named group is absent 
 name 
 The name of the group to remove'"
"def quitWindows(event=None): 
    global globalRepositoryDialogListTable 
   globalRepositoryDialogValues = euclidean.getListTableElements(globalRepositoryDialogListTable) 
   for globalRepositoryDialogValue in globalRepositoryDialogValues: 
      quitWindow(globalRepositoryDialogValue.root)", 'Quit all windows that have been created','Quit all windows.'
"def get(key, profile=None): 
    data = _get_values(profile) 
   return salt.utils.traverse_dict_and_list(data, key, None)", 'Retrieve a value from the salt configuration file.','Get a value from the REST interface'
"def for_all_dtypes(name='dtype', no_float16=False, no_bool=False): 
    return for_dtypes(_make_all_dtypes(no_float16, no_bool), name=name)", 'For all dtypes (including numpy.bool_)',"'Decorator that checks the fixture with all dtypes. 
 Args: 
 name(str): Argument name to which specified dtypes are passed. 
 no_float16(bool): If, True, ``numpy.float16`` is 
 omitted from candidate dtypes. 
 no_bool(bool): If, True, ``numpy.bool_`` is 
 omitted from candidate dtypes. 
 dtypes to be tested: ``numpy.float16`` (optional), ``numpy.float32``, 
 ``numpy.float64``, ``numpy.dtype(\'b\')``, ``numpy.dtype(\'h\')``, 
 ``numpy.dtype(\'i\')``, ``numpy.dtype(\'l\')``, ``numpy.dtype(\'q\')``, 
 ``numpy.dtype(\'B\')``, ``numpy.dtype(\'H\')``, ``numpy.dtype(\'I\')``, 
 ``numpy.dtype(\'L\')``, ``numpy.dtype(\'Q\')``, and ``numpy.bool_`` (optional). 
 The usage is as follows. 
 This test fixture checks if ``cPickle`` successfully reconstructs 
 :class:`cupy.ndarray` for various dtypes. 
 ``dtype`` is an argument inserted by the decorator. 
 >>> import unittest 
 >>> from cupy import testing 
 >>> @testing.gpu 
 ... class TestNpz(unittest.TestCase): 
 ...     @testing.for_all_dtypes() 
 ...     def test_pickle(self, dtype): 
 ...         a = testing.shaped_arange((2, 3, 4), dtype=dtype) 
 ...         s = six.moves.cPickle.dumps(a) 
 ...         b = six.moves.cPickle.loads(s) 
 ...         testing.assert_array_equal(a, b) 
 Typically, we use this decorator in combination with 
 decorators that check consistency between NumPy and CuPy like 
 :func:`cupy.testing.numpy_cupy_allclose`. 
 The following is such an example. 
 >>> import unittest 
 >>> from cupy import testing 
 >>> @testing.gpu 
 ... class TestMean(unittest.TestCase): 
 ...     @testing.for_all_dtypes() 
 ...     @testing.numpy_cupy_allclose() 
 ...     def test_mean_all(self, xp, dtype): 
 ...         a = testing.shaped_arange((2, 3), xp, dtype) 
 ...         return a.mean() 
 .. seealso:: :func:`cupy.testing.for_dtypes`'"
"def mapping_file_to_dict(mapping_data, header): 
    map_dict = {} 
   for i in range(len(mapping_data)): 
      sam = mapping_data[i] 
      map_dict[sam[0]] = {} 
      for j in range(len(header)): 
         if (j == 0): 
            continue 
         map_dict[sam[0]][header[j]] = sam[j] 
   return map_dict"," 'Returns a dictionary mapping the file names to the dicts containing 
 the mapping data. 
 :param mapping_data: A list of dictionaries. 
 :param header: A list of strings. 
 :return: A dictionary mapping the file names to the dicts containing 
 the mapping data.'",'processes mapping data in list of lists format into a 2 deep dict'
"def firstof(parser, token): 
    bits = token.split_contents()[1:] 
   if (len(bits) < 1): 
      raise TemplateSyntaxError(""'firstof'   statement   requires   at   least   one   argument"") 
   return FirstOfNode([parser.compile_filter(bit) for bit in bits])"," 'Parse a template variable as a list of filters. 
 :param parser: The parser instance. 
 :param token: The token to parse. 
 :return: The compiled node. 
 :rtype: CompiledNode'","'Outputs the first variable passed that is not False, without escaping. 
 Outputs nothing if all the passed variables are False. 
 Sample usage:: 
 {% firstof var1 var2 var3 %} 
 This is equivalent to:: 
 {% if var1 %} 
 {{ var1|safe }} 
 {% else %}{% if var2 %} 
 {{ var2|safe }} 
 {% else %}{% if var3 %} 
 {{ var3|safe }} 
 {% endif %}{% endif %}{% endif %} 
 but obviously much cleaner! 
 You can also use a literal string as a fallback value in case all 
 passed variables are False:: 
 {% firstof var1 var2 var3 ""fallback value"" %} 
 If you want to escape the output, use a filter tag:: 
 {% filter force_escape %} 
 {% firstof var1 var2 var3 ""fallback value"" %} 
 {% endfilter %}'"
"@app.route('/scans/<int:scan_id>/exceptions/', methods=['GET']) 
 @requires_auth 
 def list_exceptions(scan_id): 
    scan_info = get_scan_info_from_id(scan_id) 
   if (scan_info is None): 
      abort(404, 'Scan   not   found') 
   data = [] 
   all_exceptions = scan_info.w3af_core.exception_handler.get_all_exceptions() 
   for (exception_id, exception_data) in enumerate(all_exceptions): 
      data.append(exception_to_json(exception_data, scan_id, exception_id)) 
   return jsonify({'items': data})", 'Get a list of all exceptions raised during the scan.',"'List all exceptions found during a scan 
 :return: A JSON containing a list of: 
 - Exception resource URL (eg. /scans/0/exceptions/3) 
 - The exceptions id (eg. 3) 
 - Exception string 
 - Exception file name 
 - Exception line number'"
"def _check_even_rewrite(func, arg): 
    return (func(arg).args[0] == (- arg))", 'Checks whether the function is even or not.',"'Checks that the expr has been rewritten using f(-x) -> f(x) 
 arg : -x'"
"def is_jid(jid): 
    if (not isinstance(jid, six.string_types)): 
      return False 
   if (len(jid) != 20): 
      return False 
   try: 
      int(jid) 
      return True 
   except ValueError: 
      return False", 'Check if the given string is a valid JID','Returns True if the passed in value is a job id'
"def encode_single(typ, arg): 
    (base, sub, _) = typ 
   if (base == 'uint'): 
      sub = int(sub) 
      if (not ((0 < sub <= 256) and ((sub % 8) == 0))): 
         raise ValueError('invalid   unsigned   integer   bit   length   {}'.format(sub)) 
      try: 
         i = decint(arg, signed=False) 
      except EncodingError: 
         raise ValueOutOfBounds(repr(arg)) 
      if (not (0 <= i < (2 ** sub))): 
         raise ValueOutOfBounds(repr(arg)) 
      value_encoded = int_to_big_endian(i) 
      return zpad(value_encoded, 32) 
   if (base == 'int'): 
      sub = int(sub) 
      bits = (sub - 1) 
      if (not ((0 < sub <= 256) and ((sub % 8) == 0))): 
         raise ValueError('invalid   integer   bit   length   {}'.format(sub)) 
      try: 
         i = decint(arg, signed=True) 
      except EncodingError: 
         raise ValueOutOfBounds(repr(arg)) 
      if (not ((- (2 ** bits)) <= i < (2 ** bits))): 
         raise ValueOutOfBounds(repr(arg)) 
      value = (i % (2 ** sub)) 
      value_encoded = int_to_big_endian(value) 
      return zpad(value_encoded, 32) 
   if (base == 'bool'): 
      if (arg is True): 
         value_encoded = int_to_big_endian(1) 
      elif (arg is False): 
         value_encoded = int_to_big_endian(0) 
      else: 
         raise ValueError(('%r   is   not   bool' % arg)) 
      return zpad(value_encoded, 32) 
   if (base == 'ufixed'): 
      sub = str(sub) 
      (high_str, low_str) = sub.split('x') 
      high = int(high_str) 
      low = int(low_str) 
      if (not ((0 < (high + low) <= 256) and ((high % 8) == 0) and ((low % 8) == 0))): 
         raise ValueError('invalid   unsigned   fixed   length   {}'.format(sub)) 
      if (not (0 <= arg < (2 ** high))): 
         raise ValueOutOfBounds(repr(arg)) 
      float_point = (arg * (2 ** low)) 
      fixed_point = int(float_point) 
      return zpad(int_to_big_endian(fixed_point), 32) 
   if (base == 'fixed'): 
      sub = str(sub) 
      (high_str, low_str) = sub.split('x') 
      high = int(high_str) 
      low = int(low_str) 
      bits = (high - 1) 
      if (not ((0 < (high + low) <= 256) and ((high % 8) == 0) and ((low % 8) == 0))): 
         raise ValueError('invalid   unsigned   fixed   length   {}'.format(sub)) 
      if (not ((- (2 ** bits)) <= arg < (2 ** bits))): 
         raise ValueOutOfBounds(repr(arg)) 
      float_point = (arg * (2 ** low)) 
      fixed_point = int(float_point) 
      value = (fixed_point % (2 ** 256)) 
      return zpad(int_to_big_endian(value), 32) 
   if (base == 'string'): 
      if isinstance(arg, utils.unicode): 
         arg = arg.encode('utf8') 
      else: 
         try: 
            arg.decode('utf8') 
         except UnicodeDecodeError: 
            raise ValueError('string   must   be   utf8   encoded') 
      if len(sub): 
         if (not (0 <= len(arg) <= int(sub))): 
            raise ValueError('invalid   string   length   {}'.format(sub)) 
         if (not (0 <= int(sub) <= 32)): 
            raise ValueError('invalid   string   length   {}'.format(sub)) 
         return rzpad(arg, 32) 
      if (not (0 <= len(arg) < TT256)): 
         raise Exception(('Integer   invalid   or   out   of   range:   %r' % arg)) 
      length_encoded = zpad(int_to_big_endian(len(arg)), 32) 
      value_encoded = rzpad(arg, utils.ceil32(len(arg))) 
      return (length_encoded + value_encoded) 
   if (base == 'bytes'): 
      if (not is_string(arg)): 
         raise EncodingError(('Expecting   string:   %r' % arg)) 
      arg = utils.to_string(arg) 
      if len(sub): 
         if (not (0 <= len(arg) <= int(sub))): 
            raise ValueError('string   must   be   utf8   encoded') 
         if (not (0 <= int(sub) <= 32)): 
            raise ValueError('string   must   be   utf8   encoded') 
         return rzpad(arg, 32) 
      if (not (0 <= len(arg) < TT256)): 
         raise Exception(('Integer   invalid   or   out   of   range:   %r' % arg)) 
      length_encoded = zpad(int_to_big_endian(len(arg)), 32) 
      value_encoded = rzpad(arg, utils.ceil32(len(arg))) 
      return (length_encoded + value_encoded) 
   if (base == 'hash'): 
      if (not (int(sub) and (int(sub) <= 32))): 
         raise EncodingError(('too   long:   %r' % arg)) 
      if isnumeric(arg): 
         return zpad(encode_int(arg), 32) 
      if (len(arg) == int(sub)): 
         return zpad(arg, 32) 
      if (len(arg) == (int(sub) * 2)): 
         return zpad(decode_hex(arg), 32) 
      raise EncodingError(('Could   not   parse   hash:   %r' % arg)) 
   if (base == 'address'): 
      assert (sub == '') 
      if isnumeric(arg): 
         return zpad(encode_int(arg), 32) 
      if (len(arg) == 20): 
         return zpad(arg, 32) 
      if (len(arg) == 40): 
         return zpad(decode_hex(arg), 32) 
      if ((len(arg) == 42) and (arg[:2] == '0x')): 
         return zpad(decode_hex(arg[2:]), 32) 
      raise EncodingError(('Could   not   parse   address:   %r' % arg)) 
   raise EncodingError(('Unhandled   type:   %r   %r' % (base, sub)))"," 'Encode a single value into a TT format. 
 Parameters 
 arg : value to encode 
 sub : type of value to encode 
 base : base type of value to encode 
 Returns 
 value_encoded : encoded value 
 Raises 
 ValueError 
 ValueOutOfBounds 
 EncodingError 
 Exception'","'Encode `arg` as `typ`. 
 `arg` will be encoded in a best effort manner, were necessary the function 
 will try to correctly define the underlying binary representation (ie. 
 decoding a hex-encoded address/hash). 
 Args: 
 typ (Tuple[(str, int, list)]): A 3-tuple defining the `arg` type. 
 The first element defines the type name. 
 The second element defines the type length in bits. 
 The third element defines if it\'s an array type. 
 Together the first and second defines the elementary type, the third 
 element must be present but is ignored. 
 Valid type names are: 
 - uint 
 - int 
 - bool 
 - ufixed 
 - fixed 
 - string 
 - bytes 
 - hash 
 - address 
 arg (object): The object to be encoded, it must be a python object 
 compatible with the `typ`. 
 Raises: 
 ValueError: when an invalid `typ` is supplied. 
 ValueOutOfBounds: when `arg` cannot be encoded as `typ` because of the 
 binary contraints. 
 Note: 
 This function don\'t work with array types, for that use the `enc` 
 function.'"
"def init_runspace(): 
    global shell 
   cmds = {} 
   for cmdlet in InvokeCommand('get-command'): 
      cmds[translate(cmdlet.Name)] = ShellCommand(cmdlet.Name) 
   for alias in InvokeCommand('get-alias'): 
      cmdName = translate(alias.ReferencedCommand.Name) 
      if (cmdName in cmds): 
         cmds[translate(alias.Name)] = cmds[cmdName] 
   shell = Shell(cmds) 
   for key in cmds.keys(): 
      setattr(ShellOutput, key, cmds[key])", 'Initialize the runspace and alias commands',''
"def test_hsl_to_rgb_part_3(): 
    assert (hsl_to_rgb(6120, 100, 50) == (255, 0, 0)) 
   assert (hsl_to_rgb((-9660), 100, 50) == (255, 255, 0)) 
   assert (hsl_to_rgb(99840, 100, 50) == (0, 255, 0)) 
   assert (hsl_to_rgb((-900), 100, 50) == (0, 255, 255)) 
   assert (hsl_to_rgb((-104880), 100, 50) == (0, 0, 255)) 
   assert (hsl_to_rgb(2820, 100, 50) == (255, 0, 255))"," 'Test hsl_to_rgb function with input values 
 -6120, 100, 50 
 -9660, 100, 50 
 99840, 100, 50 
 -900, 100, 50 
 -104880, 100, 50 
 2820, 100, 50'",'Test hsl to rgb color function'
"def test_cx_Oracle(): 
    if (('ORACLE_HOME' not in os.environ) and ('ORACLE_INSTANTCLIENT_HOME' not in os.environ)): 
      raise SkipTest 
   try: 
      import cx_Oracle 
      return 
   except ImportError as ex: 
      if ('No   module   named' in ex.message): 
         assert_true(False, 'cx_Oracle   skipped   its   build.   This   happens   if   env   var   ORACLE_HOME   or   ORACLE_INSTANTCLIENT_HOME   is   not   defined.   So   ignore   this   test   failure   if   your   build   does   not   need   to   work   with   an   oracle   backend.')", 'Test cx_Oracle','Tests that cx_Oracle (external dependency) is built correctly.'
"def default_channel(): 
    try: 
      chan = _open_session() 
   except ssh.SSHException as err: 
      if (str(err) == 'SSH   session   not   active'): 
         connections[env.host_string].close() 
         del connections[env.host_string] 
         chan = _open_session() 
      else: 
         raise 
   chan.settimeout(0.1) 
   chan.input_enabled = True 
   return chan"," 'Get a default session to the remote host. 
 This is the session that will be used for all SSH operations. 
 :rtype: :class:`~fabtools.ssh.Session`'",'Return a channel object based on ``env.host_string``.'
"def pixels_to_EMU(value): 
    return int((value * 9525))", 'Convert pixels to EMU','1 pixel = 9525 EMUs'
"def global_fixes(): 
    for function in list(globals().values()): 
      if inspect.isfunction(function): 
         arguments = inspect.getargspec(function)[0] 
         if (arguments[:1] != [u'source']): 
            continue 
         code = extract_code_from_function(function) 
         if code: 
            (yield (code, function))"," 'Return a generator that yields (code, function) tuples. 
 The function is the original function and the code is the 
 code that is generated by the function. 
 :return: A generator that yields (code, function) tuples.'","'Yield multiple (code, function) tuples.'"
"def get_metadata(stream): 
    lrf = (stream if isinstance(stream, LRFMetaFile) else LRFMetaFile(stream)) 
   authors = string_to_authors(lrf.author) 
   mi = MetaInformation(lrf.title.strip(), authors) 
   mi.author = lrf.author.strip() 
   mi.comments = lrf.free_text.strip() 
   mi.category = ((lrf.category.strip() + ',   ') + lrf.classification.strip()) 
   tags = [x.strip() for x in mi.category.split(',') if x.strip()] 
   if tags: 
      mi.tags = tags 
   if (mi.category.strip() == ','): 
      mi.category = None 
   mi.publisher = lrf.publisher.strip() 
   mi.cover_data = lrf.get_cover() 
   try: 
      mi.title_sort = lrf.title_reading.strip() 
      if (not mi.title_sort): 
         mi.title_sort = None 
   except: 
      pass 
   try: 
      mi.author_sort = lrf.author_reading.strip() 
      if (not mi.author_sort): 
         mi.author_sort = None 
   except: 
      pass 
   if ((not mi.title) or ('unknown' in mi.title.lower())): 
      mi.title = None 
   if (not mi.authors): 
      mi.authors = None 
   if ((not mi.author) or ('unknown' in mi.author.lower())): 
      mi.author = None 
   if ((not mi.category) or ('unknown' in mi.category.lower())): 
      mi.category = None 
   if ((not mi.publisher) or ('unknown' in mi.publisher.lower()) or ('some   publisher' in mi.publisher.lower())): 
      mi.publisher = None 
   return mi"," 'Reads the metadata from a LRF file and returns a MetaInformation object. 
 :param stream: A file-like object containing the LRF file. 
 :returns: A MetaInformation object.'","'Return basic meta-data about the LRF file in C{stream} as a 
 L{MetaInformation} object. 
 @param stream: A file like object or an instance of L{LRFMetaFile}'"
"def _make_triplets(seq, phase=0): 
    pre = seq[:phase] 
   np_seq = seq[phase:] 
   non_triplets = (len(np_seq) % 3) 
   post = ('' if (not non_triplets) else np_seq[((-1) * non_triplets):]) 
   intacts = [np_seq[(3 * i):(3 * (i + 1))] for i in range((len(np_seq) // 3))] 
   return (pre, intacts, post)"," 'Returns triplets of the sequence. 
 :param seq: a sequence 
 :param phase: the phase of the triplets to return. 
 :return: a tuple of (pre, intacts, post) 
 :rtype: tuple of str'","'Selects a valid amino acid sequence given a 3-letter code input. 
 This function takes a single three-letter amino acid sequence and the phase 
 of the sequence to return the longest intact amino acid sequence possible. 
 Parts of the input sequence before and after the selected sequence are also 
 returned. 
 This is an internal private function and is meant for parsing Exonerate\'s 
 three-letter amino acid output. 
 >>> from Bio.SearchIO.ExonerateIO._base import _make_triplets 
 >>> _make_triplets(\'GlyThrSerAlaPro\') 
 (\'\', [\'Gly\', \'Thr\', \'Ser\', \'Ala\', \'Pro\'], \'\') 
 >>> _make_triplets(\'yThSerAla\', phase=1) 
 (\'y\', [\'Thr\', \'Ser\', \'Ala\'], \'\') 
 >>> _make_triplets(\'yThSerAlaPr\', phase=1) 
 (\'y\', [\'Thr\', \'Ser\', \'Ala\'], \'Pr\')'"
"def loadClassPath(classpath): 
    if (':' in classpath): 
      (modname, objname) = classpath.split(':', 1) 
      try: 
         __import__(modname) 
         module = modules[modname] 
         _class = eval(objname, module.__dict__) 
         if (_class is None): 
            raise Exception(('eval(%(objname)s)   in   %(modname)s   came   up   None' % locals())) 
      except Exception as e: 
         raise KnownUnknown(('Tried   to   import   %s,   but:   %s' % (classpath, e))) 
   else: 
      classpath = classpath.split('.') 
      try: 
         module = __import__('.'.join(classpath[:(-1)]), fromlist=str(classpath[(-1)])) 
      except ImportError as e: 
         raise KnownUnknown(('Tried   to   import   %s,   but:   %s' % ('.'.join(classpath), e))) 
      try: 
         _class = getattr(module, classpath[(-1)]) 
      except AttributeError as e: 
         raise KnownUnknown(('Tried   to   import   %s,   but:   %s' % ('.'.join(classpath), e))) 
   return _class"," 'Load a class from a classpath. 
 classpath can be a path, or a string of the form ""module:classname"". 
 Returns the class, or raises an exception if it can\'t be loaded.'","'Load external class based on a path. 
 Example classpath: ""Module.Submodule:Classname"". 
 Equivalent soon-to-be-deprecated classpath: ""Module.Submodule.Classname"".'"
"def _memoize(func, *args, **opts): 
    if opts: 
      key = (args, frozenset(opts.items())) 
   else: 
      key = args 
   cache = func.cache 
   try: 
      result = cache[key] 
   except KeyError: 
      result = cache[key] = func(*args, **opts) 
   return result"," 'Memoize a function. 
 Parameters 
 func : function 
 The function to memoize. 
 args : tuple 
 The arguments to pass to the function. 
 opts : dict 
 The keyword arguments to pass to the function. 
 Returns 
 func : function 
 The memoized version of func. 
 Examples 
 >>> @_memoize 
 ... def f(x): 
 ...     return x 
 >>> f(1) 
 1 
 >>> f(1) 
 1'",'Implements memoized cache lookups'
"def git_pull(): 
    app = get_app() 
   if (not have_git): 
      session.flash = GIT_MISSING 
      redirect(URL('site')) 
   dialog = FORM.confirm(T('Pull'), {T('Cancel'): URL('site')}) 
   if dialog.accepted: 
      try: 
         repo = git.Repo(os.path.join(apath(r=request), app)) 
         origin = repo.remotes.origin 
         origin.fetch() 
         origin.pull() 
         session.flash = T('Application   updated   via   git   pull') 
         redirect(URL('site')) 
      except git.CheckoutError: 
         session.flash = T('Pull   failed,   certain   files   could   not   be   checked   out.   Check   logs   for   details.') 
         redirect(URL('site')) 
      except git.UnmergedEntriesError: 
         session.flash = T('Pull   is   not   possible   because   you   have   unmerged   files.   Fix   them   up   in   the   work   tree,   and   then   try   again.') 
         redirect(URL('site')) 
      except git.GitCommandError: 
         session.flash = T('Pull   failed,   git   exited   abnormally.   See   logs   for   details.') 
         redirect(URL('site')) 
      except AssertionError: 
         session.flash = T('Pull   is   not   possible   because   you   have   unmerged   files.   Fix   them   up   in   the   work   tree,   and   then   try   again.') 
         redirect(URL('site')) 
   elif ('cancel' in request.vars): 
      redirect(URL('site')) 
   return dict(app=app, dialog=dialog)"," 'Pull the application from the remote repository. 
 :return: a dictionary with the application and dialog'",'Git Pull handler'
"def _get_css_imports_cssutils(data, inline=False): 
    try: 
      import cssutils 
   except (ImportError, re.error): 
      return None 
   parser = cssutils.CSSParser(loglevel=100, fetcher=(lambda url: (None, '')), validate=False) 
   if (not inline): 
      sheet = parser.parseString(data) 
      return list(cssutils.getUrls(sheet)) 
   else: 
      urls = [] 
      declaration = parser.parseStyle(data) 
      for prop in declaration: 
         for value in prop.propertyValue: 
            if isinstance(value, cssutils.css.URIValue): 
               if value.uri: 
                  urls.append(value.uri) 
      return urls"," 'Returns a list of css urls from a css string. 
 This function uses cssutils to parse the css string. 
 It returns a list of urls found in the css string. 
 If the css string is inline, then it returns the urls in the css string. 
 If the css string is not inline, then it returns the urls found in the css 
 string. 
 :param data: The css string to parse. 
 :param inline: If True, then the css string is assumed to be inline, 
 otherwise it is assumed to be a css file. 
 :returns: A list of urls found in the css string.'","'Return all assets that are referenced in the given CSS document. 
 The returned URLs are relative to the stylesheet\'s URL. 
 Args: 
 data: The content of the stylesheet to scan as string. 
 inline: True if the argument is an inline HTML style attribute.'"
"def make_script_path(script): 
    s_path = None 
   path = cfg.script_dir.get_path() 
   if (path and script): 
      if (script.lower() not in ('none', 'default')): 
         s_path = os.path.join(path, script) 
         if (not os.path.exists(s_path)): 
            s_path = None 
   return s_path"," 'Return the script path for a given script. 
 :param script: The script to find the path for. 
 :return: The path to the script, or None if the script does not exist.'","'Return full script path, if any valid script exists, else None'"
"def follow(): 
    t = Twitter(auth=authen()) 
   screen_name = g['stuff'].split()[0] 
   if screen_name.startswith('@'): 
      t.friendships.create(screen_name=screen_name[1:], follow=True) 
      printNicely(green((('You   are   following   ' + screen_name) + '   now!'))) 
   else: 
      printNicely(red(""A   name   should   begin   with   a   '@'""))", 'Follows the specified user.','Follow a user'
"def select_template(template_name_list): 
    not_found = [] 
   for template_name in template_name_list: 
      try: 
         return get_template(template_name) 
      except TemplateDoesNotExist as e: 
         if (e.args[0] not in not_found): 
            not_found.append(e.args[0]) 
         continue 
   raise TemplateDoesNotExist(',   '.join(not_found))"," 'Select a template from a list of template names. 
 If the template is not found, raise TemplateDoesNotExist with the template 
 name.'","'Given a list of template names, returns the first that can be loaded.'"
"def _update_data(entity_id, public_key, sso_url, expires_at): 
    data_obj = SAMLProviderData.current(entity_id) 
   fetched_at = datetime.datetime.now() 
   if (data_obj and ((data_obj.public_key == public_key) and (data_obj.sso_url == sso_url))): 
      data_obj.expires_at = expires_at 
      data_obj.fetched_at = fetched_at 
      data_obj.save() 
      return False 
   else: 
      SAMLProviderData.objects.create(entity_id=entity_id, fetched_at=fetched_at, expires_at=expires_at, sso_url=sso_url, public_key=public_key) 
      return True"," 'Updates the SAMLProviderData object with the given public key and expiration time. 
 :param entity_id: The entity_id of the SAMLProviderData object. 
 :type entity_id: str 
 :param public_key: The public key of the SAMLProviderData object. 
 :type public_key: str 
 :param sso_url: The sso_url of the SAMLProviderData object. 
 :type sso_url: str 
 :param expires_at: The expiration time of the SAMLProviderData object. 
 :type expires_at: datetime.datetime'","'Update/Create the SAMLProviderData for the given entity ID. 
 Return value: 
 False if nothing has changed and existing data\'s ""fetched at"" timestamp is just updated. 
 True if a new record was created. (Either this is a new provider or something changed.)'"
"def InstallDriver(kext_path): 
    km = objc.KextManager() 
   cf_kext_path = km.PyStringToCFString(kext_path) 
   kext_url = km.dll.CFURLCreateWithFileSystemPath(objc.CF_DEFAULT_ALLOCATOR, cf_kext_path, objc.POSIX_PATH_STYLE, True) 
   status = km.iokit.KextManagerLoadKextWithURL(kext_url, None) 
   km.dll.CFRelease(kext_url) 
   km.dll.CFRelease(cf_kext_path) 
   if (status is not objc.OS_SUCCESS): 
      raise OSError('Failed   to   load   kext   at   {0}:   {1}'.format(kext_path, status))"," 'Install a kext at a given path. 
 :param kext_path: Path to the kext. 
 :type kext_path: str 
 :return: None'","'Calls into the IOKit to load a kext by file-system path. 
 Apple kext API doco here: 
 http://developer.apple.com/library/mac/#documentation/IOKit/Reference/ 
 KextManager_header_reference/Reference/reference.html 
 Args: 
 kext_path: Absolute or relative POSIX path to the kext. 
 Raises: 
 OSError: On failure to load the kext.'"
"@depends(HAS_PYVMOMI) 
 def list_hosts(host, username, password, protocol=None, port=None): 
    service_instance = salt.utils.vmware.get_service_instance(host=host, username=username, password=password, protocol=protocol, port=port) 
   return salt.utils.vmware.list_hosts(service_instance)", 'List all hosts in a vSphere cluster.',"'Returns a list of hosts for the the specified VMware environment. 
 host 
 The location of the host. 
 username 
 The username used to login to the host, such as ``root``. 
 password 
 The password used to login to the host. 
 protocol 
 Optionally set to alternate protocol if the host is not using the default 
 protocol. Default protocol is ``https``. 
 port 
 Optionally set to alternate port if the host is not using the default 
 port. Default port is ``443``. 
 .. code-block:: bash 
 salt \'*\' vsphere.list_hosts 1.2.3.4 root bad-password'"
"def get_cluster(options, env): 
    cluster_option = options['cluster'] 
   if cluster_option: 
      try: 
         cluster = BenchmarkCluster.from_cluster_yaml(FilePath(cluster_option)) 
      except IOError as e: 
         usage(options, 'Cluster   file   {!r}   not   found.'.format(e.filename)) 
   else: 
      try: 
         cluster = BenchmarkCluster.from_acceptance_test_env(env) 
      except KeyError as e: 
         usage(options, 'Environment   variable   {!r}   not   set.'.format(e.args[0])) 
      except ValueError as e: 
         usage(options, e.args[0]) 
      except ValidationError as e: 
         usage(options, e.message) 
   return cluster"," 'Get a cluster from the options or the environment. 
 :param options: Benchmark options 
 :param env: Environment to use 
 :return: Cluster'","'Obtain a cluster from the command line options and environment. 
 :param BenchmarkOption options: Parsed command line options. 
 :param dict env: Dictionary of environment variables. 
 :return BenchmarkCluster: Cluster to benchmark.'"
"def runsimple(func, port=8080): 
    import SimpleHTTPServer, SocketServer, BaseHTTPServer, urlparse 
   import socket, errno 
   import traceback 
   class WSGIHandler(SimpleHTTPServer.SimpleHTTPRequestHandler, ): 
      def run_wsgi_app(self): 
         (protocol, host, path, parameters, query, fragment) = urlparse.urlparse(('http://dummyhost%s' % self.path)) 
         env = {'wsgi.version': (1, 0), 'wsgi.url_scheme': 'http', 'wsgi.input': self.rfile, 'wsgi.errors': sys.stderr, 'wsgi.multithread': 1, 'wsgi.multiprocess': 0, 'wsgi.run_once': 0, 'REQUEST_METHOD': self.command, 'REQUEST_URI': self.path, 'PATH_INFO': path, 'QUERY_STRING': query, 'CONTENT_TYPE': self.headers.get('Content-Type', ''), 'CONTENT_LENGTH': self.headers.get('Content-Length', ''), 'REMOTE_ADDR': self.client_address[0], 'SERVER_NAME': self.server.server_address[0], 'SERVER_PORT': str(self.server.server_address[1]), 'SERVER_PROTOCOL': self.request_version} 
         for (http_header, http_value) in self.headers.items(): 
            env[('HTTP_%s' % http_header.replace('-', '_').upper())] = http_value 
         self.wsgi_sent_headers = 0 
         self.wsgi_headers = [] 
         try: 
            result = self.server.app(env, self.wsgi_start_response) 
            try: 
               try: 
                  for data in result: 
                     if data: 
                        self.wsgi_write_data(data) 
               finally: 
                  if hasattr(result, 'close'): 
                     result.close() 
            except socket.error as socket_err: 
               if (socket_err.args[0] in (errno.ECONNABORTED, errno.EPIPE)): 
                  return 
            except socket.timeout as socket_timeout: 
               return 
         except: 
            print >>debug, traceback.format_exc(), 
            internalerror() 
            if (not self.wsgi_sent_headers): 
               self.wsgi_start_response(ctx.status, ctx.headers) 
            self.wsgi_write_data(ctx.output) 
         if (not self.wsgi_sent_headers): 
            self.wsgi_write_data('   ') 
         return 
      do_POST = run_wsgi_app 
      def do_GET(self): 
         if self.path.startswith('/static/'): 
            SimpleHTTPServer.SimpleHTTPRequestHandler.do_GET(self) 
         else: 
            self.run_wsgi_app() 
      def wsgi_start_response(self, response_status, response_headers, exc_info=None): 
         if self.wsgi_sent_headers: 
            raise Exception('Headers   already   sent   and   start_response   called   again!') 
         self.wsgi_headers = (response_status, response_headers) 
         return self.wsgi_write_data 
      def wsgi_write_data(self, data): 
         if (not self.wsgi_sent_headers): 
            (status, headers) = self.wsgi_headers 
            status_code = status[:status.find('   ')] 
            status_msg = status[(status.find('   ') + 1):] 
            self.send_response(int(status_code), status_msg) 
            for (header, value) in headers: 
               self.send_header(header, value) 
            self.end_headers() 
            self.wsgi_sent_headers = 1 
         self.wfile.write(data) 
   class WSGIServer(SocketServer.ThreadingMixIn, BaseHTTPServer.HTTPServer, ): 
      def __init__(self, func): 
         BaseHTTPServer.HTTPServer.__init__(self, ('0.0.0.0', int(port)), WSGIHandler) 
         self.app = func 
         self.serverShuttingDown = 0 
   print (('Launching   server:   http://0.0.0.0:' + str(port)) + '/') 
   WSGIServer(func).serve_forever()"," 'Runs a simple HTTP server on the given port. 
 This is a very simple HTTP server that is used to test the WSGI middleware 
 and is not meant to be used in production. 
 :param func: A WSGI application function that takes one argument, the request 
 object, and returns a response object. 
 :param port: The port on which the server should listen.'","'Runs a simple HTTP server hosting WSGI app `func`. The directory `static/` 
 is hosted statically. 
 Based on [WsgiServer](http://www.owlfish.com/software/wsgiutils/documentation/wsgi-server-api.html) 
 from [Colin Stewart](http://www.owlfish.com/).'"
"def setup_form_view(view, request, form, *args, **kwargs): 
    view.request = request 
   try: 
      view.request.user = request.user 
   except AttributeError: 
      view.request.user = UserFactory() 
   view.args = args 
   view.kwargs = kwargs 
   view.form = form 
   return view"," 'Decorator to set up the view for a form view. 
 :param view: 
 :type view: django.views.generic.FormView 
 :param request: 
 :type request: django.http.HttpRequest 
 :param form: 
 :type form: django.forms.Form 
 :param args: 
 :type args: tuple 
 :param kwargs: 
 :type kwargs: dict 
 :return: 
 :rtype: django.views.generic.FormView'",'Mimic as_view and with forms to skip some of the context'
"def safeCSValue(value): 
    retVal = value 
   if (retVal and isinstance(retVal, basestring)): 
      if (not (retVal[0] == retVal[(-1)] == '""')): 
         if any(((_ in retVal) for _ in (conf.get('csvDel', defaults.csvDel), '""', '\n'))): 
            retVal = ('""%s""' % retVal.replace('""', '""""')) 
   return retVal"," 'Safe string value for csv output. 
 :param value: 
 :type value: 
 :return: 
 :rtype: 
 :raise: 
 :example: 
 >>> safeCSValue(""foo"") 
 \'foo\' 
 >>> safeCSValue(""foo\nbar"") 
 \'foo\\nbar\' 
 >>> safeCSValue(""foo\nbar\n"") 
 \'foo\\nbar\\n\' 
 >>> safeCSValue(""foo\nbar\n"") 
 \'foo\\nbar\\n\' 
 >>> safeCSValue(""foo\nbar\n"") 
 \'foo\\nbar\\n\' 
 >>> safeCSValue(""foo\nbar\n"") 
 \'foo\\nbar\\n\' 
 >>> safeCSValue(""foo\nbar\n"") 
 \'foo\\nbar\\n\' 
 >>> safeCSValue(""foo\nbar\n"") 
 \'foo\\nbar\\n\' 
 >>> safeCSValue(""foo\nbar\n"") 
 \'foo\\nbar\\n\' 
 >>>","'Returns value safe for CSV dumping 
 Reference: http://tools.ietf.org/html/rfc4180 
 >>> safeCSValue(u\'foo, bar\') 
 u\'""foo, bar""\' 
 >>> safeCSValue(u\'foobar\') 
 u\'foobar\''"
"def cmp(field, value, comparison='=', escape=(lambda v: _escape(v)), table=''): 
    if table: 
      field = abs(table, field) 
   if (isinstance(value, basestring) and (value.startswith(('*', '%')) or value.endswith(('*', '%')))): 
      if (comparison in ('=', 'i=', '==', LIKE)): 
         return ('%s   like   %s' % (field, escape(value.replace('*', '%')))) 
      if (comparison in ('!=', '<>')): 
         return ('%s   not   like   %s' % (field, escape(value.replace('*', '%')))) 
   if isinstance(value, basestring): 
      if (comparison == 'i='): 
         return ('%s   like   %s' % (field, escape(value))) 
   if isinstance(value, (list, tuple)): 
      if find((lambda v: (isinstance(v, basestring) and (v.startswith('*') or v.endswith('*')))), value): 
         return ('(%s)' % any(*[(field, v) for v in value]).sql(escape=escape)) 
      if (comparison in ('=', '==', IN)): 
         return ('%s   in   (%s)' % (field, ','.join((escape(v) for v in value)))) 
      if (comparison in ('!=', '<>')): 
         return ('%s   not   in   (%s)' % (field, ','.join((escape(v) for v in value)))) 
      if (comparison in (':', BETWEEN)): 
         return ('%s   between   %s   and   %s' % (field, escape(value[0]), escape(value[1]))) 
   if isinstance(value, type(None)): 
      if (comparison in ('=', '==')): 
         return ('%s   is   null' % field) 
      if (comparison in ('!=', '<>')): 
         return ('%s   is   not   null' % field) 
   if isinstance(value, Query): 
      if (comparison in ('=', '==', IN)): 
         return ('%s   in   %s' % (field, escape(value))) 
      if (comparison in ('!=', '<>')): 
         return ('%s   not   in   %s' % (field, escape(value))) 
   return ('%s%s%s' % (field, comparison, escape(value)))"," 'Compare a field against a value. 
 :param field: The field to compare. 
 :param value: The value to compare against. 
 :param comparison: The comparison operator. 
 :param escape: The function to use to escape the value. 
 :param table: The table to use for the field. 
 :return: The SQL statement to use to compare the field against the value. 
 :rtype: str'","'Returns an SQL WHERE comparison string using =, i=, !=, >, <, >=, <= or BETWEEN. 
 Strings may contain wildcards (*) at the start or at the end. 
 A list or tuple of values can be given when using =, != or BETWEEN.'"
"def parse_bdist_wininst(name): 
    lower = name.lower() 
   (base, py_ver) = (None, None) 
   if lower.endswith('.exe'): 
      if lower.endswith('.win32.exe'): 
         base = name[:(-10)] 
      elif lower.startswith('.win32-py', (-16)): 
         py_ver = name[(-7):(-4)] 
         base = name[:(-16)] 
   return (base, py_ver)"," 'Parse the base name and version from a bdist_wininst name. 
 Returns a tuple (base, py_ver). 
 :param name: The bdist_wininst name to parse. 
 :return: A tuple of the base name and version.'","'Return (base,pyversion) or (None,None) for possible .exe name'"
"def run_pylint_gui(): 
    try: 
      from pylint.gui import Run 
      Run(sys.argv[1:]) 
   except ImportError: 
      sys.exit('tkinter   is   not   available')", 'Run pylint on the files in the current directory.','run pylint-gui'
"def _process_caption(caption): 
    tokenized_caption = [FLAGS.start_word] 
   tokenized_caption.extend(nltk.tokenize.word_tokenize(caption.lower())) 
   tokenized_caption.append(FLAGS.end_word) 
   return tokenized_caption", 'Processes the caption string into a list of tokens.',"'Processes a caption string into a list of tonenized words. 
 Args: 
 caption: A string caption. 
 Returns: 
 A list of strings; the tokenized caption.'"
"@pytest.fixture 
 def Worksheet(): 
    from openpyxl.worksheet import Worksheet 
   return Worksheet", 'Fixture to create a Worksheet object.','Worksheet Class'
"def allclose(x, y, rtol=1e-05, atol=1e-08): 
    for (xi, yi) in zip(x, y): 
      if (not (abs((xi - yi)) <= (atol + (rtol * abs(yi))))): 
         return False 
   return True"," 'Returns True if all elements of x are within rtol of y, False otherwise.'","'Returns True if x and y are sufficiently close, elementwise. 
 Parameters 
 rtol : float 
 The relative error tolerance. 
 atol : float 
 The absolute error tolerance.'"
"def abort_merge(): 
    title = N_(u'Abort   Merge...') 
   txt = N_(u'Aborting   the   current   merge   will   cause   *ALL*   uncommitted   changes   to   be   lost.\nRecovering   uncommitted   changes   is   not   possible.') 
   info_txt = N_(u'Aborting   the   current   merge?') 
   ok_txt = N_(u'Abort   Merge') 
   if qtutils.confirm(title, txt, info_txt, ok_txt, default=False, icon=icons.undo()): 
      gitcmds.abort_merge()"," 'Abort the current merge. 
 This will cause *ALL* uncommitted changes to be lost. 
 Recovering uncommitted changes is not possible.'",'Prompts before aborting a merge in progress'
"def _patched_list_steps(emr_conn, *args, **kwargs): 
    try: 
      boto.emr.emrobject.ClusterTimeline = _PatchedClusterTimeline 
      return emr_conn.list_steps(*args, **kwargs) 
   finally: 
      boto.emr.emrobject.ClusterTimeline = ClusterTimeline"," 'Patch the list_steps method to return a list of ClusterTimeline objects 
 instead of a list of dicts.'","'Wrapper for :py:meth:`boto.emr.EmrConnection.list_steps()` 
 that works around around `boto\'s startdatetime bug 
 <https://github.com/boto/boto/issues/3268>`__.'"
"@register.filter 
 def has_unrendered_errors(bound_field): 
    return (bound_field.errors and (not hasattr(bound_field.field.widget, u'render_with_errors')))", 'Check if a form field has unrendered errors.',"'Return true if this field has errors that were not accounted for by render_with_errors, because 
 the widget does not support the render_with_errors method'"
"def scrub_text(text): 
    scrubbed_text = text.rstrip().replace('\n', '\\n').replace(' DCTB ', ('   ' * 4)) 
   return scrubbed_text"," 'Scrub text for use in the output. 
 Scrubbing is done to remove trailing newlines, and to remove the 
 DCTB tag from the beginning of the text. 
 :param text: Text to scrub. 
 :type text: str 
 :return: Scrubbed text. 
 :rtype: str'","'Cleans up text. 
 Escapes newlines and tabs. 
 Parameters 
 text : str 
 Text to clean up.'"
"def formatStatResponse(msgs): 
    i = 0 
   bytes = 0 
   for size in msgs: 
      i += 1 
      bytes += size 
      (yield None) 
   (yield successResponse(('%d   %d' % (i, bytes))))"," 'Format a stat response for a stream. 
 :param msgs: list of messages 
 :return: a generator of stat responses'","'Format a list of message sizes into a STAT response. 
 This generator function is intended to be used with 
 L{Cooperator <twisted.internet.task.Cooperator>}. 
 @type msgs: L{list} of L{int} 
 @param msgs: A list of message sizes. 
 @rtype: L{None} or L{bytes} 
 @return: Yields none until a result is available, then a string that is 
 suitable for use in a STAT response. The string consists of the number 
 of messages and the total size of the messages in octets.'"
"def find_exact(tracks, query=None, limit=100, offset=0, uris=None): 
    if (query is None): 
      query = {} 
   _validate_query(query) 
   for (field, values) in query.items(): 
      for value in values: 
         if (field == u'track_no'): 
            q = _convert_to_int(value) 
         else: 
            q = value.strip() 
         def uri_filter(t): 
            return (q == t.uri) 
         def track_name_filter(t): 
            return (q == t.name) 
         def album_filter(t): 
            return (q == getattr(getattr(t, u'album', None), u'name', None)) 
         def artist_filter(t): 
            return filter((lambda a: (q == a.name)), t.artists) 
         def albumartist_filter(t): 
            return any([(q == a.name) for a in getattr(t.album, u'artists', [])]) 
         def composer_filter(t): 
            return any([(q == a.name) for a in getattr(t, u'composers', [])]) 
         def performer_filter(t): 
            return any([(q == a.name) for a in getattr(t, u'performers', [])]) 
         def track_no_filter(t): 
            return (q == t.track_no) 
         def genre_filter(t): 
            return (t.genre and (q == t.genre)) 
         def date_filter(t): 
            return (q == t.date) 
         def comment_filter(t): 
            return (q == t.comment) 
         def any_filter(t): 
            return (uri_filter(t) or track_name_filter(t) or album_filter(t) or artist_filter(t) or albumartist_filter(t) or composer_filter(t) or performer_filter(t) or track_no_filter(t) or genre_filter(t) or date_filter(t) or comment_filter(t)) 
         if (field == u'uri'): 
            tracks = filter(uri_filter, tracks) 
         elif (field == u'track_name'): 
            tracks = filter(track_name_filter, tracks) 
         elif (field == u'album'): 
            tracks = filter(album_filter, tracks) 
         elif (field == u'artist'): 
            tracks = filter(artist_filter, tracks) 
         elif (field == u'albumartist'): 
            tracks = filter(albumartist_filter, tracks) 
         elif (field == u'composer'): 
            tracks = filter(composer_filter, tracks) 
         elif (field == u'performer'): 
            tracks = filter(performer_filter, tracks) 
         elif (field == u'track_no'): 
            tracks = filter(track_no_filter, tracks) 
         elif (field == u'genre'): 
            tracks = filter(genre_filter, tracks) 
         elif (field == u'date'): 
            tracks = filter(date_filter, tracks) 
         elif (field == u'comment'): 
            tracks = filter(comment_filter, tracks) 
         elif (field == u'any'): 
            tracks = filter(any_filter, tracks) 
         else: 
            raise LookupError((u'Invalid   lookup   field:   %s' % field)) 
   if (limit is None): 
      tracks = tracks[offset:] 
   else: 
      tracks = tracks[offset:(offset + limit)] 
   return SearchResult(uri=u'local:search', tracks=tracks)"," 'Find tracks matching the query. 
 :param tracks: A list of tracks to search. 
 :param query: A dict of fields to search for. 
 :param limit: The number of results to return. 
 :param offset: The number of results to skip. 
 :param uris: A list of URIs to exclude from the search. 
 :returns: A SearchResult object. 
 :rtype: SearchResult'","'Filter a list of tracks where ``field`` is ``values``. 
 :param list tracks: a list of :class:`~mopidy.models.Track` 
 :param dict query: one or more field/value pairs to search for 
 :param int limit: maximum number of results to return 
 :param int offset: offset into result set to use. 
 :param uris: zero or more URI roots to limit the search to 
 :type uris: list of strings or :class:`None` 
 :rtype: :class:`~mopidy.models.SearchResult`'"
"@transaction.non_atomic_requests 
 @cache_control(no_cache=True, no_store=True, must_revalidate=True) 
 @coach_dashboard 
 def ccx_grades_csv(request, course, ccx=None): 
    if (not ccx): 
      raise Http404 
   ccx_key = CCXLocator.from_course_locator(course.id, unicode(ccx.id)) 
   with ccx_course(ccx_key) as course: 
      prep_course_for_grading(course, request) 
      enrolled_students = User.objects.filter(courseenrollment__course_id=ccx_key, courseenrollment__is_active=1).order_by('username').select_related('profile') 
      grades = CourseGradeFactory().iter(course, enrolled_students) 
      header = None 
      rows = [] 
      for (student, course_grade, __) in grades: 
         if course_grade: 
            if (not header): 
               header = [section['label'].encode('utf-8') for section in course_grade.summary[u'section_breakdown']] 
               rows.append((['id', 'email', 'username', 'grade'] + header)) 
            percents = {section['label']: section.get('percent', 0.0) for section in course_grade.summary[u'section_breakdown'] if ('label' in section)} 
            row_percents = [percents.get(label, 0.0) for label in header] 
            rows.append(([student.id, student.email, student.username, course_grade.percent] + row_percents)) 
      buf = StringIO() 
      writer = csv.writer(buf) 
      for row in rows: 
         writer.writerow(row) 
      response = HttpResponse(buf.getvalue(), content_type='text/csv') 
      response['Content-Disposition'] = 'attachment' 
      return response",'.csv export of a CCX\'s grades','Download grades as CSV.'
"@register_canonicalize 
 @register_uncanonicalize 
 @register_specialize 
 @gof.local_optimizer(ALL_REDUCE) 
 def local_reduce_broadcastable(node): 
    if isinstance(node.op, T.CAReduce): 
      (reduced,) = node.inputs 
      odtype = node.outputs[0].dtype 
      if (node.op.axis is None): 
         if all(reduced.broadcastable): 
            return [reduced.dimshuffle().astype(odtype)] 
      else: 
         axis = list(node.op.axis) 
         cuttable = [a for a in axis if reduced.broadcastable[a]] 
         if cuttable: 
            new_axis = [] 
            pattern = [] 
            ii = 0 
            for p in xrange(reduced.ndim): 
               if (p not in cuttable): 
                  if (p in axis): 
                     new_axis.append(ii) 
                  pattern.append(p) 
                  ii += 1 
            new_reduced = reduced.dimshuffle(*pattern) 
            if new_axis: 
               if (type(node.op) == theano.tensor.elemwise.CAReduce): 
                  new_op = node.op.__class__(node.op.scalar_op, axis=new_axis) 
               else: 
                  new_op = node.op.__class__(axis=new_axis) 
               return [new_op(new_reduced)] 
            else: 
               return [new_reduced.astype(odtype)]", 'Local reduction on broadcastable tensors','Remove reduction over broadcastable dimensions.'
"def _get_immediate_cls_attr(cls, attrname, strict=False): 
    if (not issubclass(cls, object)): 
      return None 
   for base in cls.__mro__: 
      _is_declarative_inherits = hasattr(base, '_decl_class_registry') 
      if ((attrname in base.__dict__) and ((base is cls) or (((base in cls.__bases__) if strict else True) and (not _is_declarative_inherits)))): 
         return getattr(base, attrname) 
   else: 
      return None"," 'Given a class, return the immediate base class attribute with the given 
 name. 
 This is a helper function for the _get_declarative_attr_cls() function. 
 If strict is True, only the base class itself will be searched. 
 If strict is False, the base class itself and all of its declared 
 base classes will be searched. 
 :param cls: the class to search for the attribute 
 :param attrname: the attribute name to look for 
 :param strict: whether to only search the immediate base class or 
 all of the declared base classes as well 
 :returns: the attribute if found, or None if not found 
 :rtype: object or None'","'return an attribute of the class that is either present directly 
 on the class, e.g. not on a superclass, or is from a superclass but 
 this superclass is a mixin, that is, not a descendant of 
 the declarative base. 
 This is used to detect attributes that indicate something about 
 a mapped class independently from any mapped classes that it may 
 inherit from.'"
"def wiki_escape(s): 
    ret = [] 
   for word in s.split(): 
      if re.match('[A-Z]+[a-z]+[A-Z]', word): 
         word = ('!%s' % word) 
      ret.append(word) 
   return '   '.join(ret)"," 'Returns a wiki-escaped version of the string s. 
 This is used to escape wiki links in the text. 
 >>> wiki_escape(""This is a link: http://www.example.com"") 
 \'This is a link: !http://www.example.com\''","'Detect WikiSyntax (i.e. InterCaps, a.k.a. CamelCase) and escape it.'"
"def getaddresses(fieldvalues): 
    all = COMMASPACE.join(fieldvalues) 
   a = _AddressList(all) 
   return a.addresslist"," 'Returns the addresses from a fieldvalue list. 
 :param fieldvalues: A list of fieldvalues to get addresses from. 
 :return: A list of addresses.'","'Return a list of (REALNAME, EMAIL) for each fieldvalue.'"
"def main(): 
    errors = 0 
   fits_files = handle_options(sys.argv[1:]) 
   setup_logging() 
   for filename in fits_files: 
      errors += process_file(filename) 
   if errors: 
      log.warning('{}   errors'.format(errors)) 
   return int(bool(errors))"," 'Main function for the program. 
 :returns: 0 if all files were processed successfully, 1 otherwise.'","'Processes command line parameters into options and files,  then checks 
 or update FITS DATASUM and CHECKSUM keywords for the specified files.'"
"def _extend_external_network_default(core_plugin, net_res, net_db): 
    if (net_db.external is not None): 
      net_res[IS_DEFAULT] = net_db.external.is_default 
   return net_res", 'Extend the external network default value.','Add is_default field to \'show\' response.'
"def cg_optimization_mnist(n_epochs=50, mnist_pkl_gz='mnist.pkl.gz'): 
    datasets = load_data(mnist_pkl_gz) 
   (train_set_x, train_set_y) = datasets[0] 
   (valid_set_x, valid_set_y) = datasets[1] 
   (test_set_x, test_set_y) = datasets[2] 
   batch_size = 600 
   n_train_batches = (train_set_x.get_value(borrow=True).shape[0] // batch_size) 
   n_valid_batches = (valid_set_x.get_value(borrow=True).shape[0] // batch_size) 
   n_test_batches = (test_set_x.get_value(borrow=True).shape[0] // batch_size) 
   n_in = (28 * 28) 
   n_out = 10 
   print('...   building   the   model') 
   minibatch_offset = T.lscalar() 
   x = T.matrix() 
   y = T.ivector() 
   classifier = LogisticRegression(input=x, n_in=(28 * 28), n_out=10) 
   cost = classifier.negative_log_likelihood(y).mean() 
   test_model = theano.function([minibatch_offset], classifier.errors(y), givens={x: test_set_x[minibatch_offset:(minibatch_offset + batch_size)], y: test_set_y[minibatch_offset:(minibatch_offset + batch_size)]}, name='test') 
   validate_model = theano.function([minibatch_offset], classifier.errors(y), givens={x: valid_set_x[minibatch_offset:(minibatch_offset + batch_size)], y: valid_set_y[minibatch_offset:(minibatch_offset + batch_size)]}, name='validate') 
   batch_cost = theano.function([minibatch_offset], cost, givens={x: train_set_x[minibatch_offset:(minibatch_offset + batch_size)], y: train_set_y[minibatch_offset:(minibatch_offset + batch_size)]}, name='batch_cost') 
   batch_grad = theano.function([minibatch_offset], T.grad(cost, classifier.theta), givens={x: train_set_x[minibatch_offset:(minibatch_offset + batch_size)], y: train_set_y[minibatch_offset:(minibatch_offset + batch_size)]}, name='batch_grad') 
   def train_fn(theta_value): 
      classifier.theta.set_value(theta_value, borrow=True) 
      train_losses = [batch_cost((i * batch_size)) for i in range(n_train_batches)] 
      return numpy.mean(train_losses) 
   def train_fn_grad(theta_value): 
      classifier.theta.set_value(theta_value, borrow=True) 
      grad = batch_grad(0) 
      for i in range(1, n_train_batches): 
         grad += batch_grad((i * batch_size)) 
      return (grad / n_train_batches) 
   validation_scores = [numpy.inf, 0] 
   def callback(theta_value): 
      classifier.theta.set_value(theta_value, borrow=True) 
      validation_losses = [validate_model((i * batch_size)) for i in range(n_valid_batches)] 
      this_validation_loss = numpy.mean(validation_losses) 
      print(('validation   error   %f   %%' % ((this_validation_loss * 100.0),))) 
      if (this_validation_loss < validation_scores[0]): 
         validation_scores[0] = this_validation_loss 
         test_losses = [test_model((i * batch_size)) for i in range(n_test_batches)] 
         validation_scores[1] = numpy.mean(test_losses) 
   import scipy.optimize 
   print('Optimizing   using   scipy.optimize.fmin_cg...') 
   start_time = timeit.default_timer() 
   best_w_b = scipy.optimize.fmin_cg(f=train_fn, x0=numpy.zeros(((n_in + 1) * n_out), dtype=x.dtype), fprime=train_fn_grad, callback=callback, disp=0, maxiter=n_epochs) 
   end_time = timeit.default_timer() 
   print(('Optimization   complete   with   best   validation   score   of   %f   %%,   with   test   performance   %f   %%' % ((validation_scores[0] * 100.0), (validation_scores[1] * 100.0)))) 
   print((('The   code   for   file   ' + os.path.split(__file__)[1]) + ('   ran   for   %.1fs' % (end_time - start_time))), file=sys.stderr)"," 'Optimize the Logistic Regression model using the Conjugate Gradient algorithm. 
 The optimization is performed on the MNIST dataset. 
 Parameters 
 n_epochs : int, optional 
 The number of epochs to train the model for. 
 mnist_pkl_gz : str, optional 
 The path to the pickled MNIST dataset. 
 Returns 
 None 
 Notes 
 The code is based on the following paper: 
 ""A Fast Conjugate Gradient Algorithm for Large Scale Linear Algebra Problems"" 
 by David P. Anderson, 2000. 
 See also 
 LogisticRegression 
 References 
 Anderson, D. P. (2000). A Fast Conjugate Gradient Algorithm for Large Scale Linear Algebra Problems. 
 http://www.cs.ubc.ca/~dpd/papers/fastcg.pdf'","'Demonstrate conjugate gradient optimization of a log-linear model 
 This is demonstrated on MNIST. 
 :type n_epochs: int 
 :param n_epochs: number of epochs to run the optimizer 
 :type mnist_pkl_gz: string 
 :param mnist_pkl_gz: the path of the mnist training file from 
 http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'"
"def modify_node(hostname, username, password, name, connection_limit=None, description=None, dynamic_ratio=None, logging=None, monitor=None, rate_limit=None, ratio=None, session=None, state=None, trans_label=None): 
    params = {'connection-limit': connection_limit, 'description': description, 'dynamic-ratio': dynamic_ratio, 'logging': logging, 'monitor': monitor, 'rate-limit': rate_limit, 'ratio': ratio, 'session': session, 'state': state} 
   bigip_session = _build_session(username, password, trans_label) 
   payload = _loop_payload(params) 
   payload['name'] = name 
   try: 
      response = bigip_session.put((BIG_IP_URL_BASE.format(host=hostname) + '/ltm/node/{name}'.format(name=name)), data=json.dumps(payload)) 
   except requests.exceptions.ConnectionError as e: 
      return _load_connection_error(hostname, e) 
   return _load_response(response)"," 'Modifies the specified node. 
 .. code-block:: python 
 # modify a node 
 from fabtools import sysadmin 
 sysadmin.bigip.modify_node(hostname, \'admin\' , \'admin\' , \'node1\' , \'description\' , \'connection-limit\' , 100, \'monitor\' , 500, \'logging\' , \'true\') 
 :param hostname: the hostname of the device 
 :param username: the user name to use to connect to the device 
 :param password: the password to use to connect to the device 
 :param name: the name of the node to modify 
 :param connection_limit: the maximum number of connections allowed to this node 
 :param description: the description of the node 
 :param dynamic_ratio: the dynamic ratio of the node 
 :param logging: the logging level of the node 
 :param monitor: the monitor level of the node 
 :param rate_limit: the rate limit of the node 
 :param ratio: the ratio of the node 
 :param session: the session object","'A function to connect to a bigip device and modify an existing node. 
 hostname 
 The host/address of the bigip device 
 username 
 The iControl REST username 
 password 
 The iControl REST password 
 name 
 The name of the node to modify 
 connection_limit 
 [integer] 
 description 
 [string] 
 dynamic_ratio 
 [integer] 
 logging 
 [enabled | disabled] 
 monitor 
 [[name] | none | default] 
 rate_limit 
 [integer] 
 ratio 
 [integer] 
 session 
 [user-enabled | user-disabled] 
 state 
 [user-down | user-up ] 
 trans_label 
 The label of the transaction stored within the grain: 
 ``bigip_f5_trans:<label>`` 
 CLI Example:: 
 salt \'*\' bigip.modify_node bigip admin admin 10.1.1.2 ratio=2 logging=enabled'"
"def get_manager(cls): 
    db_user = boto.config.get('DB', 'db_user', None) 
   db_passwd = boto.config.get('DB', 'db_passwd', None) 
   db_type = boto.config.get('DB', 'db_type', 'SimpleDB') 
   db_name = boto.config.get('DB', 'db_name', None) 
   db_table = boto.config.get('DB', 'db_table', None) 
   db_host = boto.config.get('DB', 'db_host', 'sdb.amazonaws.com') 
   db_port = boto.config.getint('DB', 'db_port', 443) 
   enable_ssl = boto.config.getbool('DB', 'enable_ssl', True) 
   sql_dir = boto.config.get('DB', 'sql_dir', None) 
   debug = boto.config.getint('DB', 'debug', 0) 
   module_name = cls.__module__.replace('.', '_') 
   db_section = ((('DB_' + module_name) + '_') + cls.__name__) 
   if (not boto.config.has_section(db_section)): 
      db_section = ('DB_' + cls.__name__) 
   if boto.config.has_section(db_section): 
      db_user = boto.config.get(db_section, 'db_user', db_user) 
      db_passwd = boto.config.get(db_section, 'db_passwd', db_passwd) 
      db_type = boto.config.get(db_section, 'db_type', db_type) 
      db_name = boto.config.get(db_section, 'db_name', db_name) 
      db_table = boto.config.get(db_section, 'db_table', db_table) 
      db_host = boto.config.get(db_section, 'db_host', db_host) 
      db_port = boto.config.getint(db_section, 'db_port', db_port) 
      enable_ssl = boto.config.getint(db_section, 'enable_ssl', enable_ssl) 
      debug = boto.config.getint(db_section, 'debug', debug) 
   elif (hasattr(cls, '_db_name') and (cls._db_name is not None)): 
      db_name = cls._db_name 
   elif hasattr(cls.__bases__[0], '_manager'): 
      return cls.__bases__[0]._manager 
   if (db_type == 'SimpleDB'): 
      from boto.sdb.db.manager.sdbmanager import SDBManager 
      return SDBManager(cls, db_name, db_user, db_passwd, db_host, db_port, db_table, sql_dir, enable_ssl) 
   elif (db_type == 'XML'): 
      from boto.sdb.db.manager.xmlmanager import XMLManager 
      return XMLManager(cls, db_name, db_user, db_passwd, db_host, db_port, db_table, sql_dir, enable_ssl) 
   else: 
      raise ValueError(('Unknown   db_type:   %s' % db_type))"," 'Returns a DBManager instance for the given class. 
 :param cls: The class to get a manager for 
 :return: A DBManager instance for the given class'","'Returns the appropriate Manager class for a given Model class.  It 
 does this by looking in the boto config for a section like this:: 
 [DB] 
 db_type = SimpleDB 
 db_user = <aws access key id> 
 db_passwd = <aws secret access key> 
 db_name = my_domain 
 [DB_TestBasic] 
 db_type = SimpleDB 
 db_user = <another aws access key id> 
 db_passwd = <another aws secret access key> 
 db_name = basic_domain 
 db_port = 1111 
 The values in the DB section are ""generic values"" that will be used 
 if nothing more specific is found.  You can also create a section for 
 a specific Model class that gives the db info for that class. 
 In the example above, TestBasic is a Model subclass.'"
"def I(attr): 
    return range_property(attr, 0, 4294967295)"," 'Returns a property that returns the integer value of the attribute. 
 :param attr: name of the attribute to get the value of 
 :return: property 
 :rtype: property'",'Unsigned Long'
"def set_edit_mode(request, flag): 
    if (flag and could_edit(request)): 
      request.session[EDIT_FLAG_NAME] = True 
   else: 
      request.session.pop(EDIT_FLAG_NAME, None)"," 'Sets the edit mode flag for the request. 
 This is a convenience function that sets the edit mode flag for the 
 request. If the flag is True, then the user is allowed to edit the 
 page. If the flag is False, then the user is not allowed to edit the 
 page.'","'Enable or disable edit mode for the request. 
 :param request: HTTP request 
 :type request: django.http.HttpRequest 
 :param flag: Enable flag 
 :type flag: bool'"
"def register(func, msg_type=None, dispatcher_name=None, active_by_default=True): 
    if (msg_type and (msg_type not in MSG_TYPE_MAP)): 
      raise InvalidHandlerType(('Invalid   message   type   [%s]:   type   should   be   in   %s' % (msg_type, str(MSG_TYPES)))) 
   handler = Handler(func=func, name=dispatcher_name) 
   if (msg_type is None): 
      _registry[RAW_TYPE].append(handler) 
   else: 
      _registry[msg_type].append(handler) 
   if active_by_default: 
      _active.add(dispatcher_name)"," 'Register a handler for a message type. 
 :param func: The function to call when the message is received. 
 :type func: callable 
 :param msg_type: The message type to register the handler for. 
 :type msg_type: str 
 :param dispatcher_name: The name of the dispatcher to register the handler for. 
 :type dispatcher_name: str 
 :param active_by_default: Whether the handler should be active by default. 
 :type active_by_default: bool 
 :return: None 
 :rtype: None'","'Register handler to RAW if msg_type not given. 
 :type func: callable 
 :type msg_type: str or unicode'"
"def query(params=None): 
    path = 'https://api.qingcloud.com/iaas/' 
   access_key_id = config.get_cloud_config_value('access_key_id', get_configured_provider(), __opts__, search_global=False) 
   access_key_secret = config.get_cloud_config_value('secret_access_key', get_configured_provider(), __opts__, search_global=False) 
   real_parameters = {'access_key_id': access_key_id, 'signature_version': DEFAULT_QINGCLOUD_SIGNATURE_VERSION, 'time_stamp': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()), 'version': DEFAULT_QINGCLOUD_API_VERSION} 
   if params: 
      for (key, value) in params.items(): 
         if isinstance(value, list): 
            for i in range(1, (len(value) + 1)): 
               if isinstance(value[(i - 1)], dict): 
                  for (sk, sv) in value[(i - 1)].items(): 
                     if (isinstance(sv, dict) or isinstance(sv, list)): 
                        sv = json.dumps(sv, separators=(',', ':')) 
                     real_parameters['{0}.{1}.{2}'.format(key, i, sk)] = sv 
               else: 
                  real_parameters['{0}.{1}'.format(key, i)] = value[(i - 1)] 
         else: 
            real_parameters[key] = value 
   signature = _compute_signature(real_parameters, access_key_secret, 'GET', '/iaas/') 
   real_parameters['signature'] = signature 
   request = requests.get(path, params=real_parameters, verify=False) 
   if (request.status_code != 200): 
      raise SaltCloudSystemExit(""An   error   occurred   while   querying   QingCloud.   HTTP   Code:   {0}      Error:   '{1}'"".format(request.status_code, request.text)) 
   log.debug(request.url) 
   content = request.text 
   result = json.loads(content, object_hook=salt.utils.decode_dict) 
   if (result['ret_code'] != 0): 
      raise SaltCloudSystemExit(pprint.pformat(result.get('message', {}))) 
   return result", 'Query QingCloud API','Make a web call to QingCloud IaaS API.'
"def is_list_of_ints(intlist): 
    if (not isinstance(intlist, list)): 
      return False 
   for i in intlist: 
      if (not isinstance(i, int)): 
         return False 
   return True"," 'Check if a list is of integers. 
 :param intlist: A list of integers 
 :return: True if all elements are integers, False otherwise'",'Return True if list is a list of ints.'
"def pade(an, m): 
    an = asarray(an) 
   N = (len(an) - 1) 
   n = (N - m) 
   if (n < 0): 
      raise ValueError('Order   of   q   <m>   must   be   smaller   than   len(an)-1.') 
   Akj = eye((N + 1), (n + 1)) 
   Bkj = zeros(((N + 1), m), 'd') 
   for row in range(1, (m + 1)): 
      Bkj[row, :row] = (- an[:row][::(-1)]) 
   for row in range((m + 1), (N + 1)): 
      Bkj[row, :] = (- an[(row - m):row][::(-1)]) 
   C = hstack((Akj, Bkj)) 
   pq = linalg.solve(C, an) 
   p = pq[:(n + 1)] 
   q = r_[(1.0, pq[(n + 1):])] 
   return (poly1d(p[::(-1)]), poly1d(q[::(-1)]))"," 'Return the Pade approximant of an analytic function. 
 Parameters 
 an : ndarray 
 The analytic function. 
 m : int 
 The order of the Pade approximant. 
 Returns 
 (poly1d, poly1d) 
 The Pade approximant. 
 Examples 
 >>> from sympy.physics.pade import pade 
 >>> from sympy import Symbol, exp, I, sin, cos, pi 
 >>> x = Symbol(\'x\') 
 >>> pade(exp(x), 2) 
 (exp(x), exp(-x)) 
 >>> pade(sin(x), 2) 
 (sin(x), -cos(x)) 
 >>> pade(cos(x), 2) 
 (cos(x), sin(x)) 
 >>> pade(pi, 2) 
 (pi, -pi) 
 >>> pade(exp(x), 3) 
 (exp(x), -exp(-x), exp(-2*x)) 
 >>> pade(sin(x), 3) 
 (sin","'Return Pade approximation to a polynomial as the ratio of two polynomials. 
 Parameters 
 an : (N,) array_like 
 Taylor series coefficients. 
 m : int 
 The order of the returned approximating polynomials. 
 Returns 
 p, q : Polynomial class 
 The Pade approximation of the polynomial defined by `an` is 
 ``p(x)/q(x)``. 
 Examples 
 >>> from scipy.interpolate import pade 
 >>> e_exp = [1.0, 1.0, 1.0/2.0, 1.0/6.0, 1.0/24.0, 1.0/120.0] 
 >>> p, q = pade(e_exp, 2) 
 >>> e_exp.reverse() 
 >>> e_poly = np.poly1d(e_exp) 
 Compare ``e_poly(x)`` and the Pade approximation ``p(x)/q(x)`` 
 >>> e_poly(1) 
 2.7166666666666668 
 >>> p(1)/q(1) 
 2.7179487179487181'"
"def count(session, query): 
    counts = query.selectable.with_only_columns([func.count()]) 
   num_results = session.execute(counts.order_by(None)).scalar() 
   if ((num_results is None) or (query._limit is not None)): 
      return query.order_by(None).count() 
   return num_results"," 'Count the number of rows in the query. 
 :param session: An active Session. 
 :param query: The Query to count. 
 :return: The number of rows in the query. 
 :rtype: int'","'Returns the count of the specified `query`. 
 This function employs an optimization that bypasses the 
 :meth:`sqlalchemy.orm.Query.count` method, which can be very slow 
 for large queries.'"
"def test_create_angles(): 
    u'   The   ""angle""   is   a   fundamental   object.   The   internal\n            representation   is   stored   in   radians,   but   this   is   transparent   to   the   user.\n            Units   *must*   be   specified   rather   than   a   default   value   be   assumed.   This   is\n            as   much   for   self-documenting   code   as   anything   else.\n\n            Angle   objects   simply   represent   a   single   angular   coordinate.   More   specific\n            angular   coordinates   (e.g.   Longitude,   Latitude)   are   subclasses   of   Angle.' 
   a1 = Angle(54.12412, unit=u.degree) 
   a2 = Angle(u'54.12412', unit=u.degree) 
   a3 = Angle(u'54:07:26.832', unit=u.degree) 
   a4 = Angle(u'54.12412   deg') 
   a5 = Angle(u'54.12412   degrees') 
   a6 = Angle(u'54.12412\xb0') 
   a7 = Angle((54, 7, 26.832), unit=u.degree) 
   a8 = Angle(u'54\xb007\'26.832""') 
   a9 = Angle([54, 7, 26.832], unit=u.degree) 
   assert_allclose(a9.value, [54, 7, 26.832]) 
   assert (a9.unit is u.degree) 
   a10 = Angle(3.60827466667, unit=u.hour) 
   a11 = Angle(u'3:36:29.7888000120', unit=u.hour) 
   a12 = Angle((3, 36, 29.788800012), unit=u.hour) 
   a13 = Angle((3, 36, 29.788800012), unit=u'hour') 
   Angle(0.944644098745, unit=u.radian) 
   with pytest.raises(u.UnitsError): 
      Angle(54.12412) 
   with pytest.raises(u.UnitsError): 
      Angle(54.12412, unit=u.m) 
   with pytest.raises(ValueError): 
      Angle(12.34, unit=u'not   a   unit') 
   a14 = Angle(u'03h36m29.7888000120') 
   a15 = Angle(u'5h4m3s') 
   assert (a15.unit == u.hourangle) 
   a16 = Angle(u'1   d') 
   a17 = Angle(u'1   degree') 
   assert (a16.degree == 1) 
   assert (a17.degree == 1) 
   a18 = Angle(u'54   07.4472', unit=u.degree) 
   a19 = Angle(u'54:07.4472', unit=u.degree) 
   a20 = Angle(u'54d07.4472m', unit=u.degree) 
   a21 = Angle(u'3h36m', unit=u.hour) 
   a22 = Angle(u'3.6h', unit=u.hour) 
   a23 = Angle(u'-   3h', unit=u.hour) 
   a24 = Angle(u'+   3h', unit=u.hour) 
   assert (a1 == a2 == a3 == a4 == a5 == a6 == a7 == a8 == a18 == a19 == a20) 
   assert_allclose(a1.radian, a2.radian) 
   assert_allclose(a2.degree, a3.degree) 
   assert_allclose(a3.radian, a4.radian) 
   assert_allclose(a4.radian, a5.radian) 
   assert_allclose(a5.radian, a6.radian) 
   assert_allclose(a6.radian, a7.radian) 
   assert_allclose(a10.degree, a11.degree) 
   assert (a11 == a12 == a13 == a14) 
   assert (a21 == a22) 
   assert (a23 == (- a24)) 
   with pytest.raises(IllegalSecondError): 
      a = Angle(u'12   32   99', unit=u.degree) 
   with pytest.raises(IllegalMinuteError): 
      a = Angle(u'12   99   23', unit=u.degree) 
   with pytest.raises(IllegalSecondError): 
      a = Angle(u'12   32   99', unit=u.hour) 
   with pytest.raises(IllegalMinuteError): 
      a = Angle(u'12   99   23', unit=u.hour) 
   with pytest.raises(IllegalHourError): 
      a = Angle(u'99   25   51.0', unit=u.hour) 
   with pytest.raises(ValueError): 
      a = Angle(u'12   25   51.0xxx', unit=u.hour) 
   with pytest.raises(ValueError): 
      a = Angle(u'12h34321m32.2s') 
   assert (a1 is not None)", 'Test Angle class','Tests creating and accessing Angle objects'
"def getVector3RemoveByPrefix(prefix, vector3, xmlElement): 
    vector3RemoveByPrefix = getVector3ByPrefix(vector3, prefix, xmlElement) 
   euclidean.removePrefixFromDictionary(xmlElement.attributeDictionary, prefix) 
   return vector3RemoveByPrefix"," 'Get the vector3 by removing the prefix from the vector3 
 :param prefix: the prefix to be removed from the vector3 
 :type prefix: str 
 :param vector3: the vector3 to be modified 
 :type vector3: Vector3 
 :param xmlElement: the xml element containing the vector3 
 :type xmlElement: Element'","'Get vector3 from prefix and xml element, then remove prefix attributes from dictionary.'"
"def _escape_jid(jid): 
    jid = str(jid) 
   jid = re.sub(""'*"", '', jid) 
   return jid"," 'Escape JIDs to prevent XSS attacks. 
 :param jid: JID to escape. 
 :type jid: str 
 :return: Escaped JID. 
 :rtype: str'",'Do proper formatting of the jid'
"def parse_time(value): 
    match = time_re.match(value) 
   if match: 
      kw = match.groupdict() 
      if kw['microsecond']: 
         kw['microsecond'] = kw['microsecond'].ljust(6, '0') 
      kw = dict(((k, int(v)) for (k, v) in six.iteritems(kw) if (v is not None))) 
      return datetime.time(**kw)", 'Parse a string into a time object.',"'Parses a string and return a datetime.time. 
 This function doesn\'t support time zone offsets. 
 Raises ValueError if the input is well formatted but not a valid time. 
 Returns None if the input isn\'t well formatted, in particular if it 
 contains an offset.'"
"def unparse_vs(tup): 
    return '.'.join(map(str, tup))"," 'Unparse a tuple of Variables. 
 This is used for printing in the interactive session. 
 >>> from sympy.matrices import DiagonalMatrix 
 >>> from sympy.matrices.dense import Matrix, MatrixSymbol 
 >>> from sympy.matrices.denseops import unparse_vs 
 >>> A = Matrix([[1, 2], [3, 4]]) 
 >>> A.unparse_vs() 
 \'[1 2 3 4]\' 
 >>> A = MatrixSymbol(\'A\', 2, 2) 
 >>> A.unparse_vs() 
 \'A\' 
 >>> B = DiagonalMatrix([[1, 2], [3, 4]]) 
 >>> B.unparse_vs() 
 \'[1 2 3 4]\' 
 >>> B = DiagonalMatrix(MatrixSymbol(\'A\', 2, 2)) 
 >>> B.unparse_vs() 
 \'A\' 
 >>> A = Matrix([[1, 2], [3, 4]], \'A\')",'version list to string'
"def _convert(expected_type, value): 
    if (not isinstance(value, expected_type)): 
      try: 
         value = expected_type(value) 
      except: 
         raise TypeError(('expected   ' + str(expected_type))) 
   return value", 'Convert value to expected type','Check value is of or can be converted to expected type.'
"@apply_to_binary_file 
 def xmlminify(data): 
    parser = lxml.etree.XMLParser(remove_blank_text=True) 
   newdata = lxml.etree.XML(data, parser=parser) 
   return lxml.etree.tostring(newdata, encoding='utf-8', method='xml', xml_declaration=True)"," 'Minify XML document by removing all whitespace and comments. 
 :param data: XML document in string format. 
 :type data: str 
 :return: XML document in string format.'",'Minify XML files (strip whitespace and use minimal separators).'
"def qd(A, B, output='real', lwork=None, sort=None, overwrite_a=False, overwrite_b=False, check_finite=True): 
    (result, _) = _qd(A, B, output=output, lwork=lwork, sort=sort, overwrite_a=overwrite_a, overwrite_b=overwrite_b, check_finite=check_finite) 
   return (result[0], result[1], result[(-4)], result[(-3)])"," 'Computes the quadratic discriminant of two polynomials. 
 Parameters 
 A : array 
 A polynomial of degree at most `degree`. 
 B : array 
 B polynomial of degree at most `degree`. 
 output : string 
 One of \'real\' or \'complex\'. 
 lwork : int, optional 
 The workspace size. 
 sort : bool, optional 
 Sort the roots of the discriminant. 
 overwrite_a : bool, optional 
 If True, overwrites `A` with the result. 
 overwrite_b : bool, optional 
 If True, overwrites `B` with the result. 
 check_finite : bool, optional 
 If True, checks if the roots are finite. 
 Returns 
 result : tuple 
 A tuple containing the following information: 
 - `result[0]` : the quadratic discriminant. 
 - `result[1]` : the number of real roots. 
 - `result[2]` : the number of complex roots. 
 - `result[3]` : the number of real and complex roots. 
 Examples 
","'QZ decomposition for generalized eigenvalues of a pair of matrices. 
 The QZ, or generalized Schur, decomposition for a pair of N x N 
 nonsymmetric matrices (A,B) is:: 
 (A,B) = (Q*AA*Z\', Q*BB*Z\') 
 where AA, BB is in generalized Schur form if BB is upper-triangular 
 with non-negative diagonal and AA is upper-triangular, or for real QZ 
 decomposition (``output=\'real\'``) block upper triangular with 1x1 
 and 2x2 blocks.  In this case, the 1x1 blocks correspond to real 
 generalized eigenvalues and 2x2 blocks are \'standardized\' by making 
 the corresponding elements of BB have the form:: 
 [ a 0 ] 
 [ 0 b ] 
 and the pair of corresponding 2x2 blocks in AA and BB will have a complex 
 conjugate pair of generalized eigenvalues.  If (``output=\'complex\'``) or 
 A and B are complex matrices, Z\' denotes the conjugate-transpose of Z. 
 Q and Z are unitary matrices. 
 Parameters 
 A : (N, N) array_like 
 2d array to decompose 
 B : (N, N) array_like 
 2d array to decompose 
 output : {\'real\', \'complex\'}, optional 
 Construct the real or complex QZ decomposition for real matrices. 
 Default is \'real\'. 
 lwork : int, optional 
 Work array size.  If None or -1, it is automatically computed. 
 sort : {None, callable, \'lhp\', \'rhp\', \'iuc\', \'ouc\'}, optional 
 NOTE: THIS INPUT IS DISABLED FOR NOW. Use ordqd instead. 
 Specifies whether the upper eigenvalues should be sorted.  A callable 
 may be passed that, given a eigenvalue, returns a boolean denoting 
 whether the eigenvalue should be sorted to the top-left (True). For 
 real matrix pairs, the sort function takes three real arguments 
 (alphar, alphai, beta). The eigenvalue 
 ``x = (alphar + alphai*1j)/beta``.  For complex matrix pairs or 
 output=\'complex\', the sort function takes two complex arguments 
 (alpha, beta). The eigenvalue ``x = (alpha/beta)``.  Alternatively, 
 string parameters may be used: 
 - \'lhp\'   Left-hand plane (x.real < 0.0) 
 - \'rhp\'   Right-hand plane (x.real > 0.0) 
 - \'iuc\'   Inside the unit circle (x*x.conjugate() < 1.0) 
 - \'ouc\'   Outside the unit circle (x*x.conjugate() > 1.0) 
 Defaults to None (no sorting). 
 overwrite_a : bool, optional 
 Whether to overwrite data in a (may improve performance) 
 overwrite_b : bool, optional 
 Whether to overwrite data in b (may improve performance) 
 check_finite : bool, optional 
 If true checks the elements of `A` and `B` are finite numbers. If 
 false does no checking and passes matrix through to 
 underlying algorithm. 
 Returns 
 AA : (N, N) ndarray 
 Generalized Schur form of A. 
 BB : (N, N) ndarray 
 Generalized Schur form of B. 
 Q : (N, N) ndarray 
 The left Schur vectors. 
 Z : (N, N) ndarray 
 The right Schur vectors. 
 Notes 
 Q is transposed versus the equivalent function in Matlab. 
 .. versionadded:: 0.11.0 
 Examples 
 >>> from scipy import linalg 
 >>> np.random.seed(1234) 
 >>> A = np.arange(9).reshape((3, 3)) 
 >>> B = np.random.randn(3, 3) 
 >>> AA, BB, Q, Z = linalg.qd(A, B) 
 >>> AA 
 array([[-13.40928183,  -4.62471562,   1.09215523], 
 [  0.        ,   0.        ,   1.22805978], 
 [  0.        ,   0.        ,   0.31973817]]) 
 >>> BB 
 array([[ 0.33362547, -1.37393632,  0.02179805], 
 [ 0.        ,  1.68144922,  0.74683866], 
 [ 0.        ,  0.        ,  0.9258294 ]]) 
 >>> Q 
 array([[ 0.14134727, -0.97562773,  0.16784365], 
 [ 0.49835904, -0.07636948, -0.86360059], 
 [ 0.85537081,  0.20571399,  0.47541828]]) 
 >>> Z 
 array([[-0.24900855, -0.51772687,  0.81850696], 
 [-0.79813178,  0.58842606,  0.12938478], 
 [-0.54861681, -0.6210585 , -0.55973739]]) 
 See also 
 ordqd'"
"def GetMostRecentClient(client_list, token=None): 
    last = rdfvalue.RDFDatetime(0) 
   client_urn = None 
   for client in aff4.FACTORY.MultiOpen(client_list, token=token): 
      client_last = client.Get(client.Schema.LAST) 
      if (client_last > last): 
         last = client_last 
         client_urn = client.urn 
   return client_urn"," 'Returns the most recent client in the given list of clients. 
 If the list is empty, returns None.'",'Return most recent client from list of clients.'
"def _fwd_eeg_fit_berg_scherg(m, nterms, nfit): 
    from scipy.optimize import fmin_cobyla 
   assert (nfit >= 2) 
   u = dict(y=np.zeros((nterms - 1)), resi=np.zeros((nterms - 1)), nfit=nfit, nterms=nterms, M=np.zeros(((nterms - 1), (nfit - 1)))) 
   u['fn'] = _fwd_eeg_get_multi_sphere_model_coeffs(m, (nterms + 1)) 
   f = (min([layer['rad'] for layer in m['layers']]) / max([layer['rad'] for layer in m['layers']])) 
   k = np.arange(1, (nterms + 1)) 
   u['w'] = (np.sqrt(((((2.0 * k) + 1) * ((3.0 * k) + 1.0)) / k)) * np.power(f, (k - 1.0))) 
   u['w'][(-1)] = 0 
   mu_0 = (np.random.RandomState(0).rand(nfit) * f) 
   fun = partial(_one_step, u=u) 
   max_ = (1.0 - 0.0002) 
   cons = [(lambda x: (max_ - np.abs(x[ii]))) for ii in range(nfit)] 
   mu = fmin_cobyla(fun, mu_0, cons, rhobeg=0.5, rhoend=0.005, disp=0) 
   (rv, lambda_) = _compute_linear_parameters(mu, u) 
   order = np.argsort(mu)[::(-1)] 
   (mu, lambda_) = (mu[order], lambda_[order]) 
   m['mu'] = mu 
   m['lambda'] = (lambda_ / m['layers'][(-1)]['sigma']) 
   m['nfit'] = nfit 
   return rv"," 'Fit the Berg-Scherg model to the data. 
 Parameters 
 m : dict 
 Model parameters. 
 nterms : int 
 Number of layers in the model. 
 nfit : int 
 Number of free parameters in the fit. 
 Returns 
 rv : dict 
 Result of the fit. 
 Notes 
 This function implements the Berg-Scherg model. 
 References 
 .. [1] Berg, P., & Scherg, H. (1994). ""A new method for the 
 estimation of the source-receptor distance in EEG-source 
 analysis."" Electroencephalography and Clinical Neurophysiology, 91, 
 243-250.'",'Fit the Berg-Scherg equivalent spherical model dipole parameters.'
"@pytest.mark.parametrize('stream', ['stdout', 'stderr']) 
 def test_exit_unsuccessful_output(qtbot, proc, caplog, py_proc, stream): 
    with caplog.at_level(logging.ERROR): 
      with qtbot.waitSignal(proc.finished, timeout=10000): 
         proc.start(*py_proc('\n                                                import   sys\n                                                print(""test"",   file=sys.{})\n                                                sys.exit(1)\n                                    '.format(stream))) 
   assert (len(caplog.records) == 2) 
   assert (caplog.records[1].msg == 'Process   {}:\ntest'.format(stream))", 'Test that the output of the process is written to stdout/stderr',"'When a process fails, its output should be logged.'"
"def create_move(project, resource, offset=None): 
    if (offset is None): 
      return MoveModule(project, resource) 
   this_pymodule = project.get_pymodule(resource) 
   pyname = evaluate.eval_location(this_pymodule, offset) 
   if (pyname is not None): 
      pyobject = pyname.get_object() 
      if (isinstance(pyobject, pyobjects.PyModule) or isinstance(pyobject, pyobjects.PyPackage)): 
         return MoveModule(project, pyobject.get_resource()) 
      if (isinstance(pyobject, pyobjects.PyFunction) and isinstance(pyobject.parent, pyobjects.PyClass)): 
         return MoveMethod(project, resource, offset) 
      if ((isinstance(pyobject, pyobjects.PyDefinedObject) and isinstance(pyobject.parent, pyobjects.PyModule)) or isinstance(pyname, pynames.AssignedName)): 
         return MoveGlobal(project, resource, offset) 
   raise exceptions.RefactoringError('Move   only   works   on   global   classes/functions/variables,   modules   and   methods.')"," 'Create a move refactoring object. 
 :param project: The refactoring project. 
 :param resource: The resource to refactor. 
 :param offset: The offset to move to. 
 :returns: A refactoring object. 
 :raises: RefactoringError'","'A factory for creating Move objects 
 Based on `resource` and `offset`, return one of `MoveModule`, 
 `MoveGlobal` or `MoveMethod` for performing move refactoring.'"
"def in6_isaddr6to4(x): 
    x = inet_pton(socket.AF_INET6, x) 
   return (x[:2] == '   \x02')", 'Return True if x is an IPv6 address in IPv4-mapped format.',"'Return True if provided address (in printable format) is a 6to4 
 address (being in 2002::/16).'"
"def __virtual__(): 
    if (HAS_NAPALM and ('proxy' in __opts__)): 
      return __virtualname__ 
   else: 
      return (False, 'The   network   NTP   state   (netntp)   cannot   be   loaded:                                                   NAPALM   or   proxy   could   not   be   loaded.')", 'Load the NTP state module if NAPALM or proxy are available.',"'NAPALM library must be installed for this module to work. 
 Also, the key proxymodule must be set in the __opts___ dictionary.'"
"def extract_python(fileobj, keywords, comment_tags, options): 
    funcname = lineno = message_lineno = None 
   call_stack = (-1) 
   buf = [] 
   messages = [] 
   translator_comments = [] 
   in_def = in_translator_comments = False 
   comment_tag = None 
   encoding = (parse_encoding(fileobj) or options.get('encoding', 'iso-8859-1')) 
   tokens = generate_tokens(fileobj.readline) 
   for (tok, value, (lineno, _), _, _) in tokens: 
      if ((call_stack == (-1)) and (tok == NAME) and (value in ('def', 'class'))): 
         in_def = True 
      elif ((tok == OP) and (value == '(')): 
         if in_def: 
            in_def = False 
            continue 
         if funcname: 
            message_lineno = lineno 
            call_stack += 1 
      elif (in_def and (tok == OP) and (value == ':')): 
         in_def = False 
         continue 
      elif ((call_stack == (-1)) and (tok == COMMENT)): 
         value = value.decode(encoding)[1:].strip() 
         if (in_translator_comments and (translator_comments[(-1)][0] == (lineno - 1))): 
            translator_comments.append((lineno, value)) 
            continue 
         for comment_tag in comment_tags: 
            if value.startswith(comment_tag): 
               in_translator_comments = True 
               translator_comments.append((lineno, value)) 
               break 
      elif (funcname and (call_stack == 0)): 
         if ((tok == OP) and (value == ')')): 
            if buf: 
               messages.append(''.join(buf)) 
               del buf[:] 
            else: 
               messages.append(None) 
            if (len(messages) > 1): 
               messages = tuple(messages) 
            else: 
               messages = messages[0] 
            if (translator_comments and (translator_comments[(-1)][0] < (message_lineno - 1))): 
               translator_comments = [] 
            (yield (message_lineno, funcname, messages, [comment[1] for comment in translator_comments])) 
            funcname = lineno = message_lineno = None 
            call_stack = (-1) 
            messages = [] 
            translator_comments = [] 
            in_translator_comments = False 
         elif (tok == STRING): 
            value = eval(('#   coding=%s\n%s' % (encoding, value)), {'__builtins__': {}}, {}) 
            if isinstance(value, str): 
               value = value.decode(encoding) 
            buf.append(value) 
         elif ((tok == OP) and (value == ',')): 
            if buf: 
               messages.append(''.join(buf)) 
               del buf[:] 
            else: 
               messages.append(None) 
            if translator_comments: 
               (old_lineno, old_comment) = translator_comments.pop() 
               translator_comments.append(((old_lineno + 1), old_comment)) 
      elif ((call_stack > 0) and (tok == OP) and (value == ')')): 
         call_stack -= 1 
      elif (funcname and (call_stack == (-1))): 
         funcname = None 
      elif ((tok == NAME) and (value in keywords)): 
         funcname = value"," 'Extracts messages from Python source code. 
 :param fileobj: The file object to read from. 
 :param keywords: A list of keywords to search for. 
 :param comment_tags: A list of comment tags to search for. 
 :param options: A dict of options to pass to the tokenizer. 
 :return: A generator of tuples containing the message line number, 
 the function name, and the message text. 
 :rtype: generator[tuple[int, str, str, list[str]]]'","'Extract messages from Python source code. 
 :param fileobj: the seekable, file-like object the messages should be 
 extracted from 
 :param keywords: a list of keywords (i.e. function names) that should be 
 recognized as translation functions 
 :param comment_tags: a list of translator tags to search for and include 
 in the results 
 :param options: a dictionary of additional options (optional) 
 :return: an iterator over ``(lineno, funcname, message, comments)`` tuples 
 :rtype: ``iterator``'"
"def generate_replace_result_xml(result_sourcedid, score): 
    elem = ElementMaker(nsmap={None: 'http://www.imsglobal.org/services/ltiv1p1/xsd/imsoms_v1p0'}) 
   xml = elem.imsx_POXEnvelopeRequest(elem.imsx_POXHeader(elem.imsx_POXRequestHeaderInfo(elem.imsx_version('V1.0'), elem.imsx_messageIdentifier(str(uuid.uuid4())))), elem.imsx_POXBody(elem.replaceResultRequest(elem.resultRecord(elem.sourcedGUID(elem.sourcedId(result_sourcedid)), elem.result(elem.resultScore(elem.language('en'), elem.textString(str(score)))))))) 
   return etree.tostring(xml, xml_declaration=True, encoding='UTF-8')"," 'Generate the replace result xml 
 :param str result_sourcedid: The sourcedid of the result 
 :param float score: The score of the result'","'Create the XML document that contains the new score to be sent to the LTI 
 consumer. The format of this message is defined in the LTI 1.1 spec.'"
"def _get_params(mapper_spec, allowed_keys=None): 
    if ('input_reader' not in mapper_spec.params): 
      message = ""Input   reader's   parameters   should   be   specified   in   input_reader   subdictionary."" 
      if allowed_keys: 
         raise errors.BadReaderParamsError(message) 
      params = mapper_spec.params 
      params = dict(((str(n), v) for (n, v) in params.iteritems())) 
   else: 
      if (not isinstance(mapper_spec.params.get('input_reader'), dict)): 
         raise errors.BadReaderParamsError('Input   reader   parameters   should   be   a   dictionary') 
      params = mapper_spec.params.get('input_reader') 
      params = dict(((str(n), v) for (n, v) in params.iteritems())) 
      if allowed_keys: 
         params_diff = (set(params.keys()) - allowed_keys) 
         if params_diff: 
            raise errors.BadReaderParamsError(('Invalid   input_reader   parameters:   %s' % ','.join(params_diff))) 
   return params"," 'Helper method to get reader\'s parameters. 
 :param mapper_spec: Mapper spec 
 :param allowed_keys: List of allowed keys 
 :returns: Dictionary of reader\'s parameters'","'Obtain input reader parameters. 
 Utility function for input readers implementation. Fetches parameters 
 from mapreduce specification giving appropriate usage warnings. 
 Args: 
 mapper_spec: The MapperSpec for the job 
 allowed_keys: set of all allowed keys in parameters as strings. If it is not 
 None, then parameters are expected to be in a separate ""input_reader"" 
 subdictionary of mapper_spec parameters. 
 Returns: 
 mapper parameters as dict 
 Raises: 
 BadReaderParamsError: if parameters are invalid/missing or not allowed.'"
"def dir_list(load): 
    gitfs = salt.utils.gitfs.GitFS(__opts__) 
   gitfs.init_remotes(__opts__['gitfs_remotes'], PER_REMOTE_OVERRIDES, PER_REMOTE_ONLY) 
   return gitfs.dir_list(load)"," 'Lists the contents of a directory. 
 This function is a wrapper for the gitfs.dir_list function. 
 It accepts a dictionary of load options:: 
 .. code-block:: python 
 { 
 \'remotes\' : [ 
 \'origin\' 
 ] 
 \'glob\' : \'*.py\' 
 \'exclude\' : \'*.pyc\' 
 \'exclude_glob\' : \'*.pyc\' 
 \'exclude_glob_regex\' : \'*.pyc\' 
 \'exclude_glob_regex_negate\' : \'*.pyc\' 
 \'include\' : \'*.py\' 
 \'include_glob\' : \'*.py\' 
 \'include_glob_regex\' : \'*.py\' 
 \'include_glob_regex_negate\' : \'*.py\' 
 \'include_glob_regex_exclude\' : \'*.py\' 
 \'include_glob_regex_exclude\' : \'*.py\' 
 \'include_glob_regex_ex",'Return a list of all directories on the master'
"def find_duplicative_certs(config, domains): 
    def update_certs_for_domain_matches(candidate_lineage, rv): 
      'Return   cert   as   identical_names_cert   if   it   matches,\n                                 or   subset_names_cert   if   it   matches   as   subset\n                        ' 
      (identical_names_cert, subset_names_cert) = rv 
      candidate_names = set(candidate_lineage.names()) 
      if (candidate_names == set(domains)): 
         identical_names_cert = candidate_lineage 
      elif candidate_names.issubset(set(domains)): 
         if (subset_names_cert is None): 
            subset_names_cert = candidate_lineage 
         elif (len(candidate_names) > len(subset_names_cert.names())): 
            subset_names_cert = candidate_lineage 
      return (identical_names_cert, subset_names_cert) 
   return _search_lineages(config, update_certs_for_domain_matches, (None, None))"," 'Searches the lineage for a cert that matches the given domains. 
 :param config: The config object. 
 :param domains: The domains to match. 
 :return: The matching cert, or None if no match was found.'",'Find existing certs that duplicate the request.'
"def assemble(block): 
    return [('\n'.join(sub_block) + '\n') for sub_block in block]"," 'Assemble a block of code into a single string. 
 Parameters 
 block : list 
 A list of sub-blocks, each a list of strings. 
 Returns 
 A string containing the assembled code.'",'Assemble a block into multi-line sub-blocks.'
"def getNewDerivation(elementNode): 
    return evaluate.EmptyObject(elementNode)"," 'Returns the new derivation for an element node. 
 :param elementNode: the element node 
 :type elementNode: ElementNode 
 :returns: the new derivation for the element node 
 :rtype: Derivation'",'Get new derivation.'
"def make_colorizer(color): 
    def inner(text): 
      return colorizer.colorize(color, text) 
   return inner"," 'Returns a function that colorizes the text it\'s given. 
 Parameters 
 color : str 
 The color to use. 
 Returns 
 A function that colorizes the text it\'s given.'","'Creates a function that colorizes text with the given color. 
 For example: 
 green = make_colorizer(\'darkgreen\') 
 red = make_colorizer(\'red\') 
 Then, you can use: 
 print ""It\'s either "" + green(\'OK\') + \' or \' + red(\'Oops\')'"
"def is_var(exp): 
    return isinstance(exp, Var)"," 'Checks if a given expression is a variable. 
 :param exp: Expression to check. 
 :type exp: Expression 
 :return: True if expression is a variable, False otherwise. 
 :rtype: bool'",'Is `exp` a `Var` node.'
"def make_middleware(app, global_conf=None, warning_level=None): 
    if warning_level: 
      warning_level = int(warning_level) 
   return HTTPExceptionHandler(app, warning_level=warning_level)"," 'Create a middleware for handling HTTP exceptions. 
 :param app: The application to use as the base for the middleware. 
 :param global_conf: The global configuration object for the application. 
 :param warning_level: The warning level for exceptions. 
 :return: A middleware for handling HTTP exceptions. 
 :rtype: middleware'","'``httpexceptions`` middleware; this catches any 
 ``paste.httpexceptions.HTTPException`` exceptions (exceptions like 
 ``HTTPNotFound``, ``HTTPMovedPermanently``, etc) and turns them 
 into proper HTTP responses. 
 ``warning_level`` can be an integer corresponding to an HTTP code. 
 Any code over that value will be passed \'up\' the chain, potentially 
 reported on by another piece of middleware.'"
"def fromRoman(s): 
    if (not s): 
      raise InvalidRomanNumeralError, 'Input   can   not   be   blank' 
   if (not romanNumeralPattern.search(s)): 
      raise InvalidRomanNumeralError, ('Invalid   Roman   numeral:   %s' % s) 
   result = 0 
   index = 0 
   for (numeral, integer) in romanNumeralMap: 
      while (s[index:(index + len(numeral))] == numeral): 
         result += integer 
         index += len(numeral) 
   return result"," 'Converts a roman numeral to an integer. 
 Parameters 
 s : string 
 A roman numeral. 
 Returns 
 integer 
 The corresponding integer. 
 Raises 
 InvalidRomanNumeralError 
 If the input is not a valid roman numeral.'",'convert Roman numeral to integer'
"def new_figure_manager_given_figure(num, figure): 
    canvas = FigureCanvasGDK(figure) 
   manager = FigureManagerBase(canvas, num) 
   return manager"," 'Create a new FigureManagerBase instance, given a figure number and 
 the figure to be managed. 
 Parameters 
 num : int 
 The figure number to be managed. 
 figure : Figure 
 The figure to be managed. 
 Returns 
 manager : FigureManagerBase 
 A FigureManagerBase instance.'",'Create a new figure manager instance for the given figure.'
"def _py_convert_agg_to_wx_bitmap(agg, bbox): 
    if (bbox is None): 
      return wx.BitmapFromImage(_py_convert_agg_to_wx_image(agg, None)) 
   else: 
      return _clipped_image_as_bitmap(_py_convert_agg_to_wx_image(agg, None), bbox)", 'Convert a wx.Bitmap to an agg::Image.',"'Convert the region of the agg buffer bounded by bbox to a wx.Bitmap.  If 
 bbox is None, the entire buffer is converted. 
 Note: agg must be a backend_agg.RendererAgg instance.'"
"def generate_presigned_url(self, ClientMethod, Params=None, ExpiresIn=3600, HttpMethod=None): 
    client_method = ClientMethod 
   params = Params 
   expires_in = ExpiresIn 
   http_method = HttpMethod 
   request_signer = self._request_signer 
   serializer = self._serializer 
   try: 
      operation_name = self._PY_TO_OP_NAME[client_method] 
   except KeyError: 
      raise UnknownClientMethodError(method_name=client_method) 
   operation_model = self.meta.service_model.operation_model(operation_name) 
   request_dict = serializer.serialize_to_request(params, operation_model) 
   if (http_method is not None): 
      request_dict['method'] = http_method 
   prepare_request_dict(request_dict, endpoint_url=self.meta.endpoint_url) 
   return request_signer.generate_presigned_url(request_dict=request_dict, expires_in=expires_in, operation_name=operation_name)"," 'Generates a presigned url for the given method and params. 
 :param ClientMethod: The name of the method to call 
 :param Params: The parameters to pass to the method. 
 :param ExpiresIn: The number of seconds the presigned url is valid for 
 :param HttpMethod: The http method to use to call the method 
 :returns: A string representing the presigned url 
 :rtype: str'","'Generate a presigned url given a client, its method, and arguments 
 :type ClientMethod: string 
 :param ClientMethod: The client method to presign for 
 :type Params: dict 
 :param Params: The parameters normally passed to 
 ``ClientMethod``. 
 :type ExpiresIn: int 
 :param ExpiresIn: The number of seconds the presigned url is valid 
 for. By default it expires in an hour (3600 seconds) 
 :type HttpMethod: string 
 :param HttpMethod: The http method to use on the generated url. By 
 default, the http method is whatever is used in the method\'s model. 
 :returns: The presigned url'"
"def get_scene_numbering_for_show(indexer_id, indexer): 
    if (indexer_id is None): 
      return {} 
   indexer_id = int(indexer_id) 
   indexer = int(indexer) 
   result = {} 
   for dbData in [x[u'doc'] for x in sickrage.srCore.mainDB.db.get_many(u'scene_numbering', indexer_id, with_doc=True)]: 
      season = int((dbData[u'season'] or 0)) 
      episode = int((dbData[u'episode'] or 0)) 
      scene_season = int((dbData[u'scene_season'] or 0)) 
      scene_episode = int((dbData[u'scene_episode'] or 0)) 
      if ((int(dbData[u'indexer']) != indexer) or ((scene_season or scene_episode) == 0)): 
         continue 
      result[(season, episode)] = (scene_season, scene_episode) 
   return result"," 'Get the scene numbering for a show from the indexer\'s db 
 :param indexer_id: (int) The indexer ID 
 :param indexer: (int) The indexer number 
 :returns: (dict) A dictionary of the form {(season, episode): (scene_season, scene_episode)}'","'Returns a dict of (season, episode) : (sceneSeason, sceneEpisode) mappings 
 for an entire show.  Both the keys and values of the dict are tuples. 
 Will be empty if there are no scene numbers set'"
"def oo_random_word(length, source='abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'): 
    return ''.join((random.choice(source) for i in range(length)))"," 'Returns a random word of the given length. 
 :param length: Length of the word. 
 :type length: int 
 :param source: Source of the letters. 
 :type source: string 
 :return: Random word. 
 :rtype: string'","'Generates a random string of given length from a set of alphanumeric characters. 
 The default source uses [a-z][A-Z][0-9] 
 Ex: 
 - oo_random_word(3)                => aB9 
 - oo_random_word(4, source=\'012\')  => 0123'"
"def removed(name, requirements=None, bin_env=None, log=None, proxy=None, timeout=None, user=None, cwd=None, use_vt=False): 
    ret = {'name': name, 'result': None, 'comment': '', 'changes': {}} 
   try: 
      pip_list = __salt__['pip.list'](bin_env=bin_env, user=user, cwd=cwd) 
   except (CommandExecutionError, CommandNotFoundError) as err: 
      ret['result'] = False 
      ret['comment'] = ""Error   uninstalling   '{0}':   {1}"".format(name, err) 
      return ret 
   if (name not in pip_list): 
      ret['result'] = True 
      ret['comment'] = 'Package   is   not   installed.' 
      return ret 
   if __opts__['test']: 
      ret['result'] = None 
      ret['comment'] = 'Package   {0}   is   set   to   be   removed'.format(name) 
      return ret 
   if __salt__['pip.uninstall'](pkgs=name, requirements=requirements, bin_env=bin_env, log=log, proxy=proxy, timeout=timeout, user=user, cwd=cwd, use_vt=use_vt): 
      ret['result'] = True 
      ret['changes'][name] = 'Removed' 
      ret['comment'] = 'Package   was   successfully   removed.' 
   else: 
      ret['result'] = False 
      ret['comment'] = 'Could   not   remove   package.' 
   return ret"," 'Remove a package from the system. 
 name: the name of the package to be removed 
 requirements: the requirements file to use 
 bin_env: the environment to use for the pip command 
 log: the log function to use for logging 
 proxy: the proxy to use for the pip command 
 timeout: the timeout for the pip command 
 user: the user to use for the pip command 
 cwd: the working directory for the pip command 
 use_vt: whether to use the virtual terminal'","'Make sure that a package is not installed. 
 name 
 The name of the package to uninstall 
 user 
 The user under which to run pip 
 bin_env : None 
 the pip executable or virtualenenv to use 
 use_vt 
 Use VT terminal emulation (see output while installing)'"
"@frappe.whitelist() 
 def take_backup(): 
    enqueue(u'frappe.integrations.doctype.dropbox_settings.dropbox_settings.take_backup_to_dropbox', queue=u'long') 
   frappe.msgprint(_(u'Queued   for   backup.   It   may   take   a   few   minutes   to   an   hour.'))", 'Queues the backup to Dropbox','Enqueue longjob for taking backup to dropbox'
"@task 
 @needs('pavelib.prereqs.install_prereqs') 
 @cmdopts([('settings=', 's', 'Django   settings   for   both   LMS   and   Studio'), ('asset-settings=', 'a', 'Django   settings   for   updating   assets   for   both   LMS   and   Studio   (defaults   to   settings)'), ('worker-settings=', 'w', 'Celery   worker   Django   settings'), ('fast', 'f', 'Skip   updating   assets'), ('optimized', 'o', 'Run   with   optimized   assets'), ('settings-lms=', 'l', 'Set   LMS   only,   overriding   the   value   from   --settings   (if   provided)'), ('asset-settings-lms=', None, 'Set   LMS   only,   overriding   the   value   from   --asset-settings   (if   provided)'), ('settings-cms=', 'c', 'Set   Studio   only,   overriding   the   value   from   --settings   (if   provided)'), ('asset-settings-cms=', None, 'Set   Studio   only,   overriding   the   value   from   --asset-settings   (if   provided)'), ('asset_settings=', None, 'deprecated   in   favor   of   asset-settings'), ('asset_settings_cms=', None, 'deprecated   in   favor   of   asset-settings-cms'), ('asset_settings_lms=', None, 'deprecated   in   favor   of   asset-settings-lms'), ('settings_cms=', None, 'deprecated   in   favor   of   settings-cms'), ('settings_lms=', None, 'deprecated   in   favor   of   settings-lms'), ('worker_settings=', None, 'deprecated   in   favor   of   worker-settings')]) 
 def run_all_servers(options): 
    settings = getattr(options, 'settings', DEFAULT_SETTINGS) 
   asset_settings = getattr(options, 'asset_settings', settings) 
   worker_settings = getattr(options, 'worker_settings', 'dev_with_worker') 
   fast = getattr(options, 'fast', False) 
   optimized = getattr(options, 'optimized', False) 
   if optimized: 
      settings = OPTIMIZED_SETTINGS 
      asset_settings = OPTIMIZED_ASSETS_SETTINGS 
   settings_lms = getattr(options, 'settings_lms', settings) 
   settings_cms = getattr(options, 'settings_cms', settings) 
   asset_settings_lms = getattr(options, 'asset_settings_lms', asset_settings) 
   asset_settings_cms = getattr(options, 'asset_settings_cms', asset_settings) 
   if (not fast): 
      args = ['lms', 'studio', '--settings={}'.format(asset_settings), '--skip-collect'] 
      call_task('pavelib.assets.update_assets', args=args) 
      if (settings != DEFAULT_SETTINGS): 
         collect_assets(['lms'], asset_settings_lms) 
         collect_assets(['studio'], asset_settings_cms) 
      call_task('pavelib.assets.watch_assets', options={'background': True}) 
   lms_port = DEFAULT_PORT['lms'] 
   cms_port = DEFAULT_PORT['studio'] 
   lms_runserver_args = ['0.0.0.0:{}'.format(lms_port)] 
   cms_runserver_args = ['0.0.0.0:{}'.format(cms_port)] 
   run_multi_processes([django_cmd('lms', settings_lms, 'runserver', '--traceback', '--pythonpath=.', *lms_runserver_args), django_cmd('studio', settings_cms, 'runserver', '--traceback', '--pythonpath=.', *cms_runserver_args), django_cmd('lms', worker_settings, 'celery', 'worker', '--beat', '--loglevel=INFO', '--pythonpath=.')])", 'Run all servers for LMS and Studio',"'Runs Celery workers, Studio, and LMS.'"
"def _get_info_slice(obj, indexer): 
    if (not hasattr(obj, '_info_axis_number')): 
      raise TypeError(('object   of   type   %r   has   no   info   axis' % type(obj).__name__)) 
   slices = ([slice(None)] * obj.ndim) 
   slices[obj._info_axis_number] = indexer 
   return tuple(slices)", 'Return a slice object for the given indexer.','Slice the info axis of `obj` with `indexer`.'
"def device_memory_size(devmem): 
    sz = getattr(devmem, '_cuda_memsize_', None) 
   if (sz is None): 
      (s, e) = device_extents(devmem) 
      sz = (e - s) 
      devmem._cuda_memsize_ = sz 
   assert (sz > 0), 'zero   length   array' 
   return sz"," 'Return the size of the memory allocated for a device array. 
 Parameters 
 devmem : DeviceArray 
 The device array. 
 Returns 
 size : int 
 The size of the memory allocated for the array. 
 Raises 
 RuntimeError 
 If the device array is not allocated. 
 Notes 
 The memory size is the number of bytes allocated by the device for the 
 array. 
 This function is only available for CUDA 5.0 and later.'","'Check the memory size of the device memory. 
 The result is cached in the device memory object. 
 It may query the driver for the memory size of the device memory allocation.'"
"@register.tag('filter') 
 def do_filter(parser, token): 
    (_, rest) = token.contents.split(None, 1) 
   filter_expr = parser.compile_filter(('var|%s' % rest)) 
   for (func, unused) in filter_expr.filters: 
      filter_name = getattr(func, '_filter_name', None) 
      if (filter_name in ('escape', 'safe')): 
         raise TemplateSyntaxError(('""filter   %s""   is   not   permitted.      Use   the   ""autoescape""   tag   instead.' % filter_name)) 
   nodelist = parser.parse(('endfilter',)) 
   parser.delete_first_token() 
   return FilterNode(filter_expr, nodelist)"," 'Renders the given filter expression. 
 .. versionchanged:: 0.9 
 The `autoescape` filter was added. 
 .. versionchanged:: 0.8 
 The `safe` filter was added. 
 .. versionchanged:: 0.7 
 The `var` filter was added. 
 .. versionchanged:: 0.6 
 The `escape` filter was added. 
 .. versionchanged:: 0.5 
 The `autoescape` filter was added. 
 .. versionchanged:: 0.4 
 The `safe` filter was added. 
 .. versionchanged:: 0.3 
 The `autoescape` filter was added. 
 .. versionchanged:: 0.2 
 The `safe` filter was added. 
 .. versionchanged:: 0.1 
 The `autoescape` filter was added. 
 .. versionchanged:: 0.0 
 The `autoescape` filter was added. 
 .. versionchanged:: 0.0 
 The `autoescape` filter was added. 
 .. versionchanged:: 0.0 
 The `autoescape` filter was added. ","'Filters the contents of the block through variable filters. 
 Filters can also be piped through each other, and they can have 
 arguments -- just like in variable syntax. 
 Sample usage:: 
 {% filter force_escape|lower %} 
 This text will be HTML-escaped, and will appear in lowercase. 
 {% endfilter %} 
 Note that the ``escape`` and ``safe`` filters are not acceptable arguments. 
 Instead, use the ``autoescape`` tag to manage autoescaping for blocks of 
 template code.'"
"def get_queue_names(app_id=None, max_rows=100): 
    rpc = apiproxy_stub_map.UserRPC('taskqueue') 
   request = taskqueue_service_pb.TaskQueueFetchQueuesRequest() 
   response = taskqueue_service_pb.TaskQueueFetchQueuesResponse() 
   if app_id: 
      request.set_app_id(app_id) 
   request.set_max_rows(max_rows) 
   queues = ['default'] 
   try: 
      rpc.make_call('FetchQueues', request, response) 
      rpc.check_success() 
      for queue in response.queue_list(): 
         if ((queue.mode() == taskqueue_service_pb.TaskQueueMode.PUSH) and (not queue.queue_name().startswith('__')) and (queue.queue_name() != 'default')): 
            queues.append(queue.queue_name()) 
   except Exception: 
      logging.exception('Failed   to   get   queue   names.') 
   return queues"," 'Returns a list of queue names. 
 :param app_id: Optional app_id to limit the list of queues to. 
 :param max_rows: Optional maximum number of rows to return. 
 :returns: A list of queue names.'",'Returns a list with all non-special queue names for app_id.'
"def add_key_to_url(url, scheme, key): 
    query = request.args.to_dict() 
   query['view_only'] = key 
   replacements = {'query': urllib.urlencode(query)} 
   if scheme: 
      replacements['scheme'] = scheme 
   parsed_url = urlparse.urlparse(url) 
   if parsed_url.fragment: 
      replacements['path'] = '{}%23{}'.format(parsed_url.path, parsed_url.fragment) 
      replacements['fragment'] = '' 
   parsed_redirect_url = parsed_url._replace(**replacements) 
   return urlparse.urlunparse(parsed_redirect_url)"," 'Adds a key to the URL and returns the updated URL. 
 :param url: The URL to be modified. 
 :param scheme: The scheme of the URL. 
 :param key: The key to be added to the URL. 
 :return: The modified URL.'",'Redirects the user to the requests URL with the given key appended to the query parameters.'
"def route_exists(destination_cidr_block, route_table_name=None, route_table_id=None, gateway_id=None, instance_id=None, interface_id=None, tags=None, region=None, key=None, keyid=None, profile=None, vpc_peering_connection_id=None): 
    if (not any((route_table_name, route_table_id))): 
      raise SaltInvocationError('At   least   one   of   the   following   must   be   specified:   route   table   name   or   route   table   id.') 
   if (not any((gateway_id, instance_id, interface_id, vpc_peering_connection_id))): 
      raise SaltInvocationError('At   least   one   of   the   following   must   be   specified:   gateway   id,   instance   id,   interface   id   or   VPC   peering   connection   id.') 
   try: 
      conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) 
      filter_parameters = {'filters': {}} 
      if route_table_id: 
         filter_parameters['route_table_ids'] = [route_table_id] 
      if route_table_name: 
         filter_parameters['filters']['tag:Name'] = route_table_name 
      if tags: 
         for (tag_name, tag_value) in six.iteritems(tags): 
            filter_parameters['filters']['tag:{0}'.format(tag_name)] = tag_value 
      route_tables = conn.get_all_route_tables(**filter_parameters) 
      if (len(route_tables) != 1): 
         raise SaltInvocationError('Found   more   than   one   route   table.') 
      route_check = {'destination_cidr_block': destination_cidr_block, 'gateway_id': gateway_id, 'instance_id': instance_id, 'interface_id': interface_id, 'vpc_peering_connection_id': vpc_peering_connection_id} 
      for route_match in route_tables[0].routes: 
         route_dict = {'destination_cidr_block': route_match.destination_cidr_block, 'gateway_id': route_match.gateway_id, 'instance_id': route_match.instance_id, 'interface_id': route_match.interface_id, 'vpc_peering_connection_id': vpc_peering_connection_id} 
         route_comp = (set(route_dict.items()) ^ set(route_check.items())) 
         if (len(route_comp) == 0): 
            log.info('Route   {0}   exists.'.format(destination_cidr_block)) 
            return {'exists': True} 
      log.warning('Route   {0}   does   not   exist.'.format(destination_cidr_block)) 
      return {'exists': False} 
   except BotoServerError as e: 
      return {'error': salt.utils.boto.get_error(e)}"," 'Check if a route exists. 
 This function checks if a route exists in a route table. 
 This function can be used to check if a route exists for a route table, 
 a route table with a specified tag, or a route table with a specified 
 route. 
 .. versionchanged:: 2016.7.0 
 Added support for VPC peering connections. 
 .. versionchanged:: 2016.6.1 
 Added support for route table tags. 
 .. versionchanged:: 2016.6.0 
 Added support for route table names. 
 .. versionchanged:: 2016.5.0 
 Added support for route table IDs. 
 .. versionchanged:: 2016.4.0 
 Added support for VPC peering connection IDs. 
 .. versionchanged:: 2016.3.0 
 Added support for destination CIDR blocks. 
 .. versionchanged:: 2016.2.0 
 Added support for gateway IDs. 
 .. versionchanged:: 2016.1.0 
 Added support for instance IDs","'Checks if a route exists. 
 .. versionadded:: 2015.8.0 
 CLI Example: 
 .. code-block:: bash 
 salt myminion boto_vpc.route_exists destination_cidr_block=\'10.0.0.0/20\' gateway_id=\'local\' route_table_name=\'test\''"
"def protected(callback=None): 
    def wrapper(f): 
      @functools.wraps(f) 
      def inner(self, request, *args, **kwargs): 
         request.assert_authenticated() 
         if request.context.is_admin: 
            LOG.warning(_LW('RBAC:   Bypassing   authorization')) 
         elif (callback is not None): 
            prep_info = {'f_name': f.__name__, 'input_attr': kwargs} 
            callback(self, request, prep_info, *args, **kwargs) 
         else: 
            action = ('identity:%s' % f.__name__) 
            creds = _build_policy_check_credentials(self, action, request.context_dict, kwargs) 
            policy_dict = {} 
            if (hasattr(self, 'get_member_from_driver') and (self.get_member_from_driver is not None)): 
               key = ('%s_id' % self.member_name) 
               if (key in kwargs): 
                  ref = self.get_member_from_driver(kwargs[key]) 
                  policy_dict['target'] = {self.member_name: ref} 
            if (request.context_dict.get('subject_token_id') is not None): 
               window_seconds = self._token_validation_window(request) 
               token_ref = token_model.KeystoneToken(token_id=request.context_dict['subject_token_id'], token_data=self.token_provider_api.validate_token(request.context_dict['subject_token_id'], window_seconds=window_seconds)) 
               policy_dict.setdefault('target', {}) 
               policy_dict['target'].setdefault(self.member_name, {}) 
               policy_dict['target'][self.member_name]['user_id'] = token_ref.user_id 
               try: 
                  user_domain_id = token_ref.user_domain_id 
               except exception.UnexpectedError: 
                  user_domain_id = None 
               if user_domain_id: 
                  policy_dict['target'][self.member_name].setdefault('user', {}) 
                  policy_dict['target'][self.member_name]['user'].setdefault('domain', {}) 
                  policy_dict['target'][self.member_name]['user']['domain']['id'] = user_domain_id 
            policy_dict.update(kwargs) 
            self.policy_api.enforce(creds, action, utils.flatten_dict(policy_dict)) 
            LOG.debug('RBAC:   Authorization   granted') 
         return f(self, request, *args, **kwargs) 
      return inner 
   return wrapper"," 'Protects a view function with authorization checks. 
 :param callback: Callback function to be called before the view function 
 is called. 
 :returns: A decorator function that wraps the view function.'","'Wrap API calls with role based access controls (RBAC). 
 This handles both the protection of the API parameters as well as any 
 target entities for single-entity API calls. 
 More complex API calls (for example that deal with several different 
 entities) should pass in a callback function, that will be subsequently 
 called to check protection for these multiple entities. This callback 
 function should gather the appropriate entities needed and then call 
 check_protection() in the V3Controller class.'"
"def _domain_variants(domain): 
    parts = domain.split('.') 
   for i in range(len(parts), 1, (-1)): 
      (yield '.'.join(parts[(- i):]))"," 'Yield all the possible domain variants of a given domain. 
 :param domain: The domain to generate variants of. 
 :return: A generator of all possible variants of the domain.'","'>>> list(_domain_variants(""foo.bar.example.com"")) 
 [\'foo.bar.example.com\', \'bar.example.com\', \'example.com\']'"
"def calculateDeltaSeconds(start): 
    return (time.time() - start)"," 'Calculate the difference between the start time and the current time in 
 seconds.'",'Returns elapsed time from start till now'
"def _collect_dirs(start_dir, blacklist=set(['conftest.py', 'nox.py']), suffix='_test.py'): 
    for (parent, subdirs, files) in os.walk(start_dir): 
      if any((f for f in files if (f.endswith(suffix) and (f not in blacklist)))): 
         del subdirs[:] 
         (yield parent) 
      else: 
         subdirs[:] = [s for s in subdirs if (s[0].isalpha() and (os.path.join(parent, s) not in blacklist))]", 'Returns all directories that contain test files',"'Recursively collects a list of dirs that contain a file matching the 
 given suffix. 
 This works by listing the contents of directories and finding 
 directories that have `*_test.py` files.'"
"def expand_default(self, option): 
    if ((self.parser is None) or (not self.default_tag)): 
      return option.help 
   optname = option._long_opts[0][2:] 
   try: 
      provider = self.parser.options_manager._all_options[optname] 
   except KeyError: 
      value = None 
   else: 
      optdict = provider.get_option_def(optname) 
      optname = provider.option_attrname(optname, optdict) 
      value = getattr(provider.config, optname, optdict) 
      value = format_option_value(optdict, value) 
   if ((value is optik_ext.NO_DEFAULT) or (not value)): 
      value = self.NO_DEFAULT_VALUE 
   return option.help.replace(self.default_tag, str(value))"," 'Expand the default option value. 
 :param option: The option to expand. 
 :return: The expanded value.'","'monkey patch OptionParser.expand_default since we have a particular 
 way to handle defaults to avoid overriding values in the configuration 
 file'"
"def password_option(*param_decls, **attrs): 
    def decorator(f): 
      attrs.setdefault('prompt', True) 
      attrs.setdefault('confirmation_prompt', True) 
      attrs.setdefault('hide_input', True) 
      return option(*(param_decls or ('--password',)), **attrs)(f) 
   return decorator"," 'Decorator for option that prompts for a password. 
 :param param_decls: 
 :param attrs: 
 :return: 
 :rtype: 
 :raises: 
 :example: 
 >>> @password_option 
 ... def foo(options, args): 
 ...     return 1 
 ... 
 >>> foo(help=""Do something"") 
 >>> foo(help=""Do something"", confirmation_prompt=False) 
 >>> foo(help=""Do something"", confirmation_prompt=False, hide_input=True)'","'Shortcut for password prompts. 
 This is equivalent to decorating a function with :func:`option` with 
 the following parameters:: 
 @click.command() 
 @click.option(\'--password\', prompt=True, confirmation_prompt=True, 
 hide_input=True) 
 def changeadmin(password): 
 pass'"
"def stop(name): 
    cmd = '/etc/rc.d/{0}   -f   stop'.format(name) 
   return (not __salt__['cmd.retcode'](cmd))"," 'Stop a service. 
 name: The service name to stop. 
 Example: 
 .. code-block:: bash 
 salt-call service.stop apache'","'Stop the specified service 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' service.stop <service name>'"
"@click.command() 
 @click.option('--count', default=2, callback=validate_count, help='A   positive   even   number.') 
 @click.option('--foo', help='A   mysterious   parameter.') 
 @click.option('--url', help='A   URL', type=URL()) 
 @click.version_option() 
 def cli(count, foo, url): 
    if ((foo is not None) and (foo != 'wat')): 
      raise click.BadParameter('If   a   value   is   provided   it   needs   to   be   the   value   ""wat"".', param_hint=['--foo']) 
   click.echo(('count:   %s' % count)) 
   click.echo(('foo:   %s' % foo)) 
   click.echo(('url:   %s' % repr(url)))", 'A CLI example.',"'Validation. 
 This example validates parameters in different ways.  It does it 
 through callbacks, through a custom type as well as by validating 
 manually in the function.'"
"def _collectWarnings(observeWarning, f, *args, **kwargs): 
    def showWarning(message, category, filename, lineno, file=None, line=None): 
      assert isinstance(message, Warning) 
      observeWarning(_Warning(message.args[0], category, filename, lineno)) 
   for v in sys.modules.itervalues(): 
      if (v is not None): 
         try: 
            v.__warningregistry__ = None 
         except: 
            pass 
   origFilters = warnings.filters[:] 
   origShow = warnings.showwarning 
   warnings.simplefilter('always') 
   try: 
      warnings.showwarning = showWarning 
      result = f(*args, **kwargs) 
   finally: 
      warnings.filters[:] = origFilters 
      warnings.showwarning = origShow 
   return result"," 'Collect warnings and return the result of the function. 
 This is used for functions that have side effects, so that we can 
 collect warnings that are raised during the execution of the function. 
 For example, if you want to collect warnings raised by a function that 
 calls `numpy.savetxt`, you can use this function. 
 Parameters 
 observeWarning : callable 
 A callable that takes a warning as input and returns a boolean. 
 If the callable returns True, the warning is printed. 
 f : callable 
 The function to call. 
 *args 
 *kwargs 
 Arguments to pass to the function. 
 Returns 
 The return value of the function. 
 Examples 
 >>> def f(): 
 ...     return 1 
 >>> _collectWarnings(lambda w: True)(f) 
 1'","'Call C{f} with C{args} positional arguments and C{kwargs} keyword arguments 
 and collect all warnings which are emitted as a result in a list. 
 @param observeWarning: A callable which will be invoked with a L{_Warning} 
 instance each time a warning is emitted. 
 @return: The return value of C{f(*args, **kwargs)}.'"
"def comment_counts(user, all_comments, filediff, interfilediff=None): 
    comment_dict = {} 
   if interfilediff: 
      key = (filediff.pk, interfilediff.pk) 
   else: 
      key = (filediff.pk, None) 
   comments = all_comments.get(key, []) 
   for comment in comments: 
      review = comment.get_review() 
      if (review and (review.public or (review.user == user))): 
         key = (comment.first_line, comment.num_lines) 
         comment_dict.setdefault(key, []).append({u'comment_id': comment.id, u'text': normalize_text_for_edit(user, comment.text, comment.rich_text), u'html': markdown_render_conditional(comment.text, comment.rich_text), u'rich_text': comment.rich_text, u'line': comment.first_line, u'num_lines': comment.num_lines, u'user': {u'username': review.user.username, u'name': (review.user.get_full_name() or review.user.username)}, u'url': comment.get_review_url(), u'localdraft': ((review.user == user) and (not review.public)), u'review_id': review.id, u'issue_opened': comment.issue_opened, u'issue_status': BaseComment.issue_status_to_string(comment.issue_status), u'reply_to_id': comment.reply_to_id}) 
   comments_array = [] 
   for (key, value) in six.iteritems(comment_dict): 
      comments_array.append({u'linenum': key[0], u'num_lines': key[1], u'comments': value}) 
   comments_array.sort(cmp=(lambda x, y: cmp(x[u'linenum'], (y[u'linenum'] or cmp(x[u'num_lines'], y[u'num_lines']))))) 
   return comments_array"," 'Returns a list of comment objects sorted by line number and num_lines. 
 Each comment object has the following fields: 
 linenum: The line number of the comment. 
 num_lines: The number of lines in the comment. 
 comments: A list of comment objects. 
 Each comment object has the following fields: 
 comment_id: The ID of the comment. 
 text: The text of the comment. 
 html: The HTML version of the comment. 
 rich_text: The rich text version of the comment. 
 line: The line number of the comment. 
 num_lines: The number of lines in the comment. 
 user: Information about the user who left the comment. 
 url: The URL to the comment. 
 localdraft: Whether the comment is a local draft. 
 review_id: The ID of the review. 
 issue_opened: Whether the issue was opened in the comment. 
 issue_status: The status of the issue. 
 reply_to_id: The ID of the comment that this comment is a reply to.'","'Returns an array of current comments for a filediff, sorted by line number. 
 Each entry in the array has a dictionary containing the following keys: 
 Key                Description 
 comment_id         The ID of the comment 
 text               The plain or rich text of the comment 
 rich_text          The rich text flag for the comment 
 line               The first line number 
 num_lines          The number of lines this comment spans 
 user               A dictionary containing ""username"" and ""name"" keys 
 for the user 
 url                The URL to the comment 
 localdraft         True if this is the current user\'s draft comment 
 review_id          The ID of the review this comment is associated with'"
"def req_match(): 
    return s3db.req_match()", 'Get the Requirements Matching table.','Match Requests for Sites'
"def _get_TV(codon_lst1, codon_lst2, codon_table=default_codon_table): 
    purine = ('A', 'G') 
   pyrimidine = ('C', 'T') 
   TV = [0, 0] 
   sites = 0 
   for (codon1, codon2) in zip(codon_lst1, codon_lst2): 
      if ('---' not in (codon1, codon2)): 
         for (i, j) in zip(codon1, codon2): 
            if (i == j): 
               pass 
            elif ((i in purine) and (j in purine)): 
               TV[0] += 1 
            elif ((i in pyrimidine) and (j in pyrimidine)): 
               TV[0] += 1 
            else: 
               TV[1] += 1 
            sites += 1 
   return ((TV[0] / sites), (TV[1] / sites))"," 'Return the TV score of two codon sequences 
 Parameters 
 codon_lst1 : list of codons 
 codon_lst2 : list of codons 
 codon_table : CodonTable 
 Returns 
 tuple 
 (TV_A, TV_G) 
 where TV_A and TV_G are the TV scores for A and G, respectively.'","'Get TV (PRIVATE). 
 Arguments: 
 - T - proportions of transitional differences 
 - V - proportions of transversional differences'"
"def failing_checks(node_address, timeout=30): 
    ctx = ssl.create_default_context() 
   ctx.check_hostname = False 
   ctx.load_verify_locations(cafile='ssl/cacert.pem') 
   ctx.verify_mode = ssl.CERT_REQUIRED 
   url = 'https://{}:8500/v1/health/state/any'.format(node_address) 
   request = urllib2.Request(url) 
   auth = ('Basic   ' + base64.b64encode(get_credentials())) 
   request.add_header('Authorization', auth) 
   f = urllib2.urlopen(request, None, timeout, context=ctx) 
   checks = json.loads(f.read().decode('utf8')) 
   return [c for c in checks if (c.get('Status', '').lower() != 'passing')]"," 'Returns a list of failing checks. 
 :param node_address: node address to connect to 
 :param timeout: timeout in seconds'",'Returns a list of failing checks.'
"def batch_indices(batch_nb, data_length, batch_size): 
    start = int((batch_nb * batch_size)) 
   end = int(((batch_nb + 1) * batch_size)) 
   if (end > data_length): 
      shift = (end - data_length) 
      start -= shift 
      end -= shift 
   return (start, end)"," 'Compute the indices of the batch of data to be read. 
 :param batch_nb: number of batches to read 
 :param data_length: total length of the data 
 :param batch_size: batch size 
 :return: start, end indices of the batch to be read'","'This helper function computes a batch start and end index 
 :param batch_nb: the batch number 
 :param data_length: the total length of the data being parsed by batches 
 :param batch_size: the number of inputs in each batch 
 :return: pair of (start, end) indices'"
"def import_doc(d, doctype, overwrite, row_idx, submit=False, ignore_links=False): 
    if (d.get(u'name') and frappe.db.exists(doctype, d[u'name'])): 
      if overwrite: 
         doc = frappe.get_doc(doctype, d[u'name']) 
         doc.flags.ignore_links = ignore_links 
         doc.update(d) 
         if (d.get(u'docstatus') == 1): 
            doc.update_after_submit() 
         else: 
            doc.save() 
         return (u'Updated   row   (#%d)   %s' % ((row_idx + 1), getlink(doctype, d[u'name']))) 
      else: 
         return (u'Ignored   row   (#%d)   %s   (exists)' % ((row_idx + 1), getlink(doctype, d[u'name']))) 
   else: 
      doc = frappe.get_doc(d) 
      doc.flags.ignore_links = ignore_links 
      doc.insert() 
      if submit: 
         doc.submit() 
      return (u'Inserted   row   (#%d)   %s' % ((row_idx + 1), getlink(doctype, doc.get(u'name'))))", 'Import doc and update the row in the table','import main (non child) document'
"def fmtstr_payload(offset, writes, numbwritten=0, write_size='byte'): 
    config = {32: {'byte': (4, 1, 255, 'hh', 8), 'short': (2, 2, 65535, 'h', 16), 'int': (1, 4, 4294967295, '', 32)}, 64: {'byte': (8, 1, 255, 'hh', 8), 'short': (4, 2, 65535, 'h', 16), 'int': (2, 4, 4294967295, '', 32)}} 
   if (write_size not in ['byte', 'short', 'int']): 
      log.error(""write_size   must   be   'byte',   'short'   or   'int'"") 
   (number, step, mask, formatz, decalage) = config[context.bits][write_size] 
   payload = '' 
   for (where, what) in writes.items(): 
      for i in range(0, (number * step), step): 
         payload += pack((where + i)) 
   numbwritten += len(payload) 
   fmtCount = 0 
   for (where, what) in writes.items(): 
      for i in range(0, number): 
         current = (what & mask) 
         if ((numbwritten & mask) <= current): 
            to_add = (current - (numbwritten & mask)) 
         else: 
            to_add = ((current | (mask + 1)) - (numbwritten & mask)) 
         if (to_add != 0): 
            payload += '%{}c'.format(to_add) 
         payload += '%{}${}n'.format((offset + fmtCount), formatz) 
         numbwritten += to_add 
         what >>= decalage 
         fmtCount += 1 
   return payload"," 'Format a payload to be written in a context. 
 :param offset: 
 :param writes: 
 :param numbwritten: 
 :param write_size: 
 :return: 
 :rtype: 
 :raises: 
 :version: 
 :author: 
 :date: 
 :copyright: 
 :license: 
 :requires: 
 :depends: 
 :sphinx: 
 :doc: 
 :todo: 
 :tracker: 
 :lifecycle: 
 :maintainer: 
 :provides: 
 :bin: 
 :example: 
 :ref: 
 :references: 
 :test: 
 :related: 
 :deprecated: 
 :deprecated since: 
 :deprecated reason: 
 :deprecated when: 
 :deprecated details: 
 :deprecated alternative: 
 :deprecated version: 
 :deprecated since: 
 :deprecated reason: 
 :deprecated when: 
 :deprecated details","'fmtstr_payload(offset, writes, numbwritten=0, write_size=\'byte\') -> str 
 Makes payload with given parameter. 
 It can generate payload for 32 or 64 bits architectures. 
 The size of the addr is taken from ``context.bits`` 
 Arguments: 
 offset(int): the first formatter\'s offset you control 
 writes(dict): dict with addr, value ``{addr: value, addr2: value2}`` 
 numbwritten(int): number of byte already written by the printf function 
 write_size(str): must be ``byte``, ``short`` or ``int``. Tells if you want to write byte by byte, short by short or int by int (hhn, hn or n) 
 Returns: 
 The payload in order to do needed writes 
 Examples: 
 >>> context.clear(arch = \'amd64\') 
 >>> print repr(fmtstr_payload(1, {0x0: 0x1337babe}, write_size=\'int\')) 
 \'\x00\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x00\x00\x00\x00\x00%322419374c%1$n%3972547906c%2$n\' 
 >>> print repr(fmtstr_payload(1, {0x0: 0x1337babe}, write_size=\'short\')) 
 \'\x00\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x00\x00\x00\x00\x00\x06\x00\x00\x00\x00\x00\x00\x00%47774c%1$hn%22649c%2$hn%60617c%3$hn%4$hn\' 
 >>> print repr(fmtstr_payload(1, {0x0: 0x1337babe}, write_size=\'byte\')) 
 \'\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x03\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x00\x00\x00\x00\x00\x05\x00\x00\x00\x00\x00\x00\x00\x06\x00\x00\x00\x00\x00\x00\x00\x07\x00\x00\x00\x00\x00\x00\x00%126c%1$hhn%252c%2$hhn%125c%3$hhn%220c%4$hhn%237c%5$hhn%6$hhn%7$hhn%8$hhn\' 
 >>> context.clear(arch = \'i386\') 
 >>> print repr(fmtstr_payload(1, {0x0: 0x1337babe}, write_size=\'int\')) 
 \'\x00\x00\x00\x00%322419386c%1$n\' 
 >>> print repr(fmtstr_payload(1, {0x0: 0x1337babe}, write_size=\'short\')) 
 \'\x00\x00\x00\x00\x02\x00\x00\x00%47798c%1$hn%22649c%2$hn\' 
 >>> print repr(fmtstr_payload(1, {0x0: 0x1337babe}, write_size=\'byte\')) 
 \'\x00\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x00\x03\x00\x00\x00%174c%1$hhn%252c%2$hhn%125c%3$hhn%220c%4$hhn\''"
"def makeTerms(nodes, title='Node', term='xterm'): 
    terms = [] 
   for node in nodes: 
      terms += makeTerm(node, title, term) 
   return terms", 'Returns a list of terms from a list of nodes.',"'Create terminals. 
 nodes: list of Node objects 
 title: base title for each 
 returns: list of created tunnel/terminal processes'"
"def set_using_network_time(enable): 
    state = salt.utils.mac_utils.validate_enabled(enable) 
   cmd = 'systemsetup   -setusingnetworktime   {0}'.format(state) 
   salt.utils.mac_utils.execute_return_success(cmd) 
   return (state == salt.utils.mac_utils.validate_enabled(get_using_network_time()))"," 'Set the system to use network time. 
 This will use the NTP server on the network to determine the time. 
 :param bool enable: Set to True to enable the network time, False to disable. 
 :returns: The current setting. 
 :rtype: bool'","'Set whether network time is on or off. 
 :param enable: True to enable, False to disable. Can also use \'on\' or \'off\' 
 :type: str bool 
 :return: True if successful, False if not 
 :rtype: bool 
 :raises: CommandExecutionError on failure 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' timezone.set_using_network_time True'"
"def read_cache(stream): 
    (version, num_entries) = read_header(stream) 
   count = 0 
   entries = dict() 
   read = stream.read 
   tell = stream.tell 
   while (count < num_entries): 
      beginoffset = tell() 
      ctime = unpack('>8s', read(8))[0] 
      mtime = unpack('>8s', read(8))[0] 
      (dev, ino, mode, uid, gid, size, sha, flags) = unpack('>LLLLLL20sH', read(((20 + (4 * 6)) + 2))) 
      path_size = (flags & CE_NAMEMASK) 
      path = read(path_size).decode(defenc) 
      real_size = (((tell() - beginoffset) + 8) & (~ 7)) 
      read(((beginoffset + real_size) - tell())) 
      entry = IndexEntry((mode, sha, flags, path, ctime, mtime, dev, ino, uid, gid, size)) 
      entries[(path, entry.stage)] = entry 
      count += 1 
   extension_data = stream.read((~ 0)) 
   assert (len(extension_data) > 19), ('Index   Footer   was   not   at   least   a   sha   on   content   as   it   was   only   %i   bytes   in   size' % len(extension_data)) 
   content_sha = extension_data[(-20):] 
   extension_data = extension_data[:(-20)] 
   return (version, entries, extension_data, content_sha)"," 'Reads a cache file and returns the version number, a dictionary of 
 entries, the extension data and the content sha.'","'Read a cache file from the given stream 
 :return: tuple(version, entries_dict, extension_data, content_sha) 
 * version is the integer version number 
 * entries dict is a dictionary which maps IndexEntry instances to a path at a stage 
 * extension_data is \'\' or 4 bytes of type + 4 bytes of size + size bytes 
 * content_sha is a 20 byte sha on all cache file contents'"
"def test_construction(): 
    s3_deleter.Deleter()", 'Test that Deleter is constructible.','The constructor basically works.'
"def getGeometryOutput(elementNode): 
    derivation = HeightmapDerivation(elementNode) 
   heightGrid = derivation.heightGrid 
   if (derivation.fileName != ''): 
      heightGrid = getHeightGrid(archive.getAbsoluteFolderPath(elementNode.getOwnerDocument().fileName, derivation.fileName)) 
   return getGeometryOutputByHeightGrid(derivation, elementNode, heightGrid)"," 'Get the geometry output for the given element node. 
 :param elementNode: The element node. 
 :return: The geometry output. 
 :rtype: :class:`~ospy.core.geometry.GeometryOutput`'",'Get vector3 vertexes from attribute dictionary.'
"def maybe_download(filename, work_directory): 
    if (not os.path.exists(work_directory)): 
      os.mkdir(work_directory) 
   filepath = os.path.join(work_directory, filename) 
   if (not os.path.exists(filepath)): 
      (filepath, _) = urllib.urlretrieve((SOURCE_URL + filename), filepath) 
      statinfo = os.stat(filepath) 
      print('Succesfully   downloaded', filename, statinfo.st_size, 'bytes.') 
   return filepath", 'Download the file `filename` to `work_directory` and return the path.',"'Download the data from Yann\'s website, unless it\'s already here.'"
"def _set_rpm_probes(probes): 
    return __salt__['probes.set_probes'](_ordered_dict_to_dict(probes), commit=False)"," 'Set the RPM probes to use. 
 .. versionchanged:: 2015.7.0 
 Adds support for setting the probes to use.'","'Calls the Salt module ""probes"" to configure the probes on the device.'"
"def find_subsections(section): 
    result = [] 
   for child in section.children: 
      if isinstance(child, nodes.section): 
         result.append(child) 
         continue 
      result.extend(find_subsections(child)) 
   return result"," 'Find all the subsections of the given section. 
 :param section: Section to find subsections of 
 :return: List of subsections'",'Return a list of subsections for the given ``section``.'
"def recreate_tags_from_list(list_of_tags): 
    tags = list() 
   i = 0 
   list_of_tags = list_of_tags 
   for i in range(len(list_of_tags)): 
      key_name = list_of_tags[i][0] 
      key_val = list_of_tags[i][1] 
      tags.append({'Key': key_name, 'Value': key_val}) 
   return tags"," 'Creates tags from a list of tags 
 :param list_of_tags: list of tags 
 :return: list of tags'","'Recreate tags from a list of tuples into the Amazon Tag format. 
 Args: 
 list_of_tags (list): List of tuples. 
 Basic Usage: 
 >>> list_of_tags = [(\'Env\', \'Development\')] 
 >>> recreate_tags_from_list(list_of_tags) 
 ""Value"": ""Development"", 
 ""Key"": ""Env"" 
 Returns: 
 List'"
"def parse_sphinx_searchindex(searchindex): 
    def _select_block(str_in, start_tag, end_tag): 
      'Select   first   block   delimited   by   start_tag   and   end_tag' 
      start_pos = str_in.find(start_tag) 
      if (start_pos < 0): 
         raise ValueError('start_tag   not   found') 
      depth = 0 
      for pos in range(start_pos, len(str_in)): 
         if (str_in[pos] == start_tag): 
            depth += 1 
         elif (str_in[pos] == end_tag): 
            depth -= 1 
         if (depth == 0): 
            break 
      sel = str_in[(start_pos + 1):pos] 
      return sel 
   def _parse_dict_recursive(dict_str): 
      'Parse   a   dictionary   from   the   search   index' 
      dict_out = dict() 
      pos_last = 0 
      pos = dict_str.find(':') 
      while (pos >= 0): 
         key = dict_str[pos_last:pos] 
         if (dict_str[(pos + 1)] == '['): 
            pos_tmp = dict_str.find(']', (pos + 1)) 
            if (pos_tmp < 0): 
               raise RuntimeError('error   when   parsing   dict') 
            value = dict_str[(pos + 2):pos_tmp].split(',') 
            for i in range(len(value)): 
               try: 
                  value[i] = int(value[i]) 
               except ValueError: 
                  pass 
         elif (dict_str[(pos + 1)] == '{'): 
            subdict_str = _select_block(dict_str[pos:], '{', '}') 
            value = _parse_dict_recursive(subdict_str) 
            pos_tmp = (pos + len(subdict_str)) 
         else: 
            raise ValueError('error   when   parsing   dict:   unknown   elem') 
         key = key.strip('""') 
         if (len(key) > 0): 
            dict_out[key] = value 
         pos_last = dict_str.find(',', pos_tmp) 
         if (pos_last < 0): 
            break 
         pos_last += 1 
         pos = dict_str.find(':', pos_last) 
      return dict_out 
   query = 'objects:' 
   pos = searchindex.find(query) 
   if (pos < 0): 
      raise ValueError('""objects:""   not   found   in   search   index') 
   sel = _select_block(searchindex[pos:], '{', '}') 
   objects = _parse_dict_recursive(sel) 
   query = 'filenames:' 
   pos = searchindex.find(query) 
   if (pos < 0): 
      raise ValueError('""filenames:""   not   found   in   search   index') 
   filenames = searchindex[((pos + len(query)) + 1):] 
   filenames = filenames[:filenames.find(']')] 
   filenames = [f.strip('""') for f in filenames.split(',')] 
   return (filenames, objects)"," 'Parses the search index in a Sphinx search result and returns 
 the filenames and the objects. 
 :param searchindex: the search index string 
 :return: a tuple of filenames and objects'","'Parse a Sphinx search index 
 Parameters 
 searchindex : str 
 The Sphinx search index (contents of searchindex.js) 
 Returns 
 filenames : list of str 
 The file names parsed from the search index. 
 objects : dict 
 The objects parsed from the search index.'"
"def _get_deep(data_structure, dot_path_or_list, default_value=None): 
    search_path = None 
   param_type = type(dot_path_or_list) 
   if (param_type in (tuple, list)): 
      search_path = dot_path_or_list 
   elif (param_type == str): 
      search_path = dot_path_or_list.split('.') 
   assert (len(search_path) > 0), 'Missing   valid   search   path' 
   try: 
      current_item = data_structure 
      for search_key in search_path: 
         current_item = current_item[search_key] 
   except (KeyError, IndexError, TypeError): 
      return default_value 
   return current_item"," 'Returns the value of a data structure at a specified path. 
 If the path is a list, the value is returned for the first element. 
 If the path is a tuple, the value is returned for the first element. 
 If the path is a string, the value is returned for the first element.'","'Attempts access nested data structures and not blow up on a gross key 
 error 
 ""hello"": { 
 ""hi"": 5'"
"def strip_unneeded(bkts, sufficient_funds): 
    bkts = sorted(bkts, key=(lambda bkt: bkt.value)) 
   for i in range(len(bkts)): 
      if (not sufficient_funds(bkts[(i + 1):])): 
         return bkts[i:] 
   return bkts"," 'Returns the bkts in sorted order, but with the bkts that have 
 insufficient funds removed. 
 :param bkts: 
 :param sufficient_funds: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :",'Remove buckets that are unnecessary in achieving the spend amount'
"def run(cmd, **kwargs): 
    log('-', cmd) 
   cmd = cmd.split() 
   arg0 = cmd[0] 
   if (not find_executable(arg0)): 
      raise Exception((('Cannot   find   executable   ""%s"";' % arg0) + ('you   might   try   %s   --depend' % argv[0]))) 
   return check_output(cmd, **kwargs)", 'Runs a command and returns its output.','Convenient interface to check_output'
"@treeio_login_required 
 def ajax_object_lookup(request, response_format='html'): 
    objects = [] 
   if (request.GET and ('term' in request.GET)): 
      objects = Object.filter_permitted(request.user.profile, Object.objects.filter(object_name__icontains=request.GET['term']), mode='x')[:10] 
   return render_to_response('core/ajax_object_lookup', {'objects': objects}, context_instance=RequestContext(request), response_format=response_format)"," 'Ajax-only lookup for objects. 
 :param request: 
 :param response_format: 
 :return: 
 :rtype: 
 :raises: 
 :example: 
 .. sourcecode:: html 
 <script src=""/core/ajax_object_lookup?term=object"" type=""text/javascript""></script>'",'Returns a list of matching objects'
"def scope2index(scope, descr, where=None): 
    try: 
      return scopes.index(scope) 
   except ValueError: 
      raise ValueError(""{0}   {1}has   an   unsupported   scope   value   '{2}'"".format(descr, ('from   {0}   '.format(where) if where else ''), scope))"," 'Returns the index of the scope for the given scope and description. 
 :param scope: the scope to check 
 :param descr: the description of the scope 
 :param where: the where clause for the scope 
 :return: the index of the scope'","'Look up the index of ``scope`` and raise a descriptive value error 
 if not defined.'"
"def get_context(context): 
    host = get_request_site_address() 
   blog_list = frappe.db.sql(u' DCTB  DCTB select   route   as   name,   published_on,   modified,   title,   content   from   `tabBlog   Post`\n DCTB  DCTB where   ifnull(published,0)=1\n DCTB  DCTB order   by   published_on   desc   limit   20', as_dict=1) 
   for blog in blog_list: 
      blog_page = cstr(urllib.quote(blog.route.encode(u'utf-8'))) 
      blog.link = urllib.basejoin(host, blog_page) 
      blog.content = escape_html((blog.content or u'')) 
   if blog_list: 
      modified = max((blog[u'modified'] for blog in blog_list)) 
   else: 
      modified = now() 
   blog_settings = frappe.get_doc(u'Blog   Settings', u'Blog   Settings') 
   context = {u'title': (blog_settings.blog_title or u'Blog'), u'description': (blog_settings.blog_introduction or u''), u'modified': modified, u'items': blog_list, u'link': (host + u'/blog')} 
   return context", 'Get context for blog page','generate rss feed'
"def publish_cmdline(reader=None, reader_name='standalone', parser=None, parser_name='restructuredtext', writer=None, writer_name='pseudoxml', settings=None, settings_spec=None, settings_overrides=None, config_section=None, enable_exit_status=True, argv=None, usage=default_usage, description=default_description): 
    pub = Publisher(reader, parser, writer, settings=settings) 
   pub.set_components(reader_name, parser_name, writer_name) 
   output = pub.publish(argv, usage, description, settings_spec, settings_overrides, config_section=config_section, enable_exit_status=enable_exit_status) 
   return output"," 'Publish a document to the specified output format. 
 This is a convenience function that takes care of all the details of 
 publishing a document to a particular output format. 
 :param reader: A :class:`Reader` instance to read the document from. 
 :param reader_name: The name of the :class:`Reader` instance. 
 :param parser: A :class:`Parser` instance to parse the document. 
 :param parser_name: The name of the :class:`Parser` instance. 
 :param writer: A :class:`Writer` instance to write the document. 
 :param writer_name: The name of the :class:`Writer` instance. 
 :param settings: A :class:`Settings` instance to use. 
 :param settings_spec: A dictionary of settings to use. 
 :param settings_overrides: A dictionary of settings to override. 
 :param config_section: The config section to use. 
 :param enable_exit_status: Whether to set the exit status to 0 if 
 the document is successfully published. 
 :param argv: The command line arguments to use. 
 :param","'Set up & run a `Publisher` for command-line-based file I/O (input and 
 output file paths taken automatically from the command line).  Return the 
 encoded string output also. 
 Parameters: see `publish_programmatically` for the remainder. 
 - `argv`: Command-line argument list to use instead of ``sys.argv[1:]``. 
 - `usage`: Usage string, output if there\'s a problem parsing the command 
 line. 
 - `description`: Program description, output for the ""--help"" option 
 (along with command-line option descriptions).'"
"def user_remove(name, database=None, user=None, password=None, host=None, port=None): 
    if (not user_exists(name, database, user, password, host, port)): 
      if database: 
         log.info(""User   '{0}'   does   not   exist   for   DB   '{1}'"".format(name, database)) 
      else: 
         log.info(""Cluster   admin   '{0}'   does   not   exist"".format(name)) 
      return False 
   client = _client(user=user, password=password, host=host, port=port) 
   if (not database): 
      return client.delete_cluster_admin(name) 
   client.switch_database(database) 
   return client.delete_database_user(name)"," 'Remove the specified user. 
 :param name: The name of the user to remove. 
 :param database: The name of the database to remove the user from. 
 :param user: The name of the user to remove. 
 :param password: The password of the user to remove. 
 :param host: The host to connect to. 
 :param port: The port to connect to. 
 :return: True if the user was successfully removed, False otherwise.'","'Remove a cluster admin or a database user. 
 If a database is specified: it will remove the database user. 
 If a database is not specified: it will remove the cluster admin. 
 name 
 User name to remove 
 database 
 The database to remove the user from 
 user 
 User name for the new user to delete 
 user 
 The user to connect as 
 password 
 The password of the user 
 host 
 The host to connect to 
 port 
 The port to connect to 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' influxdb08.user_remove <name> 
 salt \'*\' influxdb08.user_remove <name> <database> 
 salt \'*\' influxdb08.user_remove <name> <database> <user> <password> <host> <port>'"
"def get_base_domain(url): 
    domain = get_domain(url) 
   tld = '.'.join(domain.split('.')[(-2):]) 
   if (tld in ['co.uk', 'com.au', 'au.com']): 
      end_chunks = domain.split('.')[(-3):] 
   else: 
      end_chunks = domain.split('.')[(-2):] 
   base_domain = '.'.join(end_chunks) 
   return base_domain"," 'Returns the base domain of the given url. 
 The base domain is the domain without the TLD. 
 :param url: The url to parse. 
 :type url: str 
 :return: The base domain. 
 :rtype: str'","'For example, the base url of uk.reuters.com => reuters.com'"
"def relativize_classpath(classpath, root_dir, followlinks=True): 
    def relativize_url(url, root_dir): 
      url = (os.path.realpath(url) if followlinks else url) 
      root_dir = (os.path.realpath(root_dir) if followlinks else root_dir) 
      url_in_bundle = os.path.relpath(url, root_dir) 
      if os.path.isdir(url): 
         url_in_bundle += u'/' 
      return url_in_bundle 
   return [relativize_url(url, root_dir) for url in classpath]"," 'Returns a list of URLs relative to the given root_dir, which may be a 
 directory or a bundle. 
 :param classpath: 
 The list of URLs to relativize. 
 :param root_dir: 
 The root directory. 
 :param followlinks: 
 If true, follow links. 
 :returns: 
 The list of URLs relative to root_dir.'","'Convert into classpath relative to a directory. 
 This is eventually used by a jar file located in this directory as its manifest 
 attribute Class-Path. See 
 https://docs.oracle.com/javase/7/docs/technotes/guides/extensions/spec.html#bundled 
 :param list classpath: Classpath to be relativized. 
 :param string root_dir: directory to relativize urls in the classpath, does not 
 have to exist yet. 
 :param bool followlinks: whether to follow symlinks to calculate relative path. 
 :returns: Converted classpath of the same size as input classpath. 
 :rtype: list of strings'"
"def byte_compile(py_files, optimize=0, force=0, prefix=None, base_dir=None, verbose=1, dry_run=0, direct=None): 
    if sys.dont_write_bytecode: 
      raise DistutilsByteCompileError('byte-compiling   is   disabled.') 
   if (direct is None): 
      direct = (__debug__ and (optimize == 0)) 
   if (not direct): 
      try: 
         from tempfile import mkstemp 
         (script_fd, script_name) = mkstemp('.py') 
      except ImportError: 
         from tempfile import mktemp 
         (script_fd, script_name) = (None, mktemp('.py')) 
      log.info(""writing   byte-compilation   script   '%s'"", script_name) 
      if (not dry_run): 
         if (script_fd is not None): 
            script = os.fdopen(script_fd, 'w') 
         else: 
            script = open(script_name, 'w') 
         script.write('from   distutils.util   import   byte_compile\nfiles   =   [\n') 
         script.write((string.join(map(repr, py_files), ',\n') + ']\n')) 
         script.write(('\nbyte_compile(files,   optimize=%r,   force=%r,\n                                       prefix=%r,   base_dir=%r,\n                                       verbose=%r,   dry_run=0,\n                                       direct=1)\n' % (optimize, force, prefix, base_dir, verbose))) 
         script.close() 
      cmd = [sys.executable, script_name] 
      if (optimize == 1): 
         cmd.insert(1, '-O') 
      elif (optimize == 2): 
         cmd.insert(1, '-OO') 
      spawn(cmd, dry_run=dry_run) 
      execute(os.remove, (script_name,), ('removing   %s' % script_name), dry_run=dry_run) 
   else: 
      from py_compile import compile 
      for file in py_files: 
         if (file[(-3):] != '.py'): 
            continue 
         cfile = (file + ((__debug__ and 'c') or 'o')) 
         dfile = file 
         if prefix: 
            if (file[:len(prefix)] != prefix): 
               raise ValueError, (""invalid   prefix:   filename   %r   doesn't   start   with   %r"" % (file, prefix)) 
            dfile = dfile[len(prefix):] 
         if base_dir: 
            dfile = os.path.join(base_dir, dfile) 
         cfile_base = os.path.basename(cfile) 
         if direct: 
            if (force or newer(file, cfile)): 
               log.info('byte-compiling   %s   to   %s', file, cfile_base) 
               if (not dry_run): 
                  compile(file, cfile, dfile) 
            else: 
               log.debug('skipping   byte-compilation   of   %s   to   %s', file, cfile_base)"," 'Compile the given Python files to bytecode. 
 This is done by writing a temporary script that calls the 
 ``byte_compile`` function from the Python standard library. 
 The script is written to a temporary file, which is then run. 
 The output is then written to stdout. 
 :param py_files: The Python files to be compiled. 
 :param optimize: The optimization level to use. 0, 1, or 2. 
 :param force: Whether to force recompilation even if the source files 
 have not changed. 
 :param prefix: The prefix to prepend to the file name. 
 :param base_dir: The base directory to use. 
 :param verbose: The verbosity level. 0, 1, or 2. 
 :param dry_run: Whether to run the compiled script or not. 
 :param direct: Whether to use direct compilation or not. 
 :returns: None.'","'Byte-compile a collection of Python source files to either .pyc 
 or .pyo files in the same directory.  \'py_files\' is a list of files 
 to compile; any files that don\'t end in "".py"" are silently skipped. 
 \'optimize\' must be one of the following: 
 0 - don\'t optimize (generate .pyc) 
 1 - normal optimization (like ""python -O"") 
 2 - extra optimization (like ""python -OO"") 
 If \'force\' is true, all files are recompiled regardless of 
 timestamps. 
 The source filename encoded in each bytecode file defaults to the 
 filenames listed in \'py_files\'; you can modify these with \'prefix\' and 
 \'basedir\'.  \'prefix\' is a string that will be stripped off of each 
 source filename, and \'base_dir\' is a directory name that will be 
 prepended (after \'prefix\' is stripped).  You can supply either or both 
 (or neither) of \'prefix\' and \'base_dir\', as you wish. 
 If \'dry_run\' is true, doesn\'t actually do anything that would 
 affect the filesystem. 
 Byte-compilation is either done directly in this interpreter process 
 with the standard py_compile module, or indirectly by writing a 
 temporary script and executing it.  Normally, you should let 
 \'byte_compile()\' figure out to use direct compilation or not (see 
 the source for details).  The \'direct\' flag is used by the script 
 generated in indirect mode; unless you know what you\'re doing, leave 
 it set to None.'"
"def HoursSince(timestamp): 
    return (SecondsSince(timestamp) / 3600.0)"," 'Returns the number of hours since the timestamp. 
 :param timestamp: A timestamp to convert. 
 :returns: The number of hours since the timestamp. 
 :rtype: float'",'Hours since a given timestamp. Floating point.'
"def volume_present(name, volume_size, sparse=False, create_parent=False, properties=None, cloned_from=None): 
    ret = {'name': name, 'changes': {}, 'result': True, 'comment': ''} 
   if (not properties): 
      properties = {} 
   log.debug('zfs.volume_present::{0}::config::volume_size   =   {1}'.format(name, volume_size)) 
   log.debug('zfs.volume_present::{0}::config::sparse   =   {1}'.format(name, sparse)) 
   log.debug('zfs.volume_present::{0}::config::create_parent   =   {1}'.format(name, create_parent)) 
   log.debug('zfs.volume_present::{0}::config::cloned_from   =   {1}'.format(name, cloned_from)) 
   log.debug('zfs.volume_present::{0}::config::properties   =   {1}'.format(name, properties)) 
   for prop in properties.keys(): 
      if isinstance(properties[prop], bool): 
         properties[prop] = ('on' if properties[prop] else 'off') 
   if (('@' in name) or ('#' in name)): 
      ret['result'] = False 
      ret['comment'] = 'invalid   filesystem   or   volume   name:   {0}'.format(name) 
   if cloned_from: 
      cloned_parent = cloned_from[:cloned_from.index('@')] 
      if ('@' not in cloned_from): 
         ret['result'] = False 
         ret['comment'] = '{0}   is   not   a   snapshot'.format(cloned_from) 
      elif (cloned_from not in __salt__['zfs.list'](cloned_from, **{'type': 'snapshot'})): 
         ret['result'] = False 
         ret['comment'] = 'snapshot   {0}   does   not   exist'.format(cloned_from) 
      elif (cloned_parent not in __salt__['zfs.list'](cloned_parent, **{'type': 'volume'})): 
         ret['result'] = False 
         ret['comment'] = 'snapshot   {0}   is   not   from   a   volume'.format(cloned_from) 
   if ret['result']: 
      if (name in __salt__['zfs.list'](name, **{'type': 'volume'})): 
         properties['volsize'] = volume_size 
         result = __salt__['zfs.get'](name, **{'properties': ','.join(properties.keys()), 'fields': 'value', 'depth': 1}) 
         for prop in properties.keys(): 
            if (properties[prop] != result[name][prop]['value']): 
               if (name not in ret['changes']): 
                  ret['changes'][name] = {} 
               ret['changes'][name][prop] = properties[prop] 
         if (len(ret['changes']) > 0): 
            if (not __opts__['test']): 
               result = __salt__['zfs.set'](name, **ret['changes'][name]) 
               if (name not in result): 
                  ret['result'] = False 
               else: 
                  for prop in result[name].keys(): 
                     if (result[name][prop] != 'set'): 
                        ret['result'] = False 
            if ret['result']: 
               ret['comment'] = 'volume   {0}   was   updated'.format(name) 
            else: 
               ret['changes'] = {} 
               ret['comment'] = 'volume   {0}   failed   to   be   updated'.format(name) 
         else: 
            ret['comment'] = 'volume   {0}   is   up   to   date'.format(name) 
      else: 
         result = {name: 'created'} 
         if (not __opts__['test']): 
            if (not cloned_from): 
               result = __salt__['zfs.create'](name, **{'volume_size': volume_size, 'sparse': sparse, 'create_parent': create_parent, 'properties': properties}) 
            else: 
               result = __salt__['zfs.clone'](cloned_from, name, **{'create_parent': create_parent, 'properties': properties}) 
         ret['result'] = (name in result) 
         if ret['result']: 
            ret['result'] = ((result[name] == 'created') or result[name].startswith('cloned')) 
         if ret['result']: 
            ret['changes'][name] = (properties if (len(properties) > 0) else result[name]) 
            ret['comment'] = 'volume   {0}   was   created'.format(name) 
         else: 
            ret['comment'] = 'failed   to   create   volume   {0}'.format(name) 
            if (name in result): 
               ret['comment'] = result[name] 
   return ret"," 'Check if a volume exists and if it does, update the properties of the volume. 
 This function will check if a volume exists and if it does, update the properties 
 of the volume. If the volume does not exist, it will be created. 
 The properties of the volume can be updated by setting the properties in the 
 dictionary. 
 :param name: The name of the volume to check 
 :param volume_size: The size of the volume 
 :param sparse: Whether the volume is sparse or not 
 :param create_parent: Whether the volume should be created in a parent 
 :param properties: The properties to set on the volume 
 :param cloned_from: The name of the snapshot to clone from 
 :return: The result of the operation'","'ensure volume exists and has properties set 
 name : string 
 name of volume 
 volume_size : string 
 size of volume 
 sparse : boolean 
 create sparse volume 
 create_parent : boolean 
 creates all the non-existing parent datasets. 
 any property specified on the command line using the -o option is ignored. 
 cloned_from : string 
 name of snapshot to clone 
 properties : dict 
 additional zfs properties (-o) 
 .. note:: 
 ``cloned_from`` is only use if the volume does not exist yet, 
 when ``cloned_from`` is set after the volume exists it will be ignored. 
 .. note:: 
 Properties do not get cloned, if you specify the properties in the state file 
 they will be applied on a subsequent run. 
 ``volume_size`` is considered a property, so the volume\'s size will be 
 corrected when the properties get updated if it differs from the 
 original volume. 
 The sparse parameter is ignored when using ``cloned_from``.'"
"def prepare_grant_uri(uri, client_id, response_type, redirect_uri=None, scope=None, state=None, **kwargs): 
    if (not is_secure_transport(uri)): 
      raise InsecureTransportError() 
   params = [(u'response_type', response_type), (u'client_id', client_id)] 
   if redirect_uri: 
      params.append((u'redirect_uri', redirect_uri)) 
   if scope: 
      params.append((u'scope', list_to_scope(scope))) 
   if state: 
      params.append((u'state', state)) 
   for k in kwargs: 
      if kwargs[k]: 
         params.append((unicode_type(k), kwargs[k])) 
   return add_params_to_uri(uri, params)"," 'Prepare an authorization URI for granting access to an application. 
 :param uri: The URI to append parameters to. 
 :param client_id: The client ID of the application. 
 :param response_type: The response type. 
 :param redirect_uri: The redirect URI of the application. 
 :param scope: The scope of the access requested. 
 :param state: A state value that is passed back to the application. 
 :param kwargs: Additional parameters to append to the URI. 
 :return: The URI with parameters appended.'","'Prepare the authorization grant request URI. 
 The client constructs the request URI by adding the following 
 parameters to the query component of the authorization endpoint URI 
 using the ``application/x-www-form-urlencoded`` format as defined by 
 [`W3C.REC-html401-19991224`_]: 
 :param response_type: To indicate which OAuth 2 grant/flow is required, 
 ""code"" and ""token"". 
 :param client_id: The client identifier as described in `Section 2.2`_. 
 :param redirect_uri: The client provided URI to redirect back to after 
 authorization as described in `Section 3.1.2`_. 
 :param scope: The scope of the access request as described by 
 `Section 3.3`_. 
 :param state: An opaque value used by the client to maintain 
 state between the request and callback.  The authorization 
 server includes this value when redirecting the user-agent 
 back to the client.  The parameter SHOULD be used for 
 preventing cross-site request forgery as described in 
 `Section 10.12`_. 
 :param kwargs: Extra arguments to embed in the grant/authorization URL. 
 An example of an authorization code grant authorization URL: 
 .. code-block:: http 
 GET /authorize?response_type=code&client_id=s6BhdRkqt3&state=xyz 
 &redirect_uri=https%3A%2F%2Fclient%2Eexample%2Ecom%2Fcb HTTP/1.1 
 Host: server.example.com 
 .. _`W3C.REC-html401-19991224`: http://tools.ietf.org/html/rfc6749#ref-W3C.REC-html401-19991224 
 .. _`Section 2.2`: http://tools.ietf.org/html/rfc6749#section-2.2 
 .. _`Section 3.1.2`: http://tools.ietf.org/html/rfc6749#section-3.1.2 
 .. _`Section 3.3`: http://tools.ietf.org/html/rfc6749#section-3.3 
 .. _`section 10.12`: http://tools.ietf.org/html/rfc6749#section-10.12'"
"def ctcpStringify(messages): 
    coded_messages = [] 
   for (tag, data) in messages: 
      if data: 
         if (not isinstance(data, str)): 
            try: 
               data = '   '.join(map(str, data)) 
            except TypeError: 
               pass 
         m = ('%s   %s' % (tag, data)) 
      else: 
         m = str(tag) 
      m = ctcpQuote(m) 
      m = ('%s%s%s' % (X_DELIM, m, X_DELIM)) 
      coded_messages.append(m) 
   line = ''.join(coded_messages) 
   return line"," 'Convert a list of CTCP messages into a string. 
 :param messages: 
 A list of (tag, data) tuples. 
 :returns: 
 A string of all the CTCP messages.'","'@type messages: a list of extended messages.  An extended 
 message is a (tag, data) tuple, where \'data\' may be L{None}, a 
 string, or a list of strings to be joined with whitespace. 
 @returns: String'"
"def beacon(config): 
    ret = [] 
   changes = {} 
   txt = {} 
   global LAST_GRAINS 
   _validate = __validate__(config) 
   if (not _validate[0]): 
      log.warning('Beacon   {0}   configuration   invalid,   not   adding.   {1}'.format(__virtualname__, _validate[1])) 
      return ret 
   if ('servicename' in config): 
      servicename = config['servicename'] 
   else: 
      servicename = __grains__['host'] 
   for item in config['txt']: 
      if config['txt'][item].startswith('grains.'): 
         grain = config['txt'][item][7:] 
         grain_index = None 
         square_bracket = grain.find('[') 
         if ((square_bracket != (-1)) and (grain[(-1)] == ']')): 
            grain_index = int(grain[(square_bracket + 1):(-1)]) 
            grain = grain[:square_bracket] 
         grain_value = __grains__.get(grain, '') 
         if isinstance(grain_value, list): 
            if (grain_index is not None): 
               grain_value = grain_value[grain_index] 
            else: 
               grain_value = ','.join(grain_value) 
         txt[item] = grain_value 
         if (LAST_GRAINS and (LAST_GRAINS.get(grain, '') != __grains__.get(grain, ''))): 
            changes[str(('txt.' + item))] = txt[item] 
      else: 
         txt[item] = config['txt'][item] 
      if (not LAST_GRAINS): 
         changes[str(('txt.' + item))] = txt[item] 
   if changes: 
      if (not LAST_GRAINS): 
         changes['servicename'] = servicename 
         changes['servicetype'] = config['servicetype'] 
         changes['port'] = config['port'] 
         GROUP.AddService(avahi.IF_UNSPEC, avahi.PROTO_UNSPEC, dbus.UInt32(0), servicename, config['servicetype'], '', '', dbus.UInt16(config['port']), avahi.dict_to_txt_array(txt)) 
         GROUP.Commit() 
      elif config.get('reset_on_change', False): 
         GROUP.Reset() 
         reset_wait = config.get('reset_wait', 0) 
         if (reset_wait > 0): 
            time.sleep(reset_wait) 
         GROUP.AddService(avahi.IF_UNSPEC, avahi.PROTO_UNSPEC, dbus.UInt32(0), servicename, config['servicetype'], '', '', dbus.UInt16(config['port']), avahi.dict_to_txt_array(txt)) 
         GROUP.Commit() 
      else: 
         GROUP.UpdateServiceTxt(avahi.IF_UNSPEC, avahi.PROTO_UNSPEC, dbus.UInt32(0), servicename, config['servicetype'], '', avahi.dict_to_txt_array(txt)) 
      ret.append({'tag': 'result', 'changes': changes}) 
   if config.get('copy_grains', False): 
      LAST_GRAINS = __grains__.copy() 
   else: 
      LAST_GRAINS = __grains__ 
   return ret"," 'Adds the beacon service to the Avahi database. 
 @param config: dictionary of beacon configuration 
 @return: list of results 
 @rtype: list of dict'","'Broadcast values via zeroconf 
 If the announced values are static, it is adviced to set run_once: True 
 (do not poll) on the beacon configuration. 
 The following are required configuration settings: 
 \'servicetype\': The service type to announce. 
 \'port\': The port of the service to announce. 
 \'txt\': The TXT record of the service being announced as a dict. 
 Grains can be used to define TXT values using the syntax: 
 grains.<grain_name> 
 or: 
 grains.<grain_name>[i] 
 where i is an integer representing the index of the grain to 
 use. If the grain is not a list, the index is ignored. 
 The following are optional configuration settings: 
 \'servicename\': Set the name of the service. Will use the hostname from 
 __grains__[\'host\'] if not set. 
 \'reset_on_change\': If true and there is a change in TXT records 
 detected, it will stop announcing the service and 
 then restart announcing the service. This 
 interruption in service announcement may be 
 desirable if the client relies on changes in the 
 browse records to update its cache of the TXT 
 records. 
 Defaults to False. 
 \'reset_wait\': The number of seconds to wait after announcement stops 
 announcing and before it restarts announcing in the 
 case where there is a change in TXT records detected 
 and \'reset_on_change\' is True. 
 Defaults to 0. 
 \'copy_grains\': If set to True, it will copy the grains passed into 
 the beacon when it backs them up to check for changes 
 on the next iteration. Normally, instead of copy, it 
 would use straight value assignment. This will allow 
 detection of changes to grains where the grains are 
 modified in-place instead of completely replaced. 
 In-place grains changes are not currently done in the 
 main Salt code but may be done due to a custom 
 plug-in. 
 Defaults to False. 
 Example Config 
 .. code-block:: yaml 
 beacons: 
 avahi_announce: 
 run_once: True 
 servicetype: _demo._tcp 
 port: 1234 
 txt: 
 ProdName: grains.productname 
 SerialNo: grains.serialnumber 
 Comments: \'this is a test\''"
"@loader_option() 
 def noload(loadopt, attr): 
    return loadopt.set_relationship_strategy(attr, {'lazy': 'noload'})"," 'Sets the relationship strategy for the given attribute to 
 \'noload\' (default).'","'Indicate that the given relationship attribute should remain unloaded. 
 This function is part of the :class:`.Load` interface and supports 
 both method-chained and standalone operation. 
 :func:`.orm.noload` applies to :func:`.relationship` attributes; for 
 column-based attributes, see :func:`.orm.defer`.'"
"def masterserviceHandle(target): 
    GlobalObject().masterremote._reference._service.mapTarget(target)"," 'Registers a service with the master remote. 
 This is the same as calling the service\'s ``mapTarget`` method. 
 :param target: The target of the service. 
 :type target: RemoteTarget'",''
"def fetch_species_distributions(data_home=None, download_if_missing=True): 
    data_home = get_data_home(data_home) 
   if (not exists(data_home)): 
      makedirs(data_home) 
   extra_params = dict(x_left_lower_corner=(-94.8), Nx=1212, y_left_lower_corner=(-56.05), Ny=1592, grid_size=0.05) 
   dtype = np.int16 
   archive_path = _pkl_filepath(data_home, DATA_ARCHIVE_NAME) 
   if (not exists(archive_path)): 
      if (not download_if_missing): 
         raise IOError('Data   not   found   and   `download_if_missing`   is   False') 
      print ('Downloading   species   data   from   %s   to   %s' % (SAMPLES_URL, data_home)) 
      X = np.load(BytesIO(urlopen(SAMPLES_URL).read())) 
      for f in X.files: 
         fhandle = BytesIO(X[f]) 
         if ('train' in f): 
            train = _load_csv(fhandle) 
         if ('test' in f): 
            test = _load_csv(fhandle) 
      print ('Downloading   coverage   data   from   %s   to   %s' % (COVERAGES_URL, data_home)) 
      X = np.load(BytesIO(urlopen(COVERAGES_URL).read())) 
      coverages = [] 
      for f in X.files: 
         fhandle = BytesIO(X[f]) 
         print ('   -   converting', f) 
         coverages.append(_load_coverage(fhandle)) 
      coverages = np.asarray(coverages, dtype=dtype) 
      bunch = Bunch(coverages=coverages, test=test, train=train, **extra_params) 
      joblib.dump(bunch, archive_path, compress=9) 
   else: 
      bunch = joblib.load(archive_path) 
   return bunch"," 'Loads the species distribution data from the internet and saves it to 
 disk. 
 Parameters 
 data_home : str, optional 
 The path to the directory where the data will be saved. 
 download_if_missing : bool, optional 
 If True, download the data if it is not present on disk. 
 Returns 
 Bunch 
 A Bunch containing the following attributes: 
 - train 
 - test 
 - coverages 
 - extra_params 
 - dtype 
 See Also 
 :func:`fetch_species_distributions_from_disk` 
 Examples 
 >>> from sklearn.datasets import fetch_species_distributions 
 >>> species_data = fetch_species_distributions()'","'Loader for species distribution dataset from Phillips et. al. (2006) 
 Read more in the :ref:`User Guide <datasets>`. 
 Parameters 
 data_home : optional, default: None 
 Specify another download and cache folder for the datasets. By default 
 all scikit learn data is stored in \'~/scikit_learn_data\' subfolders. 
 download_if_missing : optional, True by default 
 If False, raise a IOError if the data is not locally available 
 instead of trying to download the data from the source site. 
 Returns 
 The data is returned as a Bunch object with the following attributes: 
 coverages : array, shape = [14, 1592, 1212] 
 These represent the 14 features measured at each point of the map grid. 
 The latitude/longitude values for the grid are discussed below. 
 Missing data is represented by the value -9999. 
 train : record array, shape = (1623,) 
 The training points for the data.  Each point has three fields: 
 - train[\'species\'] is the species name 
 - train[\'dd long\'] is the longitude, in degrees 
 - train[\'dd lat\'] is the latitude, in degrees 
 test : record array, shape = (619,) 
 The test points for the data.  Same format as the training data. 
 Nx, Ny : integers 
 The number of longitudes (x) and latitudes (y) in the grid 
 x_left_lower_corner, y_left_lower_corner : floats 
 The (x,y) position of the lower-left corner, in degrees 
 grid_size : float 
 The spacing between points of the grid, in degrees 
 Notes 
 This dataset represents the geographic distribution of species. 
 The dataset is provided by Phillips et. al. (2006). 
 The two species are: 
 - `""Bradypus variegatus"" 
 <http://www.iucnredlist.org/details/3038/0>`_ , 
 the Brown-throated Sloth. 
 - `""Microryzomys minutus"" 
 <http://www.iucnredlist.org/details/13408/0>`_ , 
 also known as the Forest Small Rice Rat, a rodent that lives in Peru, 
 Colombia, Ecuador, Peru, and Venezuela. 
 References 
 * `""Maximum entropy modeling of species geographic distributions"" 
 <http://www.cs.princeton.edu/~schapire/papers/ecolmod.pdf>`_ 
 S. J. Phillips, R. P. Anderson, R. E. Schapire - Ecological Modelling, 
 190:231-259, 2006. 
 Notes 
 * See examples/applications/plot_species_distribution_modeling.py 
 for an example of using this dataset with scikit-learn'"
"def automaster(config='/etc/auto_salt'): 
    ret = {} 
   if (not os.path.isfile(config)): 
      return ret 
   with salt.utils.fopen(config) as ifile: 
      for line in ifile: 
         if line.startswith('#'): 
            continue 
         if (not line.strip()): 
            continue 
         comps = line.split() 
         if (len(comps) != 3): 
            continue 
         prefix = '/..' 
         name = comps[0].replace(prefix, '') 
         device_fmt = comps[2].split(':') 
         opts = comps[1].split(',') 
         ret[name] = {'device': device_fmt[1], 'fstype': opts[0], 'opts': opts[1:]} 
   return ret", 'Returns the contents of /etc/auto_salt',"'List the contents of the auto master 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' mount.automaster'"
"@requires_sklearn 
 def test_ica_reject_buffer(): 
    raw = read_raw_fif(raw_fname).crop(1.5, stop).load_data() 
   picks = pick_types(raw.info, meg=True, stim=False, ecg=False, eog=False, exclude='bads') 
   ica = ICA(n_components=3, max_pca_components=4, n_pca_components=4) 
   raw._data[2, 1000:1005] = 5e-12 
   with catch_logging() as drop_log: 
      with warnings.catch_warnings(record=True): 
         ica.fit(raw, picks[:5], reject=dict(mag=2.5e-12), decim=2, tstep=0.01, verbose=True) 
      assert_true(((raw._data[:5, ::2].shape[1] - 4) == ica.n_samples_)) 
   log = [l for l in drop_log.getvalue().split('\n') if ('detected' in l)] 
   assert_equal(len(log), 1)", 'Test that ICA rejects the buffer.','Test ICA data raw buffer rejection.'
"def check_valid_naming(pattern=None, multi=None, anime_type=None): 
    if (pattern is None): 
      pattern = sickbeard.NAMING_PATTERN 
   if (anime_type is None): 
      anime_type = sickbeard.NAMING_ANIME 
   logger.log(((u'Checking   whether   the   pattern   ' + pattern) + u'   is   valid   for   a   single   episode'), logger.DEBUG) 
   valid = validate_name(pattern, None, anime_type) 
   if (multi is not None): 
      logger.log(((u'Checking   whether   the   pattern   ' + pattern) + u'   is   valid   for   a   multi   episode'), logger.DEBUG) 
      valid = (valid and validate_name(pattern, multi, anime_type)) 
   return valid"," 'Checks whether a naming pattern is valid for a single episode or a multi 
 episode. 
 :param pattern: The naming pattern to check. 
 :param multi: Whether the pattern is for a multi episode. 
 :param anime_type: The type of anime to check. 
 :return: Whether the pattern is valid or not. 
 :rtype: bool'","'Checks if the name is can be parsed back to its original form for both single and multi episodes. 
 :return: true if the naming is valid, false if not.'"
"def constrain_rgb(r, g, b): 
    w = (- min([0, r, g, b])) 
   if (w > 0): 
      r += w 
      g += w 
      b += w 
   return (r, g, b)"," 'Constrain the RGB values to be in the range [0, 255]. 
 Parameters 
 r : int 
 The red value. 
 g : int 
 The green value. 
 b : int 
 The blue value. 
 Returns 
 r : int 
 The red value. 
 g : int 
 The green value. 
 b : int 
 The blue value. 
 Raises 
 ValueError 
 If the red, green, or blue value is outside the range [0, 255].'","'If the requested RGB shade contains a negative weight for 
 one of the primaries, it lies outside the colour gamut 
 accessible from the given triple of primaries.  Desaturate 
 it by adding white, equal quantities of R, G, and B, enough 
 to make RGB all positive.  The function returns 1 if the 
 components were modified, zero otherwise.'"
"def _parse_split_test_data_str(): 
    tuple_class = collections.namedtuple('TestCase', 'input,   keep,   no_keep') 
   for line in test_data_str.splitlines(): 
      if (not line): 
         continue 
      data = line.split('/') 
      item = tuple_class(input=data[0], keep=data[1].split('|'), no_keep=data[2].split('|')) 
      (yield item) 
   (yield tuple_class(input='', keep=[], no_keep=[]))"," 'Parse the split test data. 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
","'Parse the test data set into a namedtuple to use in tests. 
 Returns: 
 A list of namedtuples with str attributes: input, keep, no_keep'"
"def parse_network(rule): 
    parser = argparse.ArgumentParser() 
   rules = shlex.split(rule) 
   rules.pop(0) 
   parser.add_argument('--bootproto', dest='bootproto', action='store', choices=['dhcp', 'bootp', 'static', 'ibft']) 
   parser.add_argument('--device', dest='device', action='store') 
   parser.add_argument('--ip', dest='ip', action='store') 
   parser.add_argument('--ipv6', dest='ipv6', action='store') 
   parser.add_argument('--gateway', dest='gateway', action='store') 
   parser.add_argument('--nodefroute', dest='nodefroute', action='store_true') 
   parser.add_argument('--nameserver', dest='nameserver', action='store') 
   parser.add_argument('--nodns', dest='nodns', action='store_true') 
   parser.add_argument('--netmask', dest='netmask', action='store') 
   parser.add_argument('--hostname', dest='hostname', action='store') 
   parser.add_argument('--ethtool', dest='ethtool', action='store') 
   parser.add_argument('--essid', dest='essid', action='store') 
   parser.add_argument('--wepkey', dest='wepkey', action='store') 
   parser.add_argument('--wpakey', dest='wpakey', action='store') 
   parser.add_argument('--onboot', dest='onboot', action='store') 
   parser.add_argument('--dhcpclass', dest='dhcpclass', action='store') 
   parser.add_argument('--mtu', dest='mtu', action='store') 
   parser.add_argument('--noipv4', dest='noipv4', action='store_true') 
   parser.add_argument('--noipv6', dest='noipv6', action='store_true') 
   parser.add_argument('--activate', dest='activate', action='store_true') 
   args = clean_args(vars(parser.parse_args(rules))) 
   parser = None 
   return args"," 'Parse the network configuration from the rule. 
 :param rule: The network configuration rule 
 :return: A dictionary with the parsed arguments'",'Parse the network line'
"def is_field_shared_factory(field_name): 
    class IsFieldShared(permissions.BasePermission, ): 
      '\n                        Grants   access   if   a   particular   profile   field   is   shared   with   the   requesting   user.\n                        ' 
      def has_permission(self, request, view): 
         url_username = request.parser_context.get('kwargs', {}).get('username', '') 
         if (request.user.username.lower() == url_username.lower()): 
            return True 
         if request.user.is_staff: 
            return True 
         user = get_object_or_404(User, username__iexact=url_username) 
         if (field_name in visible_fields(user.profile, user)): 
            return True 
         raise Http404() 
   return IsFieldShared", 'Checks if a particular profile field is shared with the requesting user.',"'Generates a permission class that grants access if a particular profile field is 
 shared with the requesting user.'"
"def fix_epoch(epoch): 
    epoch = int(epoch) 
   if (len(str(epoch)) <= 10): 
      return epoch 
   elif (len(str(epoch)) == 13): 
      return int((epoch / 1000)) 
   elif ((len(str(epoch)) > 10) and (len(str(epoch)) < 13)): 
      raise ValueError('Unusually   formatted   epoch   timestamp.      Should   be   10,   13,   or   more   digits') 
   else: 
      orders_of_magnitude = (len(str(epoch)) - 10) 
      powers_of_ten = (10 ** orders_of_magnitude) 
      epoch = int((epoch / powers_of_ten)) 
   return epoch"," 'Fixes epoch timestamps by converting them to integers. 
 :param epoch: epoch timestamp as a string 
 :return: epoch timestamp as an integer'","'Fix value of `epoch` to be epoch, which should be 10 or fewer digits long. 
 :arg epoch: An epoch timestamp, in epoch + milliseconds, or microsecond, or 
 even nanoseconds. 
 :rtype: int'"
"def uninstall(pecls): 
    if isinstance(pecls, six.string_types): 
      pecls = [pecls] 
   return _pecl('uninstall   {0}'.format(_cmd_quote('   '.join(pecls))))"," 'Uninstall a PECL package. 
 :param pecls: PECL package name or names. 
 :return: None'","'Uninstall one or several pecl extensions. 
 pecls 
 The pecl extensions to uninstall. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' pecl.uninstall fuse'"
"def make_user_coach(user, master_course_key): 
    coach_role_on_master_course = CourseCcxCoachRole(master_course_key) 
   coach_role_on_master_course.add_users(user)", 'Adds a user as a coach for a course.',"'Makes an user coach on the master course. 
 This function is needed because an user cannot become a coach of the CCX if s/he is not 
 coach on the master course. 
 Args: 
 user (User): User object 
 master_course_key (CourseKey): Key locator object for the course'"
"def weights_uniform(nlags): 
    return np.ones((nlags + 1))", 'Returns weights for uniform lag length',"'uniform weights for HAC 
 this will be moved to another module 
 Parameters 
 nlags : int 
 highest lag in the kernel window, this does not include the zero lag 
 Returns 
 kernel : ndarray, (nlags+1,) 
 weights for uniform kernel'"
"def literalquery(statement, dialect=None): 
    if isinstance(statement, sqlalchemy.orm.Query): 
      if (dialect is None): 
         dialect = statement.session.get_bind(statement._mapper_zero_or_none()).dialect 
      statement = statement.statement 
   if (dialect is None): 
      dialect = getattr(statement.bind, 'dialect', None) 
   if (dialect is None): 
      from sqlalchemy.dialects import mysql 
      dialect = mysql.dialect() 
   Compiler = type(statement._compiler(dialect)) 
   class LiteralCompiler(Compiler, ): 
      visit_bindparam = Compiler.render_literal_bindparam 
      def render_literal_value(self, value, type_): 
         if isinstance(value, (Decimal, long)): 
            return str(value) 
         elif isinstance(value, datetime): 
            return repr(str(value)) 
         else: 
            value = super(LiteralCompiler, self).render_literal_value(value, type_) 
            if isinstance(value, unicode): 
               return value.encode('UTF-8') 
            else: 
               return value 
   return LiteralCompiler(dialect, statement)"," 'A decorator to render a SQLAlchemy statement as a literal query. 
 The decorated statement is rendered as a literal query, with all bind 
 parameters rendered as literal values, and all bind values rendered as 
 literal values. 
 This is useful for generating SQLAlchemy statements that can be passed to 
 the MySQL client library, which does not support bind parameters. 
 :param statement: A SQLAlchemy statement to be rendered as a literal query. 
 :param dialect: A dialect to be used for rendering the literal query. 
 :return: A LiteralCompiler instance, which is used to render the literal 
 query. 
 :rtype: :class:`~sqlalchemy.dialects.mysql.compiler.LiteralCompiler`'","'Generate an SQL expression string with bound parameters rendered inline 
 for the given SQLAlchemy statement. 
 WARNING: This method of escaping is insecure, incomplete, and for debugging 
 purposes only. Executing SQL statements with inline-rendered user values is 
 extremely insecure.'"
"def available(): 
    ret = [] 
   mod_dir = os.path.join('/lib/modules/', os.uname()[2]) 
   for (root, dirs, files) in os.walk(mod_dir): 
      for fn_ in files: 
         if ('.ko' in fn_): 
            ret.append(fn_[:fn_.index('.ko')].replace('-', '_')) 
   if ('Arch' in __grains__['os_family']): 
      mod_dir_arch = (('/lib/modules/extramodules-' + os.uname()[2][0:3]) + '-ARCH') 
      for (root, dirs, files) in os.walk(mod_dir_arch): 
         for fn_ in files: 
            if ('.ko' in fn_): 
               ret.append(fn_[:fn_.index('.ko')].replace('-', '_')) 
   return sorted(list(ret))"," 'Returns a list of available kernel modules. 
 :return: A list of available kernel modules. 
 :rtype: list'","'Return a list of all available kernel modules 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' kmod.available'"
"def get_clonespec_for_valid_snapshot(config_spec, object_ref, reloc_spec, template, vm_): 
    moving = True 
   if (QUICK_LINKED_CLONE == vm_['snapshot']['disk_move_type']): 
      reloc_spec.diskMoveType = QUICK_LINKED_CLONE 
   elif (CURRENT_STATE_LINKED_CLONE == vm_['snapshot']['disk_move_type']): 
      reloc_spec.diskMoveType = CURRENT_STATE_LINKED_CLONE 
   elif (COPY_ALL_DISKS_FULL_CLONE == vm_['snapshot']['disk_move_type']): 
      reloc_spec.diskMoveType = COPY_ALL_DISKS_FULL_CLONE 
   elif (FLATTEN_DISK_FULL_CLONE == vm_['snapshot']['disk_move_type']): 
      reloc_spec.diskMoveType = FLATTEN_DISK_FULL_CLONE 
   else: 
      moving = False 
   if moving: 
      return build_clonespec(config_spec, object_ref, reloc_spec, template) 
   else: 
      return None"," 'Get clone specification for valid snapshot. 
 If the snapshot is valid, return clone specification, else return None. 
 :param config_spec: Specification of the configuration to be cloned. 
 :param object_ref: Object reference of the snapshot. 
 :param reloc_spec: Specification of the relocation. 
 :param template: Template to clone. 
 :param vm_: VM object. 
 :returns: Clone specification. 
 :raises: InvalidParameterValue if invalid snapshot is specified. 
 :raises: InvalidRequest if invalid clone type is specified. 
 :raises: InvalidInput if invalid template is specified. 
 :raises: InvalidOperation if clone type is invalid.'",'return clonespec only if values are valid'
"def test_email(): 
    os.environ['PYTHONPATH'] = current_directory 
   os.environ['DJANGO_SETTINGS_MODULE'] = 'djangoapp' 
   (status, out) = commands.getstatusoutput('django-admin.py   harvest   email.feature   --verbosity=2') 
   assert_not_equals(status, 0)", 'Test email harvesting','lettuce should be able to receive emails sent from django server'
"@decorators.which('chef-client') 
 def client(whyrun=False, localmode=False, logfile=None, **kwargs): 
    if (logfile is None): 
      logfile = _default_logfile('chef-client') 
   args = ['chef-client', '--no-color', '--once', '--logfile   ""{0}""'.format(logfile), '--format   doc'] 
   if whyrun: 
      args.append('--why-run') 
   if localmode: 
      args.append('--local-mode') 
   return _exec_cmd(*args, **kwargs)"," 'Run the chef-client. 
 This will run the chef-client with the specified options. 
 :param whyrun: If set to ``True``, will print the whyrun message. 
 :param localmode: If set to ``True``, will run the chef-client in local mode. 
 :param logfile: If set to ``None``, will use the default logfile. 
 :param kwargs: Any other keyword arguments will be passed to the chef-client. 
 :return: The exit code of the chef-client.'","'Execute a chef client run and return a dict with the stderr, stdout, 
 return code, and pid. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' chef.client server=https://localhost 
 server 
 The chef server URL 
 client_key 
 Set the client key file location 
 config 
 The configuration file to use 
 config-file-jail 
 Directory under which config files are allowed to be loaded 
 (no client.rb or knife.rb outside this path will be loaded). 
 environment 
 Set the Chef Environment on the node 
 group 
 Group to set privilege to 
 json-attributes 
 Load attributes from a JSON file or URL 
 localmode 
 Point chef-client at local repository if True 
 log_level 
 Set the log level (debug, info, warn, error, fatal) 
 logfile 
 Set the log file location 
 node-name 
 The node name for this client 
 override-runlist 
 Replace current run list with specified items for a single run 
 pid 
 Set the PID file location, defaults to /tmp/chef-client.pid 
 run-lock-timeout 
 Set maximum duration to wait for another client run to finish, 
 default is indefinitely. 
 runlist 
 Permanently replace current run list with specified items 
 user 
 User to set privilege to 
 validation_key 
 Set the validation key file location, used for registering new clients 
 whyrun 
 Enable whyrun mode when set to True'"
"def forwards_move_repo_source(apps, schema_editor): 
    RemoteRepository = apps.get_model(u'oauth', u'RemoteRepository') 
   SocialAccount = apps.get_model(u'socialaccount', u'SocialAccount') 
   for account in SocialAccount.objects.all(): 
      rows = RemoteRepository.objects.filter(users=account.user, source=account.provider).update(account=account)", 'Update the social account source for all remote repositories','Use source field to set repository account'
"def set_wake_on_network(enabled): 
    state = salt.utils.mac_utils.validate_enabled(enabled) 
   cmd = 'systemsetup   -setwakeonnetworkaccess   {0}'.format(state) 
   salt.utils.mac_utils.execute_return_success(cmd) 
   return salt.utils.mac_utils.confirm_updated(state, get_wake_on_network)"," 'Enable or disable wake on network on Mac OS X. 
 This will set the wake on network setting to enabled or disabled. 
 If the setting is already enabled or disabled, it will not change. 
 :param bool enabled: 
 :return: 
 :rtype: 
 :raises: 
 :example: 
 .. code-block:: bash 
 salt '*' macosx.set_wake_on_network True'","'Set whether or not the computer will wake from sleep when network activity 
 is detected. 
 :param bool enabled: True to enable, False to disable. ""On"" and ""Off"" are 
 also acceptable values. Additionally you can pass 1 and 0 to represent True 
 and False respectively 
 :return: True if successful, False if not 
 :rtype: bool 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' power.set_wake_on_network True'"
"def check_files(files, file_check, exclusions=set(), pattern=None): 
    if (not files): 
      return 
   for fname in files: 
      if ((not exists(fname)) or (not isfile(fname))): 
         continue 
      if any(((ex in fname) for ex in exclusions)): 
         continue 
      if ((pattern is None) or re.match(pattern, fname)): 
         file_check(fname)"," 'Check all files in `files` against `file_check` function. 
 If `pattern` is specified, only files matching that pattern will be checked. 
 If `exclusions` is specified, files matching any of those will be skipped. 
 If `file_check` raises an exception, the entire `files` list will be skipped.'","'Checks all files with the file_check function provided, skipping files 
 that contain any of the strings in the set provided by exclusions.'"
"def determine_format(request, serializer, default_format=u'application/json'): 
    format = request.GET.get(u'format') 
   if format: 
      if (format in serializer.formats): 
         return serializer.get_mime_for_format(format) 
   if ((u'callback' in request.GET) and (u'jsonp' in serializer.formats)): 
      return serializer.get_mime_for_format(u'jsonp') 
   accept = request.META.get(u'HTTP_ACCEPT', u'*/*') 
   if (accept != u'*/*'): 
      try: 
         best_format = mimeparse.best_match(serializer.supported_formats_reversed, accept) 
      except ValueError: 
         raise BadRequest(u'Invalid   Accept   header') 
      if best_format: 
         return best_format 
   return default_format"," 'Returns the format to use for the serializer. 
 If a format is requested, this function will return it. 
 If a format is not requested, and the Accept header is not 
 ""*/*"", the first format that is supported by the serializer will be 
 returned. 
 If the Accept header is ""*/*"", the default format will be returned. 
 :param request: The request object 
 :param serializer: The serializer to use 
 :param default_format: The default format to use if no format is requested 
 :returns: The format to use for the serializer 
 :rtype: str'","'Tries to ""smartly"" determine which output format is desired. 
 First attempts to find a ``format`` override from the request and supplies 
 that if found. 
 If no request format was demanded, it falls back to ``mimeparse`` and the 
 ``Accepts`` header, allowing specification that way. 
 If still no format is found, returns the ``default_format`` (which defaults 
 to ``application/json`` if not provided). 
 NOTE: callers *must* be prepared to handle BadRequest exceptions due to 
 malformed HTTP request headers!'"
"def remove_packages(module, port_path, packages): 
    remove_c = 0 
   for package in packages: 
      if (not query_package(module, port_path, package)): 
         continue 
      (rc, out, err) = module.run_command(('%s   uninstall   %s' % (port_path, package))) 
      if query_package(module, port_path, package): 
         module.fail_json(msg=('failed   to   remove   %s:   %s' % (package, out))) 
      remove_c += 1 
   if (remove_c > 0): 
      module.exit_json(changed=True, msg=('removed   %s   package(s)' % remove_c)) 
   module.exit_json(changed=False, msg='package(s)   already   absent')", 'Remove packages from the port.','Uninstalls one or more packages if installed.'
"def _get_options(ret=None): 
    defaults = {'level': 'LOG_INFO', 'facility': 'LOG_USER', 'options': []} 
   attrs = {'level': 'level', 'facility': 'facility', 'tag': 'tag', 'options': 'options'} 
   _options = salt.returners.get_returner_options(__virtualname__, ret, attrs, __salt__=__salt__, __opts__=__opts__, defaults=defaults) 
   return _options", 'Returns the options for the logging module.','Get the returner options from salt.'
"def listdir(path): 
    if (not hasattr(sys, 'frozen')): 
      return os.listdir(path) 
   (zipPath, archivePath) = splitZip(path) 
   if (archivePath is None): 
      return os.listdir(path) 
   with zipfile.ZipFile(zipPath, 'r') as zipobj: 
      contents = zipobj.namelist() 
   results = set() 
   for name in contents: 
      if (name.startswith(archivePath) and (len(name) > len(archivePath))): 
         name = name[len(archivePath):].split('/')[0] 
         results.add(name) 
   return list(results)"," 'Returns the list of all files in the given directory. 
 This function is used to implement the file system interface. 
 This function is not used by the user.'",'Replacement for os.listdir that works in frozen environments.'
"def object_id(value): 
    _object_id = '{}_{}_{}'.format(slugify(_value_name(value)), value.node.node_id, value.index) 
   if (value.instance > 1): 
      return '{}_{}'.format(_object_id, value.instance) 
   return _object_id"," 'Returns the object ID of the value. 
 :param value: the value to get the object ID for 
 :type value: Value'","'Return the object_id of the device value. 
 The object_id contains node_id and value instance id 
 to not collide with other entity_ids.'"
"def mquantiles_cimj(data, prob=[0.25, 0.5, 0.75], alpha=0.05, axis=None): 
    alpha = min(alpha, (1 - alpha)) 
   z = norm.ppf((1 - (alpha / 2.0))) 
   xq = mstats.mquantiles(data, prob, alphap=0, betap=0, axis=axis) 
   smj = mjci(data, prob, axis=axis) 
   return ((xq - (z * smj)), (xq + (z * smj)))"," 'Return the range of values in the data set that contain a given percentage of 
 the values. 
 Parameters 
 data : array_like 
 Input data. 
 prob : array_like 
 Percentage of values to include in the range. 
 alpha : float 
 Level of significance. 
 axis : int, optional 
 Axis along which to calculate the quantiles. 
 Returns 
 xq : 2-element array 
 The range of values in the data set that contain a given percentage of the 
 values. 
 Examples 
 >>> from scipy.stats import norm, mstats 
 >>> from scipy.stats.mstats import mquantiles_cimj 
 >>> data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20] 
 >>> mquantiles_cimj(data, prob=[0.25, 0.","'Computes the alpha confidence interval for the selected quantiles of the 
 data, with Maritz-Jarrett estimators. 
 Parameters 
 data : ndarray 
 Data array. 
 prob : sequence, optional 
 Sequence of quantiles to compute. 
 alpha : float, optional 
 Confidence level of the intervals. 
 axis : int or None, optional 
 Axis along which to compute the quantiles. 
 If None, use a flattened array.'"
"def get_pools(client): 
    return [server.pool for server in client._get_topology().select_servers(any_server_selector)]", 'Get all pools from the cluster.','Get all pools.'
"def get_cpu_list(ip, user, passwd): 
    cmd = 'statcpu   -iter   1   -t' 
   showcpu_list = run_ssh_thread(ip, user, passwd, cmd) 
   cpu_list = [] 
   line_num = 0 
   for line in showcpu_list: 
      line_num += 1 
      if (line_num >= 3): 
         cpu_stats = line.split() 
         if (len(cpu_stats) > 2): 
            cpu_list.append(cpu_stats[0].split(',')[0]) 
   return cpu_list"," 'Get cpu list from the remote host. 
 :param ip: IP of the remote host 
 :param user: User name of the remote host 
 :param passwd: Password of the remote host 
 :return: List of cpu name.'",'Get a list of cpus to build metric definitions with'
"def test_append(hist, config_stub): 
    config_stub.data = CONFIG_NOT_PRIVATE 
   hist.append('new   item') 
   assert ('new   item' in hist.history) 
   hist.history.remove('new   item') 
   assert (hist.history == HISTORY)", 'Test that we can append items to the history.','Test append outside private mode.'
"def new(rsa_key): 
    return PKCS115_SigScheme(rsa_key)"," 'Return a new PKCS#11-5 signature scheme. 
 :param rsa_key: A RSA key object. 
 :type rsa_key: Crypto.PublicKey.RSA.RSAObject'","'Return a signature scheme object `PKCS115_SigScheme` that 
 can create or verify PKCS#1 v1.5 signatures. 
 :Parameters: 
 rsa_key : RSA key object 
 The RSA key to use to sign or verify the message. 
 This is a `Cryptodome.PublicKey.RSA` object. 
 Signing is only possible if ``rsa_key`` is a private RSA key.'"
"def get_connection(backend=None, fail_silently=False, **kwds): 
    path = (backend or settings.EMAIL_BACKEND) 
   try: 
      (mod_name, klass_name) = path.rsplit('.', 1) 
      mod = import_module(mod_name) 
   except ImportError as e: 
      raise ImproperlyConfigured(('Error   importing   email   backend   module   %s:   ""%s""' % (mod_name, e))) 
   try: 
      klass = getattr(mod, klass_name) 
   except AttributeError: 
      raise ImproperlyConfigured(('Module   ""%s""   does   not   define   a   ""%s""   class' % (mod_name, klass_name))) 
   return klass(fail_silently=fail_silently, **kwds)"," 'Return an instance of the given email backend. 
 The backend is specified by the EMAIL_BACKEND setting. 
 If the backend is not defined, the default is \'django.core.mail.backends.smtp.EmailBackend\'. 
 The backend can be a string or a callable. 
 If a string is given, it is assumed to be the name of a module, 
 which should contain a class with the same name as the backend. 
 If a callable is given, it should return an instance of the 
 appropriate class. 
 :param backend: The backend to use. 
 :type backend: string or callable 
 :param fail_silently: If set to True, raise exceptions if the backend 
 could not be instantiated. 
 :type fail_silently: bool 
 :param kwds: Additional keyword arguments to pass to the backend. 
 :type kwds: dict 
 :rtype: :class:`~django.core.mail.backends.EmailBackend` 
 :returns: An instance of the backend class. 
 :raises ImproperlyConfigured:","'Load an email backend and return an instance of it. 
 If backend is None (default) settings.EMAIL_BACKEND is used. 
 Both fail_silently and other keyword arguments are used in the 
 constructor of the backend.'"
"def detect(): 
    devlist = list_devices() 
   print(('Found   %d   CUDA   devices' % len(devlist))) 
   supported_count = 0 
   for dev in devlist: 
      attrs = [] 
      cc = dev.compute_capability 
      attrs += [('compute   capability', ('%d.%d' % cc))] 
      attrs += [('pci   device   id', dev.PCI_DEVICE_ID)] 
      attrs += [('pci   bus   id', dev.PCI_BUS_ID)] 
      if (cc < (2, 0)): 
         support = '[NOT   SUPPORTED:   CC   <   2.0]' 
      else: 
         support = '[SUPPORTED]' 
         supported_count += 1 
      print(('id   %d            %20s   %40s' % (dev.id, dev.name, support))) 
      for (key, val) in attrs: 
         print(('%40s:   %s' % (key, val))) 
   print('Summary:') 
   print((' DCTB %d/%d   devices   are   supported' % (supported_count, len(devlist)))) 
   return (supported_count > 0)"," 'Detects if CUDA is supported on this machine. 
 If the CUDA driver is not installed, this will return False. 
 If the CUDA driver is installed, this will return True.'","'Detect supported CUDA hardware and print a summary of the detected hardware. 
 Returns a boolean indicating whether any supported devices were detected.'"
"def _collect_filetree_revs(obj_store, tree_sha, kset): 
    filetree = obj_store[tree_sha] 
   for (name, mode, sha) in filetree.iteritems(): 
      if ((not S_ISGITLINK(mode)) and (sha not in kset)): 
         kset.add(sha) 
         if stat.S_ISDIR(mode): 
            _collect_filetree_revs(obj_store, sha, kset)"," 'Collects the sha1 of all files in a filetree. 
 :param obj_store: An ObjectStore 
 :param tree_sha: The sha1 of the filetree 
 :param kset: The set of sha1\'s we\'ve already seen. 
 :type kset: set[str]'","'Collect SHA1s of files and directories for specified tree. 
 :param obj_store: Object store to get objects by SHA from 
 :param tree_sha: tree reference to walk 
 :param kset: set to fill with references to files and directories'"
"def destroy_vm(session, instance, vm_ref=None): 
    try: 
      if (not vm_ref): 
         vm_ref = get_vm_ref(session, instance) 
      LOG.debug('Destroying   the   VM', instance=instance) 
      destroy_task = session._call_method(session.vim, 'Destroy_Task', vm_ref) 
      session._wait_for_task(destroy_task) 
      LOG.info(_LI('Destroyed   the   VM'), instance=instance) 
   except Exception: 
      LOG.exception(_LE('Destroy   VM   failed'), instance=instance)", 'Destroy the VM.','Destroy a VM instance. Assumes VM is powered off.'
"def dirichlet_likelihood(weights, alpha=None): 
    if (type(weights) is Variable): 
      n_topics = weights.data.shape[1] 
   else: 
      n_topics = weights.W.data.shape[1] 
   if (alpha is None): 
      alpha = (1.0 / n_topics) 
   if (type(weights) is Variable): 
      log_proportions = F.log_softmax(weights) 
   else: 
      log_proportions = F.log_softmax(weights.W) 
   loss = ((alpha - 1.0) * log_proportions) 
   return (- F.sum(loss))"," 'Computes the Dirichlet log-likelihood. 
 Parameters 
 weights : Variable 
 The topic weights. 
 alpha : float, optional 
 The alpha parameter. 
 Returns 
 loss : Tensor 
 The loss. 
 Examples 
 >>> weights = tf.Variable(tf.constant([[0.2, 0.3, 0.5], 
 ... [0.1, 0.4, 0.5]])) 
 >>> alpha = 0.1 
 >>> loss = tf.nn.dirichlet_loss(weights, alpha) 
 >>> print(loss.eval()) 
 -0.001'","'Calculate the log likelihood of the observed topic proportions. 
 A negative likelihood is more likely than a negative likelihood. 
 Args: 
 weights (chainer.Variable): Unnormalized weight vector. The vector 
 will be passed through a softmax function that will map the input 
 onto a probability simplex. 
 alpha (float): The Dirichlet concentration parameter. Alpha 
 greater than 1.0 results in very dense topic weights such 
 that each document belongs to many topics. Alpha < 1.0 results 
 in sparser topic weights. The default is to set alpha to 
 1.0 / n_topics, effectively enforcing the prior belief that a 
 document belong to very topics at once. 
 Returns: 
 ~chainer.Variable: Output loss variable.'"
"def _urlopen_cached(url, cache): 
    from_cache = False 
   if (cache is not None): 
      cache_path = join(cache, (url.split('://')[(-1)].replace('/', ',') + '.zip')) 
      try: 
         data = _open_cache(cache_path) 
         from_cache = True 
      except: 
         pass 
   if (not from_cache): 
      data = urlopen(url).read() 
      if (cache is not None): 
         _cache_it(data, cache_path) 
   return (data, from_cache)"," 'Return the contents of a URL, and whether or not it was retrieved from 
 cache.'","'Tries to load data from cache location otherwise downloads it. If it 
 downloads the data and cache is not None then it will put the downloaded 
 data in the cache path.'"
"def create_network_acl_entry(network_acl_id=None, rule_number=None, protocol=None, rule_action=None, cidr_block=None, egress=None, network_acl_name=None, icmp_code=None, icmp_type=None, port_range_from=None, port_range_to=None, region=None, key=None, keyid=None, profile=None): 
    kwargs = locals() 
   return _create_network_acl_entry(**kwargs)"," 'Creates a new Network ACL entry. 
 If the Network ACL does not exist, it will be created. 
 If the Network ACL already exists, the entry will be added. 
 :param network_acl_id: The ID of the Network ACL. 
 :type network_acl_id: string 
 :param rule_number: The number of the rule. 
 :type rule_number: int 
 :param protocol: The protocol of the rule. 
 :type protocol: string 
 :param rule_action: The action of the rule. 
 :type rule_action: string 
 :param cidr_block: The CIDR block of the rule. 
 :type cidr_block: string 
 :param egress: Indicates if the rule is an egress rule. 
 :type egress: bool 
 :param network_acl_name: The name of the Network ACL. 
 :type network_acl_name: string 
 :param icmp_code: The ICMP code of the rule. 
 :type icmp_code: int 
 :","'Creates a network acl entry. 
 CLI Example: 
 .. code-block:: bash 
 salt myminion boto_vpc.create_network_acl_entry \'acl-5fb85d36\' \'32767\' \ 
 \'all\' \'deny\' \'0.0.0.0/0\' egress=true'"
"def interface_field(interfaces, **field_kwargs): 
    if (not isinstance(interfaces, tuple)): 
      raise TypeError('The   ``interfaces``   argument   must   be   a   tuple.   Got:   {!r}'.format(interfaces)) 
   original_invariant = field_kwargs.pop('invariant', None) 
   def invariant(value): 
      error_messages = [] 
      if (original_invariant is not None): 
         (original_invariant_result, _original_invariant_message) = original_invariant(value) 
         if original_invariant_result: 
            error_messages.append(original_invariant_result) 
      missing_interfaces = [] 
      for interface in interfaces: 
         if (not interface.providedBy(value)): 
            missing_interfaces.append(interface.getName()) 
      if missing_interfaces: 
         error_messages.append('The   value   {!r}   did   not   provide   these   required   interfaces:   {}'.format(value, ',   '.join(missing_interfaces))) 
      if error_messages: 
         return (False, '\n'.join(error_messages)) 
      else: 
         return (True, '') 
   field_kwargs['invariant'] = invariant 
   return field(**field_kwargs)"," 'Decorator to create a field that checks that the value of the field 
 is an interface that provides all the interfaces in the `interfaces` 
 argument. 
 :param tuple interfaces: a tuple of interfaces that the value of the field 
 must provide. 
 :param dict field_kwargs: keyword arguments to be passed to the field 
 constructor. 
 :returns: A field instance. 
 :rtype: Field 
 :raises: `ValidationError` if the value of the field is not an interface 
 that provides all the interfaces in `interfaces`. 
 .. versionchanged:: 0.12 
 Added support for keyword arguments.'","'A ``PClass`` field which checks that the assigned value provides all the 
 ``interfaces``. 
 :param tuple interfaces: The ``Interface`` that a value must provide.'"
"def resource_delete(context, data_dict): 
    model = context['model'] 
   id = _get_or_bust(data_dict, 'id') 
   entity = model.Resource.get(id) 
   if (entity is None): 
      raise NotFound 
   _check_access('resource_delete', context, data_dict) 
   package_id = entity.get_package_id() 
   pkg_dict = _get_action('package_show')(context, {'id': package_id}) 
   for plugin in plugins.PluginImplementations(plugins.IResourceController): 
      plugin.before_delete(context, data_dict, pkg_dict.get('resources', [])) 
   if pkg_dict.get('resources'): 
      pkg_dict['resources'] = [r for r in pkg_dict['resources'] if (not (r['id'] == id))] 
   try: 
      pkg_dict = _get_action('package_update')(context, pkg_dict) 
   except ValidationError as e: 
      errors = e.error_dict['resources'][(-1)] 
      raise ValidationError(errors) 
   for plugin in plugins.PluginImplementations(plugins.IResourceController): 
      plugin.after_delete(context, pkg_dict.get('resources', [])) 
   model.repo.commit()", 'Delete a resource.',"'Delete a resource from a dataset. 
 You must be a sysadmin or the owner of the resource to delete it. 
 :param id: the id of the resource 
 :type id: string'"
"def multisplit(container, name, xpath, before=True): 
    root = container.parsed(name) 
   nodes = root.xpath(xpath, namespaces=XPNSMAP) 
   if (not nodes): 
      raise AbortError((_(u'The   expression   %s   did   not   match   any   nodes') % xpath)) 
   for split_point in nodes: 
      if in_table(split_point): 
         raise AbortError(u'Cannot   split   inside   tables') 
      if split_point.tag.endswith(u'}body'): 
         raise AbortError(u'Cannot   split   on   the   <body>   tag') 
   for (i, tag) in enumerate(nodes): 
      tag.set(u'calibre-split-point', str(i)) 
   current = name 
   all_names = [name] 
   for i in xrange(len(nodes)): 
      current = split(container, current, (u'//*[@calibre-split-point=""%d""]' % i), before=before) 
      all_names.append(current) 
   for x in all_names: 
      for tag in container.parsed(x).xpath(u'//*[@calibre-split-point]'): 
         tag.attrib.pop(u'calibre-split-point') 
      container.dirty(x) 
   return all_names[1:]"," 'Split the container into multiple containers, each containing a 
 single node. 
 :param container: The container to split. 
 :param name: The name of the container to split. 
 :param xpath: The xpath expression to use to split. 
 :param before: If True, split before the nodes, otherwise split 
 after the nodes. 
 :return: A list of the names of the containers that were split.'","'Split the specified file at multiple locations (all tags that match the specified XPath expression. See also: :func:`split`. 
 Splitting automatically migrates all links and references to the affected 
 files. 
 :param before: If True the splits occur before the identified element otherwise after it.'"
"def create_pool(lb, name, method='ROUND_ROBIN'): 
    if __opts__['load_balancers'].get(lb, None): 
      (username, password) = list(__opts__['load_balancers'][lb].values()) 
   else: 
      raise Exception('Unable   to   find   `{0}`   load   balancer'.format(lb)) 
   F5 = F5Mgmt(lb, username, password) 
   F5.create_pool(name, method) 
   return True"," 'Create a new pool on a load balancer 
 :param lb: The load balancer to create the pool on 
 :param name: The name of the pool 
 :param method: The method to use for load balancing 
 :returns: True if the pool was created, False otherwise'","'Create a pool on the F5 load balancer 
 CLI Examples: 
 .. code-block:: bash 
 salt-run f5.create_pool load_balancer pool_name loadbalance_method 
 salt-run f5.create_pool load_balancer my_pool ROUND_ROBIN'"
"def threads(request, document_slug): 
    doc = get_document(document_slug, request) 
   try: 
      sort = int(request.GET.get('sort', 0)) 
   except ValueError: 
      sort = 0 
   try: 
      desc = int(request.GET.get('desc', 0)) 
   except ValueError: 
      desc = 0 
   desc_toggle = (0 if desc else 1) 
   threads_ = sort_threads(doc.thread_set, sort, desc) 
   threads_ = paginate(request, threads_, per_page=kbforums.THREADS_PER_PAGE) 
   feed_urls = ((reverse('wiki.discuss.threads.feed', args=[document_slug]), ThreadsFeed().title(doc)),) 
   is_watching_forum = (request.user.is_authenticated() and NewThreadEvent.is_notifying(request.user, doc)) 
   return render(request, 'kbforums/threads.html', {'document': doc, 'threads': threads_, 'is_watching_forum': is_watching_forum, 'sort': sort, 'desc_toggle': desc_toggle, 'feeds': feed_urls})"," 'Displays the list of threads for the given document. 
 The view uses the :doc:`kbforums/threads.html` template.'",'View all the threads in a discussion forum.'
"def _read_uint64(f): 
    return np.uint64(struct.unpack('>Q', f.read(8))[0])", 'Read a 64 bit unsigned integer from the file.','Read an unsigned 64-bit integer'
"def get(name, default=_UNSET, scope='global', window=None, tab=None): 
    reg = _get_registry(scope, window, tab) 
   try: 
      return reg[name] 
   except KeyError: 
      if (default is not _UNSET): 
         return default 
      else: 
         raise"," 'Get a value from the registry. 
 If the registry does not contain the value, the default value is returned. 
 If the default value is not set, a KeyError is raised. 
 :param name: the name of the value to retrieve 
 :param default: the default value to return if the registry does not contain 
 the requested value 
 :param scope: the scope of the registry to use, can be ""global"", ""window"", or 
 ""tab"" 
 :param window: the window to use, if scope is ""window"" 
 :param tab: the tab to use, if scope is ""tab"" 
 :return: the value or the default value if the value is not in the registry'","'Helper function to get an object. 
 Args: 
 default: A default to return if the object does not exist.'"
"@command(usage='parse   links') 
 def extend_links(args): 
    args = parse_command_line(args, [], ['name']) 
   import lixian_tasks_extended 
   for x in (lixian_tasks_extended.extend_links if (not args.name) else lixian_tasks_extended.extend_links_name)(args): 
      print x", 'Extend links',"'usage: lx extend-links http://kuai.xunlei.com/d/... http://www.verycd.com/topics/... 
 parse and print links from pages 
 lx extend-links urls... 
 lx extend-links --name urls...'"
"def send(text, connections, **kwargs): 
    if (not isinstance(connections, collections.Iterable)): 
      connections = [connections] 
   router = get_router() 
   message = router.new_outgoing_message(text=text, connections=connections, **kwargs) 
   router.send_outgoing(message) 
   return message"," 'Send a message to the given connections. 
 :param text: The message to send. 
 :param connections: A list of connections to send the message to. 
 :param kwargs: Additional kwargs to pass to the router.'","'Creates an outgoing message and passes it to the router to be processed 
 and sent via the respective backend. 
 Arbitrary arguments are passed along to 
 :py:meth:`~rapidsms.router.blocking.BlockingRouter.new_outgoing_message`. 
 :param text: text message 
 :param connections: list or QuerySet of RapidSMS 
 :py:class:`~rapidsms.models.Connection` objects 
 :param kwargs: Extra kwargs to pass to 
 :py:class:`~rapidsms.messages.outgoing.OutgoingMessage` constructor 
 :returns: message constructed by router. A returned 
 message object does not indicate that router processing has 
 finished or even started, as this depends on the router defined 
 in :setting:`RAPIDSMS_ROUTER`. 
 :rtype: :py:class:`~rapidsms.messages.outgoing.OutgoingMessage`'"
"def load_template(template_name, template_source=None, template_path=None, template_hash=None, template_hash_name=None, template_user='root', template_group='root', template_mode='755', saltenv=None, template_engine='jinja', skip_verify=True, defaults=None, test=False, commit=True, debug=False, replace=False, **template_vars): 
    _rendered = '' 
   _loaded = {'result': True, 'comment': '', 'out': None} 
   loaded_config = None 
   if (template_engine not in salt.utils.templates.TEMPLATE_REGISTRY): 
      _loaded.update({'result': False, 'comment': 'Invalid   templating   engine!   Choose   between:   {tpl_eng_opts}'.format(tpl_eng_opts=',   '.join(list(salt.utils.templates.TEMPLATE_REGISTRY.keys())))}) 
      return _loaded 
   salt_render_prefixes = ('salt://', 'http://', 'https://', 'ftp://') 
   salt_render = False 
   for salt_render_prefix in salt_render_prefixes: 
      if (not salt_render): 
         salt_render = (salt_render or template_name.startswith(salt_render_prefix) or (template_path and template_path.startswith(salt_render_prefix))) 
   file_exists = __salt__['file.file_exists'](template_name) 
   if (template_source or template_path or file_exists or salt_render): 
      if template_source: 
         if (not saltenv): 
            saltenv = (template_path if template_path else 'base') 
         _rendered = __salt__['file.apply_template_on_contents'](contents=template_source, template=template_engine, context=template_vars, defaults=defaults, saltenv=saltenv) 
         if (not isinstance(_rendered, six.string_types)): 
            if ('result' in _rendered): 
               _loaded['result'] = _rendered['result'] 
            else: 
               _loaded['result'] = False 
            if ('comment' in _rendered): 
               _loaded['comment'] = _rendered['comment'] 
            else: 
               _loaded['comment'] = 'Error   while   rendering   the   template.' 
            return _loaded 
      else: 
         if (template_path and (not file_exists)): 
            template_name = __salt__['file.join'](template_path, template_name) 
            if (not saltenv): 
               saltenv = (template_path if (not salt_render) else 'base') 
         elif (salt_render and (not saltenv)): 
            saltenv = (template_path if template_path else 'base') 
         if (not saltenv): 
            saltenv = 'base' 
         _rand_filename = __salt__['random.hash'](template_name, 'md5') 
         _temp_file = __salt__['file.join']('/tmp', _rand_filename) 
         _managed = __salt__['file.get_managed'](name=_temp_file, source=template_name, source_hash=template_hash, source_hash_name=template_hash_name, user=template_user, group=template_group, mode=template_mode, template=template_engine, context=template_vars, defaults=defaults, saltenv=saltenv, skip_verify=skip_verify) 
         if ((not isinstance(_managed, (list, tuple))) and isinstance(_managed, six.string_types)): 
            _loaded['comment'] = _managed 
            _loaded['result'] = False 
         elif (isinstance(_managed, (list, tuple)) and (not (len(_managed) > 0))): 
            _loaded['result'] = False 
            _loaded['comment'] = 'Error   while   rendering   the   template.' 
         elif (isinstance(_managed, (list, tuple)) and (not (len(_managed[0]) > 0))): 
            _loaded['result'] = False 
            _loaded['comment'] = _managed[(-1)] 
         if _loaded['result']: 
            _temp_tpl_file = _managed[0] 
            _temp_tpl_file_exists = __salt__['file.file_exists'](_temp_tpl_file) 
            if (not _temp_tpl_file_exists): 
               _loaded['result'] = False 
               _loaded['comment'] = 'Error   while   rendering   the   template.' 
               return _loaded 
            _rendered = open(_temp_tpl_file).read() 
         else: 
            return _loaded 
      if debug: 
         loaded_config = _rendered 
      if _loaded['result']: 
         fun = 'load_merge_candidate' 
         if replace: 
            fun = 'load_replace_candidate' 
         _loaded = __proxy__['napalm.call'](fun, **{'config': _rendered}) 
   else: 
      load_templates_params = (defaults if defaults else {}) 
      load_templates_params.update(template_vars) 
      load_templates_params.update({'template_name': template_name, 'template_source': template_source, 'template_path': template_path, 'pillar': __pillar__, 'grains': __grains__, 'opts': __opts__}) 
      _loaded = __proxy__['napalm.call']('load_template', **load_templates_params) 
   return _config_logic(_loaded, test=test, commit_config=commit, loaded_config=loaded_config)"," 'Load a template from a source (file, string, etc) 
 :param template_name: The name of the template to load 
 :param template_source: The source to load the template from 
 :param template_path: The path to the template 
 :param template_hash: The hash of the template 
 :param template_hash_name: The name of the template hash 
 :param template_user: The user to set the template\'s owner to 
 :param template_group: The group to set the template\'s group to 
 :param template_mode: The mode to set the template\'s permissions to 
 :param saltenv: The salt environment to set the template\'s saltenv to 
 :param template_engine: The templating engine to use for rendering the template 
 :param skip_verify: Skip verification of the template\'s hash 
 :param defaults: The defaults to use for the template 
 :param test: Set to ``True`` to test the template 
 :param commit: Set to ``True`` to commit the template to the pillar 
 :param debug: Set to ``True``","'Renders a configuration template (default: Jinja) and loads the result on the device. 
 By default this function will commit the changes. If there are no changes, 
 it does not commit, discards he config and the flag ``already_configured`` 
 will be set as ``True`` to point this out. 
 To avoid committing the configuration, set the argument ``test`` to ``True`` 
 and will discard (dry run). 
 To preserve the chnages, set ``commit`` to ``False``. 
 However, this is recommended to be used only in exceptional cases 
 when there are applied few consecutive states 
 and/or configuration changes. 
 Otherwise the user might forget that the config DB is locked 
 and the candidate config buffer is not cleared/merged in the running config. 
 To replace the config, set ``replace`` to ``True``. 
 template_name 
 Identifies path to the template source. 
 The template can be either stored on the local machine, either remotely. 
 The recommended location is under the ``file_roots`` 
 as specified in the master config file. 
 For example, let\'s suppose the ``file_roots`` is configured as: 
 .. code-block:: yaml 
 file_roots: 
 base: 
 - /etc/salt/states 
 Placing the template under ``/etc/salt/states/templates/example.jinja``, 
 it can be used as ``salt://templates/example.jinja``. 
 Alternatively, for local files, the user can specify the abolute path. 
 If remotely, the source can be retrieved via ``http``, ``https`` or ``ftp``. 
 Examples: 
 - ``salt://my_template.jinja`` 
 - ``/absolute/path/to/my_template.jinja`` 
 - ``http://example.com/template.cheetah`` 
 - ``https:/example.com/template.mako`` 
 - ``ftp://example.com/template.py`` 
 template_source: None 
 Inline config template to be rendered and loaded on the device. 
 template_path: None 
 Required only in case the argument ``template_name`` provides only the file basename 
 when referencing a local template using the absolute path. 
 E.g.: if ``template_name`` is specified as ``my_template.jinja``, 
 in order to find the template, this argument must be provided: 
 ``template_path: /absolute/path/to/``. 
 template_hash: None 
 Hash of the template file. Format: ``{hash_type: \'md5\', \'hsum\': <md5sum>}`` 
 .. versionadded:: 2016.11.2 
 template_hash_name: None 
 When ``template_hash`` refers to a remote file, 
 this specifies the filename to look for in that file. 
 .. versionadded:: 2016.11.2 
 template_group: root 
 Owner of file. 
 .. versionadded:: 2016.11.2 
 template_user: root 
 Group owner of file. 
 .. versionadded:: 2016.11.2 
 template_user: 755 
 Permissions of file. 
 .. versionadded:: 2016.11.2 
 saltenv: base 
 Specifies the template environment. 
 This will influence the relative imports inside the templates. 
 .. versionadded:: 2016.11.2 
 template_engine: jinja 
 The following templates engines are supported: 
 - :mod:`cheetah<salt.renderers.cheetah>` 
 - :mod:`genshi<salt.renderers.genshi>` 
 - :mod:`jinja<salt.renderers.jinja>` 
 - :mod:`mako<salt.renderers.mako>` 
 - :mod:`py<salt.renderers.py>` 
 - :mod:`wempy<salt.renderers.wempy>` 
 .. versionadded:: 2016.11.2 
 skip_verify: True 
 If ``True``, hash verification of remote file sources 
 (``http://``, ``https://``, ``ftp://``) will be skipped, 
 and the ``source_hash`` argument will be ignored. 
 .. versionadded:: 2016.11.2 
 test: False 
 Dry run? If set to ``True``, will apply the config, 
 discard and return the changes. 
 Default: ``False`` and will commit the changes on the device. 
 commit: True 
 Commit? (default: ``True``) 
 debug: False 
 Debug mode. Will insert a new key under the output dictionary, 
 as ``loaded_config`` contaning the raw result after the template was rendered. 
 .. versionadded:: 2016.11.2 
 replace: False 
 Load and replace the configuration. 
 .. versionadded:: 2016.11.2 
 defaults: None 
 Default variables/context passed to the template. 
 .. versionadded:: 2016.11.2 
 **template_vars 
 Dictionary with the arguments/context to be used when the template is rendered. 
 .. note:: 
 Do not explicitely specify this argument. 
 This represents any other variable that will be sent 
 to the template rendering system. 
 Please see the examples below! 
 :return: a dictionary having the following keys: 
 * result (bool): if the config was applied successfully. It is ``False`` only in case of failure. In case     there are no changes to be applied and successfully performs all operations it is still ``True`` and so will be     the ``already_configured`` flag (example below) 
 * comment (str): a message for the user 
 * already_configured (bool): flag to check if there were no changes applied 
 * loaded_config (str): the configuration loaded on the device, after rendering the template. Requires ``debug``     to be set as ``True`` 
 * diff (str): returns the config changes applied 
 The template can use variables from the ``grains``, ``pillar`` or ``opts``, for example: 
 .. code-block:: jinja 
 {% set router_model = grains.get(\'model\') -%} 
 {% set router_vendor = grains.get(\'vendor\') -%} 
 {% set os_version = grains.get(\'version\') -%} 
 {% set hostname = pillar.get(\'proxy\', {}).get(\'host\') -%} 
 {% if router_vendor|lower == \'juniper\' %} 
 system { 
 host-name {{hostname}}; 
 {% elif router_vendor|lower == \'cisco\' %} 
 hostname {{hostname}} 
 {% endif %} 
 CLI Examples: 
 .. code-block:: bash 
 salt \'*\' net.load_template set_ntp_peers peers=[192.168.0.1]  # uses NAPALM default templates 
 # inline template: 
 salt -G \'os:junos\' net.load_template set_hostname template_source=\'system { host-name {{host_name}}; }\'         host_name=\'MX480.lab\' 
 # inline template using grains info: 
 salt -G \'os:junos\' net.load_template set_hostname         template_source=\'system { host-name {{grains.model}}.lab; }\' 
 # if the device is a MX480, the command above will set the hostname as: MX480.lab 
 # inline template using pillar data: 
 salt -G \'os:junos\' net.load_template set_hostname template_source=\'system { host-name {{pillar.proxy.host}}; }\' 
 salt \'*\' net.load_template my_template template_path=\'/tmp/tpl/\' my_param=\'aaa\'  # will commit 
 salt \'*\' net.load_template my_template template_path=\'/tmp/tpl/\' my_param=\'aaa\' test=True  # dry run 
 salt \'*\' net.load_template salt://templates/my_stuff.jinja debug=True  # equivalent of the next command 
 salt \'*\' net.load_template my_stuff.jinja template_path=salt://templates/ debug=True 
 # in case the template needs to include files that are not under the same path (e.g. http://), 
 # to help the templating engine find it, you will need to specify the `saltenv` argument: 
 salt \'*\' net.load_template my_stuff.jinja template_path=salt://templates saltenv=/path/to/includes debug=True 
 # render a mako template: 
 salt \'*\' net.load_template salt://templates/my_stuff.mako template_engine=mako debug=True 
 # render remote template 
 salt -G \'os:junos\' net.load_template http://bit.ly/2fReJg7 test=True debug=True peers=[\'192.168.0.1\'] 
 salt -G \'os:ios\' net.load_template http://bit.ly/2gKOj20 test=True debug=True peers=[\'192.168.0.1\'] 
 Example output: 
 .. code-block:: python 
 \'comment\': \'\', 
 \'already_configured\': False, 
 \'result\': True, 
 \'diff\': \'[edit system]+  host-name edge01.bjm01\', 
 \'loaded_config\': \'system { host-name edge01.bjm01; }\'\''"
"def _array_to_datum(image, label, encoding): 
    if (not encoding): 
      if (image.ndim == 3): 
         image = image.transpose((2, 0, 1)) 
         if (image.shape[0] == 3): 
            image = image[[2, 1, 0], ...] 
      elif (image.ndim == 2): 
         image = image[np.newaxis, :, :] 
      else: 
         raise Exception(('Image   has   unrecognized   shape:   ""%s""' % image.shape)) 
      datum = caffe.io.array_to_datum(image, label) 
   else: 
      datum = caffe_pb2.Datum() 
      if (image.ndim == 3): 
         datum.channels = image.shape[2] 
      else: 
         datum.channels = 1 
      datum.height = image.shape[0] 
      datum.width = image.shape[1] 
      datum.label = label 
      s = StringIO() 
      if (encoding == 'png'): 
         PIL.Image.fromarray(image).save(s, format='PNG') 
      elif (encoding == 'jpg'): 
         PIL.Image.fromarray(image).save(s, format='JPEG', quality=90) 
      else: 
         raise ValueError('Invalid   encoding   type') 
      datum.data = s.getvalue() 
      datum.encoded = True 
   return datum"," 'Convert an image to a caffe datum. 
 Parameters 
 image : numpy array 
 The image to convert. 
 label : int 
 The label of the image. 
 encoding : str 
 The encoding type of the image. 
 Returns 
 datum : caffe.io.Datum 
 The converted datum.'",'Create a caffe Datum from a numpy.ndarray'
"def self_test(): 
    with tf.Session() as sess: 
      print('Self-test   for   neural   translation   model.') 
      model = seq2seq_model.Seq2SeqModel(10, 10, [(3, 3), (6, 6)], 32, 2, 5.0, 32, 0.3, 0.99, num_samples=8) 
      sess.run(tf.global_variables_initializer()) 
      data_set = ([([1, 1], [2, 2]), ([3, 3], [4]), ([5], [6])], [([1, 1, 1, 1, 1], [2, 2, 2, 2, 2]), ([3, 3, 3], [5, 6])]) 
      for _ in xrange(5): 
         bucket_id = random.choice([0, 1]) 
         (encoder_inputs, decoder_inputs, target_weights) = model.get_batch(data_set, bucket_id) 
         model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, False)", 'Run self-test for neural translation model.','Test the translation model.'
"def _calibrate_comp(comp, chs, row_names, col_names, mult_keys=('range', 'cal'), flip=False): 
    ch_names = [c['ch_name'] for c in chs] 
   row_cals = np.zeros(len(row_names)) 
   col_cals = np.zeros(len(col_names)) 
   for (names, cals, inv) in zip((row_names, col_names), (row_cals, col_cals), (False, True)): 
      for ii in range(len(cals)): 
         p = ch_names.count(names[ii]) 
         if (p != 1): 
            raise RuntimeError(('Channel   %s   does   not   appear   exactly   once   in   data' % names[ii])) 
         idx = ch_names.index(names[ii]) 
         val = (chs[idx][mult_keys[0]] * chs[idx][mult_keys[1]]) 
         val = (float((1.0 / val)) if inv else float(val)) 
         val = ((1.0 / val) if flip else val) 
         cals[ii] = val 
   comp['rowcals'] = row_cals 
   comp['colcals'] = col_cals 
   comp['data']['data'] = ((row_cals[:, None] * comp['data']['data']) * col_cals[None, :])"," 'Calibrate the data from the given channels, using the given row and column names. 
 Parameters 
 comp : Comp object 
 The Comp object to calibrate. 
 chs : list 
 The channels to calibrate. 
 row_names : list 
 The row names to use for the calibration. 
 col_names : list 
 The column names to use for the calibration. 
 mult_keys : list 
 The keys of the calibration matrix to use. 
 flip : bool 
 Whether to flip the calibration matrix. 
 Returns 
 comp : Comp object 
 The Comp object with the calibrated data. 
 Examples 
 >>> comp = Comp() 
 >>> comp.add_channels(chs) 
 >>> comp.add_row_names(row_names) 
 >>> comp.add_column_names(col_names) 
 >>> comp.calibrate_comp(chs, row_names, col_names, mult_keys=(\'range\', \'cal\')) 
 >>> comp.data 
 >>> comp.data.data'",'Get row and column cals.'
"@treeio_login_required 
 @handle_response_format 
 def opportunity_view(request, opportunity_id, response_format='html'): 
    profile = request.user.profile 
   opportunity = get_object_or_404(Opportunity, pk=opportunity_id) 
   if ((not profile.has_permission(opportunity)) and (not profile.is_admin('treeio.sales'))): 
      return user_denied(request, message=""You   don't   have   access   to   this   Opportunity"") 
   form = _do_update_record(profile, request, opportunity) 
   return render_to_response('sales/opportunity_view', {'opportunity': opportunity, 'record_form': form}, context_instance=RequestContext(request), response_format=response_format)"," 'View an Opportunity. 
 This view is intended for use by sales people to view an Opportunity.'",'Opportunity view'
"def start(name, call=None): 
    if (call != 'action'): 
      raise SaltCloudSystemExit('The   start   action   must   be   called   with   -a   or   --action.') 
   data = show_instance(name, call='action') 
   if (data.get('status') == 'active'): 
      return {'success': True, 'action': 'start', 'status': 'active', 'msg': 'Machine   is   already   running.'} 
   ret = query(droplet_id=data['id'], command='actions', args={'type': 'power_on'}, http_method='post') 
   return {'success': True, 'action': ret['action']['type'], 'state': ret['action']['status']}", 'Start a machine',"'Start a droplet in DigitalOcean. 
 .. versionadded:: 2015.8.8 
 name 
 The name of the droplet to start. 
 CLI Example: 
 .. code-block:: bash 
 salt-cloud -a start droplet_name'"
"def inpaint_biharmonic(img, mask, multichannel=False): 
    if (img.ndim < 1): 
      raise ValueError('Input   array   has   to   be   at   least   1D') 
   img_baseshape = (img.shape[:(-1)] if multichannel else img.shape) 
   if (img_baseshape != mask.shape): 
      raise ValueError('Input   arrays   have   to   be   the   same   shape') 
   if np.ma.isMaskedArray(img): 
      raise TypeError('Masked   arrays   are   not   supported') 
   img = skimage.img_as_float(img) 
   mask = mask.astype(np.bool) 
   kernel = ndi.morphology.generate_binary_structure(mask.ndim, 1) 
   mask_dilated = ndi.morphology.binary_dilation(mask, structure=kernel) 
   (mask_labeled, num_labels) = label(mask_dilated, return_num=True) 
   mask_labeled *= mask 
   if (not multichannel): 
      img = img[..., np.newaxis] 
   out = np.copy(img) 
   for idx_channel in range(img.shape[(-1)]): 
      known_points = img[..., idx_channel][(~ mask)] 
      limits = (np.min(known_points), np.max(known_points)) 
      for idx_region in range(1, (num_labels + 1)): 
         mask_region = (mask_labeled == idx_region) 
         _inpaint_biharmonic_single_channel(img[..., idx_channel], mask_region, out[..., idx_channel], limits) 
   if (not multichannel): 
      out = out[..., 0] 
   return out"," 'Inpaint using biharmonic filtering. 
 Parameters 
 img : ndarray 
 Input image. 
 mask : ndarray 
 Mask. 
 multichannel : bool, optional 
 If True, the function operates on multichannel input. 
 Returns 
 out : ndarray 
 Inpainted image. 
 Examples 
 >>> from skimage import data, filters 
 >>> from skimage.morphology import square 
 >>> from skimage.restoration.denoise import inpaint_biharmonic 
 >>> img = data.astronaut() 
 >>> mask = square(img.shape[:2], 1) 
 >>> inpaint_biharmonic(img, mask) 
 array([[[ 0.24326234,  0.16810746,  0.25007795], 
        [ 0.16810746,  0.25007795,  0.24326234], 
        [ ","'Inpaint masked points in image with biharmonic equations. 
 Parameters 
 img : (M[, N[, ..., P]][, C]) ndarray 
 Input image. 
 mask : (M[, N[, ..., P]]) ndarray 
 Array of pixels to be inpainted. Have to be the same shape as one 
 of the \'img\' channels. Unknown pixels have to be represented with 1, 
 known pixels - with 0. 
 multichannel : boolean, optional 
 If True, the last `img` dimension is considered as a color channel, 
 otherwise as spatial. 
 Returns 
 out : (M[, N[, ..., P]][, C]) ndarray 
 Input image with masked pixels inpainted. 
 References 
 .. [1]  N.S.Hoang, S.B.Damelin, ""On surface completion and image inpainting 
 by biharmonic functions: numerical aspects"", 
 http://www.ima.umn.edu/~damelin/biharmonic 
 Examples 
 >>> img = np.tile(np.square(np.linspace(0, 1, 5)), (5, 1)) 
 >>> mask = np.zeros_like(img) 
 >>> mask[2, 2:] = 1 
 >>> mask[1, 3:] = 1 
 >>> mask[0, 4:] = 1 
 >>> out = inpaint_biharmonic(img, mask)'"
"def get_test_result(): 
    global _TEST_RESULT 
   res = _TEST_RESULT 
   _TEST_RESULT = [] 
   return res", 'Returns the test result.','get test result and reset test_results'
"def savgol_filter(x, window_length, polyorder, deriv=0, delta=1.0, axis=(-1), mode='interp', cval=0.0): 
    if (mode not in ['mirror', 'constant', 'nearest', 'interp', 'wrap']): 
      raise ValueError(""mode   must   be   'mirror',   'constant',   'nearest'   'wrap'   or   'interp'."") 
   x = np.asarray(x) 
   if ((x.dtype != np.float64) and (x.dtype != np.float32)): 
      x = x.astype(np.float64) 
   coeffs = savgol_coeffs(window_length, polyorder, deriv=deriv, delta=delta) 
   if (mode == 'interp'): 
      y = convolve1d(x, coeffs, axis=axis, mode='constant') 
      _fit_edges_polyfit(x, window_length, polyorder, deriv, delta, axis, y) 
   else: 
      y = convolve1d(x, coeffs, axis=axis, mode=mode, cval=cval) 
   return y"," 'Savitzky-Golay filter. 
 Parameters 
 x : array_like 
 Input array. 
 window_length : int 
 The length of the sliding window. 
 polyorder : int 
 The order of the polynomial. 
 deriv : int 
 The order of the derivative. 
 delta : float 
 The width of the Gaussian window. 
 axis : int 
 Axis along which the filter is applied. 
 mode : str 
 The interpolation mode. 
 cval : float 
 The value to use for NaNs. 
 Returns 
 y : ndarray 
 The filtered output array. 
 Notes 
 This is the implementation of the Savitzky-Golay filter algorithm. 
 The Savitzky-Golay filter is a low-pass filter that suppresses noise 
 while preserving high-frequency content. 
 The Savitzky-Golay filter is computed using a polynomial of order 
 `polyorder`. 
 References 
 .. [1] http://en.wikipedia.org/wiki/Savitzky-Golay_filter 
 Examples ","'Apply a Savitzky-Golay filter to an array. 
 This is a 1-d filter.  If `x`  has dimension greater than 1, `axis` 
 determines the axis along which the filter is applied. 
 Parameters 
 x : array_like 
 The data to be filtered.  If `x` is not a single or double precision 
 floating point array, it will be converted to type `numpy.float64` 
 before filtering. 
 window_length : int 
 The length of the filter window (i.e. the number of coefficients). 
 `window_length` must be a positive odd integer. 
 polyorder : int 
 The order of the polynomial used to fit the samples. 
 `polyorder` must be less than `window_length`. 
 deriv : int, optional 
 The order of the derivative to compute.  This must be a 
 nonnegative integer.  The default is 0, which means to filter 
 the data without differentiating. 
 delta : float, optional 
 The spacing of the samples to which the filter will be applied. 
 This is only used if deriv > 0.  Default is 1.0. 
 axis : int, optional 
 The axis of the array `x` along which the filter is to be applied. 
 Default is -1. 
 mode : str, optional 
 Must be \'mirror\', \'constant\', \'nearest\', \'wrap\' or \'interp\'.  This 
 determines the type of extension to use for the padded signal to 
 which the filter is applied.  When `mode` is \'constant\', the padding 
 value is given by `cval`.  See the Notes for more details on \'mirror\', 
 \'constant\', \'wrap\', and \'nearest\'. 
 When the \'interp\' mode is selected (the default), no extension 
 is used.  Instead, a degree `polyorder` polynomial is fit to the 
 last `window_length` values of the edges, and this polynomial is 
 used to evaluate the last `window_length // 2` output values. 
 cval : scalar, optional 
 Value to fill past the edges of the input if `mode` is \'constant\'. 
 Default is 0.0. 
 Returns 
 y : ndarray, same shape as `x` 
 The filtered data. 
 See Also 
 savgol_coeffs 
 Notes 
 Details on the `mode` options: 
 \'mirror\': 
 Repeats the values at the edges in reverse order.  The value 
 closest to the edge is not included. 
 \'nearest\': 
 The extension contains the nearest input value. 
 \'constant\': 
 The extension contains the value given by the `cval` argument. 
 \'wrap\': 
 The extension contains the values from the other end of the array. 
 For example, if the input is [1, 2, 3, 4, 5, 6, 7, 8], and 
 `window_length` is 7, the following shows the extended data for 
 the various `mode` options (assuming `cval` is 0):: 
 mode       |   Ext   |         Input          |   Ext 
 \'mirror\'   | 4  3  2 | 1  2  3  4  5  6  7  8 | 7  6  5 
 \'nearest\'  | 1  1  1 | 1  2  3  4  5  6  7  8 | 8  8  8 
 \'constant\' | 0  0  0 | 1  2  3  4  5  6  7  8 | 0  0  0 
 \'wrap\'     | 6  7  8 | 1  2  3  4  5  6  7  8 | 1  2  3 
 .. versionadded:: 0.14.0 
 Examples 
 >>> from scipy.signal import savgol_filter 
 >>> np.set_printoptions(precision=2)  # For compact display. 
 >>> x = np.array([2, 2, 5, 2, 1, 0, 1, 4, 9]) 
 Filter with a window length of 5 and a degree 2 polynomial.  Use 
 the defaults for all other parameters. 
 >>> savgol_filter(x, 5, 2) 
 array([ 1.66,  3.17,  3.54,  2.86,  0.66,  0.17,  1.  ,  4.  ,  9.  ]) 
 Note that the last five values in x are samples of a parabola, so 
 when mode=\'interp\' (the default) is used with polyorder=2, the last 
 three values are unchanged.  Compare that to, for example, 
 `mode=\'nearest\'`: 
 >>> savgol_filter(x, 5, 2, mode=\'nearest\') 
 array([ 1.74,  3.03,  3.54,  2.86,  0.66,  0.17,  1.  ,  4.6 ,  7.97])'"
"def test_takes_kwargs(): 
    assert hug.introspect.takes_kwargs(function_with_kwargs) 
   assert (not hug.introspect.takes_kwargs(function_with_args)) 
   assert (not hug.introspect.takes_kwargs(function_with_neither)) 
   assert hug.introspect.takes_kwargs(function_with_both)", 'Test if the function takes kwargs.','Test to ensure hug introspection can correctly identify when a function takes kwargs'
"def load_app(target): 
    global NORUN 
   (NORUN, nr_old) = (True, NORUN) 
   tmp = default_app.push() 
   try: 
      rv = load(target) 
      return (rv if callable(rv) else tmp) 
   finally: 
      default_app.remove(tmp) 
      NORUN = nr_old"," 'Loads an app into the default app stack. 
 If the app is already loaded, it is pushed on top of the stack. 
 If the app is not loaded, it is loaded and pushed on top of the stack. 
 :param target: The name of the app to load. 
 :type target: str 
 :return: The app that was loaded, or the app that was already loaded.'","'Load a bottle application from a module and make sure that the import 
 does not affect the current default application, but returns a separate 
 application object. See :func:`load` for the target parameter.'"
"def compute_f1(precision, recall): 
    f1 = (((2.0 * precision) * recall) / T.maximum(1, (precision + recall))) 
   return f1"," 'Computes the F1 score for the given precision and recall. 
 Args: 
 precision: float. 
 recall: float. 
 Returns: 
 float. 
 The F1 score. 
 Examples: 
 >>> compute_f1(0.9, 0.9) 
 0.9'","'Computes the f1 score for the binary classification. 
 Computed as, 
 f1 = 2 * precision * recall / (precision + recall) 
 Parameters 
 precision : Variable 
 Precision score of the binary decisions. 
 recall : Variable 
 Recall score of the binary decisions. 
 Returns 
 f1 : Variable 
 f1 score for the binary decisions.'"
"def colorize(lead, num, color): 
    s = (u'%s=%-4s' % (lead, str(num))) 
   if ((num != 0) and ANSIBLE_COLOR and (color is not None)): 
      s = stringc(s, color) 
   return s"," 'Colorize a string with the given color. 
 :param lead: The string to prepend to the colorized string. 
 :param num: The number to colorize. 
 :param color: The color to use.'",'Print \'lead\' = \'num\' in \'color\''
"def hstack(tup): 
    arrs = [cupy.atleast_1d(a) for a in tup] 
   axis = 1 
   if (arrs[0].ndim == 1): 
      axis = 0 
   return concatenate(arrs, axis)"," 'Concatenate along the last axis. 
 Parameters 
 tup : tuple 
 Tuple of arrays to be concatenated. 
 Returns 
 out : ndarray 
 Concatenated array. 
 Notes 
 This is a convenience function that can be used to stack a list of 
 arrays. 
 Examples 
 >>> from cupy import linspace, stack, hstack, zeros 
 >>> x = linspace(0, 10, 10) 
 >>> y = linspace(0, 10, 10) 
 >>> z = linspace(0, 10, 10) 
 >>> stack(x, y, z) 
 array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], 
 [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], 
 [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.","'Stacks arrays horizontally. 
 If an input array has one dimension, then the array is treated as a 
 horizontal vector and stacked along the first axis. Otherwise, the array is 
 stacked along the second axis. 
 Args: 
 tup (sequence of arrays): Arrays to be stacked. 
 Returns: 
 cupy.ndarray: Stacked array. 
 .. seealso:: :func:`numpy.hstack`'"
"def dump_psutil(): 
    output_file = (PROFILING_OUTPUT_FMT % get_filename_fmt()) 
   process_info = {} 
   for proc in psutil.process_iter(): 
      try: 
         pinfo = proc.as_dict(attrs=['pid', 'name', 'parent', 'status', 'io_counters', 'num_threads', 'cpu_times', 'cpu_percent', 'memory_info_ex', 'memory_percent', 'exe', 'cmdline']) 
      except psutil.NoSuchProcess: 
         pass 
      else: 
         for (info_name, info_data) in pinfo.iteritems(): 
            if hasattr(info_data, '_asdict'): 
               pinfo[info_name] = dict(info_data._asdict()) 
            else: 
               pinfo[info_name] = info_data 
         process_info[pinfo['pid']] = pinfo 
   netinfo = psutil.net_io_counters(pernic=True) 
   for (key, value) in netinfo.iteritems(): 
      netinfo[key] = value._asdict() 
   pids_to_show = [] 
   for (pid, pinfo) in process_info.iteritems(): 
      exe = str(pinfo['exe']) 
      if (('python' in exe) and ('w3af' in exe)): 
         pids_to_show.append(pid) 
   ps_mem_data = ps_mem_to_json(*get_memory_usage(pids_to_show, True)) 
   psutil_data = {'CPU': psutil.cpu_times()._asdict(), 'Load   average': os.getloadavg(), 'Virtual   memory': psutil.virtual_memory()._asdict(), 'Swap   memory': psutil.swap_memory()._asdict(), 'Network': netinfo, 'Processes': process_info, 'ps_mem': ps_mem_data, 'Thread   CPU   usage': get_threads_cpu_percent()} 
   json.dump(psutil_data, file(output_file, 'w'), indent=4, sort_keys=True)"," 'Dumps all the psutil data to the output file. 
 :param output_file: The output file to dump to. 
 :type output_file: str 
 :return: None'",'Dumps operating system information to file'
"def inputs(eval_data, data_dir, batch_size): 
    if (not eval_data): 
      filenames = [os.path.join(data_dir, ('data_batch_%d.bin' % i)) for i in xrange(1, 6)] 
      num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN 
   else: 
      filenames = [os.path.join(data_dir, 'test_batch.bin')] 
      num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_EVAL 
   for f in filenames: 
      if (not tf.gfile.Exists(f)): 
         raise ValueError(('Failed   to   find   file:   ' + f)) 
   filename_queue = tf.train.string_input_producer(filenames) 
   read_input = read_cifar10(filename_queue) 
   reshaped_image = tf.cast(read_input.uint8image, tf.float32) 
   height = IMAGE_SIZE 
   width = IMAGE_SIZE 
   resized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image, width, height) 
   float_image = tf.image.per_image_standardization(resized_image) 
   float_image.set_shape([height, width, 3]) 
   read_input.label.set_shape([1]) 
   min_fraction_of_examples_in_queue = 0.4 
   min_queue_examples = int((num_examples_per_epoch * min_fraction_of_examples_in_queue)) 
   return _generate_image_and_label_batch(float_image, read_input.label, min_queue_examples, batch_size, shuffle=False)"," 'Returns a tuple of the form (images, labels) where 
 images is a batch of size [batch_size] of float32 tensors with shape [height, width, 3] 
 and labels is a batch of size [batch_size] of int32 tensors with shape [1].'","'Construct input for CIFAR evaluation using the Reader ops. 
 Args: 
 eval_data: bool, indicating if one should use the train or eval data set. 
 data_dir: Path to the CIFAR-10 data directory. 
 batch_size: Number of images per batch. 
 Returns: 
 images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size. 
 labels: Labels. 1D tensor of [batch_size] size.'"
"def scope_to_list(scope): 
    if isinstance(scope, list): 
      return [unicode_type(s) for s in scope] 
   if isinstance(scope, set): 
      scope_to_list(list(scope)) 
   elif (scope is None): 
      return None 
   else: 
      return scope.split(u'   ')"," 'Convert a scope to a list of unicode strings. 
 If the scope is a list, return it as-is. 
 If the scope is a set, return a list of the strings in the set. 
 If the scope is None, return None. 
 If the scope is a string, return a list of the string\'s characters.'",'Convert a space separated string to a list of scopes.'
"@cython.locals(n=int, i=int, j=int) 
 def permutations(iterable): 
    pool = tuple(iterable) 
   n = len(pool) 
   indices = list(range(n)) 
   cycles = list(range(1, (n + 1)))[::(-1)] 
   (yield [pool[i] for i in indices]) 
   while n: 
      for i in reversed(range(n)): 
         j = (cycles[i] - 1) 
         if (j == 0): 
            indices[i:] = (indices[(i + 1):] + indices[i:(i + 1)]) 
            cycles[i] = (n - i) 
         else: 
            cycles[i] = j 
            (indices[i], indices[(- j)]) = (indices[(- j)], indices[i]) 
            (yield [pool[i] for i in indices]) 
            break 
      else: 
         return"," 'Returns all possible permutations of a list of elements. 
 >>> permutations([1, 2, 3]) 
 [[1, 2, 3], [1, 3, 2], [2, 1, 3], [2, 3, 1], [3, 1, 2], [3, 2, 1]] 
 >>> permutations([1, 2, 3, 4]) 
 [[1, 2, 3, 4], [1, 2, 4, 3], [1, 3, 2, 4], [1, 3, 4, 2], [1, 4, 2, 3], [1, 4, 3, 2], [2, 1, 3, 4], [2, 1, 4, 3], [2, 3, 1, 4], [2, 3, 4, 1], [2, 4, 1, 3], [2, 4, 3, 1], [3, 1, 2, ","'permutations(range(3), 2) --> (0,1) (0,2) (1,0) (1,2) (2,0) (2,1)'"
"def _push_assemble_error_status(status, ret, logs): 
    comment = 'An   error   occurred   pushing   your   image' 
   status['out'] = '' 
   try: 
      status['out'] += ('\n' + ret) 
      for err_log in logs: 
         if isinstance(err_log, dict): 
            if ('errorDetail' in err_log): 
               if ('code' in err_log['errorDetail']): 
                  msg = '\n{0}\n{1}:   {2}'.format(err_log['error'], err_log['errorDetail']['code'], err_log['errorDetail']['message']) 
               else: 
                  msg = '\n{0}\n{1}'.format(err_log['error'], err_log['errorDetail']['message']) 
               comment += msg 
   except Exception: 
      trace = traceback.format_exc() 
      status['out'] = 'An   error   occurred   while   parsing   error   output:\n{0}'.format(trace) 
   _invalid(status, comment=comment) 
   return status", 'Push an image and return the status.',"'Given input in this form:: 
 u\'{""status"":""Pulling repository foo/ubuntubox""}: 
 ""image (latest) from foo/  ... 
 rogress"":""complete"",""id"":""2c80228370c9""}\' 
 construct something like that (load json data is possible):: 
 [u\'{""status"":""Pulling repository foo/ubuntubox""\', 
 {""status"":""Download"",""progress"":""complete"",""id"":""2c80228370c9""}]'"
"def url(context, link_url): 
    site = context['__CACTUS_SITE__'] 
   page = context['__CACTUS_CURRENT_PAGE__'] 
   url = site.get_url_for_page(link_url) 
   if (url is None): 
      link_url_index = os.path.join(link_url, 'index.html') 
      url_link_url_index = site.get_url_for_page(link_url_index) 
      if (url_link_url_index is None): 
         logger.warning('%s:   page   resource   does   not   exist:   %s', page.link_url, link_url) 
      url = link_url 
   if site.prettify_urls: 
      return url.rsplit('index.html', 1)[0] 
   return url", 'Returns the url of the given link_url.',"'Get the path for a page in the Cactus build. 
 We\'ll need this because paths can be rewritten with prettifying.'"
"def histograms_plot(): 
    (mu, sigma) = (100, 15) 
   x = (mu + (sigma * np.random.randn(10000))) 
   num_bins = 50 
   (n, bins, patches) = plt.hist(x, bins=num_bins, normed=1, color='green', alpha=0.6, label='hist') 
   y = mlab.normpdf(bins, mu, sigma) 
   plt.plot(bins, y, 'r--', label='line') 
   plt.legend(loc='upper   left', shadow=True) 
   plt.show() 
   return", 'Plot a histogram with a fitted normal distribution','histograms plot'
"def test_array___setitem__(): 
    pass", 'Test array.__setitem__','TODO'
"def register_linker(name, linker): 
    if (name in predefined_linkers): 
      raise ValueError(('Linker   name   already   taken:   %s' % name)) 
   predefined_linkers[name] = linker"," 'Registers a linker with the given name. 
 :param name: The name of the linker. 
 :param linker: The actual linker object. 
 :raises ValueError: If the given name is already taken.'",'Add a `Linker` which can be referred to by `name` in `Mode`.'
"def pkcs_mgf1(mgfSeed, maskLen, h): 
    if (not _hashFuncParams.has_key(h)): 
      warning('pkcs_mgf1:   invalid   hash   (%s)   provided') 
      return None 
   hLen = _hashFuncParams[h][0] 
   hFunc = _hashFuncParams[h][1] 
   if (maskLen > ((2 ** 32) * hLen)): 
      warning('pkcs_mgf1:   maskLen   >   2**32   *   hLen') 
      return None 
   T = '' 
   maxCounter = math.ceil((float(maskLen) / float(hLen))) 
   counter = 0 
   while (counter < maxCounter): 
      C = pkcs_i2osp(counter, 4) 
      T += hFunc((mgfSeed + C)) 
      counter += 1 
   return T[:maskLen]"," 'Generates a mask of length maskLen using the MGF1 algorithm 
 and the given hash function h. 
 The mask is a string of length maskLen. 
 The MGF1 algorithm is described in RFC2898, section 4.1. 
 Parameters 
 mgfSeed : string 
 The MGF1 seed. 
 maskLen : int 
 The length of the mask to generate. 
 h : hash function 
 The hash function to use. 
 Returns 
 A string of length maskLen. 
 Raises 
 ValueError if maskLen > 2**32 * hLen 
 Notes 
 The MGF1 algorithm is described in RFC2898, section 4.1. 
 The MGF1 seed is a string of length 4. 
 The mask is a string of length maskLen. 
 Examples 
 >>> from Crypto.Hash import SHA 
 >>> from Crypto.Util.mgf import pkcs_mgf1 
 >>> h = SHA.new() 
 >>> mgfSeed =","'Implements generic MGF1 Mask Generation function as described in 
 Appendix B.2.1 of RFC 3447. The hash function is passed by name. 
 valid values are \'md2\', \'md4\', \'md5\', \'sha1\', \'tls, \'sha256\', 
 \'sha384\' and \'sha512\'. Returns None on error. 
 Input: 
 mgfSeed: seed from which mask is generated, an octet string 
 maskLen: intended length in octets of the mask, at most 2^32 * hLen 
 hLen (see below) 
 h      : hash function name (in \'md2\', \'md4\', \'md5\', \'sha1\', \'tls\', 
 \'sha256\', \'sha384\'). hLen denotes the length in octets of 
 the hash function output. 
 Output: 
 an octet string of length maskLen'"
"def _to_gapic_image(image): 
    if (image.content is not None): 
      return image_annotator_pb2.Image(content=image.content) 
   if (image.source is not None): 
      return image_annotator_pb2.Image(source=image_annotator_pb2.ImageSource(gcs_image_uri=image.source)) 
   raise ValueError('No   image   content   or   source   found.')", 'Convert a Google Cloud Vision API Image to a Gapic Image.',"'Helper function to convert an ``Image`` to a gRPC ``Image``. 
 :type image: :class:`~google.cloud.vision.image.Image` 
 :param image: Local ``Image`` class to be converted to gRPC ``Image``. 
 :rtype: :class:`~google.cloud.grpc.vision.v1.image_annotator_pb2.Image` 
 :returns: gRPC ``Image`` converted from 
 :class:`~google.cloud.vision.image.Image`.'"
"def init_cachedir(base=None): 
    if (base is None): 
      base = __opts__['cachedir'] 
   needed_dirs = (base, os.path.join(base, 'requested'), os.path.join(base, 'active')) 
   for dir_ in needed_dirs: 
      if (not os.path.exists(dir_)): 
         os.makedirs(dir_) 
      os.chmod(base, 493) 
   return base"," 'Initialize the cache directory if it doesn\'t exist. 
 :param base: The base directory for the cache. 
 :type base: str'",'Initialize the cachedir needed for Salt Cloud to keep track of minions'
"def load_extensions(): 
    installed_extensions = [] 
   for entry_point in pkg_resources.iter_entry_points(u'mopidy.ext'): 
      logger.debug(u'Loading   entry   point:   %s', entry_point) 
      try: 
         extension_class = entry_point.load(require=False) 
      except Exception as e: 
         logger.exception((u'Failed   to   load   extension   %s:   %s' % (entry_point.name, e))) 
         continue 
      try: 
         if (not issubclass(extension_class, Extension)): 
            raise TypeError 
      except TypeError: 
         logger.error(u'Entry   point   %s   did   not   contain   a   valid   extensionclass:   %r', entry_point.name, extension_class) 
         continue 
      try: 
         extension = extension_class() 
         config_schema = extension.get_config_schema() 
         default_config = extension.get_default_config() 
         command = extension.get_command() 
      except Exception: 
         logger.exception(u'Setup   of   extension   from   entry   point   %s   failed,   ignoring   extension.', entry_point.name) 
         continue 
      installed_extensions.append(ExtensionData(extension, entry_point, config_schema, default_config, command)) 
      logger.debug(u'Loaded   extension:   %s   %s', extension.dist_name, extension.version) 
   names = (ed.extension.ext_name for ed in installed_extensions) 
   logger.debug(u'Discovered   extensions:   %s', u',   '.join(names)) 
   return installed_extensions"," 'Loads all available extensions and their configuration schemas. 
 :return: List of extensions.'","'Find all installed extensions. 
 :returns: list of installed extensions'"
"def get_checks_places(osname, agentConfig): 
    try: 
      checksd_path = get_checksd_path(osname) 
   except PathNotFound as e: 
      log.error(e.args[0]) 
      sys.exit(3) 
   places = [(lambda name: os.path.join(agentConfig['additional_checksd'], ('%s.py' % name)))] 
   try: 
      sdk_integrations = get_sdk_integrations_path(osname) 
      places.append((lambda name: os.path.join(sdk_integrations, name, 'check.py'))) 
   except PathNotFound: 
      log.debug('No   sdk   integrations   path   found') 
   places.append((lambda name: os.path.join(checksd_path, ('%s.py' % name)))) 
   return places"," 'Get a list of places where checks are located. 
 :param osname: The operating system name. 
 :param agentConfig: The agent configuration. 
 :return: A list of places where checks are located.'","'Return a list of methods which, when called with a check name, will each return a check path to inspect'"
"@require_admin_context 
 def group_type_access_remove(context, type_id, project_id): 
    group_type_id = _group_type_get_id_from_group_type(context, type_id) 
   count = _group_type_access_query(context).filter_by(group_type_id=group_type_id).filter_by(project_id=project_id).soft_delete(synchronize_session=False) 
   if (count == 0): 
      raise exception.GroupTypeAccessNotFound(group_type_id=type_id, project_id=project_id)"," 'Remove access to the group type for the project. 
 :param context: context to query under 
 :param type_id: group type id 
 :param project_id: project id 
 :raises: GroupTypeAccessNotFound if the group type does not exist or has no access to the project.'",'Remove given tenant from the group type access list.'
"def _plot_traces(params): 
    params['text'].set_visible(False) 
   ax = params['ax'] 
   butterfly = params['butterfly'] 
   if butterfly: 
      ch_start = 0 
      n_channels = len(params['picks']) 
      data = (params['data'] * params['butterfly_scale']) 
   else: 
      ch_start = params['ch_start'] 
      n_channels = params['n_channels'] 
      data = (params['data'] * params['scale_factor']) 
   offsets = params['offsets'] 
   lines = params['lines'] 
   epochs = params['epochs'] 
   n_times = len(epochs.times) 
   tick_list = list() 
   start_idx = int((params['t_start'] / n_times)) 
   end = (params['t_start'] + params['duration']) 
   end_idx = int((end / n_times)) 
   xlabels = params['labels'][start_idx:] 
   event_ids = params['epochs'].events[:, 2] 
   params['ax2'].set_xticklabels(event_ids[start_idx:]) 
   ax.set_xticklabels(xlabels) 
   ylabels = ax.yaxis.get_ticklabels() 
   for line_idx in range(n_channels): 
      ch_idx = (line_idx + ch_start) 
      if (line_idx >= len(lines)): 
         break 
      elif (ch_idx < len(params['ch_names'])): 
         if butterfly: 
            ch_type = params['types'][ch_idx] 
            if (ch_type == 'grad'): 
               offset = offsets[0] 
            elif (ch_type == 'mag'): 
               offset = offsets[1] 
            elif (ch_type == 'eeg'): 
               offset = offsets[2] 
            elif (ch_type == 'eog'): 
               offset = offsets[3] 
            elif (ch_type == 'ecg'): 
               offset = offsets[4] 
            else: 
               lines[line_idx].set_segments(list()) 
         else: 
            tick_list += [params['ch_names'][ch_idx]] 
            offset = offsets[line_idx] 
         this_data = data[ch_idx] 
         ydata = (offset - this_data) 
         xdata = params['times'][:params['duration']] 
         num_epochs = np.min([params['n_epochs'], len(epochs.events)]) 
         segments = np.split(np.array((xdata, ydata)).T, num_epochs) 
         ch_name = params['ch_names'][ch_idx] 
         if (ch_name in params['info']['bads']): 
            if (not butterfly): 
               this_color = params['bad_color'] 
               ylabels[line_idx].set_color(this_color) 
            this_color = np.tile(params['bad_color'], (num_epochs, 1)) 
            for bad_idx in params['bads']: 
               if ((bad_idx < start_idx) or (bad_idx > end_idx)): 
                  continue 
               this_color[(bad_idx - start_idx)] = (1.0, 0.0, 0.0) 
            lines[line_idx].set_zorder(2) 
         else: 
            this_color = params['colors'][ch_idx][start_idx:end_idx] 
            lines[line_idx].set_zorder(3) 
            if (not butterfly): 
               ylabels[line_idx].set_color('black') 
         lines[line_idx].set_segments(segments) 
         lines[line_idx].set_color(this_color) 
      else: 
         lines[line_idx].set_segments(list()) 
   ax.set_xlim(params['times'][0], (params['times'][0] + params['duration']), False) 
   params['ax2'].set_xlim(params['times'][0], (params['times'][0] + params['duration']), False) 
   if butterfly: 
      factor = ((-1.0) / params['butterfly_scale']) 
      labels = np.empty(20, dtype='S15') 
      labels.fill('') 
      ticks = ax.get_yticks() 
      idx_offset = 1 
      if ('grad' in params['types']): 
         labels[(idx_offset + 1)] = '0.00' 
         for idx in [idx_offset, (idx_offset + 2)]: 
            labels[idx] = '{0:.2f}'.format(((((ticks[idx] - offsets[0]) * params['scalings']['grad']) * 10000000000000.0) * factor)) 
         idx_offset += 4 
      if ('mag' in params['types']): 
         labels[(idx_offset + 1)] = '0.00' 
         for idx in [idx_offset, (idx_offset + 2)]: 
            labels[idx] = '{0:.2f}'.format(((((ticks[idx] - offsets[1]) * params['scalings']['mag']) * 1000000000000000.0) * factor)) 
         idx_offset += 4 
      if ('eeg' in params['types']): 
         labels[(idx_offset + 1)] = '0.00' 
         for idx in [idx_offset, (idx_offset + 2)]: 
            labels[idx] = '{0:.2f}'.format(((((ticks[idx] - offsets[2]) * params['scalings']['eeg']) * 1000000.0) * factor)) 
         idx_offset += 4 
      if ('eog' in params['types']): 
         labels[(idx_offset + 1)] = '0.00' 
         for idx in [idx_offset, (idx_offset + 2)]: 
            labels[idx] = '{0:.2f}'.format(((((ticks[idx] - offsets[3]) * params['scalings']['eog']) * 1000000.0) * factor)) 
         idx_offset += 4 
      if ('ecg' in params['types']): 
         labels[(idx_offset + 1)] = '0.00' 
         for idx in [idx_offset, (idx_offset + 2)]: 
            labels[idx] = '{0:.2f}'.format(((((ticks[idx] - offsets[4]) * params['scalings']['ecg']) * 1000000.0) * factor)) 
      ax.set_yticklabels(labels, fontsize=12, color='black') 
   else: 
      ax.set_yticklabels(tick_list, fontsize=12) 
   if (params['events'] is not None): 
      _draw_event_lines(params) 
   params['vsel_patch'].set_y(ch_start) 
   params['fig'].canvas.draw() 
   if (params['fig_proj'] is not None): 
      params['fig_proj'].canvas.draw()", 'Plot the traces of the channels.','Plot concatenated epochs.'
"def refine_Determinant(expr, assumptions): 
    if ask(Q.orthogonal(expr.arg), assumptions): 
      return S.One 
   elif ask(Q.singular(expr.arg), assumptions): 
      return S.Zero 
   elif ask(Q.unit_triangular(expr.arg), assumptions): 
      return S.One 
   return expr"," 'Refines the determinant to be a unit determinant or a one. 
 Examples 
 >>> from sympy import Determinant, refine_Determinant 
 >>> from sympy.abc import x, y, z 
 >>> Determinant(x*y + z*y + x*z) 
 Determinant(x*y + z*y + x*z, x, y, z) 
 >>> refine_Determinant(Determinant(x*y + z*y + x*z), {x: True}) 
 Determinant(x*y + z*y + x*z, x, y, z) 
 >>> refine_Determinant(Determinant(x*y + z*y + x*z), {y: True}) 
 Determinant(x*y + z*y + x*z, x, y, z) 
 >>> refine_Determinant(Determinant(x*y + z*y + x*z), {z: True}) 
 Determinant(x*","'>>> from sympy import MatrixSymbol, Q, assuming, refine, det 
 >>> X = MatrixSymbol(\'X\', 2, 2) 
 >>> det(X) 
 Determinant(X) 
 >>> with assuming(Q.orthogonal(X)): 
 ...     print(refine(det(X))) 
 1'"
"def get_rate_limit(): 
    return getattr(g, '_rate_limit', None)"," 'Get the current rate limit. 
 :return: The current rate limit or None.'","'If available, returns a RateLimit instance which is valid for the 
 current request-response. 
 .. versionadded:: 0.0.7'"
"@with_setup(prepare_stdout) 
 def test_output_outlines_success_colorful(): 
    runner = Runner(join_path('zh-TW', 'success', 'outlines.feature'), verbosity=3, no_color=False) 
   runner.run() 
   assert_stdout_lines(u'\n\x1b[1;37m\u7279\u6027:   \u4e2d\u6587\u5834\u666f\u6a21\u677f                                 \x1b[1;30m#   tests/functional/language_specific_features/zh-TW/success/outlines.feature:3\x1b[0m\n\x1b[1;37m      \u4e2d\u6587\u5834\u666f\u6a21\u677f\u5716\u8868\u6e2c\u8a66                     \x1b[1;30m#   tests/functional/language_specific_features/zh-TW/success/outlines.feature:4\x1b[0m\n\n\x1b[1;37m      \u5834\u666f\u6a21\u677f:   \u7528\u8868\u683c\u63cf\u8ff0\u5834\u666f         \x1b[1;30m#   tests/functional/language_specific_features/zh-TW/success/outlines.feature:6\x1b[0m\n\x1b[0;36m            \u5982\u679c   \u8f38\u5165\u662f<\u8f38\u5165>                        \x1b[1;30m#   tests/functional/language_specific_features/zh-TW/success/outlines_steps.py:13\x1b[0m\n\x1b[0;36m            \u7576   \u57f7\u884c<\u8655\u7406>\u6642                              \x1b[1;30m#   tests/functional/language_specific_features/zh-TW/success/outlines_steps.py:22\x1b[0m\n\x1b[0;36m            \u90a3\u9ebd   \u5f97\u5230<\u7d50\u679c>                              \x1b[1;30m#   tests/functional/language_specific_features/zh-TW/success/outlines_steps.py:31\x1b[0m\n\n\x1b[1;37m      \u4f8b\u5982:\x1b[0m\n\x1b[0;36m         \x1b[1;37m   |\x1b[0;36m   \u8f38\u5165\x1b[1;37m   |\x1b[0;36m   \u8655\u7406\x1b[1;37m   |\x1b[0;36m   \u7d50\u679c                        \x1b[1;37m   |\x1b[0;36m\x1b[0m\n\x1b[1;32m         \x1b[1;37m   |\x1b[1;32m   \u4ec0\u9ebd\x1b[1;37m   |\x1b[1;32m   \u9019\u500b\x1b[1;37m   |\x1b[1;32m   \u529f\u80fd                        \x1b[1;37m   |\x1b[1;32m\x1b[0m\n\x1b[1;32m         \x1b[1;37m   |\x1b[1;32m   \u5176\u4ed6\x1b[1;37m   |\x1b[1;32m   \u9019\u88cf\x1b[1;37m   |\x1b[1;32m   \u4e00\u6a23                        \x1b[1;37m   |\x1b[1;32m\x1b[0m\n\x1b[1;32m         \x1b[1;37m   |\x1b[1;32m   \u6578\u64da\x1b[1;37m   |\x1b[1;32m   \u52d5\u4f5c\x1b[1;37m   |\x1b[1;32m   unicode\u8f38\u51fa!\x1b[1;37m   |\x1b[1;32m\x1b[0m\n\n\x1b[1;37m1   feature   (\x1b[1;32m1   passed\x1b[1;37m)\x1b[0m\n\x1b[1;37m3   scenarios   (\x1b[1;32m3   passed\x1b[1;37m)\x1b[0m\n\x1b[1;37m9   steps   (\x1b[1;32m9   passed\x1b[1;37m)\x1b[0m\n')", 'Test that the output of the runner is colorful and success.','Language: zh-TW -> sucess outlines colorful'
"@utils.auth.requires_login 
 @blueprint.route('/upload_archive', methods=['POST']) 
 def upload_archive(): 
    files = flask.request.files 
   archive_file = get_tempfile(files['archive'], '.archive') 
   if tarfile.is_tarfile(archive_file): 
      archive = tarfile.open(archive_file, 'r') 
      names = archive.getnames() 
   elif zipfile.is_zipfile(archive_file): 
      archive = zipfile.ZipFile(archive_file, 'r') 
      names = archive.namelist() 
   else: 
      return (flask.jsonify({'status': 'Incorrect   Archive   Type'}), 500) 
   if ('info.json' in names): 
      tempdir = tempfile.mkdtemp() 
      labels_file = None 
      archive.extractall(path=tempdir) 
      with open(os.path.join(tempdir, 'info.json')) as data_file: 
         info = json.load(data_file) 
      (valid, key) = validate_archive_keys(info) 
      if (valid is False): 
         return (flask.jsonify({'status': ((""Missing   Key   '"" + key) + ""'   in   info.json"")}), 500) 
      weights_file = os.path.join(tempdir, info['snapshot   file']) 
      if ('model   file' in info): 
         model_file = os.path.join(tempdir, info['model   file']) 
      elif ('network   file' in info): 
         model_file = os.path.join(tempdir, info['network   file']) 
      else: 
         return (flask.jsonify({'status': 'Missing   model   definition   in   info.json'}), 500) 
      if ('labels   file' in info): 
         labels_file = os.path.join(tempdir, info['labels   file']) 
      job = PretrainedModelJob(weights_file, model_file, labels_file, info['framework'], username=utils.auth.get_username(), name=info['name']) 
      scheduler.add_job(job) 
      job.wait_completion() 
      shutil.rmtree(tempdir, ignore_errors=True) 
      return (flask.jsonify({'status': 'success'}), 200) 
   else: 
      return (flask.jsonify({'status': 'Missing   or   Incorrect   json   file'}), 500)"," 'Upload a model archive to the server and create a job to run it. 
 :param files: 
 :return: 
 :rtype: 
 :raise: 
 :auth: 
 :permissions: 
 :example: 
 .. code-block:: python 
 :caption: Upload a model archive 
 .. code-block:: 
 from flask import Flask, render_template, request, redirect, url_for 
 from flask_restful import Resource, reqparse, marshal, fields 
 from werkzeug.datastructures import FileStorage 
 from flask_jwt import JWT 
 from flask_uploads import IMAGES 
 from werkzeug.datastructures import FileStorage 
 from flask import Flask, render_template, request, redirect, url_for 
 from flask_restful import Resource, reqparse, marshal, fields 
 from werkzeug.datastructures import FileStorage 
 from flask_jwt import JWT 
 from flask_uploads import IMAGES 
 from werkzeug.datastructures import FileStorage 
 from flask import Flask, render",'Upload archive'
